HostName: geneva.ee.ucl.ac.uk, LoginName: uceeyc6, Current GPU ID: 1/4, CodeFile: /home/uceeyc6/AgentNet/ogb_mol_3_fast_withoutdqnloss.py
----------------------------------------------------------------------------------------------------
Hyperparameters:
dropout             : 0.0
batch_size          : 64
hidden_units        : 128
use_aux_loss        : False
lr                  : 0.0001
num_agents          : 26
num_steps           : 16
reduce              : log
epochs              : 350
warmup              : 0
self_loops          : True
node_readout        : False
use_step_readout_lin: False
gumbel_temp         : 0.66666667
gumbel_min_temp     : 0.66666667
gumbel_warmup       : -1
gumbel_decay_epochs : 50
min_lr_mult         : 1e-07
weight_decay        : 0.01
num_pos_attention_heads: 1
clip_grad           : 1.0
readout_mlp         : False
post_ln             : False
attn_dropout        : 0.0
no_time_cond        : False
mlp_width_mult      : 2
activation_function : leaky_relu
negative_slope      : 0.01
input_mlp           : True
attn_width_mult     : 1
importance_init     : False
random_agent        : False
test_argmax         : False
global_agent_pool   : True
agent_global_extra  : False
basic_global_agent  : False
basic_agent         : False
bias_attention      : True
visited_decay       : 0.9
sparse_conv         : False
mean_pool_only      : False
edge_negative_slope : 0.2
final_readout_only  : False
device              : 0
gnn                 : gin-virtual
drop_ratio          : 0.5
num_layer           : 5
emb_dim             : 300
num_workers         : 0
dataset             : ogbg-molhiv
feature             : full
slurm               : False
grid_search         : False
gpu_jobs            : False
seed                : 0
gpu_id              : 1
hpc_exp_number      : None
trials              : <bound method HyperOptArgumentParser.opt_trials of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>
optimize_parallel   : <bound method HyperOptArgumentParser.optimize_parallel of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>
optimize_parallel_gpu: <bound method HyperOptArgumentParser.optimize_parallel_gpu of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>
optimize_parallel_cpu: <bound method HyperOptArgumentParser.optimize_parallel_cpu of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>
generate_trials     : <bound method HyperOptArgumentParser.generate_trials of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>
optimize_trials_parallel_gpu: <bound method HyperOptArgumentParser.optimize_trials_parallel_gpu of HyperOptArgumentParser(prog='ogb_mol_3_fast_withoutdqnloss.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='resolve', add_help=False)>

fgs data loaded
=====Epoch 1=====
Training...
dqn reward tensor(-466., device='cuda:0') e 0.9995 loss_dqn tensor(1.1017, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.8295204639434814
dqn reward tensor(-372.2500, device='cuda:0') e 0.9990000000000001 loss_dqn tensor(1.3371, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2187662124633789
dqn reward tensor(-383.6875, device='cuda:0') e 0.9985000000000002 loss_dqn tensor(1.9570, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2552694082260132
dqn reward tensor(-377.8750, device='cuda:0') e 0.9980000000000002 loss_dqn tensor(2.8509, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1615545153617859
dqn reward tensor(-436.7500, device='cuda:0') e 0.9975000000000003 loss_dqn tensor(4.0300, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08886778354644775
dqn reward tensor(-395.6250, device='cuda:0') e 0.9970000000000003 loss_dqn tensor(5.6030, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.379786878824234
dqn reward tensor(-437., device='cuda:0') e 0.9965000000000004 loss_dqn tensor(7.5967, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.40188083052635193
dqn reward tensor(-435.7500, device='cuda:0') e 0.9960000000000004 loss_dqn tensor(9.7445, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0014164757449179888
dqn reward tensor(-462.6875, device='cuda:0') e 0.9955000000000005 loss_dqn tensor(12.7365, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2108190655708313
dqn reward tensor(-415.7500, device='cuda:0') e 0.9950000000000006 loss_dqn tensor(16.4099, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2090974748134613
dqn reward tensor(-468.8750, device='cuda:0') e 0.9945000000000006 loss_dqn tensor(20.5302, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21579089760780334
dqn reward tensor(-462.8750, device='cuda:0') e 0.9940000000000007 loss_dqn tensor(25.9890, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10917362570762634
dqn reward tensor(-373.3125, device='cuda:0') e 0.9935000000000007 loss_dqn tensor(33.3838, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0012905634939670563
dqn reward tensor(-490.6250, device='cuda:0') e 0.9930000000000008 loss_dqn tensor(37.7966, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10261037945747375
dqn reward tensor(-479.6250, device='cuda:0') e 0.9925000000000008 loss_dqn tensor(44.5769, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0015635426389053464
dqn reward tensor(-366.0625, device='cuda:0') e 0.9920000000000009 loss_dqn tensor(50.3567, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10776545107364655
dqn reward tensor(-521.3750, device='cuda:0') e 0.9915000000000009 loss_dqn tensor(57.8119, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3051416575908661
dqn reward tensor(-395.8750, device='cuda:0') e 0.991000000000001 loss_dqn tensor(66.4827, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3887719511985779
dqn reward tensor(-509.1250, device='cuda:0') e 0.990500000000001 loss_dqn tensor(75.3022, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18943636119365692
dqn reward tensor(-445.7500, device='cuda:0') e 0.9900000000000011 loss_dqn tensor(84.0416, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3608044385910034
dqn reward tensor(-479.3750, device='cuda:0') e 0.9895000000000012 loss_dqn tensor(96.6346, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.004039600491523743
dqn reward tensor(-371., device='cuda:0') e 0.9890000000000012 loss_dqn tensor(107.3903, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3400619626045227
dqn reward tensor(-472.8750, device='cuda:0') e 0.9885000000000013 loss_dqn tensor(116.1966, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16163183748722076
dqn reward tensor(-380., device='cuda:0') e 0.9880000000000013 loss_dqn tensor(133.6440, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15710243582725525
dqn reward tensor(-471.3750, device='cuda:0') e 0.9875000000000014 loss_dqn tensor(141.3086, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2279660850763321
dqn reward tensor(-666.1250, device='cuda:0') e 0.9870000000000014 loss_dqn tensor(147.4488, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14771831035614014
dqn reward tensor(-426.6875, device='cuda:0') e 0.9865000000000015 loss_dqn tensor(146.1813, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25606152415275574
dqn reward tensor(-421.6250, device='cuda:0') e 0.9860000000000015 loss_dqn tensor(191.4792, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1897062510251999
dqn reward tensor(-463.2500, device='cuda:0') e 0.9855000000000016 loss_dqn tensor(202.6377, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14327695965766907
dqn reward tensor(-453.7500, device='cuda:0') e 0.9850000000000017 loss_dqn tensor(218.0257, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23122449219226837
dqn reward tensor(-425.8125, device='cuda:0') e 0.9845000000000017 loss_dqn tensor(258.1119, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23833993077278137
dqn reward tensor(-369.3750, device='cuda:0') e 0.9840000000000018 loss_dqn tensor(284.4827, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1976601481437683
dqn reward tensor(-488.2500, device='cuda:0') e 0.9835000000000018 loss_dqn tensor(301.3722, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12419953942298889
dqn reward tensor(-412.5000, device='cuda:0') e 0.9830000000000019 loss_dqn tensor(331.4752, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17370933294296265
dqn reward tensor(-323.1250, device='cuda:0') e 0.9825000000000019 loss_dqn tensor(358.9558, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17045167088508606
dqn reward tensor(-382.9375, device='cuda:0') e 0.982000000000002 loss_dqn tensor(391.7139, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15993842482566833
dqn reward tensor(-388.1875, device='cuda:0') e 0.981500000000002 loss_dqn tensor(395.1689, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11256788671016693
dqn reward tensor(-372.7500, device='cuda:0') e 0.9810000000000021 loss_dqn tensor(409.7791, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23678822815418243
dqn reward tensor(-439.2500, device='cuda:0') e 0.9805000000000021 loss_dqn tensor(394.0744, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14379319548606873
dqn reward tensor(-582., device='cuda:0') e 0.9800000000000022 loss_dqn tensor(432.1713, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943404957652092
dqn reward tensor(-533.2500, device='cuda:0') e 0.9795000000000023 loss_dqn tensor(426.9318, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13707959651947021
dqn reward tensor(-556.3125, device='cuda:0') e 0.9790000000000023 loss_dqn tensor(444.8465, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028260182589292526
dqn reward tensor(-411.2500, device='cuda:0') e 0.9785000000000024 loss_dqn tensor(502.4470, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13530100882053375
dqn reward tensor(-464.8750, device='cuda:0') e 0.9780000000000024 loss_dqn tensor(497.5882, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2617526650428772
dqn reward tensor(-450.1250, device='cuda:0') e 0.9775000000000025 loss_dqn tensor(513.5115, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14119136333465576
dqn reward tensor(-531.2500, device='cuda:0') e 0.9770000000000025 loss_dqn tensor(490.5911, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0172007717192173
dqn reward tensor(-302.7500, device='cuda:0') e 0.9765000000000026 loss_dqn tensor(544.5975, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07920713722705841
dqn reward tensor(-410.1250, device='cuda:0') e 0.9760000000000026 loss_dqn tensor(542.2238, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20827803015708923
dqn reward tensor(-426.5000, device='cuda:0') e 0.9755000000000027 loss_dqn tensor(597.8514, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3441477119922638
dqn reward tensor(-430.1250, device='cuda:0') e 0.9750000000000028 loss_dqn tensor(573.4023, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27598240971565247
dqn reward tensor(-378.2500, device='cuda:0') e 0.9745000000000028 loss_dqn tensor(612.4847, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08159477263689041
dqn reward tensor(-423., device='cuda:0') e 0.9740000000000029 loss_dqn tensor(628.2941, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07978662848472595
dqn reward tensor(-405.6250, device='cuda:0') e 0.9735000000000029 loss_dqn tensor(619.1252, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08075114339590073
dqn reward tensor(-404.7500, device='cuda:0') e 0.973000000000003 loss_dqn tensor(662.6411, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20109499990940094
dqn reward tensor(-330.5625, device='cuda:0') e 0.972500000000003 loss_dqn tensor(674.7026, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2676798701286316
dqn reward tensor(-384.0625, device='cuda:0') e 0.9720000000000031 loss_dqn tensor(675.8259, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3098985254764557
dqn reward tensor(-431.3750, device='cuda:0') e 0.9715000000000031 loss_dqn tensor(726.8892, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13883958756923676
dqn reward tensor(-423.2500, device='cuda:0') e 0.9710000000000032 loss_dqn tensor(701.7426, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13573956489562988
dqn reward tensor(-546.4375, device='cuda:0') e 0.9705000000000032 loss_dqn tensor(721.8245, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14120909571647644
dqn reward tensor(-474.8750, device='cuda:0') e 0.9700000000000033 loss_dqn tensor(695.1777, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14393110573291779
dqn reward tensor(-374.6250, device='cuda:0') e 0.9695000000000034 loss_dqn tensor(689.2125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23865798115730286
dqn reward tensor(-455.6250, device='cuda:0') e 0.9690000000000034 loss_dqn tensor(780.5808, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18735098838806152
dqn reward tensor(-401.1250, device='cuda:0') e 0.9685000000000035 loss_dqn tensor(790.2919, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06516867876052856
dqn reward tensor(-301.5000, device='cuda:0') e 0.9680000000000035 loss_dqn tensor(829.7676, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23649895191192627
dqn reward tensor(-274.8125, device='cuda:0') e 0.9675000000000036 loss_dqn tensor(790.1754, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1863318383693695
dqn reward tensor(-373.7500, device='cuda:0') e 0.9670000000000036 loss_dqn tensor(769.3832, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10262677073478699
dqn reward tensor(-487.1250, device='cuda:0') e 0.9665000000000037 loss_dqn tensor(894.8024, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1452406495809555
dqn reward tensor(-454., device='cuda:0') e 0.9660000000000037 loss_dqn tensor(770.3133, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23535187542438507
dqn reward tensor(-366.0625, device='cuda:0') e 0.9655000000000038 loss_dqn tensor(899.2244, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09378907084465027
dqn reward tensor(-329.1875, device='cuda:0') e 0.9650000000000039 loss_dqn tensor(993.6660, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2313833236694336
dqn reward tensor(-337.2500, device='cuda:0') e 0.9645000000000039 loss_dqn tensor(944.1539, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1392536610364914
dqn reward tensor(-481., device='cuda:0') e 0.964000000000004 loss_dqn tensor(1139.3203, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33679652214050293
dqn reward tensor(-562.3750, device='cuda:0') e 0.963500000000004 loss_dqn tensor(1270.7467, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13839492201805115
dqn reward tensor(-445.3750, device='cuda:0') e 0.9630000000000041 loss_dqn tensor(1200.5811, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0876600444316864
dqn reward tensor(-460.5000, device='cuda:0') e 0.9625000000000041 loss_dqn tensor(1434.7969, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08962730318307877
dqn reward tensor(-443.3750, device='cuda:0') e 0.9620000000000042 loss_dqn tensor(1625.4320, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03560161590576172
dqn reward tensor(-271.5000, device='cuda:0') e 0.9615000000000042 loss_dqn tensor(1422.5938, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19204610586166382
dqn reward tensor(-557.7500, device='cuda:0') e 0.9610000000000043 loss_dqn tensor(1622.4497, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0846221074461937
dqn reward tensor(-384.7500, device='cuda:0') e 0.9605000000000044 loss_dqn tensor(1909.2096, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.141795814037323
dqn reward tensor(-402.5000, device='cuda:0') e 0.9600000000000044 loss_dqn tensor(1965.5897, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08706191927194595
dqn reward tensor(-366.0625, device='cuda:0') e 0.9595000000000045 loss_dqn tensor(2093.4573, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023511897772550583
dqn reward tensor(-367.1250, device='cuda:0') e 0.9590000000000045 loss_dqn tensor(2239.9155, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07758613675832748
dqn reward tensor(-224., device='cuda:0') e 0.9585000000000046 loss_dqn tensor(1978.0570, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01887105591595173
dqn reward tensor(-475.7500, device='cuda:0') e 0.9580000000000046 loss_dqn tensor(2360.6877, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14502128958702087
dqn reward tensor(-356.7500, device='cuda:0') e 0.9575000000000047 loss_dqn tensor(2699.7390, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20322880148887634
dqn reward tensor(-485., device='cuda:0') e 0.9570000000000047 loss_dqn tensor(2917.3735, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20259851217269897
dqn reward tensor(-506.5000, device='cuda:0') e 0.9565000000000048 loss_dqn tensor(2973.9387, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1388632357120514
dqn reward tensor(-274.8750, device='cuda:0') e 0.9560000000000048 loss_dqn tensor(2426.0608, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1448138803243637
dqn reward tensor(-323.1250, device='cuda:0') e 0.9555000000000049 loss_dqn tensor(3278.0237, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07781130075454712
dqn reward tensor(-272.4375, device='cuda:0') e 0.955000000000005 loss_dqn tensor(3033.8105, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08014340698719025
dqn reward tensor(-433.6250, device='cuda:0') e 0.954500000000005 loss_dqn tensor(3397.7368, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2008545845746994
dqn reward tensor(-233.2500, device='cuda:0') e 0.9540000000000051 loss_dqn tensor(3242.2761, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20612552762031555
dqn reward tensor(-372.5625, device='cuda:0') e 0.9535000000000051 loss_dqn tensor(2983.0549, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1914932131767273
dqn reward tensor(-396.8750, device='cuda:0') e 0.9530000000000052 loss_dqn tensor(3245.3540, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0794893354177475
dqn reward tensor(-363.8750, device='cuda:0') e 0.9525000000000052 loss_dqn tensor(3364.6848, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13553091883659363
dqn reward tensor(-452.6250, device='cuda:0') e 0.9520000000000053 loss_dqn tensor(3451.1387, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08788872510194778
dqn reward tensor(-520.7500, device='cuda:0') e 0.9515000000000053 loss_dqn tensor(3748.5713, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24976833164691925
dqn reward tensor(-423., device='cuda:0') e 0.9510000000000054 loss_dqn tensor(3339.6843, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09073498845100403
dqn reward tensor(-508.2500, device='cuda:0') e 0.9505000000000055 loss_dqn tensor(3586.7290, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1911313682794571
dqn reward tensor(-321.8750, device='cuda:0') e 0.9500000000000055 loss_dqn tensor(3682.9607, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19299481809139252
dqn reward tensor(-343.3750, device='cuda:0') e 0.9495000000000056 loss_dqn tensor(3693.0317, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32700812816619873
dqn reward tensor(-322.6250, device='cuda:0') e 0.9490000000000056 loss_dqn tensor(3506.2593, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05276355519890785
dqn reward tensor(-363.3750, device='cuda:0') e 0.9485000000000057 loss_dqn tensor(3674.2178, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10136477649211884
dqn reward tensor(-300.6875, device='cuda:0') e 0.9480000000000057 loss_dqn tensor(3721.8970, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14442002773284912
dqn reward tensor(-427.1250, device='cuda:0') e 0.9475000000000058 loss_dqn tensor(4112.0586, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24247857928276062
dqn reward tensor(-366.6250, device='cuda:0') e 0.9470000000000058 loss_dqn tensor(3901.2861, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22385632991790771
dqn reward tensor(-540.1250, device='cuda:0') e 0.9465000000000059 loss_dqn tensor(3997.9768, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14110425114631653
dqn reward tensor(-524.6250, device='cuda:0') e 0.946000000000006 loss_dqn tensor(4300.1953, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14009597897529602
dqn reward tensor(-494.8750, device='cuda:0') e 0.945500000000006 loss_dqn tensor(4117.4814, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23941929638385773
dqn reward tensor(-351.4375, device='cuda:0') e 0.9450000000000061 loss_dqn tensor(3733.2456, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08823312073945999
dqn reward tensor(-446.5625, device='cuda:0') e 0.9445000000000061 loss_dqn tensor(3625.0286, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19350522756576538
dqn reward tensor(-299.0625, device='cuda:0') e 0.9440000000000062 loss_dqn tensor(3559.9128, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09157642722129822
dqn reward tensor(-348.4375, device='cuda:0') e 0.9435000000000062 loss_dqn tensor(3140.2791, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13779917359352112
dqn reward tensor(-314.2500, device='cuda:0') e 0.9430000000000063 loss_dqn tensor(2422.0381, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19444920122623444
dqn reward tensor(-466.8750, device='cuda:0') e 0.9425000000000063 loss_dqn tensor(3120.7686, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14826907217502594
dqn reward tensor(-221.2500, device='cuda:0') e 0.9420000000000064 loss_dqn tensor(2713.4148, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029550500214099884
dqn reward tensor(-322.6250, device='cuda:0') e 0.9415000000000064 loss_dqn tensor(2361.8184, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08395179361104965
dqn reward tensor(-377., device='cuda:0') e 0.9410000000000065 loss_dqn tensor(2644.5095, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1892048716545105
dqn reward tensor(-261.1250, device='cuda:0') e 0.9405000000000066 loss_dqn tensor(2234.0093, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08048952370882034
dqn reward tensor(-517.1250, device='cuda:0') e 0.9400000000000066 loss_dqn tensor(2389.1035, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02475021965801716
dqn reward tensor(-425., device='cuda:0') e 0.9395000000000067 loss_dqn tensor(1870.3781, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3289812207221985
dqn reward tensor(-293.6250, device='cuda:0') e 0.9390000000000067 loss_dqn tensor(1562.3380, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1452702283859253
dqn reward tensor(-159.3750, device='cuda:0') e 0.9385000000000068 loss_dqn tensor(1411.0482, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20356528460979462
dqn reward tensor(-252.5000, device='cuda:0') e 0.9380000000000068 loss_dqn tensor(1129.7534, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14046716690063477
dqn reward tensor(-384.5000, device='cuda:0') e 0.9375000000000069 loss_dqn tensor(1544.6642, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19966106116771698
dqn reward tensor(-533.3125, device='cuda:0') e 0.9370000000000069 loss_dqn tensor(1421.9049, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13694335520267487
dqn reward tensor(-387.5000, device='cuda:0') e 0.936500000000007 loss_dqn tensor(927.5034, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3001924753189087
dqn reward tensor(-213.1250, device='cuda:0') e 0.936000000000007 loss_dqn tensor(669.0724, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1411888152360916
dqn reward tensor(-356.7500, device='cuda:0') e 0.9355000000000071 loss_dqn tensor(825.2994, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04032284766435623
dqn reward tensor(-341.5000, device='cuda:0') e 0.9350000000000072 loss_dqn tensor(729.3536, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14500415325164795
dqn reward tensor(-388.3750, device='cuda:0') e 0.9345000000000072 loss_dqn tensor(419.8460, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04596766456961632
dqn reward tensor(-304.5000, device='cuda:0') e 0.9340000000000073 loss_dqn tensor(424.5970, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19207939505577087
dqn reward tensor(-404.6250, device='cuda:0') e 0.9335000000000073 loss_dqn tensor(453.8579, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09484955668449402
dqn reward tensor(-481.5000, device='cuda:0') e 0.9330000000000074 loss_dqn tensor(443.5555, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2828616797924042
dqn reward tensor(-306.2500, device='cuda:0') e 0.9325000000000074 loss_dqn tensor(405.5242, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038919854909181595
dqn reward tensor(-436.6250, device='cuda:0') e 0.9320000000000075 loss_dqn tensor(458.7282, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23638179898262024
dqn reward tensor(-473.6250, device='cuda:0') e 0.9315000000000075 loss_dqn tensor(487.4400, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2948073148727417
dqn reward tensor(-260.7500, device='cuda:0') e 0.9310000000000076 loss_dqn tensor(632.1349, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19067201018333435
dqn reward tensor(-108.2500, device='cuda:0') e 0.9305000000000077 loss_dqn tensor(788.4463, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13458994030952454
dqn reward tensor(-213., device='cuda:0') e 0.9300000000000077 loss_dqn tensor(919.9546, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1338740587234497
dqn reward tensor(-403.3125, device='cuda:0') e 0.9295000000000078 loss_dqn tensor(1117.1244, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23235011100769043
dqn reward tensor(-397.2500, device='cuda:0') e 0.9290000000000078 loss_dqn tensor(1159.8752, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1886684000492096
dqn reward tensor(-426.1875, device='cuda:0') e 0.9285000000000079 loss_dqn tensor(1259.8950, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1883130669593811
dqn reward tensor(-389.6875, device='cuda:0') e 0.9280000000000079 loss_dqn tensor(1507.2229, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2321704477071762
dqn reward tensor(-357.1250, device='cuda:0') e 0.927500000000008 loss_dqn tensor(1989.2198, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19633060693740845
dqn reward tensor(-294.4375, device='cuda:0') e 0.927000000000008 loss_dqn tensor(2432.2319, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1795099973678589
dqn reward tensor(-415.6875, device='cuda:0') e 0.9265000000000081 loss_dqn tensor(2777.0400, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22988557815551758
dqn reward tensor(-312.0625, device='cuda:0') e 0.9260000000000081 loss_dqn tensor(3513.0391, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09925398230552673
dqn reward tensor(-374.7500, device='cuda:0') e 0.9255000000000082 loss_dqn tensor(4435.0942, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1853892207145691
dqn reward tensor(-304.8125, device='cuda:0') e 0.9250000000000083 loss_dqn tensor(4618.8267, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13952979445457458
dqn reward tensor(-340.5000, device='cuda:0') e 0.9245000000000083 loss_dqn tensor(5985.1733, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18290914595127106
dqn reward tensor(-401.5000, device='cuda:0') e 0.9240000000000084 loss_dqn tensor(7603.2603, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33142074942588806
dqn reward tensor(-309.1250, device='cuda:0') e 0.9235000000000084 loss_dqn tensor(8104.0742, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04627709090709686
dqn reward tensor(-418.3750, device='cuda:0') e 0.9230000000000085 loss_dqn tensor(9861.4629, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1900606006383896
dqn reward tensor(-303.1250, device='cuda:0') e 0.9225000000000085 loss_dqn tensor(12031.6465, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24106138944625854
dqn reward tensor(-452.8750, device='cuda:0') e 0.9220000000000086 loss_dqn tensor(13040.6562, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14306838810443878
dqn reward tensor(-426.1250, device='cuda:0') e 0.9215000000000086 loss_dqn tensor(14491.4941, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13324196636676788
dqn reward tensor(-380.8750, device='cuda:0') e 0.9210000000000087 loss_dqn tensor(16962.1191, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1388128101825714
dqn reward tensor(-360.3125, device='cuda:0') e 0.9205000000000088 loss_dqn tensor(20255.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03599633648991585
dqn reward tensor(-328.5625, device='cuda:0') e 0.9200000000000088 loss_dqn tensor(22272.5273, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19365328550338745
dqn reward tensor(-367.7500, device='cuda:0') e 0.9195000000000089 loss_dqn tensor(25772.7832, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.186254620552063
dqn reward tensor(-426.2500, device='cuda:0') e 0.9190000000000089 loss_dqn tensor(26773.3047, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19399940967559814
dqn reward tensor(-613.7500, device='cuda:0') e 0.918500000000009 loss_dqn tensor(29556.4863, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19305828213691711
dqn reward tensor(-309.8750, device='cuda:0') e 0.918000000000009 loss_dqn tensor(36304.8086, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2470463663339615
dqn reward tensor(-403.1875, device='cuda:0') e 0.9175000000000091 loss_dqn tensor(38444.5352, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13919201493263245
dqn reward tensor(-307.0625, device='cuda:0') e 0.9170000000000091 loss_dqn tensor(45639.9844, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18650317192077637
dqn reward tensor(-306., device='cuda:0') e 0.9165000000000092 loss_dqn tensor(45444.6484, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13711020350456238
dqn reward tensor(-186.5000, device='cuda:0') e 0.9160000000000093 loss_dqn tensor(55359.9297, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13620609045028687
dqn reward tensor(-490.8750, device='cuda:0') e 0.9155000000000093 loss_dqn tensor(57692.9375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19254544377326965
dqn reward tensor(-293.0625, device='cuda:0') e 0.9150000000000094 loss_dqn tensor(62600.5547, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09179992973804474
dqn reward tensor(-409.7500, device='cuda:0') e 0.9145000000000094 loss_dqn tensor(72031.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14180321991443634
dqn reward tensor(-361.5000, device='cuda:0') e 0.9140000000000095 loss_dqn tensor(76278.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18639126420021057
dqn reward tensor(-327.6250, device='cuda:0') e 0.9135000000000095 loss_dqn tensor(78231.5938, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19060900807380676
dqn reward tensor(-497., device='cuda:0') e 0.9130000000000096 loss_dqn tensor(83536.8672, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1472913920879364
dqn reward tensor(-470.8750, device='cuda:0') e 0.9125000000000096 loss_dqn tensor(92742.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09267222881317139
dqn reward tensor(-351.5000, device='cuda:0') e 0.9120000000000097 loss_dqn tensor(105713.6172, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18201732635498047
dqn reward tensor(-479.6250, device='cuda:0') e 0.9115000000000097 loss_dqn tensor(111002.3594, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13880741596221924
dqn reward tensor(-317.1250, device='cuda:0') e 0.9110000000000098 loss_dqn tensor(114189.1406, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13810887932777405
dqn reward tensor(-367.0625, device='cuda:0') e 0.9105000000000099 loss_dqn tensor(127723.4766, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18653474748134613
dqn reward tensor(-395.6250, device='cuda:0') e 0.9100000000000099 loss_dqn tensor(130783.0781, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08070091158151627
dqn reward tensor(-212.4375, device='cuda:0') e 0.90950000000001 loss_dqn tensor(147475.0469, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30499017238616943
dqn reward tensor(-425.5000, device='cuda:0') e 0.90900000000001 loss_dqn tensor(149372.2031, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20374825596809387
dqn reward tensor(-438.1875, device='cuda:0') e 0.9085000000000101 loss_dqn tensor(155820.8438, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08781806379556656
dqn reward tensor(-244.6250, device='cuda:0') e 0.9080000000000101 loss_dqn tensor(168316.3125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03486748784780502
dqn reward tensor(-495.8750, device='cuda:0') e 0.9075000000000102 loss_dqn tensor(155843.7188, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2480344921350479
dqn reward tensor(-438., device='cuda:0') e 0.9070000000000102 loss_dqn tensor(165844.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23307617008686066
dqn reward tensor(-444.2500, device='cuda:0') e 0.9065000000000103 loss_dqn tensor(181337.0938, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13355273008346558
dqn reward tensor(-342.6250, device='cuda:0') e 0.9060000000000104 loss_dqn tensor(189788.6562, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3290049433708191
dqn reward tensor(-430.3750, device='cuda:0') e 0.9055000000000104 loss_dqn tensor(202405.4062, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05219801515340805
dqn reward tensor(-330.1250, device='cuda:0') e 0.9050000000000105 loss_dqn tensor(188875.4375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14243800938129425
dqn reward tensor(-367.6250, device='cuda:0') e 0.9045000000000105 loss_dqn tensor(224239.6406, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05624210089445114
dqn reward tensor(-436., device='cuda:0') e 0.9040000000000106 loss_dqn tensor(218843.0312, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19644123315811157
dqn reward tensor(-538.8750, device='cuda:0') e 0.9035000000000106 loss_dqn tensor(217913.1719, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18275344371795654
dqn reward tensor(-181.6875, device='cuda:0') e 0.9030000000000107 loss_dqn tensor(244512.5469, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13082736730575562
dqn reward tensor(-399.1875, device='cuda:0') e 0.9025000000000107 loss_dqn tensor(245892.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2927176058292389
dqn reward tensor(-345.5000, device='cuda:0') e 0.9020000000000108 loss_dqn tensor(249623.0312, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23159673810005188
dqn reward tensor(-202.1250, device='cuda:0') e 0.9015000000000108 loss_dqn tensor(275262.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1347699761390686
dqn reward tensor(-471.1250, device='cuda:0') e 0.9010000000000109 loss_dqn tensor(267418.5938, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13656041026115417
dqn reward tensor(-443.5000, device='cuda:0') e 0.900500000000011 loss_dqn tensor(271780.1562, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09682462364435196
dqn reward tensor(-415.7500, device='cuda:0') e 0.900000000000011 loss_dqn tensor(318070.5312, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1346493363380432
dqn reward tensor(-442.3750, device='cuda:0') e 0.8995000000000111 loss_dqn tensor(290244.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14396685361862183
dqn reward tensor(-443.5000, device='cuda:0') e 0.8990000000000111 loss_dqn tensor(317937.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19339881837368011
dqn reward tensor(-343.1250, device='cuda:0') e 0.8985000000000112 loss_dqn tensor(322850.4062, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08770725131034851
dqn reward tensor(-201.8750, device='cuda:0') e 0.8980000000000112 loss_dqn tensor(339617.5938, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18317365646362305
dqn reward tensor(-293.3750, device='cuda:0') e 0.8975000000000113 loss_dqn tensor(325870.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1909574270248413
dqn reward tensor(-471., device='cuda:0') e 0.8970000000000113 loss_dqn tensor(365390.7812, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.45059734582901
dqn reward tensor(-354.1250, device='cuda:0') e 0.8965000000000114 loss_dqn tensor(351128., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08761660754680634
dqn reward tensor(-338.6250, device='cuda:0') e 0.8960000000000115 loss_dqn tensor(366550.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23636209964752197
dqn reward tensor(-159.2500, device='cuda:0') e 0.8955000000000115 loss_dqn tensor(411000.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18668325245380402
dqn reward tensor(-266.1250, device='cuda:0') e 0.8950000000000116 loss_dqn tensor(387035.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09662985056638718
dqn reward tensor(-448.1250, device='cuda:0') e 0.8945000000000116 loss_dqn tensor(427509.3125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18406382203102112
dqn reward tensor(-565., device='cuda:0') e 0.8940000000000117 loss_dqn tensor(425796., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19398581981658936
dqn reward tensor(-373., device='cuda:0') e 0.8935000000000117 loss_dqn tensor(446122.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2214903086423874
dqn reward tensor(-317.2500, device='cuda:0') e 0.8930000000000118 loss_dqn tensor(474013.5312, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19825206696987152
dqn reward tensor(-418.1250, device='cuda:0') e 0.8925000000000118 loss_dqn tensor(438531.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904917061328888
dqn reward tensor(-496.9375, device='cuda:0') e 0.8920000000000119 loss_dqn tensor(443659.3125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.277032732963562
dqn reward tensor(-388.3125, device='cuda:0') e 0.891500000000012 loss_dqn tensor(459681.2188, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09596040099859238
dqn reward tensor(-299.7500, device='cuda:0') e 0.891000000000012 loss_dqn tensor(511694.9375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27649521827697754
dqn reward tensor(-366.2500, device='cuda:0') e 0.8905000000000121 loss_dqn tensor(534564.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18831096589565277
dqn reward tensor(-296.3750, device='cuda:0') e 0.8900000000000121 loss_dqn tensor(530359.4375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2860581874847412
dqn reward tensor(-409.6250, device='cuda:0') e 0.8895000000000122 loss_dqn tensor(531373.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09476551413536072
dqn reward tensor(-328., device='cuda:0') e 0.8890000000000122 loss_dqn tensor(549522.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13882583379745483
dqn reward tensor(-429.5000, device='cuda:0') e 0.8885000000000123 loss_dqn tensor(564077.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13650953769683838
dqn reward tensor(-304.3750, device='cuda:0') e 0.8880000000000123 loss_dqn tensor(560297.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04153038561344147
dqn reward tensor(-424.6250, device='cuda:0') e 0.8875000000000124 loss_dqn tensor(631714., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13526880741119385
dqn reward tensor(-560.6250, device='cuda:0') e 0.8870000000000124 loss_dqn tensor(573625.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12742817401885986
dqn reward tensor(-470.6875, device='cuda:0') e 0.8865000000000125 loss_dqn tensor(622985.9375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14036673307418823
dqn reward tensor(-511.2500, device='cuda:0') e 0.8860000000000126 loss_dqn tensor(629383.3125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2532550096511841
dqn reward tensor(-384., device='cuda:0') e 0.8855000000000126 loss_dqn tensor(698721.3125, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022141367197036743
dqn reward tensor(-280.2500, device='cuda:0') e 0.8850000000000127 loss_dqn tensor(683941.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021682703867554665
dqn reward tensor(-590.3125, device='cuda:0') e 0.8845000000000127 loss_dqn tensor(730912.4375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14669260382652283
dqn reward tensor(-456., device='cuda:0') e 0.8840000000000128 loss_dqn tensor(679681.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07675506174564362
dqn reward tensor(-553.8125, device='cuda:0') e 0.8835000000000128 loss_dqn tensor(734626.4375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2587307095527649
dqn reward tensor(-526.9375, device='cuda:0') e 0.8830000000000129 loss_dqn tensor(711249.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26380014419555664
dqn reward tensor(-408., device='cuda:0') e 0.8825000000000129 loss_dqn tensor(900763.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016636844724416733
dqn reward tensor(-453.4375, device='cuda:0') e 0.882000000000013 loss_dqn tensor(750418.6875, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1430453658103943
dqn reward tensor(-436.5000, device='cuda:0') e 0.881500000000013 loss_dqn tensor(827306.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20168574154376984
dqn reward tensor(-444., device='cuda:0') e 0.8810000000000131 loss_dqn tensor(793216.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19534099102020264
dqn reward tensor(-454.8125, device='cuda:0') e 0.8805000000000132 loss_dqn tensor(793902.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24961604177951813
dqn reward tensor(-479., device='cuda:0') e 0.8800000000000132 loss_dqn tensor(864896.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13320530951023102
dqn reward tensor(-444., device='cuda:0') e 0.8795000000000133 loss_dqn tensor(830474.5625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22336675226688385
dqn reward tensor(-492.5000, device='cuda:0') e 0.8790000000000133 loss_dqn tensor(833391.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28065380454063416
dqn reward tensor(-579.3750, device='cuda:0') e 0.8785000000000134 loss_dqn tensor(885340.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19228607416152954
dqn reward tensor(-520.5625, device='cuda:0') e 0.8780000000000134 loss_dqn tensor(880766.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1642264723777771
dqn reward tensor(-479.7500, device='cuda:0') e 0.8775000000000135 loss_dqn tensor(904553.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16356921195983887
dqn reward tensor(-429., device='cuda:0') e 0.8770000000000135 loss_dqn tensor(930063.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18843555450439453
dqn reward tensor(-513.6250, device='cuda:0') e 0.8765000000000136 loss_dqn tensor(934629., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14571045339107513
dqn reward tensor(-525.2500, device='cuda:0') e 0.8760000000000137 loss_dqn tensor(1033761.0625, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27943459153175354
dqn reward tensor(-529.8125, device='cuda:0') e 0.8755000000000137 loss_dqn tensor(980464.9375, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13812881708145142
dqn reward tensor(-478.3125, device='cuda:0') e 0.8750000000000138 loss_dqn tensor(1070072.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.189375102519989
dqn reward tensor(-466.3750, device='cuda:0') e 0.8745000000000138 loss_dqn tensor(1104187.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0863398015499115
dqn reward tensor(-444.7500, device='cuda:0') e 0.8740000000000139 loss_dqn tensor(1109500.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13421031832695007
dqn reward tensor(-428.5000, device='cuda:0') e 0.8735000000000139 loss_dqn tensor(1181055.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07455524057149887
dqn reward tensor(-386.1250, device='cuda:0') e 0.873000000000014 loss_dqn tensor(1206530.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20412160456180573
dqn reward tensor(-366.3750, device='cuda:0') e 0.872500000000014 loss_dqn tensor(1099810.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13878978788852692
dqn reward tensor(-445.1250, device='cuda:0') e 0.8720000000000141 loss_dqn tensor(1178965.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08328285813331604
dqn reward tensor(-468.1250, device='cuda:0') e 0.8715000000000142 loss_dqn tensor(1132939.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08054817467927933
dqn reward tensor(-312.7500, device='cuda:0') e 0.8710000000000142 loss_dqn tensor(1253643.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019165795296430588
dqn reward tensor(-549., device='cuda:0') e 0.8705000000000143 loss_dqn tensor(1343205.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1453062891960144
dqn reward tensor(-478.2500, device='cuda:0') e 0.8700000000000143 loss_dqn tensor(1264044.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016261160373687744
dqn reward tensor(-456., device='cuda:0') e 0.8695000000000144 loss_dqn tensor(1227041.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13957199454307556
dqn reward tensor(-405.3750, device='cuda:0') e 0.8690000000000144 loss_dqn tensor(1221909.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2575903534889221
dqn reward tensor(-444.7500, device='cuda:0') e 0.8685000000000145 loss_dqn tensor(1375663.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32827553153038025
dqn reward tensor(-428.0625, device='cuda:0') e 0.8680000000000145 loss_dqn tensor(1457480.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32979559898376465
dqn reward tensor(-511.2500, device='cuda:0') e 0.8675000000000146 loss_dqn tensor(1468537.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08128580451011658
dqn reward tensor(-442.6250, device='cuda:0') e 0.8670000000000146 loss_dqn tensor(1415928., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19609272480010986
dqn reward tensor(-474.6250, device='cuda:0') e 0.8665000000000147 loss_dqn tensor(1512342.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13555946946144104
dqn reward tensor(-426., device='cuda:0') e 0.8660000000000148 loss_dqn tensor(1390065.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17980846762657166
dqn reward tensor(-545.3750, device='cuda:0') e 0.8655000000000148 loss_dqn tensor(1513756.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18162274360656738
dqn reward tensor(-417.3750, device='cuda:0') e 0.8650000000000149 loss_dqn tensor(1537124.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2280566543340683
dqn reward tensor(-522.0625, device='cuda:0') e 0.8645000000000149 loss_dqn tensor(1571659., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11476658284664154
dqn reward tensor(-237.7500, device='cuda:0') e 0.864000000000015 loss_dqn tensor(1596156.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15562814474105835
dqn reward tensor(-464.5000, device='cuda:0') e 0.863500000000015 loss_dqn tensor(1650248.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1507757008075714
dqn reward tensor(-457.3750, device='cuda:0') e 0.8630000000000151 loss_dqn tensor(1712894.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09896820783615112
dqn reward tensor(-424.6250, device='cuda:0') e 0.8625000000000151 loss_dqn tensor(1636085.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22746717929840088
dqn reward tensor(-522.8750, device='cuda:0') e 0.8620000000000152 loss_dqn tensor(1601443.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08185862004756927
dqn reward tensor(-573.6250, device='cuda:0') e 0.8615000000000153 loss_dqn tensor(1754695.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34925150871276855
dqn reward tensor(-507.2500, device='cuda:0') e 0.8610000000000153 loss_dqn tensor(1658283.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028086453676223755
dqn reward tensor(-456.3750, device='cuda:0') e 0.8605000000000154 loss_dqn tensor(1727466., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025654012337327003
dqn reward tensor(-589.7500, device='cuda:0') e 0.8600000000000154 loss_dqn tensor(1712174.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1354493796825409
dqn reward tensor(-550.8750, device='cuda:0') e 0.8595000000000155 loss_dqn tensor(1897000.3750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14649894833564758
dqn reward tensor(-405.9375, device='cuda:0') e 0.8590000000000155 loss_dqn tensor(1853809.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08685392886400223
dqn reward tensor(-532., device='cuda:0') e 0.8585000000000156 loss_dqn tensor(1830595.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07673531025648117
dqn reward tensor(-452.2500, device='cuda:0') e 0.8580000000000156 loss_dqn tensor(1879448.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1484736204147339
dqn reward tensor(-554.1250, device='cuda:0') e 0.8575000000000157 loss_dqn tensor(1710638.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14268238842487335
dqn reward tensor(-484.3750, device='cuda:0') e 0.8570000000000157 loss_dqn tensor(1873475.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01454421691596508
dqn reward tensor(-448.1250, device='cuda:0') e 0.8565000000000158 loss_dqn tensor(1922232., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21009954810142517
dqn reward tensor(-495., device='cuda:0') e 0.8560000000000159 loss_dqn tensor(1944436.6250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13711793720722198
dqn reward tensor(-520.3750, device='cuda:0') e 0.8555000000000159 loss_dqn tensor(2057764.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07988601922988892
dqn reward tensor(-508.3125, device='cuda:0') e 0.855000000000016 loss_dqn tensor(2083470., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08382625877857208
dqn reward tensor(-551., device='cuda:0') e 0.854500000000016 loss_dqn tensor(2035781., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08169730007648468
dqn reward tensor(-600.3750, device='cuda:0') e 0.8540000000000161 loss_dqn tensor(2082310.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13476291298866272
dqn reward tensor(-569.2500, device='cuda:0') e 0.8535000000000161 loss_dqn tensor(2068555.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08726153522729874
dqn reward tensor(-427.2500, device='cuda:0') e 0.8530000000000162 loss_dqn tensor(2211361.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13443690538406372
dqn reward tensor(-491.3750, device='cuda:0') e 0.8525000000000162 loss_dqn tensor(2095288., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08399435132741928
dqn reward tensor(-613., device='cuda:0') e 0.8520000000000163 loss_dqn tensor(2064037., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1835727095603943
dqn reward tensor(-444.7500, device='cuda:0') e 0.8515000000000164 loss_dqn tensor(2107363.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0765579491853714
dqn reward tensor(-494.1250, device='cuda:0') e 0.8510000000000164 loss_dqn tensor(2198682., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.245798259973526
dqn reward tensor(-541.8750, device='cuda:0') e 0.8505000000000165 loss_dqn tensor(2191025., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20374907553195953
dqn reward tensor(-525.7500, device='cuda:0') e 0.8500000000000165 loss_dqn tensor(2147127.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13075637817382812
dqn reward tensor(-551.1250, device='cuda:0') e 0.8495000000000166 loss_dqn tensor(2320302.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19648170471191406
dqn reward tensor(-526.7500, device='cuda:0') e 0.8490000000000166 loss_dqn tensor(2273398.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04708234593272209
dqn reward tensor(-606.7500, device='cuda:0') e 0.8485000000000167 loss_dqn tensor(2311915., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0893646627664566
dqn reward tensor(-608.8750, device='cuda:0') e 0.8480000000000167 loss_dqn tensor(2337574., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09129511564970016
dqn reward tensor(-473.1250, device='cuda:0') e 0.8475000000000168 loss_dqn tensor(2542486.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18399640917778015
dqn reward tensor(-527.1250, device='cuda:0') e 0.8470000000000169 loss_dqn tensor(2430894.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17667564749717712
dqn reward tensor(-484.1250, device='cuda:0') e 0.8465000000000169 loss_dqn tensor(2530670., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13531558215618134
dqn reward tensor(-599.6875, device='cuda:0') e 0.846000000000017 loss_dqn tensor(2623418., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1848522126674652
dqn reward tensor(-550.2500, device='cuda:0') e 0.845500000000017 loss_dqn tensor(2350756.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040599673986434937
dqn reward tensor(-506.3750, device='cuda:0') e 0.8450000000000171 loss_dqn tensor(2479141.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13486866652965546
dqn reward tensor(-332., device='cuda:0') e 0.8445000000000171 loss_dqn tensor(2804932., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08158112317323685
dqn reward tensor(-558.3125, device='cuda:0') e 0.8440000000000172 loss_dqn tensor(2580719.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14050428569316864
dqn reward tensor(-488.2500, device='cuda:0') e 0.8435000000000172 loss_dqn tensor(2736704., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08286377042531967
dqn reward tensor(-558.5000, device='cuda:0') e 0.8430000000000173 loss_dqn tensor(2792686.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1379106640815735
dqn reward tensor(-528.9375, device='cuda:0') e 0.8425000000000173 loss_dqn tensor(2660684., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19852983951568604
dqn reward tensor(-503.1250, device='cuda:0') e 0.8420000000000174 loss_dqn tensor(2672330.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13259227573871613
dqn reward tensor(-471.3750, device='cuda:0') e 0.8415000000000175 loss_dqn tensor(2726790.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07376403361558914
dqn reward tensor(-381.1875, device='cuda:0') e 0.8410000000000175 loss_dqn tensor(3021536.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32646018266677856
dqn reward tensor(-350.7500, device='cuda:0') e 0.8405000000000176 loss_dqn tensor(2789879., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13078545033931732
dqn reward tensor(-420.5000, device='cuda:0') e 0.8400000000000176 loss_dqn tensor(2867789., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13244298100471497
dqn reward tensor(-407.2500, device='cuda:0') e 0.8395000000000177 loss_dqn tensor(2908370., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24110162258148193
dqn reward tensor(-464.3750, device='cuda:0') e 0.8390000000000177 loss_dqn tensor(2897901., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08032623678445816
dqn reward tensor(-414.6250, device='cuda:0') e 0.8385000000000178 loss_dqn tensor(2962349.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33448588848114014
dqn reward tensor(-489., device='cuda:0') e 0.8380000000000178 loss_dqn tensor(3067306., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18701933324337006
dqn reward tensor(-368.3750, device='cuda:0') e 0.8375000000000179 loss_dqn tensor(3286270., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2267833948135376
dqn reward tensor(-532.8750, device='cuda:0') e 0.837000000000018 loss_dqn tensor(3061411.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18367516994476318
dqn reward tensor(-536.2500, device='cuda:0') e 0.836500000000018 loss_dqn tensor(3350890., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2002314180135727
dqn reward tensor(-447.8750, device='cuda:0') e 0.8360000000000181 loss_dqn tensor(3352103.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15550675988197327
dqn reward tensor(-517., device='cuda:0') e 0.8355000000000181 loss_dqn tensor(3161790.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23234766721725464
dqn reward tensor(-574., device='cuda:0') e 0.8350000000000182 loss_dqn tensor(3708168.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15435302257537842
dqn reward tensor(-424.3750, device='cuda:0') e 0.8345000000000182 loss_dqn tensor(2941139.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2773895263671875
dqn reward tensor(-451., device='cuda:0') e 0.8340000000000183 loss_dqn tensor(3104618.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.092489093542099
dqn reward tensor(-507.7500, device='cuda:0') e 0.8335000000000183 loss_dqn tensor(3358400.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17567257583141327
dqn reward tensor(-598.0625, device='cuda:0') e 0.8330000000000184 loss_dqn tensor(3107694.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03849988430738449
dqn reward tensor(-476.6250, device='cuda:0') e 0.8325000000000184 loss_dqn tensor(3328880., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030278267338871956
dqn reward tensor(-585.1250, device='cuda:0') e 0.8320000000000185 loss_dqn tensor(3536416.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08979809284210205
dqn reward tensor(-559.6250, device='cuda:0') e 0.8315000000000186 loss_dqn tensor(3518313., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1944703757762909
dqn reward tensor(-338., device='cuda:0') e 0.8310000000000186 loss_dqn tensor(3329617.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19514501094818115
dqn reward tensor(-572.8750, device='cuda:0') e 0.8305000000000187 loss_dqn tensor(3603438.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432473361492157
dqn reward tensor(-487.2500, device='cuda:0') e 0.8300000000000187 loss_dqn tensor(3499665.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06934148073196411
dqn reward tensor(-533.1250, device='cuda:0') e 0.8295000000000188 loss_dqn tensor(3588676.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21398437023162842
dqn reward tensor(-503.2500, device='cuda:0') e 0.8290000000000188 loss_dqn tensor(3434557., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20859909057617188
dqn reward tensor(-505.5000, device='cuda:0') e 0.8285000000000189 loss_dqn tensor(3669844.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2598543167114258
dqn reward tensor(-547.7500, device='cuda:0') e 0.8280000000000189 loss_dqn tensor(3885002.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017495721578598022
dqn reward tensor(-425.7500, device='cuda:0') e 0.827500000000019 loss_dqn tensor(3517658., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20389804244041443
dqn reward tensor(-465.6250, device='cuda:0') e 0.827000000000019 loss_dqn tensor(3501346., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19559496641159058
dqn reward tensor(-525.8750, device='cuda:0') e 0.8265000000000191 loss_dqn tensor(3655846., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13184520602226257
dqn reward tensor(-397.3750, device='cuda:0') e 0.8260000000000192 loss_dqn tensor(3564405.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14466336369514465
dqn reward tensor(-542.5000, device='cuda:0') e 0.8255000000000192 loss_dqn tensor(3843857.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23720583319664001
dqn reward tensor(-544.8750, device='cuda:0') e 0.8250000000000193 loss_dqn tensor(3846548.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14325667917728424
dqn reward tensor(-505.5000, device='cuda:0') e 0.8245000000000193 loss_dqn tensor(4339090., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35762661695480347
dqn reward tensor(-522.7500, device='cuda:0') e 0.8240000000000194 loss_dqn tensor(3824603., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1244724690914154
dqn reward tensor(-397.2500, device='cuda:0') e 0.8235000000000194 loss_dqn tensor(4054318.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14652130007743835
dqn reward tensor(-567.6250, device='cuda:0') e 0.8230000000000195 loss_dqn tensor(4713043.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11802597343921661
dqn reward tensor(-539.2500, device='cuda:0') e 0.8225000000000195 loss_dqn tensor(4047787.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26991593837738037
dqn reward tensor(-462.3750, device='cuda:0') e 0.8220000000000196 loss_dqn tensor(4249149., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13572292029857635
dqn reward tensor(-466.7500, device='cuda:0') e 0.8215000000000197 loss_dqn tensor(4263899.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1379946917295456
dqn reward tensor(-552.5000, device='cuda:0') e 0.8210000000000197 loss_dqn tensor(4586849., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1393178254365921
dqn reward tensor(-637.4375, device='cuda:0') e 0.8205000000000198 loss_dqn tensor(4870217., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08212300390005112
dqn reward tensor(-593.5000, device='cuda:0') e 0.8200000000000198 loss_dqn tensor(4588235., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25623854994773865
dqn reward tensor(-536., device='cuda:0') e 0.8195000000000199 loss_dqn tensor(4163922., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18910279870033264
dqn reward tensor(-627., device='cuda:0') e 0.8190000000000199 loss_dqn tensor(4703425., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08574296534061432
dqn reward tensor(-613.5000, device='cuda:0') e 0.81850000000002 loss_dqn tensor(5018585.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08392602950334549
dqn reward tensor(-559.8750, device='cuda:0') e 0.81800000000002 loss_dqn tensor(4633464., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30872660875320435
dqn reward tensor(-539.5000, device='cuda:0') e 0.8175000000000201 loss_dqn tensor(4372472., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19902990758419037
dqn reward tensor(-413.3750, device='cuda:0') e 0.8170000000000202 loss_dqn tensor(4405314., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13566718995571136
dqn reward tensor(-374.6250, device='cuda:0') e 0.8165000000000202 loss_dqn tensor(4751256., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0855952799320221
dqn reward tensor(-569., device='cuda:0') e 0.8160000000000203 loss_dqn tensor(4572252.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13926129043102264
dqn reward tensor(-430.1875, device='cuda:0') e 0.8155000000000203 loss_dqn tensor(4712719., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2796870470046997
dqn reward tensor(-563.5000, device='cuda:0') e 0.8150000000000204 loss_dqn tensor(4543717., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17774127423763275
dqn reward tensor(-471.6250, device='cuda:0') e 0.8145000000000204 loss_dqn tensor(4724048.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09462975710630417
dqn reward tensor(-424.7500, device='cuda:0') e 0.8140000000000205 loss_dqn tensor(4467857., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1466551125049591
dqn reward tensor(-494.1250, device='cuda:0') e 0.8135000000000205 loss_dqn tensor(5064322., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488465666770935
dqn reward tensor(-485., device='cuda:0') e 0.8130000000000206 loss_dqn tensor(5126799., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2422601580619812
dqn reward tensor(-466., device='cuda:0') e 0.8125000000000207 loss_dqn tensor(4859931.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22405198216438293
dqn reward tensor(-392.8750, device='cuda:0') e 0.8120000000000207 loss_dqn tensor(5878762., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13118547201156616
dqn reward tensor(-603.5625, device='cuda:0') e 0.8115000000000208 loss_dqn tensor(5146434.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14481312036514282
dqn reward tensor(-531.0625, device='cuda:0') e 0.8110000000000208 loss_dqn tensor(5230607.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17255589365959167
dqn reward tensor(-425.7500, device='cuda:0') e 0.8105000000000209 loss_dqn tensor(5472070.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08720890432596207
dqn reward tensor(-451.1250, device='cuda:0') e 0.8100000000000209 loss_dqn tensor(5465538., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08892833441495895
dqn reward tensor(-368.3750, device='cuda:0') e 0.809500000000021 loss_dqn tensor(5855768., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18406572937965393
dqn reward tensor(-300.5000, device='cuda:0') e 0.809000000000021 loss_dqn tensor(5572571., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14351694285869598
dqn reward tensor(-412.5625, device='cuda:0') e 0.8085000000000211 loss_dqn tensor(5677553., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13504526019096375
dqn reward tensor(-369.1250, device='cuda:0') e 0.8080000000000211 loss_dqn tensor(6095645.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18200768530368805
dqn reward tensor(-319.6250, device='cuda:0') e 0.8075000000000212 loss_dqn tensor(5210843., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025152312591671944
dqn reward tensor(-412.1250, device='cuda:0') e 0.8070000000000213 loss_dqn tensor(5730698., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24828991293907166
dqn reward tensor(-418.8750, device='cuda:0') e 0.8065000000000213 loss_dqn tensor(6274802., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08100022375583649
dqn reward tensor(-478.7500, device='cuda:0') e 0.8060000000000214 loss_dqn tensor(5277196., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31662291288375854
dqn reward tensor(-428.3750, device='cuda:0') e 0.8055000000000214 loss_dqn tensor(5742901., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1372886300086975
dqn reward tensor(-332., device='cuda:0') e 0.8050000000000215 loss_dqn tensor(5850565., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07274995744228363
dqn reward tensor(-368.8750, device='cuda:0') e 0.8045000000000215 loss_dqn tensor(5296220., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08147463202476501
dqn reward tensor(-327., device='cuda:0') e 0.8040000000000216 loss_dqn tensor(5961662., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12960706651210785
dqn reward tensor(-381.2500, device='cuda:0') e 0.8035000000000216 loss_dqn tensor(5747488., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3067225217819214
dqn reward tensor(-420.7500, device='cuda:0') e 0.8030000000000217 loss_dqn tensor(5949693., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23665347695350647
dqn reward tensor(-342.8750, device='cuda:0') e 0.8025000000000218 loss_dqn tensor(5377005., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12939053773880005
dqn reward tensor(-352.6875, device='cuda:0') e 0.8020000000000218 loss_dqn tensor(5836522., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3169376254081726
dqn reward tensor(-373.8750, device='cuda:0') e 0.8015000000000219 loss_dqn tensor(5595227., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07421189546585083
dqn reward tensor(-400.8750, device='cuda:0') e 0.8010000000000219 loss_dqn tensor(5781476.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2028253972530365
dqn reward tensor(-473.3125, device='cuda:0') e 0.800500000000022 loss_dqn tensor(5599750., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2534244954586029
dqn reward tensor(-438.3750, device='cuda:0') e 0.800000000000022 loss_dqn tensor(5926873.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.152032271027565
dqn reward tensor(-468.6250, device='cuda:0') e 0.7995000000000221 loss_dqn tensor(5289247.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26504796743392944
dqn reward tensor(-243.1250, device='cuda:0') e 0.7990000000000221 loss_dqn tensor(5794112., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29424387216567993
dqn reward tensor(-425.9375, device='cuda:0') e 0.7985000000000222 loss_dqn tensor(5887840.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499176323413849
dqn reward tensor(-330., device='cuda:0') e 0.7980000000000222 loss_dqn tensor(6731352.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1476406306028366
dqn reward tensor(-349.1250, device='cuda:0') e 0.7975000000000223 loss_dqn tensor(7290465., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09467475116252899
dqn reward tensor(-499.9375, device='cuda:0') e 0.7970000000000224 loss_dqn tensor(5683182., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29371291399002075
dqn reward tensor(-353.8750, device='cuda:0') e 0.7965000000000224 loss_dqn tensor(6147928.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13957729935646057
dqn reward tensor(-477.6250, device='cuda:0') e 0.7960000000000225 loss_dqn tensor(5982605.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043818891048431396
dqn reward tensor(-403.5000, device='cuda:0') e 0.7955000000000225 loss_dqn tensor(6545344., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3111504316329956
dqn reward tensor(-279., device='cuda:0') e 0.7950000000000226 loss_dqn tensor(6075461., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07966150343418121
dqn reward tensor(-371.5000, device='cuda:0') e 0.7945000000000226 loss_dqn tensor(6295669.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13621649146080017
dqn reward tensor(-472.6875, device='cuda:0') e 0.7940000000000227 loss_dqn tensor(6309657., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23658519983291626
dqn reward tensor(-375.2500, device='cuda:0') e 0.7935000000000227 loss_dqn tensor(6460901., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08441127091646194
dqn reward tensor(-512.5000, device='cuda:0') e 0.7930000000000228 loss_dqn tensor(5976283., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22767190635204315
dqn reward tensor(-445., device='cuda:0') e 0.7925000000000229 loss_dqn tensor(6631685., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17603178322315216
dqn reward tensor(-334.8750, device='cuda:0') e 0.7920000000000229 loss_dqn tensor(7529174., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28708943724632263
dqn reward tensor(-437.1250, device='cuda:0') e 0.791500000000023 loss_dqn tensor(6636291., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04622647911310196
dqn reward tensor(-298.3750, device='cuda:0') e 0.791000000000023 loss_dqn tensor(6719376., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08763976395130157
dqn reward tensor(-305.7500, device='cuda:0') e 0.7905000000000231 loss_dqn tensor(7406082., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08885610103607178
dqn reward tensor(-388.1250, device='cuda:0') e 0.7900000000000231 loss_dqn tensor(7356851.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18838679790496826
dqn reward tensor(-232., device='cuda:0') e 0.7895000000000232 loss_dqn tensor(8249745., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2560325860977173
dqn reward tensor(-341.8750, device='cuda:0') e 0.7890000000000232 loss_dqn tensor(6506812., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1897526979446411
dqn reward tensor(-353., device='cuda:0') e 0.7885000000000233 loss_dqn tensor(7324701.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08837072551250458
dqn reward tensor(-412.4375, device='cuda:0') e 0.7880000000000233 loss_dqn tensor(7209327.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19579419493675232
dqn reward tensor(-406.7500, device='cuda:0') e 0.7875000000000234 loss_dqn tensor(7500772., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21752062439918518
dqn reward tensor(-436., device='cuda:0') e 0.7870000000000235 loss_dqn tensor(6872950.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1332119256258011
dqn reward tensor(-393.3125, device='cuda:0') e 0.7865000000000235 loss_dqn tensor(6804702.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24716880917549133
dqn reward tensor(-252.5000, device='cuda:0') e 0.7860000000000236 loss_dqn tensor(7400498., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042296722531318665
dqn reward tensor(-388.7500, device='cuda:0') e 0.7855000000000236 loss_dqn tensor(6262255.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24661453068256378
dqn reward tensor(-247., device='cuda:0') e 0.7850000000000237 loss_dqn tensor(7446644., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13700127601623535
dqn reward tensor(-182.8125, device='cuda:0') e 0.7845000000000237 loss_dqn tensor(8821988., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09275209903717041
dqn reward tensor(-290.7500, device='cuda:0') e 0.7840000000000238 loss_dqn tensor(6862254.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835588350892067
dqn reward tensor(-288.3750, device='cuda:0') e 0.7835000000000238 loss_dqn tensor(6349464.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1915607750415802
dqn reward tensor(-211.3125, device='cuda:0') e 0.7830000000000239 loss_dqn tensor(7350702., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17153814435005188
dqn reward tensor(-520.8750, device='cuda:0') e 0.782500000000024 loss_dqn tensor(5523695.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2254878580570221
dqn reward tensor(-413.8750, device='cuda:0') e 0.782000000000024 loss_dqn tensor(5990374., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08290321379899979
dqn reward tensor(-332., device='cuda:0') e 0.7815000000000241 loss_dqn tensor(6825926.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17178840935230255
dqn reward tensor(-195.6250, device='cuda:0') e 0.7810000000000241 loss_dqn tensor(6472108.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13057640194892883
dqn reward tensor(-173.6250, device='cuda:0') e 0.7805000000000242 loss_dqn tensor(5722637.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2454230785369873
dqn reward tensor(-246., device='cuda:0') e 0.7800000000000242 loss_dqn tensor(4597043.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17838943004608154
dqn reward tensor(-447.4375, device='cuda:0') e 0.7795000000000243 loss_dqn tensor(3986703.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27229106426239014
dqn reward tensor(-309.5000, device='cuda:0') e 0.7790000000000243 loss_dqn tensor(5082689.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13367792963981628
dqn reward tensor(-326.8750, device='cuda:0') e 0.7785000000000244 loss_dqn tensor(4512348., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18077528476715088
dqn reward tensor(-561.5000, device='cuda:0') e 0.7780000000000244 loss_dqn tensor(3556233.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23161375522613525
dqn reward tensor(-480.6250, device='cuda:0') e 0.7775000000000245 loss_dqn tensor(3395427., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24308553338050842
dqn reward tensor(-199.2500, device='cuda:0') e 0.7770000000000246 loss_dqn tensor(3952805.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18437109887599945
dqn reward tensor(-66.1250, device='cuda:0') e 0.7765000000000246 loss_dqn tensor(4325880., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14220327138900757
dqn reward tensor(-234.2500, device='cuda:0') e 0.7760000000000247 loss_dqn tensor(3257981.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17280128598213196
dqn reward tensor(30., device='cuda:0') e 0.7755000000000247 loss_dqn tensor(4436727.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24504001438617706
dqn reward tensor(-242.7500, device='cuda:0') e 0.7750000000000248 loss_dqn tensor(3608111., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058156874030828476
dqn reward tensor(-172.3125, device='cuda:0') e 0.7745000000000248 loss_dqn tensor(3645642.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08863916993141174
dqn reward tensor(-364.8750, device='cuda:0') e 0.7740000000000249 loss_dqn tensor(3227959.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1382526159286499
dqn reward tensor(-139.1250, device='cuda:0') e 0.773500000000025 loss_dqn tensor(3693946.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21000321209430695
dqn reward tensor(-274.7500, device='cuda:0') e 0.773000000000025 loss_dqn tensor(3366464.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01817198470234871
dqn reward tensor(-229.5000, device='cuda:0') e 0.7725000000000251 loss_dqn tensor(4095689.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13993242383003235
dqn reward tensor(-267., device='cuda:0') e 0.7720000000000251 loss_dqn tensor(4250910., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1484123170375824
dqn reward tensor(-301.5000, device='cuda:0') e 0.7715000000000252 loss_dqn tensor(3906008., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2817966043949127
dqn reward tensor(-227.8750, device='cuda:0') e 0.7710000000000252 loss_dqn tensor(4096128.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2586055397987366
dqn reward tensor(-85.3750, device='cuda:0') e 0.7705000000000253 loss_dqn tensor(4850708.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21002912521362305
dqn reward tensor(-145.1250, device='cuda:0') e 0.7700000000000253 loss_dqn tensor(4631672., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17321822047233582
dqn reward tensor(-271.1250, device='cuda:0') e 0.7695000000000254 loss_dqn tensor(4082388.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027981974184513092
dqn reward tensor(-184.7500, device='cuda:0') e 0.7690000000000254 loss_dqn tensor(4033709., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24668030440807343
dqn reward tensor(-444.1250, device='cuda:0') e 0.7685000000000255 loss_dqn tensor(3721931., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34083858132362366
dqn reward tensor(-229.3750, device='cuda:0') e 0.7680000000000256 loss_dqn tensor(4066292., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22228649258613586
dqn reward tensor(-270.2500, device='cuda:0') e 0.7675000000000256 loss_dqn tensor(3691718.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1550006866455078
dqn reward tensor(-272.6875, device='cuda:0') e 0.7670000000000257 loss_dqn tensor(3618956.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13750994205474854
dqn reward tensor(-277.2500, device='cuda:0') e 0.7665000000000257 loss_dqn tensor(3968435., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17162537574768066
dqn reward tensor(-295.5000, device='cuda:0') e 0.7660000000000258 loss_dqn tensor(3867699., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11503797769546509
dqn reward tensor(-399.4375, device='cuda:0') e 0.7655000000000258 loss_dqn tensor(3337636., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499364823102951
dqn reward tensor(-98.7500, device='cuda:0') e 0.7650000000000259 loss_dqn tensor(5123356., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1663953959941864
dqn reward tensor(-230.2500, device='cuda:0') e 0.7645000000000259 loss_dqn tensor(4139473., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13883766531944275
dqn reward tensor(-271.7500, device='cuda:0') e 0.764000000000026 loss_dqn tensor(4618641.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19943058490753174
dqn reward tensor(-282.2500, device='cuda:0') e 0.763500000000026 loss_dqn tensor(4402463.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20231685042381287
dqn reward tensor(-259.2500, device='cuda:0') e 0.7630000000000261 loss_dqn tensor(4610064., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12446505576372147
dqn reward tensor(-239.7500, device='cuda:0') e 0.7625000000000262 loss_dqn tensor(3569118., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016309913247823715
dqn reward tensor(-272.1875, device='cuda:0') e 0.7620000000000262 loss_dqn tensor(3850826.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14161056280136108
dqn reward tensor(-58.8750, device='cuda:0') e 0.7615000000000263 loss_dqn tensor(5917975., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4019255042076111
dqn reward tensor(-135.2500, device='cuda:0') e 0.7610000000000263 loss_dqn tensor(5959886., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07569397985935211
dqn reward tensor(-249., device='cuda:0') e 0.7605000000000264 loss_dqn tensor(4228529., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07971829175949097
dqn reward tensor(-456., device='cuda:0') e 0.7600000000000264 loss_dqn tensor(4068262.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2958781123161316
dqn reward tensor(-340., device='cuda:0') e 0.7595000000000265 loss_dqn tensor(4068572., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17891919612884521
dqn reward tensor(-266.5000, device='cuda:0') e 0.7590000000000265 loss_dqn tensor(4523546., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09128640592098236
dqn reward tensor(-137.6250, device='cuda:0') e 0.7585000000000266 loss_dqn tensor(5698379., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18094411492347717
dqn reward tensor(-230.1875, device='cuda:0') e 0.7580000000000267 loss_dqn tensor(5470987.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20576211810112
dqn reward tensor(-211.5000, device='cuda:0') e 0.7575000000000267 loss_dqn tensor(7348529.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1313856542110443
dqn reward tensor(-252., device='cuda:0') e 0.7570000000000268 loss_dqn tensor(6781940.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.351733535528183
dqn reward tensor(-191.6250, device='cuda:0') e 0.7565000000000268 loss_dqn tensor(6270063., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20243114233016968
dqn reward tensor(-271.2500, device='cuda:0') e 0.7560000000000269 loss_dqn tensor(5476533.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1587221473455429
dqn reward tensor(-177.3750, device='cuda:0') e 0.7555000000000269 loss_dqn tensor(6666886., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11226914823055267
dqn reward tensor(-261.3750, device='cuda:0') e 0.755000000000027 loss_dqn tensor(7712263.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13559122383594513
dqn reward tensor(-423.5000, device='cuda:0') e 0.754500000000027 loss_dqn tensor(4447029., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13767006993293762
dqn reward tensor(-238.0625, device='cuda:0') e 0.7540000000000271 loss_dqn tensor(5354573., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09986569732427597
dqn reward tensor(-382.7500, device='cuda:0') e 0.7535000000000271 loss_dqn tensor(3932478.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2808457612991333
dqn reward tensor(-274.3125, device='cuda:0') e 0.7530000000000272 loss_dqn tensor(4534881., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18668967485427856
dqn reward tensor(-104.2500, device='cuda:0') e 0.7525000000000273 loss_dqn tensor(6285571., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11807700991630554
dqn reward tensor(-170.2500, device='cuda:0') e 0.7520000000000273 loss_dqn tensor(4641634.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19620844721794128
dqn reward tensor(-338.8125, device='cuda:0') e 0.7515000000000274 loss_dqn tensor(3513262., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08571748435497284
dqn reward tensor(-247.8125, device='cuda:0') e 0.7510000000000274 loss_dqn tensor(3734835.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08034627884626389
dqn reward tensor(-273.2500, device='cuda:0') e 0.7505000000000275 loss_dqn tensor(2877714.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07447279989719391
dqn reward tensor(-127.3750, device='cuda:0') e 0.7500000000000275 loss_dqn tensor(4076946.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2015128880739212
dqn reward tensor(-390.6250, device='cuda:0') e 0.7495000000000276 loss_dqn tensor(2537406.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14350363612174988
dqn reward tensor(-301.3750, device='cuda:0') e 0.7490000000000276 loss_dqn tensor(2840038.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07650937885046005
dqn reward tensor(-221.2500, device='cuda:0') e 0.7485000000000277 loss_dqn tensor(2868005.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19753015041351318
dqn reward tensor(-463.7500, device='cuda:0') e 0.7480000000000278 loss_dqn tensor(2308752.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07612321525812149
dqn reward tensor(-186.8750, device='cuda:0') e 0.7475000000000278 loss_dqn tensor(2749157.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027238503098487854
dqn reward tensor(-401.2500, device='cuda:0') e 0.7470000000000279 loss_dqn tensor(2777970.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2764884829521179
dqn reward tensor(-107., device='cuda:0') e 0.7465000000000279 loss_dqn tensor(3377413.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18831855058670044
dqn reward tensor(-157., device='cuda:0') e 0.746000000000028 loss_dqn tensor(2940568.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12241677194833755
dqn reward tensor(-131.1875, device='cuda:0') e 0.745500000000028 loss_dqn tensor(3103184., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18007409572601318
dqn reward tensor(-210.6250, device='cuda:0') e 0.7450000000000281 loss_dqn tensor(1982461., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04624983295798302
dqn reward tensor(-328.0625, device='cuda:0') e 0.7445000000000281 loss_dqn tensor(1836534., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04845396429300308
dqn reward tensor(-300.5625, device='cuda:0') e 0.7440000000000282 loss_dqn tensor(2442469.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1287251114845276
dqn reward tensor(-288.3750, device='cuda:0') e 0.7435000000000282 loss_dqn tensor(2170384.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24532067775726318
dqn reward tensor(-426.1875, device='cuda:0') e 0.7430000000000283 loss_dqn tensor(1733720.1250, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12451628595590591
dqn reward tensor(-13.5000, device='cuda:0') e 0.7425000000000284 loss_dqn tensor(1881481.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044813305139541626
Evaluating...
Train: {'rocauc': 0.6839624701611188} -3.7349812984466553
=====Epoch 2=====
Training...
dqn reward tensor(-353.1250, device='cuda:0') e 0.7420000000000284 loss_dqn tensor(1305724.8750, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18172669410705566
dqn reward tensor(-262.5000, device='cuda:0') e 0.7415000000000285 loss_dqn tensor(2331221.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08565660566091537
dqn reward tensor(-182.3750, device='cuda:0') e 0.7410000000000285 loss_dqn tensor(2301777.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07616585493087769
dqn reward tensor(-225.6250, device='cuda:0') e 0.7405000000000286 loss_dqn tensor(2895294.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24847257137298584
dqn reward tensor(-327.9375, device='cuda:0') e 0.7400000000000286 loss_dqn tensor(2670087.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1953224539756775
dqn reward tensor(-102.5000, device='cuda:0') e 0.7395000000000287 loss_dqn tensor(5316641.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07169834524393082
dqn reward tensor(-270.5000, device='cuda:0') e 0.7390000000000287 loss_dqn tensor(3661856.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08078954368829727
dqn reward tensor(-258.8750, device='cuda:0') e 0.7385000000000288 loss_dqn tensor(3575718.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14072614908218384
dqn reward tensor(-136.3750, device='cuda:0') e 0.7380000000000289 loss_dqn tensor(4996452., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14212733507156372
dqn reward tensor(-295.6250, device='cuda:0') e 0.7375000000000289 loss_dqn tensor(5477566.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1355750411748886
dqn reward tensor(-201.6250, device='cuda:0') e 0.737000000000029 loss_dqn tensor(6036336., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25528430938720703
dqn reward tensor(-244.5000, device='cuda:0') e 0.736500000000029 loss_dqn tensor(3911053.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10590405762195587
dqn reward tensor(-373.1250, device='cuda:0') e 0.7360000000000291 loss_dqn tensor(4372011.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13811802864074707
dqn reward tensor(35.6250, device='cuda:0') e 0.7355000000000291 loss_dqn tensor(7903908.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08513534069061279
dqn reward tensor(-365.9375, device='cuda:0') e 0.7350000000000292 loss_dqn tensor(4222330.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19905322790145874
dqn reward tensor(-126.3750, device='cuda:0') e 0.7345000000000292 loss_dqn tensor(3604166.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28773605823516846
dqn reward tensor(-114.2500, device='cuda:0') e 0.7340000000000293 loss_dqn tensor(6144872.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23817971348762512
dqn reward tensor(-541.8125, device='cuda:0') e 0.7335000000000294 loss_dqn tensor(4224586., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2621690332889557
dqn reward tensor(-230., device='cuda:0') e 0.7330000000000294 loss_dqn tensor(5812771., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15635119378566742
dqn reward tensor(-233.2500, device='cuda:0') e 0.7325000000000295 loss_dqn tensor(6259290., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21178919076919556
dqn reward tensor(-167.6250, device='cuda:0') e 0.7320000000000295 loss_dqn tensor(7010729.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12199850380420685
dqn reward tensor(-404.2500, device='cuda:0') e 0.7315000000000296 loss_dqn tensor(5185039.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.201360285282135
dqn reward tensor(-421.1875, device='cuda:0') e 0.7310000000000296 loss_dqn tensor(6708793., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16022029519081116
dqn reward tensor(-307.1250, device='cuda:0') e 0.7305000000000297 loss_dqn tensor(6536580., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20870822668075562
dqn reward tensor(-319.2500, device='cuda:0') e 0.7300000000000297 loss_dqn tensor(6159004., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3022247850894928
dqn reward tensor(-164.5000, device='cuda:0') e 0.7295000000000298 loss_dqn tensor(8401272., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13988061249256134
dqn reward tensor(-116.3750, device='cuda:0') e 0.7290000000000298 loss_dqn tensor(10767386., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2353196144104004
dqn reward tensor(-244.6250, device='cuda:0') e 0.7285000000000299 loss_dqn tensor(11834507., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17972306907176971
dqn reward tensor(-136., device='cuda:0') e 0.72800000000003 loss_dqn tensor(10466700., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11124507337808609
dqn reward tensor(-208.8750, device='cuda:0') e 0.72750000000003 loss_dqn tensor(9528641., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03536585345864296
dqn reward tensor(-466.7500, device='cuda:0') e 0.7270000000000301 loss_dqn tensor(5247256.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08951318264007568
dqn reward tensor(-289.3750, device='cuda:0') e 0.7265000000000301 loss_dqn tensor(9285738., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1286352574825287
dqn reward tensor(-391.6250, device='cuda:0') e 0.7260000000000302 loss_dqn tensor(8899860., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17523646354675293
dqn reward tensor(-224.5000, device='cuda:0') e 0.7255000000000302 loss_dqn tensor(11499349., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07430568337440491
dqn reward tensor(50.5000, device='cuda:0') e 0.7250000000000303 loss_dqn tensor(11932598., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22312907874584198
dqn reward tensor(-418.5000, device='cuda:0') e 0.7245000000000303 loss_dqn tensor(5391845.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08703024685382843
dqn reward tensor(-284.1875, device='cuda:0') e 0.7240000000000304 loss_dqn tensor(6197200., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3000452220439911
dqn reward tensor(-193.1250, device='cuda:0') e 0.7235000000000305 loss_dqn tensor(6559011.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13333778083324432
dqn reward tensor(-328.8750, device='cuda:0') e 0.7230000000000305 loss_dqn tensor(5424842.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23126088082790375
dqn reward tensor(-95., device='cuda:0') e 0.7225000000000306 loss_dqn tensor(7741228., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03798060864210129
dqn reward tensor(-161., device='cuda:0') e 0.7220000000000306 loss_dqn tensor(6807648.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19988349080085754
dqn reward tensor(-94.6250, device='cuda:0') e 0.7215000000000307 loss_dqn tensor(8121120., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09013193845748901
dqn reward tensor(-318.8750, device='cuda:0') e 0.7210000000000307 loss_dqn tensor(7220253.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1379299759864807
dqn reward tensor(-220.3750, device='cuda:0') e 0.7205000000000308 loss_dqn tensor(6423788., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1346156746149063
dqn reward tensor(-181.5000, device='cuda:0') e 0.7200000000000308 loss_dqn tensor(7603723., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11286965757608414
dqn reward tensor(-122.6250, device='cuda:0') e 0.7195000000000309 loss_dqn tensor(7758938., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07661375403404236
dqn reward tensor(-298.3750, device='cuda:0') e 0.719000000000031 loss_dqn tensor(6447671.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21773551404476166
dqn reward tensor(-437.7500, device='cuda:0') e 0.718500000000031 loss_dqn tensor(4487946., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0334787517786026
dqn reward tensor(-251.5000, device='cuda:0') e 0.7180000000000311 loss_dqn tensor(5729524., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23651033639907837
dqn reward tensor(-264.9375, device='cuda:0') e 0.7175000000000311 loss_dqn tensor(5862591., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3232037425041199
dqn reward tensor(-98.6250, device='cuda:0') e 0.7170000000000312 loss_dqn tensor(4265885.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0973631739616394
dqn reward tensor(-321.8750, device='cuda:0') e 0.7165000000000312 loss_dqn tensor(4185791., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03922702372074127
dqn reward tensor(-245.7500, device='cuda:0') e 0.7160000000000313 loss_dqn tensor(3804255., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08470143377780914
dqn reward tensor(-24.1250, device='cuda:0') e 0.7155000000000313 loss_dqn tensor(4760686., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1964883804321289
dqn reward tensor(-387.2500, device='cuda:0') e 0.7150000000000314 loss_dqn tensor(4279384.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13085314631462097
dqn reward tensor(-386.4375, device='cuda:0') e 0.7145000000000314 loss_dqn tensor(4284964.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17746898531913757
dqn reward tensor(-218.2500, device='cuda:0') e 0.7140000000000315 loss_dqn tensor(4750955., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03580860421061516
dqn reward tensor(-491.2500, device='cuda:0') e 0.7135000000000316 loss_dqn tensor(2920632., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20588386058807373
dqn reward tensor(-368.6250, device='cuda:0') e 0.7130000000000316 loss_dqn tensor(3842512., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14419126510620117
dqn reward tensor(-407.3125, device='cuda:0') e 0.7125000000000317 loss_dqn tensor(4163697.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24716931581497192
dqn reward tensor(-231.5000, device='cuda:0') e 0.7120000000000317 loss_dqn tensor(5004364., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10756933689117432
dqn reward tensor(-212.3750, device='cuda:0') e 0.7115000000000318 loss_dqn tensor(3527818., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10819876939058304
dqn reward tensor(-256.6250, device='cuda:0') e 0.7110000000000318 loss_dqn tensor(5233644., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07581159472465515
dqn reward tensor(-230.3750, device='cuda:0') e 0.7105000000000319 loss_dqn tensor(4588095., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2613352835178375
dqn reward tensor(-290.3750, device='cuda:0') e 0.7100000000000319 loss_dqn tensor(3745190.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27925992012023926
dqn reward tensor(-369.3750, device='cuda:0') e 0.709500000000032 loss_dqn tensor(3422526.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13852913677692413
dqn reward tensor(-329.6250, device='cuda:0') e 0.709000000000032 loss_dqn tensor(2260957.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12265743315219879
dqn reward tensor(-358.3750, device='cuda:0') e 0.7085000000000321 loss_dqn tensor(3554615., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1475444734096527
dqn reward tensor(-188.7500, device='cuda:0') e 0.7080000000000322 loss_dqn tensor(3759841.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04330742359161377
dqn reward tensor(-220.1250, device='cuda:0') e 0.7075000000000322 loss_dqn tensor(5234754., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21891279518604279
dqn reward tensor(-260.7500, device='cuda:0') e 0.7070000000000323 loss_dqn tensor(4600188.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19090931117534637
dqn reward tensor(-409.1875, device='cuda:0') e 0.7065000000000323 loss_dqn tensor(5312103., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17022594809532166
dqn reward tensor(-367.1250, device='cuda:0') e 0.7060000000000324 loss_dqn tensor(4357268.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17632506787776947
dqn reward tensor(-284.2500, device='cuda:0') e 0.7055000000000324 loss_dqn tensor(6637363., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04922682046890259
dqn reward tensor(-314.5625, device='cuda:0') e 0.7050000000000325 loss_dqn tensor(7470153., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22881849110126495
dqn reward tensor(-293.7500, device='cuda:0') e 0.7045000000000325 loss_dqn tensor(5062438.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21073436737060547
dqn reward tensor(-404.4375, device='cuda:0') e 0.7040000000000326 loss_dqn tensor(5049587., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1377750188112259
dqn reward tensor(-463.1250, device='cuda:0') e 0.7035000000000327 loss_dqn tensor(4660114., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06887476146221161
dqn reward tensor(-267., device='cuda:0') e 0.7030000000000327 loss_dqn tensor(5785237., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21347978711128235
dqn reward tensor(-393.0625, device='cuda:0') e 0.7025000000000328 loss_dqn tensor(4295390., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18776875734329224
dqn reward tensor(-211.3750, device='cuda:0') e 0.7020000000000328 loss_dqn tensor(5842905., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09306682646274567
dqn reward tensor(11.3125, device='cuda:0') e 0.7015000000000329 loss_dqn tensor(8038648.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09045489877462387
dqn reward tensor(-235.8750, device='cuda:0') e 0.7010000000000329 loss_dqn tensor(5281619., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08495950698852539
dqn reward tensor(-206.5000, device='cuda:0') e 0.700500000000033 loss_dqn tensor(10207356., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06370635330677032
dqn reward tensor(-328., device='cuda:0') e 0.700000000000033 loss_dqn tensor(4660174.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023877538740634918
dqn reward tensor(-277.8750, device='cuda:0') e 0.6995000000000331 loss_dqn tensor(7822000., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08404478430747986
dqn reward tensor(-98.1250, device='cuda:0') e 0.6990000000000332 loss_dqn tensor(10012867., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.203987717628479
dqn reward tensor(-402.2500, device='cuda:0') e 0.6985000000000332 loss_dqn tensor(5472376., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20903317630290985
dqn reward tensor(-322.6875, device='cuda:0') e 0.6980000000000333 loss_dqn tensor(7634956., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07395334541797638
dqn reward tensor(-246.6250, device='cuda:0') e 0.6975000000000333 loss_dqn tensor(6914212.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08503622561693192
dqn reward tensor(-153.4375, device='cuda:0') e 0.6970000000000334 loss_dqn tensor(9181640., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12726379930973053
dqn reward tensor(-355.1250, device='cuda:0') e 0.6965000000000334 loss_dqn tensor(8253698., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4158426821231842
dqn reward tensor(-213.9375, device='cuda:0') e 0.6960000000000335 loss_dqn tensor(7685447., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18290670216083527
dqn reward tensor(-238.8750, device='cuda:0') e 0.6955000000000335 loss_dqn tensor(6416459., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11001213639974594
dqn reward tensor(-367.1250, device='cuda:0') e 0.6950000000000336 loss_dqn tensor(7038807.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1095488965511322
dqn reward tensor(-264.8750, device='cuda:0') e 0.6945000000000336 loss_dqn tensor(9332490., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11454205960035324
dqn reward tensor(-165., device='cuda:0') e 0.6940000000000337 loss_dqn tensor(11529124., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09210029244422913
dqn reward tensor(-263.9375, device='cuda:0') e 0.6935000000000338 loss_dqn tensor(8753656., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1277010589838028
dqn reward tensor(-336., device='cuda:0') e 0.6930000000000338 loss_dqn tensor(7115053., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12275096029043198
dqn reward tensor(-267.3750, device='cuda:0') e 0.6925000000000339 loss_dqn tensor(8334785.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1149153858423233
dqn reward tensor(-227., device='cuda:0') e 0.6920000000000339 loss_dqn tensor(7814881., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12852855026721954
dqn reward tensor(-292.0625, device='cuda:0') e 0.691500000000034 loss_dqn tensor(7685850., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07371613383293152
dqn reward tensor(-351., device='cuda:0') e 0.691000000000034 loss_dqn tensor(5477843.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0961662009358406
dqn reward tensor(-321.8125, device='cuda:0') e 0.6905000000000341 loss_dqn tensor(7830115.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16757643222808838
dqn reward tensor(-269.6250, device='cuda:0') e 0.6900000000000341 loss_dqn tensor(8560856., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12815868854522705
dqn reward tensor(-395.2500, device='cuda:0') e 0.6895000000000342 loss_dqn tensor(4618568., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1559659093618393
dqn reward tensor(-374.2500, device='cuda:0') e 0.6890000000000343 loss_dqn tensor(6987837.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01468606386333704
dqn reward tensor(-179.6250, device='cuda:0') e 0.6885000000000343 loss_dqn tensor(12249179., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12532563507556915
dqn reward tensor(-150.1250, device='cuda:0') e 0.6880000000000344 loss_dqn tensor(10941997., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18363036215305328
dqn reward tensor(-157.3750, device='cuda:0') e 0.6875000000000344 loss_dqn tensor(12221452., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15085649490356445
dqn reward tensor(3.5000, device='cuda:0') e 0.6870000000000345 loss_dqn tensor(10763332., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2877171039581299
dqn reward tensor(-155.5000, device='cuda:0') e 0.6865000000000345 loss_dqn tensor(8085483.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07262444496154785
dqn reward tensor(-335.3750, device='cuda:0') e 0.6860000000000346 loss_dqn tensor(7838996.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1046045646071434
dqn reward tensor(-295., device='cuda:0') e 0.6855000000000346 loss_dqn tensor(8494762., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17802917957305908
dqn reward tensor(-260., device='cuda:0') e 0.6850000000000347 loss_dqn tensor(12621468., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17584259808063507
dqn reward tensor(-56.2500, device='cuda:0') e 0.6845000000000347 loss_dqn tensor(13225104., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10654126852750778
dqn reward tensor(-200.1250, device='cuda:0') e 0.6840000000000348 loss_dqn tensor(10472777., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15102067589759827
dqn reward tensor(-64., device='cuda:0') e 0.6835000000000349 loss_dqn tensor(12781088., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2319575697183609
dqn reward tensor(-191.3750, device='cuda:0') e 0.6830000000000349 loss_dqn tensor(9795850., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19356359541416168
dqn reward tensor(-308.6250, device='cuda:0') e 0.682500000000035 loss_dqn tensor(8668836., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12603944540023804
dqn reward tensor(-123.1250, device='cuda:0') e 0.682000000000035 loss_dqn tensor(10125870., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07476550340652466
dqn reward tensor(-166.2500, device='cuda:0') e 0.6815000000000351 loss_dqn tensor(10089248., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07936157286167145
dqn reward tensor(-72.1250, device='cuda:0') e 0.6810000000000351 loss_dqn tensor(13412215., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.185364231467247
dqn reward tensor(-155.5625, device='cuda:0') e 0.6805000000000352 loss_dqn tensor(16828648., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4433201551437378
dqn reward tensor(-408.7500, device='cuda:0') e 0.6800000000000352 loss_dqn tensor(10476780., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025921905413269997
dqn reward tensor(-454.8750, device='cuda:0') e 0.6795000000000353 loss_dqn tensor(7496819., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1798742115497589
dqn reward tensor(-89.8125, device='cuda:0') e 0.6790000000000354 loss_dqn tensor(14992670., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3001042604446411
dqn reward tensor(-212.3750, device='cuda:0') e 0.6785000000000354 loss_dqn tensor(18612346., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1874363124370575
dqn reward tensor(-247., device='cuda:0') e 0.6780000000000355 loss_dqn tensor(10209181., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11444105207920074
dqn reward tensor(-336.3750, device='cuda:0') e 0.6775000000000355 loss_dqn tensor(11223910., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18190479278564453
dqn reward tensor(-108.7500, device='cuda:0') e 0.6770000000000356 loss_dqn tensor(18694730., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.100361168384552
dqn reward tensor(-292.6250, device='cuda:0') e 0.6765000000000356 loss_dqn tensor(18462342., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12629690766334534
dqn reward tensor(-87.1250, device='cuda:0') e 0.6760000000000357 loss_dqn tensor(20002838., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19447092711925507
dqn reward tensor(-236.6250, device='cuda:0') e 0.6755000000000357 loss_dqn tensor(17568806., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08599309623241425
dqn reward tensor(-329.4375, device='cuda:0') e 0.6750000000000358 loss_dqn tensor(15235024., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14234238862991333
dqn reward tensor(-157.8750, device='cuda:0') e 0.6745000000000358 loss_dqn tensor(16735880., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15531867742538452
dqn reward tensor(-398., device='cuda:0') e 0.6740000000000359 loss_dqn tensor(10116729., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03422674909234047
dqn reward tensor(-287.6250, device='cuda:0') e 0.673500000000036 loss_dqn tensor(18064092., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1317957639694214
dqn reward tensor(-83.1250, device='cuda:0') e 0.673000000000036 loss_dqn tensor(23035890., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2828332781791687
dqn reward tensor(-255.6250, device='cuda:0') e 0.6725000000000361 loss_dqn tensor(14726172., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3046537935733795
dqn reward tensor(-65.5000, device='cuda:0') e 0.6720000000000361 loss_dqn tensor(19547810., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16196486353874207
dqn reward tensor(-252., device='cuda:0') e 0.6715000000000362 loss_dqn tensor(17772434., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09079943597316742
dqn reward tensor(-91.5000, device='cuda:0') e 0.6710000000000362 loss_dqn tensor(19739262., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13937614858150482
dqn reward tensor(-50.7500, device='cuda:0') e 0.6705000000000363 loss_dqn tensor(20489836., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33017656207084656
dqn reward tensor(-287.1875, device='cuda:0') e 0.6700000000000363 loss_dqn tensor(13912734., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08461683988571167
dqn reward tensor(-264., device='cuda:0') e 0.6695000000000364 loss_dqn tensor(16083286., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2525494396686554
dqn reward tensor(-284.7500, device='cuda:0') e 0.6690000000000365 loss_dqn tensor(10913308., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09072767943143845
dqn reward tensor(-173.3750, device='cuda:0') e 0.6685000000000365 loss_dqn tensor(15807994., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10340073704719543
dqn reward tensor(-370.5000, device='cuda:0') e 0.6680000000000366 loss_dqn tensor(13562703., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10653112828731537
dqn reward tensor(-246.8750, device='cuda:0') e 0.6675000000000366 loss_dqn tensor(13397134., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09442918002605438
dqn reward tensor(-34.1250, device='cuda:0') e 0.6670000000000367 loss_dqn tensor(15722081., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08376429229974747
dqn reward tensor(-197.3750, device='cuda:0') e 0.6665000000000367 loss_dqn tensor(10049440., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08220227062702179
dqn reward tensor(-102.3750, device='cuda:0') e 0.6660000000000368 loss_dqn tensor(11677950., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21810472011566162
dqn reward tensor(-233.7500, device='cuda:0') e 0.6655000000000368 loss_dqn tensor(11045513., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1594441831111908
dqn reward tensor(-344.5000, device='cuda:0') e 0.6650000000000369 loss_dqn tensor(6943885., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08775482326745987
dqn reward tensor(-247.7500, device='cuda:0') e 0.664500000000037 loss_dqn tensor(6323415., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35050562024116516
dqn reward tensor(-116.0625, device='cuda:0') e 0.664000000000037 loss_dqn tensor(11420204., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.312702476978302
dqn reward tensor(-208.3750, device='cuda:0') e 0.6635000000000371 loss_dqn tensor(6440036., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14114724099636078
dqn reward tensor(-266.1250, device='cuda:0') e 0.6630000000000371 loss_dqn tensor(7669273., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12797464430332184
dqn reward tensor(-131.1250, device='cuda:0') e 0.6625000000000372 loss_dqn tensor(8638137., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06857936084270477
dqn reward tensor(-220.6875, device='cuda:0') e 0.6620000000000372 loss_dqn tensor(7538484., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13994067907333374
dqn reward tensor(-242.1250, device='cuda:0') e 0.6615000000000373 loss_dqn tensor(9917917., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09862085431814194
dqn reward tensor(-310.1250, device='cuda:0') e 0.6610000000000373 loss_dqn tensor(7363921.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1350049078464508
dqn reward tensor(-192.0625, device='cuda:0') e 0.6605000000000374 loss_dqn tensor(10181891., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14348334074020386
dqn reward tensor(-214.8750, device='cuda:0') e 0.6600000000000374 loss_dqn tensor(5936791.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13814732432365417
dqn reward tensor(-200.1875, device='cuda:0') e 0.6595000000000375 loss_dqn tensor(9027633., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2132747769355774
dqn reward tensor(-341.1250, device='cuda:0') e 0.6590000000000376 loss_dqn tensor(8874521., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29099658131599426
dqn reward tensor(-367.8125, device='cuda:0') e 0.6585000000000376 loss_dqn tensor(4610211., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03449632227420807
dqn reward tensor(-360.2500, device='cuda:0') e 0.6580000000000377 loss_dqn tensor(6737195., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08299696445465088
dqn reward tensor(-253.2500, device='cuda:0') e 0.6575000000000377 loss_dqn tensor(7885371., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.094134122133255
dqn reward tensor(-262.1250, device='cuda:0') e 0.6570000000000378 loss_dqn tensor(5571114.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18117991089820862
dqn reward tensor(-340.0625, device='cuda:0') e 0.6565000000000378 loss_dqn tensor(5448510.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11248938739299774
dqn reward tensor(-342.7500, device='cuda:0') e 0.6560000000000379 loss_dqn tensor(4372435., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18091072142124176
dqn reward tensor(-430.7500, device='cuda:0') e 0.6555000000000379 loss_dqn tensor(4406108., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1279391348361969
dqn reward tensor(-255.4375, device='cuda:0') e 0.655000000000038 loss_dqn tensor(4589503.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17017289996147156
dqn reward tensor(-265.9375, device='cuda:0') e 0.654500000000038 loss_dqn tensor(4262200., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2208280861377716
dqn reward tensor(-204.1875, device='cuda:0') e 0.6540000000000381 loss_dqn tensor(6993091., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22169487178325653
dqn reward tensor(-223.6250, device='cuda:0') e 0.6535000000000382 loss_dqn tensor(5807010.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09061464667320251
dqn reward tensor(-231.7500, device='cuda:0') e 0.6530000000000382 loss_dqn tensor(5291586., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0822070837020874
dqn reward tensor(-262.2500, device='cuda:0') e 0.6525000000000383 loss_dqn tensor(3579225.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0845373272895813
dqn reward tensor(-301.6875, device='cuda:0') e 0.6520000000000383 loss_dqn tensor(3190452.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15370696783065796
dqn reward tensor(-314.8750, device='cuda:0') e 0.6515000000000384 loss_dqn tensor(4122662.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14510659873485565
dqn reward tensor(-406.2500, device='cuda:0') e 0.6510000000000384 loss_dqn tensor(3816938.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0900864526629448
dqn reward tensor(-342.6250, device='cuda:0') e 0.6505000000000385 loss_dqn tensor(3607250., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028677888214588165
dqn reward tensor(-360.4375, device='cuda:0') e 0.6500000000000385 loss_dqn tensor(7049457., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16750377416610718
dqn reward tensor(-174., device='cuda:0') e 0.6495000000000386 loss_dqn tensor(4062009.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12998197972774506
dqn reward tensor(-507.8125, device='cuda:0') e 0.6490000000000387 loss_dqn tensor(5970292., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23806601762771606
dqn reward tensor(-372.8750, device='cuda:0') e 0.6485000000000387 loss_dqn tensor(7338665.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08909788727760315
dqn reward tensor(-416.6250, device='cuda:0') e 0.6480000000000388 loss_dqn tensor(3060428.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07642704993486404
dqn reward tensor(-379.6250, device='cuda:0') e 0.6475000000000388 loss_dqn tensor(9846520., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08081856369972229
dqn reward tensor(-228.1250, device='cuda:0') e 0.6470000000000389 loss_dqn tensor(5276257., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07753041386604309
dqn reward tensor(-396.2500, device='cuda:0') e 0.6465000000000389 loss_dqn tensor(6757964., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1122620478272438
dqn reward tensor(-446.9375, device='cuda:0') e 0.646000000000039 loss_dqn tensor(4383602.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19942699372768402
dqn reward tensor(-337.9375, device='cuda:0') e 0.645500000000039 loss_dqn tensor(4489424.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09127429127693176
dqn reward tensor(-332.5000, device='cuda:0') e 0.6450000000000391 loss_dqn tensor(8408762., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14317168295383453
dqn reward tensor(-368., device='cuda:0') e 0.6445000000000392 loss_dqn tensor(9340227., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0805940106511116
dqn reward tensor(-333.6250, device='cuda:0') e 0.6440000000000392 loss_dqn tensor(6798138., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17894795536994934
dqn reward tensor(-465.6250, device='cuda:0') e 0.6435000000000393 loss_dqn tensor(10462391., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13755333423614502
dqn reward tensor(-420.1250, device='cuda:0') e 0.6430000000000393 loss_dqn tensor(10137770., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1852121651172638
dqn reward tensor(-364.2500, device='cuda:0') e 0.6425000000000394 loss_dqn tensor(17109800., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12614265084266663
dqn reward tensor(-398.6250, device='cuda:0') e 0.6420000000000394 loss_dqn tensor(13796687., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09306132793426514
dqn reward tensor(-387.7500, device='cuda:0') e 0.6415000000000395 loss_dqn tensor(6188631.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13280241191387177
dqn reward tensor(-339.7500, device='cuda:0') e 0.6410000000000395 loss_dqn tensor(6253140.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07927082479000092
dqn reward tensor(-487.0625, device='cuda:0') e 0.6405000000000396 loss_dqn tensor(5075212.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09874419867992401
dqn reward tensor(-501.5000, device='cuda:0') e 0.6400000000000396 loss_dqn tensor(2620550.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07419092208147049
dqn reward tensor(-520.6250, device='cuda:0') e 0.6395000000000397 loss_dqn tensor(2232538.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18092584609985352
dqn reward tensor(-329.2500, device='cuda:0') e 0.6390000000000398 loss_dqn tensor(5267599., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19247761368751526
dqn reward tensor(-405.8750, device='cuda:0') e 0.6385000000000398 loss_dqn tensor(2432431., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206122875213623
dqn reward tensor(-576., device='cuda:0') e 0.6380000000000399 loss_dqn tensor(6284926.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22065450251102448
dqn reward tensor(-268.5625, device='cuda:0') e 0.6375000000000399 loss_dqn tensor(3182149., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11424113810062408
dqn reward tensor(-543.8750, device='cuda:0') e 0.63700000000004 loss_dqn tensor(5999360., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13708938658237457
dqn reward tensor(-637.3125, device='cuda:0') e 0.63650000000004 loss_dqn tensor(2499439.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4361773729324341
dqn reward tensor(-575.3750, device='cuda:0') e 0.6360000000000401 loss_dqn tensor(5366807., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17186716198921204
dqn reward tensor(-427.3750, device='cuda:0') e 0.6355000000000401 loss_dqn tensor(3940587., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2284543812274933
dqn reward tensor(-565.2500, device='cuda:0') e 0.6350000000000402 loss_dqn tensor(3075084., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11278289556503296
dqn reward tensor(-355.6250, device='cuda:0') e 0.6345000000000403 loss_dqn tensor(3913986.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08872983604669571
dqn reward tensor(-404.4375, device='cuda:0') e 0.6340000000000403 loss_dqn tensor(3250648., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15176965296268463
dqn reward tensor(-398.7500, device='cuda:0') e 0.6335000000000404 loss_dqn tensor(3251899.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13722120225429535
dqn reward tensor(-434.6875, device='cuda:0') e 0.6330000000000404 loss_dqn tensor(2496231., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1289595067501068
dqn reward tensor(-389.2500, device='cuda:0') e 0.6325000000000405 loss_dqn tensor(3078339.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17725062370300293
dqn reward tensor(-421.8750, device='cuda:0') e 0.6320000000000405 loss_dqn tensor(4756802., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11433497071266174
dqn reward tensor(-498.2500, device='cuda:0') e 0.6315000000000406 loss_dqn tensor(2174320.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19203652441501617
dqn reward tensor(-457.8750, device='cuda:0') e 0.6310000000000406 loss_dqn tensor(2390699.2500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18851333856582642
dqn reward tensor(-607.2500, device='cuda:0') e 0.6305000000000407 loss_dqn tensor(3417670.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14289963245391846
dqn reward tensor(-542.8750, device='cuda:0') e 0.6300000000000407 loss_dqn tensor(2781669., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2107725441455841
dqn reward tensor(-454.1875, device='cuda:0') e 0.6295000000000408 loss_dqn tensor(4991535.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11996778845787048
dqn reward tensor(-376.2500, device='cuda:0') e 0.6290000000000409 loss_dqn tensor(3608346.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38180607557296753
dqn reward tensor(-428.5625, device='cuda:0') e 0.6285000000000409 loss_dqn tensor(4408787., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15490666031837463
dqn reward tensor(-389.6250, device='cuda:0') e 0.628000000000041 loss_dqn tensor(3738606.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0902656689286232
dqn reward tensor(-254.8750, device='cuda:0') e 0.627500000000041 loss_dqn tensor(2700390.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1772540658712387
dqn reward tensor(-476.6875, device='cuda:0') e 0.6270000000000411 loss_dqn tensor(3947964., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1859007328748703
dqn reward tensor(-281.5000, device='cuda:0') e 0.6265000000000411 loss_dqn tensor(2932961.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06991519778966904
dqn reward tensor(-493., device='cuda:0') e 0.6260000000000412 loss_dqn tensor(4942446., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16100913286209106
dqn reward tensor(-398.8125, device='cuda:0') e 0.6255000000000412 loss_dqn tensor(6593947.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18573778867721558
dqn reward tensor(-458.6250, device='cuda:0') e 0.6250000000000413 loss_dqn tensor(5072642.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23310668766498566
dqn reward tensor(-548.7500, device='cuda:0') e 0.6245000000000414 loss_dqn tensor(5514253.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32783031463623047
dqn reward tensor(-327.1250, device='cuda:0') e 0.6240000000000414 loss_dqn tensor(3734401.7500, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1891908049583435
dqn reward tensor(-432.5000, device='cuda:0') e 0.6235000000000415 loss_dqn tensor(4393104., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12196632474660873
dqn reward tensor(-384.6250, device='cuda:0') e 0.6230000000000415 loss_dqn tensor(5440022.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0887065976858139
dqn reward tensor(-582.5000, device='cuda:0') e 0.6225000000000416 loss_dqn tensor(8805944., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20259349048137665
dqn reward tensor(-439.6250, device='cuda:0') e 0.6220000000000416 loss_dqn tensor(5142746.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09605353325605392
dqn reward tensor(-301.1250, device='cuda:0') e 0.6215000000000417 loss_dqn tensor(11359669., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10433810949325562
dqn reward tensor(-385.2500, device='cuda:0') e 0.6210000000000417 loss_dqn tensor(6508408., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1711653769016266
dqn reward tensor(-416.3750, device='cuda:0') e 0.6205000000000418 loss_dqn tensor(6258793., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07817491143941879
dqn reward tensor(-384.2500, device='cuda:0') e 0.6200000000000419 loss_dqn tensor(14514149., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06027671694755554
dqn reward tensor(-521., device='cuda:0') e 0.6195000000000419 loss_dqn tensor(14134844., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08648969233036041
dqn reward tensor(-441.1250, device='cuda:0') e 0.619000000000042 loss_dqn tensor(14519243., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1858631670475006
dqn reward tensor(-504.8750, device='cuda:0') e 0.618500000000042 loss_dqn tensor(5844373.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18499013781547546
dqn reward tensor(-432.3750, device='cuda:0') e 0.6180000000000421 loss_dqn tensor(9065944., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15311995148658752
dqn reward tensor(-317.5000, device='cuda:0') e 0.6175000000000421 loss_dqn tensor(7334951.5000, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23401124775409698
dqn reward tensor(-301.1250, device='cuda:0') e 0.6170000000000422 loss_dqn tensor(25010300., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2629188001155853
dqn reward tensor(-449.1250, device='cuda:0') e 0.6165000000000422 loss_dqn tensor(17990636., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17205595970153809
dqn reward tensor(-456.5000, device='cuda:0') e 0.6160000000000423 loss_dqn tensor(44275748., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22300708293914795
dqn reward tensor(-448.3750, device='cuda:0') e 0.6155000000000423 loss_dqn tensor(22429782., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1487133651971817
dqn reward tensor(-392., device='cuda:0') e 0.6150000000000424 loss_dqn tensor(25046412., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14841511845588684
dqn reward tensor(-398.1250, device='cuda:0') e 0.6145000000000425 loss_dqn tensor(27508448., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09840308874845505
dqn reward tensor(-423.8750, device='cuda:0') e 0.6140000000000425 loss_dqn tensor(29933924., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12494413554668427
dqn reward tensor(-441.8750, device='cuda:0') e 0.6135000000000426 loss_dqn tensor(37584140., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23320330679416656
dqn reward tensor(-331.5625, device='cuda:0') e 0.6130000000000426 loss_dqn tensor(39887208., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22629642486572266
dqn reward tensor(-369.2500, device='cuda:0') e 0.6125000000000427 loss_dqn tensor(42034368., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2377486228942871
dqn reward tensor(-360.4375, device='cuda:0') e 0.6120000000000427 loss_dqn tensor(77149568., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22696015238761902
dqn reward tensor(-360.7500, device='cuda:0') e 0.6115000000000428 loss_dqn tensor(35227120., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09136034548282623
dqn reward tensor(-454.2500, device='cuda:0') e 0.6110000000000428 loss_dqn tensor(24224846., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11693688482046127
dqn reward tensor(-562.9375, device='cuda:0') e 0.6105000000000429 loss_dqn tensor(26150064., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08576862514019012
dqn reward tensor(-455.3750, device='cuda:0') e 0.610000000000043 loss_dqn tensor(26949746., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15163184702396393
dqn reward tensor(-510.2500, device='cuda:0') e 0.609500000000043 loss_dqn tensor(43442728., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1565510630607605
dqn reward tensor(-271.7500, device='cuda:0') e 0.6090000000000431 loss_dqn tensor(86201880., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29658618569374084
dqn reward tensor(-472.8750, device='cuda:0') e 0.6085000000000431 loss_dqn tensor(37868368., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1985577642917633
dqn reward tensor(-481.9375, device='cuda:0') e 0.6080000000000432 loss_dqn tensor(67746384., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1258435994386673
dqn reward tensor(-258.5625, device='cuda:0') e 0.6075000000000432 loss_dqn tensor(60199096., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10586181282997131
dqn reward tensor(-526., device='cuda:0') e 0.6070000000000433 loss_dqn tensor(52998136., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04306095093488693
dqn reward tensor(-392.6250, device='cuda:0') e 0.6065000000000433 loss_dqn tensor(73647104., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24967169761657715
dqn reward tensor(-319.0625, device='cuda:0') e 0.6060000000000434 loss_dqn tensor(50720004., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1477888822555542
dqn reward tensor(-383., device='cuda:0') e 0.6055000000000434 loss_dqn tensor(93471160., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19752269983291626
dqn reward tensor(-324.8750, device='cuda:0') e 0.6050000000000435 loss_dqn tensor(62970416., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38196301460266113
dqn reward tensor(-415.7500, device='cuda:0') e 0.6045000000000436 loss_dqn tensor(88838752., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07297375053167343
dqn reward tensor(-532.3750, device='cuda:0') e 0.6040000000000436 loss_dqn tensor(58407616., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15471944212913513
dqn reward tensor(-402.6875, device='cuda:0') e 0.6035000000000437 loss_dqn tensor(65815444., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07575267553329468
dqn reward tensor(-201.5000, device='cuda:0') e 0.6030000000000437 loss_dqn tensor(62766236., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3120321035385132
dqn reward tensor(-339.1875, device='cuda:0') e 0.6025000000000438 loss_dqn tensor(71475584., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1184883713722229
dqn reward tensor(-548., device='cuda:0') e 0.6020000000000438 loss_dqn tensor(46827168., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14147090911865234
dqn reward tensor(-425.2500, device='cuda:0') e 0.6015000000000439 loss_dqn tensor(66749548., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11342529952526093
dqn reward tensor(-288., device='cuda:0') e 0.6010000000000439 loss_dqn tensor(1.0752e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33873599767684937
dqn reward tensor(-429.6250, device='cuda:0') e 0.600500000000044 loss_dqn tensor(77452616., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02343713864684105
dqn reward tensor(-425.8750, device='cuda:0') e 0.600000000000044 loss_dqn tensor(65682660., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07385192066431046
dqn reward tensor(-585.3750, device='cuda:0') e 0.5995000000000441 loss_dqn tensor(65164000., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17462784051895142
dqn reward tensor(-504.3750, device='cuda:0') e 0.5990000000000442 loss_dqn tensor(79722952., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2989436984062195
dqn reward tensor(-379.8750, device='cuda:0') e 0.5985000000000442 loss_dqn tensor(73107344., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24694335460662842
dqn reward tensor(-361.8750, device='cuda:0') e 0.5980000000000443 loss_dqn tensor(83385280., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11138048768043518
dqn reward tensor(-379., device='cuda:0') e 0.5975000000000443 loss_dqn tensor(87177976., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2422940880060196
dqn reward tensor(-289.3750, device='cuda:0') e 0.5970000000000444 loss_dqn tensor(1.0170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24588137865066528
dqn reward tensor(-439.2500, device='cuda:0') e 0.5965000000000444 loss_dqn tensor(93334248., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2678385078907013
dqn reward tensor(-561.6875, device='cuda:0') e 0.5960000000000445 loss_dqn tensor(83507408., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08032051473855972
dqn reward tensor(-526.5000, device='cuda:0') e 0.5955000000000445 loss_dqn tensor(56965808., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14666125178337097
dqn reward tensor(-323.6250, device='cuda:0') e 0.5950000000000446 loss_dqn tensor(60178256., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13732515275478363
dqn reward tensor(-438., device='cuda:0') e 0.5945000000000447 loss_dqn tensor(56025672., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09792773425579071
dqn reward tensor(-355.5000, device='cuda:0') e 0.5940000000000447 loss_dqn tensor(58979344., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24016547203063965
dqn reward tensor(-483.6250, device='cuda:0') e 0.5935000000000448 loss_dqn tensor(90948432., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20770156383514404
dqn reward tensor(-405.5000, device='cuda:0') e 0.5930000000000448 loss_dqn tensor(60048204., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11512935906648636
dqn reward tensor(-499.7500, device='cuda:0') e 0.5925000000000449 loss_dqn tensor(55054088., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22896911203861237
dqn reward tensor(-541.6250, device='cuda:0') e 0.5920000000000449 loss_dqn tensor(83064256., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06119338795542717
dqn reward tensor(-465., device='cuda:0') e 0.591500000000045 loss_dqn tensor(1.2340e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17716442048549652
dqn reward tensor(-367.1250, device='cuda:0') e 0.591000000000045 loss_dqn tensor(1.0255e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02332475408911705
dqn reward tensor(-476., device='cuda:0') e 0.5905000000000451 loss_dqn tensor(76627760., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3189759850502014
dqn reward tensor(-470.4375, device='cuda:0') e 0.5900000000000452 loss_dqn tensor(1.0592e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13393212854862213
dqn reward tensor(-454.1875, device='cuda:0') e 0.5895000000000452 loss_dqn tensor(70026784., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044786237180233
dqn reward tensor(-519.8750, device='cuda:0') e 0.5890000000000453 loss_dqn tensor(73249040., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31462982296943665
dqn reward tensor(-498.6250, device='cuda:0') e 0.5885000000000453 loss_dqn tensor(79633808., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08291834592819214
dqn reward tensor(-610., device='cuda:0') e 0.5880000000000454 loss_dqn tensor(97912080., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13780856132507324
dqn reward tensor(-425.8750, device='cuda:0') e 0.5875000000000454 loss_dqn tensor(1.0970e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0971963107585907
dqn reward tensor(-616.5000, device='cuda:0') e 0.5870000000000455 loss_dqn tensor(53445556., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1914466917514801
dqn reward tensor(-465.6250, device='cuda:0') e 0.5865000000000455 loss_dqn tensor(53673640., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07637487351894379
dqn reward tensor(-560.6250, device='cuda:0') e 0.5860000000000456 loss_dqn tensor(94529568., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15622055530548096
dqn reward tensor(-347.3125, device='cuda:0') e 0.5855000000000457 loss_dqn tensor(1.0281e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07949459552764893
dqn reward tensor(-505.1250, device='cuda:0') e 0.5850000000000457 loss_dqn tensor(1.1354e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051026541739702225
dqn reward tensor(-486.8125, device='cuda:0') e 0.5845000000000458 loss_dqn tensor(71396080., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1440422534942627
dqn reward tensor(-431.2500, device='cuda:0') e 0.5840000000000458 loss_dqn tensor(1.0977e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025415755808353424
dqn reward tensor(-412.2500, device='cuda:0') e 0.5835000000000459 loss_dqn tensor(1.2175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16212637722492218
dqn reward tensor(-501., device='cuda:0') e 0.5830000000000459 loss_dqn tensor(1.2602e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20833849906921387
dqn reward tensor(-530.8750, device='cuda:0') e 0.582500000000046 loss_dqn tensor(58021272., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06899508088827133
dqn reward tensor(-409.6250, device='cuda:0') e 0.582000000000046 loss_dqn tensor(1.3404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1854604184627533
dqn reward tensor(-434.5000, device='cuda:0') e 0.5815000000000461 loss_dqn tensor(1.1064e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20822420716285706
dqn reward tensor(-311.3750, device='cuda:0') e 0.5810000000000461 loss_dqn tensor(71550856., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1048104465007782
dqn reward tensor(-381.7500, device='cuda:0') e 0.5805000000000462 loss_dqn tensor(1.4832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18304532766342163
dqn reward tensor(-543.3750, device='cuda:0') e 0.5800000000000463 loss_dqn tensor(78015792., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06196499988436699
dqn reward tensor(-532.8750, device='cuda:0') e 0.5795000000000463 loss_dqn tensor(1.7901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13321524858474731
dqn reward tensor(-320.5000, device='cuda:0') e 0.5790000000000464 loss_dqn tensor(1.2276e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2972879409790039
dqn reward tensor(-343.7500, device='cuda:0') e 0.5785000000000464 loss_dqn tensor(1.2744e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09206992387771606
dqn reward tensor(-467.3750, device='cuda:0') e 0.5780000000000465 loss_dqn tensor(1.4604e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08089695870876312
dqn reward tensor(-466.8750, device='cuda:0') e 0.5775000000000465 loss_dqn tensor(1.3348e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18364159762859344
dqn reward tensor(-291.8750, device='cuda:0') e 0.5770000000000466 loss_dqn tensor(1.4895e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14243821799755096
dqn reward tensor(-266.5000, device='cuda:0') e 0.5765000000000466 loss_dqn tensor(2.4419e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24261406064033508
dqn reward tensor(-425.6250, device='cuda:0') e 0.5760000000000467 loss_dqn tensor(1.6123e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052909959107637405
dqn reward tensor(-432.3750, device='cuda:0') e 0.5755000000000468 loss_dqn tensor(1.5124e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07727757096290588
dqn reward tensor(-410.6250, device='cuda:0') e 0.5750000000000468 loss_dqn tensor(1.3196e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10734459012746811
dqn reward tensor(-386.6250, device='cuda:0') e 0.5745000000000469 loss_dqn tensor(1.3158e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13375017046928406
dqn reward tensor(-401.7500, device='cuda:0') e 0.5740000000000469 loss_dqn tensor(2.4103e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07236091792583466
dqn reward tensor(-333.7500, device='cuda:0') e 0.573500000000047 loss_dqn tensor(1.6311e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13927266001701355
dqn reward tensor(-363.8750, device='cuda:0') e 0.573000000000047 loss_dqn tensor(2.0816e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07244546711444855
dqn reward tensor(-298.8750, device='cuda:0') e 0.5725000000000471 loss_dqn tensor(2.3536e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09234213829040527
dqn reward tensor(-322.2500, device='cuda:0') e 0.5720000000000471 loss_dqn tensor(1.6143e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17073595523834229
dqn reward tensor(-195.5000, device='cuda:0') e 0.5715000000000472 loss_dqn tensor(2.7163e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.5141825079917908
dqn reward tensor(-383.5000, device='cuda:0') e 0.5710000000000472 loss_dqn tensor(2.2341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15915431082248688
dqn reward tensor(-242.2500, device='cuda:0') e 0.5705000000000473 loss_dqn tensor(1.4156e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18125036358833313
dqn reward tensor(-211.7500, device='cuda:0') e 0.5700000000000474 loss_dqn tensor(2.7897e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07558223605155945
dqn reward tensor(-184.6250, device='cuda:0') e 0.5695000000000474 loss_dqn tensor(2.5466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14669941365718842
dqn reward tensor(-469.8750, device='cuda:0') e 0.5690000000000475 loss_dqn tensor(1.3319e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09837408363819122
dqn reward tensor(-335.3125, device='cuda:0') e 0.5685000000000475 loss_dqn tensor(1.7735e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20568060874938965
dqn reward tensor(-358., device='cuda:0') e 0.5680000000000476 loss_dqn tensor(2.4336e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14306609332561493
dqn reward tensor(-226.5000, device='cuda:0') e 0.5675000000000476 loss_dqn tensor(2.0262e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1517445594072342
dqn reward tensor(-333.3750, device='cuda:0') e 0.5670000000000477 loss_dqn tensor(2.5206e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13487306237220764
dqn reward tensor(-443.5000, device='cuda:0') e 0.5665000000000477 loss_dqn tensor(2.1595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3316061496734619
dqn reward tensor(-354.3750, device='cuda:0') e 0.5660000000000478 loss_dqn tensor(2.2245e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15046167373657227
dqn reward tensor(-312.8125, device='cuda:0') e 0.5655000000000479 loss_dqn tensor(2.8729e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2517246603965759
dqn reward tensor(-278.5000, device='cuda:0') e 0.5650000000000479 loss_dqn tensor(2.2340e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22092831134796143
dqn reward tensor(-167.7500, device='cuda:0') e 0.564500000000048 loss_dqn tensor(4.3409e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09535670280456543
dqn reward tensor(-233.3750, device='cuda:0') e 0.564000000000048 loss_dqn tensor(2.4303e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12191715836524963
dqn reward tensor(-442.2500, device='cuda:0') e 0.5635000000000481 loss_dqn tensor(1.9798e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.226906880736351
dqn reward tensor(-241.5000, device='cuda:0') e 0.5630000000000481 loss_dqn tensor(1.9819e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15835687518119812
dqn reward tensor(-296., device='cuda:0') e 0.5625000000000482 loss_dqn tensor(2.1678e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061224110424518585
dqn reward tensor(-47.7500, device='cuda:0') e 0.5620000000000482 loss_dqn tensor(3.6178e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17803287506103516
dqn reward tensor(-313.5000, device='cuda:0') e 0.5615000000000483 loss_dqn tensor(2.8942e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19166073203086853
dqn reward tensor(-261.8125, device='cuda:0') e 0.5610000000000483 loss_dqn tensor(2.1754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08982960879802704
dqn reward tensor(-416.2500, device='cuda:0') e 0.5605000000000484 loss_dqn tensor(2.2620e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12149612605571747
dqn reward tensor(-421.8125, device='cuda:0') e 0.5600000000000485 loss_dqn tensor(2.1430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07472803443670273
dqn reward tensor(-312., device='cuda:0') e 0.5595000000000485 loss_dqn tensor(2.8193e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1729002296924591
dqn reward tensor(-327.0625, device='cuda:0') e 0.5590000000000486 loss_dqn tensor(2.0909e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08453555405139923
dqn reward tensor(-366.5000, device='cuda:0') e 0.5585000000000486 loss_dqn tensor(2.5575e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19803453981876373
dqn reward tensor(-343.1250, device='cuda:0') e 0.5580000000000487 loss_dqn tensor(2.0485e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24931970238685608
dqn reward tensor(-305.8750, device='cuda:0') e 0.5575000000000487 loss_dqn tensor(1.8691e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20133116841316223
dqn reward tensor(-218.8750, device='cuda:0') e 0.5570000000000488 loss_dqn tensor(3.0980e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29016634821891785
dqn reward tensor(-400.2500, device='cuda:0') e 0.5565000000000488 loss_dqn tensor(2.8092e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08211623877286911
dqn reward tensor(-403., device='cuda:0') e 0.5560000000000489 loss_dqn tensor(4.0045e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09760507941246033
dqn reward tensor(-349.7500, device='cuda:0') e 0.555500000000049 loss_dqn tensor(4.2572e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18326324224472046
dqn reward tensor(-378.9375, device='cuda:0') e 0.555000000000049 loss_dqn tensor(3.3273e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19834785163402557
dqn reward tensor(-262.9375, device='cuda:0') e 0.5545000000000491 loss_dqn tensor(3.9444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1893744021654129
dqn reward tensor(-322.1250, device='cuda:0') e 0.5540000000000491 loss_dqn tensor(2.8019e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19428826868534088
dqn reward tensor(-364.6250, device='cuda:0') e 0.5535000000000492 loss_dqn tensor(2.8101e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18796254694461823
dqn reward tensor(-278.3125, device='cuda:0') e 0.5530000000000492 loss_dqn tensor(2.3999e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15043547749519348
dqn reward tensor(-415.9375, device='cuda:0') e 0.5525000000000493 loss_dqn tensor(1.8430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16899314522743225
dqn reward tensor(-432.5000, device='cuda:0') e 0.5520000000000493 loss_dqn tensor(1.8367e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08403146266937256
dqn reward tensor(-343.8750, device='cuda:0') e 0.5515000000000494 loss_dqn tensor(2.4787e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12979507446289062
dqn reward tensor(-428.3125, device='cuda:0') e 0.5510000000000495 loss_dqn tensor(2.6617e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.189564049243927
dqn reward tensor(-340.7500, device='cuda:0') e 0.5505000000000495 loss_dqn tensor(2.0420e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16802868247032166
dqn reward tensor(-441.1250, device='cuda:0') e 0.5500000000000496 loss_dqn tensor(2.4002e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23907750844955444
dqn reward tensor(-352., device='cuda:0') e 0.5495000000000496 loss_dqn tensor(3.1833e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11969168484210968
dqn reward tensor(-143.5000, device='cuda:0') e 0.5490000000000497 loss_dqn tensor(2.4409e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2507806420326233
dqn reward tensor(-352.6250, device='cuda:0') e 0.5485000000000497 loss_dqn tensor(2.0001e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16776573657989502
dqn reward tensor(-391.5625, device='cuda:0') e 0.5480000000000498 loss_dqn tensor(2.9353e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03285010904073715
dqn reward tensor(-321.5000, device='cuda:0') e 0.5475000000000498 loss_dqn tensor(3.1456e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2619796693325043
dqn reward tensor(-282.2500, device='cuda:0') e 0.5470000000000499 loss_dqn tensor(2.5171e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0784861296415329
dqn reward tensor(-243.1250, device='cuda:0') e 0.54650000000005 loss_dqn tensor(3.6146e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05663100257515907
dqn reward tensor(-385., device='cuda:0') e 0.54600000000005 loss_dqn tensor(3.7472e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16885371506214142
dqn reward tensor(-484.6250, device='cuda:0') e 0.5455000000000501 loss_dqn tensor(2.5378e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26524028182029724
dqn reward tensor(-275., device='cuda:0') e 0.5450000000000501 loss_dqn tensor(3.3754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19420817494392395
dqn reward tensor(-378., device='cuda:0') e 0.5445000000000502 loss_dqn tensor(4.2511e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11136753857135773
dqn reward tensor(-306.9375, device='cuda:0') e 0.5440000000000502 loss_dqn tensor(2.6855e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15307796001434326
dqn reward tensor(-356.3750, device='cuda:0') e 0.5435000000000503 loss_dqn tensor(2.8679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15104196965694427
dqn reward tensor(-340.1250, device='cuda:0') e 0.5430000000000503 loss_dqn tensor(4.0107e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1258566975593567
dqn reward tensor(-372.7500, device='cuda:0') e 0.5425000000000504 loss_dqn tensor(2.5309e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19168755412101746
dqn reward tensor(-331.1875, device='cuda:0') e 0.5420000000000504 loss_dqn tensor(3.2317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11901587247848511
dqn reward tensor(-439.2500, device='cuda:0') e 0.5415000000000505 loss_dqn tensor(2.7011e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1889118105173111
dqn reward tensor(-296.5000, device='cuda:0') e 0.5410000000000506 loss_dqn tensor(3.5988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025011390447616577
dqn reward tensor(-348., device='cuda:0') e 0.5405000000000506 loss_dqn tensor(2.6920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07885193824768066
dqn reward tensor(-274.7500, device='cuda:0') e 0.5400000000000507 loss_dqn tensor(2.5861e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15119239687919617
dqn reward tensor(-344.8750, device='cuda:0') e 0.5395000000000507 loss_dqn tensor(2.8693e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13749592006206512
dqn reward tensor(-521.8750, device='cuda:0') e 0.5390000000000508 loss_dqn tensor(3.0004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19621892273426056
dqn reward tensor(-352.7500, device='cuda:0') e 0.5385000000000508 loss_dqn tensor(1.9303e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20360875129699707
dqn reward tensor(-138.8750, device='cuda:0') e 0.5380000000000509 loss_dqn tensor(3.2999e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08568309247493744
dqn reward tensor(-383.9375, device='cuda:0') e 0.5375000000000509 loss_dqn tensor(2.4164e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029181141406297684
dqn reward tensor(-159., device='cuda:0') e 0.537000000000051 loss_dqn tensor(3.2563e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1974973827600479
dqn reward tensor(-351.8750, device='cuda:0') e 0.536500000000051 loss_dqn tensor(3.8064e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1691657304763794
dqn reward tensor(-395., device='cuda:0') e 0.5360000000000511 loss_dqn tensor(3.7022e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13021904230117798
dqn reward tensor(-357.8750, device='cuda:0') e 0.5355000000000512 loss_dqn tensor(2.3432e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1658322513103485
dqn reward tensor(-495.7500, device='cuda:0') e 0.5350000000000512 loss_dqn tensor(3.9427e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12142688781023026
dqn reward tensor(-453., device='cuda:0') e 0.5345000000000513 loss_dqn tensor(2.6711e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2455175518989563
dqn reward tensor(-403.3750, device='cuda:0') e 0.5340000000000513 loss_dqn tensor(4.0966e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06798779219388962
dqn reward tensor(-301.7500, device='cuda:0') e 0.5335000000000514 loss_dqn tensor(3.0130e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14213216304779053
dqn reward tensor(-387.9375, device='cuda:0') e 0.5330000000000514 loss_dqn tensor(3.5768e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06993253529071808
dqn reward tensor(-455.6250, device='cuda:0') e 0.5325000000000515 loss_dqn tensor(4.1919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3568529784679413
dqn reward tensor(-402.8750, device='cuda:0') e 0.5320000000000515 loss_dqn tensor(3.3988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19800888001918793
dqn reward tensor(-438.5000, device='cuda:0') e 0.5315000000000516 loss_dqn tensor(2.4385e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10887496173381805
dqn reward tensor(-312.8750, device='cuda:0') e 0.5310000000000517 loss_dqn tensor(3.0324e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10141696035861969
dqn reward tensor(-514.4375, device='cuda:0') e 0.5305000000000517 loss_dqn tensor(3.3376e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17077694833278656
dqn reward tensor(-478.5000, device='cuda:0') e 0.5300000000000518 loss_dqn tensor(3.5569e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22559648752212524
dqn reward tensor(-238.2500, device='cuda:0') e 0.5295000000000518 loss_dqn tensor(3.2239e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27135372161865234
dqn reward tensor(-309.6250, device='cuda:0') e 0.5290000000000519 loss_dqn tensor(2.8171e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20196719467639923
dqn reward tensor(-485.6250, device='cuda:0') e 0.5285000000000519 loss_dqn tensor(2.6098e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2979859709739685
dqn reward tensor(-269.7500, device='cuda:0') e 0.528000000000052 loss_dqn tensor(3.0862e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24250341951847076
dqn reward tensor(-521.2500, device='cuda:0') e 0.527500000000052 loss_dqn tensor(2.6013e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22243955731391907
dqn reward tensor(-486.8125, device='cuda:0') e 0.5270000000000521 loss_dqn tensor(2.2150e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15714143216609955
dqn reward tensor(-266.6250, device='cuda:0') e 0.5265000000000521 loss_dqn tensor(3.2702e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10814531147480011
dqn reward tensor(-345.3125, device='cuda:0') e 0.5260000000000522 loss_dqn tensor(2.4567e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2166106402873993
dqn reward tensor(-426.8750, device='cuda:0') e 0.5255000000000523 loss_dqn tensor(3.2207e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1597069501876831
dqn reward tensor(-461.6250, device='cuda:0') e 0.5250000000000523 loss_dqn tensor(1.8800e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1932477355003357
dqn reward tensor(-148.2500, device='cuda:0') e 0.5245000000000524 loss_dqn tensor(2.9853e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.165447399020195
dqn reward tensor(-395.1250, device='cuda:0') e 0.5240000000000524 loss_dqn tensor(2.8042e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14783458411693573
dqn reward tensor(-492.9375, device='cuda:0') e 0.5235000000000525 loss_dqn tensor(1.9604e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0763629823923111
dqn reward tensor(-469.5000, device='cuda:0') e 0.5230000000000525 loss_dqn tensor(2.9261e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07111040502786636
dqn reward tensor(-297.1250, device='cuda:0') e 0.5225000000000526 loss_dqn tensor(3.2363e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1514195054769516
dqn reward tensor(-474.0625, device='cuda:0') e 0.5220000000000526 loss_dqn tensor(3.2392e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10617198050022125
dqn reward tensor(-371.6250, device='cuda:0') e 0.5215000000000527 loss_dqn tensor(2.7113e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07538169622421265
dqn reward tensor(-393.1250, device='cuda:0') e 0.5210000000000528 loss_dqn tensor(2.6218e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18658964335918427
dqn reward tensor(-246., device='cuda:0') e 0.5205000000000528 loss_dqn tensor(2.0857e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25836217403411865
dqn reward tensor(-345.8750, device='cuda:0') e 0.5200000000000529 loss_dqn tensor(2.1944e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057007983326911926
dqn reward tensor(-368.9375, device='cuda:0') e 0.5195000000000529 loss_dqn tensor(2.0024e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18931196630001068
dqn reward tensor(-353.1250, device='cuda:0') e 0.519000000000053 loss_dqn tensor(2.8368e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1360948234796524
dqn reward tensor(-364.3125, device='cuda:0') e 0.518500000000053 loss_dqn tensor(3.3145e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07447635382413864
dqn reward tensor(-344.5000, device='cuda:0') e 0.5180000000000531 loss_dqn tensor(1.8923e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17503583431243896
dqn reward tensor(-372.7500, device='cuda:0') e 0.5175000000000531 loss_dqn tensor(2.2066e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07942625880241394
dqn reward tensor(-523., device='cuda:0') e 0.5170000000000532 loss_dqn tensor(1.5892e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18592658638954163
dqn reward tensor(-415.6250, device='cuda:0') e 0.5165000000000532 loss_dqn tensor(1.7399e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04671704024076462
dqn reward tensor(-460.3750, device='cuda:0') e 0.5160000000000533 loss_dqn tensor(1.7786e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2143068164587021
dqn reward tensor(-342.1250, device='cuda:0') e 0.5155000000000534 loss_dqn tensor(2.6800e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047917939722537994
dqn reward tensor(-377.2500, device='cuda:0') e 0.5150000000000534 loss_dqn tensor(2.5698e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15245908498764038
dqn reward tensor(-440.0625, device='cuda:0') e 0.5145000000000535 loss_dqn tensor(3.0195e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.130466490983963
dqn reward tensor(-318.5000, device='cuda:0') e 0.5140000000000535 loss_dqn tensor(2.2801e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15880900621414185
dqn reward tensor(-374.3750, device='cuda:0') e 0.5135000000000536 loss_dqn tensor(3.0636e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12730537354946136
dqn reward tensor(-394.8750, device='cuda:0') e 0.5130000000000536 loss_dqn tensor(1.8832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15598732233047485
dqn reward tensor(-459.3750, device='cuda:0') e 0.5125000000000537 loss_dqn tensor(2.5059e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11261458694934845
dqn reward tensor(-343.8750, device='cuda:0') e 0.5120000000000537 loss_dqn tensor(2.8997e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2040504515171051
dqn reward tensor(-430.2500, device='cuda:0') e 0.5115000000000538 loss_dqn tensor(2.2329e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07454712688922882
dqn reward tensor(-270.9375, device='cuda:0') e 0.5110000000000539 loss_dqn tensor(3.2490e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1961529701948166
dqn reward tensor(-414., device='cuda:0') e 0.5105000000000539 loss_dqn tensor(2.3941e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4129337668418884
dqn reward tensor(-315.6250, device='cuda:0') e 0.510000000000054 loss_dqn tensor(3.1685e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11711172759532928
dqn reward tensor(-374., device='cuda:0') e 0.509500000000054 loss_dqn tensor(2.9896e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15980678796768188
dqn reward tensor(-394.6250, device='cuda:0') e 0.5090000000000541 loss_dqn tensor(1.7514e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13627031445503235
dqn reward tensor(-440.3750, device='cuda:0') e 0.5085000000000541 loss_dqn tensor(2.1998e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19455038011074066
dqn reward tensor(-485.2500, device='cuda:0') e 0.5080000000000542 loss_dqn tensor(2.4193e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3478149175643921
dqn reward tensor(-257., device='cuda:0') e 0.5075000000000542 loss_dqn tensor(2.5170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11350449919700623
dqn reward tensor(-385.2500, device='cuda:0') e 0.5070000000000543 loss_dqn tensor(2.6934e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07387624680995941
dqn reward tensor(-442.5000, device='cuda:0') e 0.5065000000000544 loss_dqn tensor(2.5763e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27056798338890076
dqn reward tensor(-262.2500, device='cuda:0') e 0.5060000000000544 loss_dqn tensor(3.4965e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17090511322021484
dqn reward tensor(-349.3750, device='cuda:0') e 0.5055000000000545 loss_dqn tensor(2.0259e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08873017877340317
dqn reward tensor(-311.7500, device='cuda:0') e 0.5050000000000545 loss_dqn tensor(3.1378e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07977083325386047
dqn reward tensor(-305.5000, device='cuda:0') e 0.5045000000000546 loss_dqn tensor(2.0309e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17263054847717285
dqn reward tensor(-466.5625, device='cuda:0') e 0.5040000000000546 loss_dqn tensor(1.0568e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12942583858966827
dqn reward tensor(-340.1875, device='cuda:0') e 0.5035000000000547 loss_dqn tensor(1.6357e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1306469887495041
dqn reward tensor(-402.6250, device='cuda:0') e 0.5030000000000547 loss_dqn tensor(3.2704e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07736194878816605
dqn reward tensor(-293.1250, device='cuda:0') e 0.5025000000000548 loss_dqn tensor(3.3228e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22678378224372864
dqn reward tensor(-363.6250, device='cuda:0') e 0.5020000000000548 loss_dqn tensor(2.3329e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08037976920604706
dqn reward tensor(-365., device='cuda:0') e 0.5015000000000549 loss_dqn tensor(1.2676e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04684222489595413
dqn reward tensor(-318.3750, device='cuda:0') e 0.501000000000055 loss_dqn tensor(1.8679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30936357378959656
dqn reward tensor(-336., device='cuda:0') e 0.500500000000055 loss_dqn tensor(1.8229e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07079875469207764
dqn reward tensor(-390.4375, device='cuda:0') e 0.5000000000000551 loss_dqn tensor(2.2249e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02595965564250946
dqn reward tensor(-416.7500, device='cuda:0') e 0.49950000000005507 loss_dqn tensor(2.3113e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06987225264310837
dqn reward tensor(-414., device='cuda:0') e 0.49900000000005507 loss_dqn tensor(1.3461e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15329253673553467
dqn reward tensor(-528.3750, device='cuda:0') e 0.49850000000005507 loss_dqn tensor(1.4623e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025551021099090576
dqn reward tensor(-394., device='cuda:0') e 0.49800000000005507 loss_dqn tensor(1.9985e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0562683530151844
dqn reward tensor(-291.5000, device='cuda:0') e 0.49750000000005506 loss_dqn tensor(2.1937e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14116349816322327
dqn reward tensor(-332.3750, device='cuda:0') e 0.49700000000005506 loss_dqn tensor(1.9969e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023434720933437347
dqn reward tensor(-282.6250, device='cuda:0') e 0.49650000000005506 loss_dqn tensor(2.0466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23940744996070862
dqn reward tensor(-385.5000, device='cuda:0') e 0.49600000000005506 loss_dqn tensor(3.1296e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14693164825439453
dqn reward tensor(-339.6875, device='cuda:0') e 0.49550000000005506 loss_dqn tensor(2.1686e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1580878496170044
dqn reward tensor(-324.1250, device='cuda:0') e 0.49500000000005506 loss_dqn tensor(2.1787e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13765105605125427
dqn reward tensor(-317.5000, device='cuda:0') e 0.49450000000005506 loss_dqn tensor(2.8838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18550336360931396
dqn reward tensor(-293.2500, device='cuda:0') e 0.49400000000005506 loss_dqn tensor(3.5109e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09506798535585403
dqn reward tensor(-305.5000, device='cuda:0') e 0.49350000000005506 loss_dqn tensor(2.8463e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12668956816196442
dqn reward tensor(-393.6250, device='cuda:0') e 0.49300000000005506 loss_dqn tensor(1.6311e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16434359550476074
dqn reward tensor(-368., device='cuda:0') e 0.49250000000005506 loss_dqn tensor(2.6449e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07520299404859543
dqn reward tensor(-393.3125, device='cuda:0') e 0.49200000000005506 loss_dqn tensor(2.1084e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09261467307806015
dqn reward tensor(-447.8750, device='cuda:0') e 0.49150000000005506 loss_dqn tensor(1.8899e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09798969328403473
dqn reward tensor(-309.7500, device='cuda:0') e 0.49100000000005506 loss_dqn tensor(1.6282e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057741500437259674
dqn reward tensor(-234.8750, device='cuda:0') e 0.49050000000005506 loss_dqn tensor(3.4969e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13946369290351868
dqn reward tensor(-200., device='cuda:0') e 0.49000000000005506 loss_dqn tensor(2.4695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059728071093559265
dqn reward tensor(-319.2500, device='cuda:0') e 0.48950000000005506 loss_dqn tensor(1.8100e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13646554946899414
dqn reward tensor(-311.8750, device='cuda:0') e 0.48900000000005506 loss_dqn tensor(2.0631e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1935528963804245
dqn reward tensor(-220.7500, device='cuda:0') e 0.48850000000005506 loss_dqn tensor(3.5976e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23965901136398315
dqn reward tensor(-263.3750, device='cuda:0') e 0.48800000000005506 loss_dqn tensor(3.1632e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11783353239297867
dqn reward tensor(-344.5000, device='cuda:0') e 0.48750000000005506 loss_dqn tensor(2.9418e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13120557367801666
dqn reward tensor(-294.9375, device='cuda:0') e 0.48700000000005506 loss_dqn tensor(1.6136e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21366551518440247
dqn reward tensor(-250.1250, device='cuda:0') e 0.48650000000005506 loss_dqn tensor(2.9041e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19429168105125427
dqn reward tensor(-297.6250, device='cuda:0') e 0.48600000000005505 loss_dqn tensor(1.8381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09394863247871399
dqn reward tensor(-310.6250, device='cuda:0') e 0.48550000000005505 loss_dqn tensor(2.7328e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16980907320976257
dqn reward tensor(-9.3750, device='cuda:0') e 0.48500000000005505 loss_dqn tensor(2.3638e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037213291972875595
Evaluating...
Train: {'rocauc': 0.7029322275501505} -5.41689920425415
=====Epoch 3=====
Training...
dqn reward tensor(-341.2500, device='cuda:0') e 0.48450000000005505 loss_dqn tensor(2.6087e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08564642816781998
dqn reward tensor(-302.3750, device='cuda:0') e 0.48400000000005505 loss_dqn tensor(1.9175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10713768750429153
dqn reward tensor(-349.7500, device='cuda:0') e 0.48350000000005505 loss_dqn tensor(2.1870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16981449723243713
dqn reward tensor(-325.7500, device='cuda:0') e 0.48300000000005505 loss_dqn tensor(2.0250e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20623184740543365
dqn reward tensor(-296.1250, device='cuda:0') e 0.48250000000005505 loss_dqn tensor(2.9406e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07533085346221924
dqn reward tensor(-371.1250, device='cuda:0') e 0.48200000000005505 loss_dqn tensor(2.4680e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1368832290172577
dqn reward tensor(-420.5000, device='cuda:0') e 0.48150000000005505 loss_dqn tensor(2.1700e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028702521696686745
dqn reward tensor(-370.7500, device='cuda:0') e 0.48100000000005505 loss_dqn tensor(2.3004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08823825418949127
dqn reward tensor(-284.5000, device='cuda:0') e 0.48050000000005505 loss_dqn tensor(1.3794e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07692122459411621
dqn reward tensor(-303.1250, device='cuda:0') e 0.48000000000005505 loss_dqn tensor(2.1236e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08475343883037567
dqn reward tensor(-312., device='cuda:0') e 0.47950000000005505 loss_dqn tensor(2.4943e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2005690336227417
dqn reward tensor(-252.7500, device='cuda:0') e 0.47900000000005505 loss_dqn tensor(3.0299e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2280590534210205
dqn reward tensor(-372.5000, device='cuda:0') e 0.47850000000005505 loss_dqn tensor(1.9300e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1216905415058136
dqn reward tensor(-225.1250, device='cuda:0') e 0.47800000000005505 loss_dqn tensor(3.0526e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432449072599411
dqn reward tensor(-430.7500, device='cuda:0') e 0.47750000000005505 loss_dqn tensor(1.3627e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2551738917827606
dqn reward tensor(-167.1250, device='cuda:0') e 0.47700000000005505 loss_dqn tensor(2.6658e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11080875247716904
dqn reward tensor(-230.1250, device='cuda:0') e 0.47650000000005505 loss_dqn tensor(3.1806e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14729134738445282
dqn reward tensor(-195.8750, device='cuda:0') e 0.47600000000005505 loss_dqn tensor(3.0172e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12297235429286957
dqn reward tensor(-273.7500, device='cuda:0') e 0.47550000000005505 loss_dqn tensor(2.8300e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11676175892353058
dqn reward tensor(-335.2500, device='cuda:0') e 0.47500000000005504 loss_dqn tensor(3.3644e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.137477844953537
dqn reward tensor(-348.8750, device='cuda:0') e 0.47450000000005504 loss_dqn tensor(2.6521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11703422665596008
dqn reward tensor(-405.6250, device='cuda:0') e 0.47400000000005504 loss_dqn tensor(2.4048e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22755365073680878
dqn reward tensor(-405.8750, device='cuda:0') e 0.47350000000005504 loss_dqn tensor(2.5851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09467831254005432
dqn reward tensor(-349., device='cuda:0') e 0.47300000000005504 loss_dqn tensor(1.7168e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14389176666736603
dqn reward tensor(-165.5000, device='cuda:0') e 0.47250000000005504 loss_dqn tensor(2.9505e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08603895455598831
dqn reward tensor(-328.9375, device='cuda:0') e 0.47200000000005504 loss_dqn tensor(2.1622e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1895054131746292
dqn reward tensor(-381., device='cuda:0') e 0.47150000000005504 loss_dqn tensor(2.6338e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07892288267612457
dqn reward tensor(-318.0625, device='cuda:0') e 0.47100000000005504 loss_dqn tensor(2.0190e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14977268874645233
dqn reward tensor(-424.8750, device='cuda:0') e 0.47050000000005504 loss_dqn tensor(2.0072e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14240773022174835
dqn reward tensor(-431.2500, device='cuda:0') e 0.47000000000005504 loss_dqn tensor(1.8748e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14378449320793152
dqn reward tensor(-276.5000, device='cuda:0') e 0.46950000000005504 loss_dqn tensor(2.0875e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020527027547359467
dqn reward tensor(-243.4375, device='cuda:0') e 0.46900000000005504 loss_dqn tensor(3.2278e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21407680213451385
dqn reward tensor(-316.1875, device='cuda:0') e 0.46850000000005504 loss_dqn tensor(3.2196e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26166215538978577
dqn reward tensor(-361.3750, device='cuda:0') e 0.46800000000005504 loss_dqn tensor(2.5082e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03615952655673027
dqn reward tensor(-356.1250, device='cuda:0') e 0.46750000000005504 loss_dqn tensor(2.0873e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027276180684566498
dqn reward tensor(-407.1250, device='cuda:0') e 0.46700000000005504 loss_dqn tensor(2.7088e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18340793251991272
dqn reward tensor(-376.8750, device='cuda:0') e 0.46650000000005504 loss_dqn tensor(3.9055e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12356370687484741
dqn reward tensor(-277.3750, device='cuda:0') e 0.46600000000005504 loss_dqn tensor(2.4033e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4233413338661194
dqn reward tensor(-396.8125, device='cuda:0') e 0.46550000000005504 loss_dqn tensor(2.3580e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16008588671684265
dqn reward tensor(-316.8750, device='cuda:0') e 0.46500000000005504 loss_dqn tensor(2.6485e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05401978641748428
dqn reward tensor(-381.9375, device='cuda:0') e 0.46450000000005504 loss_dqn tensor(2.5025e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1040101945400238
dqn reward tensor(-294.1250, device='cuda:0') e 0.46400000000005504 loss_dqn tensor(2.9028e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1317947506904602
dqn reward tensor(-177.6250, device='cuda:0') e 0.46350000000005503 loss_dqn tensor(4.1916e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32668542861938477
dqn reward tensor(-361.5625, device='cuda:0') e 0.46300000000005503 loss_dqn tensor(1.7029e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13751401007175446
dqn reward tensor(-209.3750, device='cuda:0') e 0.46250000000005503 loss_dqn tensor(2.7774e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18613407015800476
dqn reward tensor(-434.4375, device='cuda:0') e 0.46200000000005503 loss_dqn tensor(3.0884e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06400373578071594
dqn reward tensor(-275.5000, device='cuda:0') e 0.46150000000005503 loss_dqn tensor(2.4369e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17206129431724548
dqn reward tensor(-266.6250, device='cuda:0') e 0.46100000000005503 loss_dqn tensor(2.8522e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1423029601573944
dqn reward tensor(-246.7500, device='cuda:0') e 0.46050000000005503 loss_dqn tensor(3.5080e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19626367092132568
dqn reward tensor(-382.5000, device='cuda:0') e 0.46000000000005503 loss_dqn tensor(2.1840e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1402714103460312
dqn reward tensor(-304.2500, device='cuda:0') e 0.45950000000005503 loss_dqn tensor(3.7380e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11207063496112823
dqn reward tensor(-225.7500, device='cuda:0') e 0.45900000000005503 loss_dqn tensor(4.8393e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11570888012647629
dqn reward tensor(-361.4375, device='cuda:0') e 0.45850000000005503 loss_dqn tensor(4.1118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07945241034030914
dqn reward tensor(-182.5000, device='cuda:0') e 0.45800000000005503 loss_dqn tensor(3.3981e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22228993475437164
dqn reward tensor(-251.5000, device='cuda:0') e 0.45750000000005503 loss_dqn tensor(4.2171e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33495211601257324
dqn reward tensor(-387.8750, device='cuda:0') e 0.45700000000005503 loss_dqn tensor(2.9471e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09220763295888901
dqn reward tensor(-385.3750, device='cuda:0') e 0.45650000000005503 loss_dqn tensor(2.8811e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12476971000432968
dqn reward tensor(-324.8750, device='cuda:0') e 0.45600000000005503 loss_dqn tensor(1.8584e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08903796225786209
dqn reward tensor(-229.2500, device='cuda:0') e 0.455500000000055 loss_dqn tensor(5.0822e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12241429835557938
dqn reward tensor(-336.5000, device='cuda:0') e 0.455000000000055 loss_dqn tensor(2.8494e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1754589080810547
dqn reward tensor(-282.5000, device='cuda:0') e 0.454500000000055 loss_dqn tensor(4.9325e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13888561725616455
dqn reward tensor(-402.6250, device='cuda:0') e 0.454000000000055 loss_dqn tensor(3.0183e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290430724620819
dqn reward tensor(-333.1250, device='cuda:0') e 0.453500000000055 loss_dqn tensor(2.7222e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1071716696023941
dqn reward tensor(-182., device='cuda:0') e 0.453000000000055 loss_dqn tensor(3.7314e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18221697211265564
dqn reward tensor(-386.7500, device='cuda:0') e 0.452500000000055 loss_dqn tensor(3.0267e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15144501626491547
dqn reward tensor(-369.3750, device='cuda:0') e 0.452000000000055 loss_dqn tensor(2.8426e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553613513708115
dqn reward tensor(-321., device='cuda:0') e 0.451500000000055 loss_dqn tensor(4.2055e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1172315776348114
dqn reward tensor(-287.6250, device='cuda:0') e 0.451000000000055 loss_dqn tensor(2.9547e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08404102176427841
dqn reward tensor(-267.7500, device='cuda:0') e 0.450500000000055 loss_dqn tensor(4.4931e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13617059588432312
dqn reward tensor(35.1250, device='cuda:0') e 0.450000000000055 loss_dqn tensor(4.6965e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14362482726573944
dqn reward tensor(-257., device='cuda:0') e 0.449500000000055 loss_dqn tensor(2.6757e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0931835025548935
dqn reward tensor(-118.6250, device='cuda:0') e 0.449000000000055 loss_dqn tensor(3.9638e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365369379520416
dqn reward tensor(-159.5000, device='cuda:0') e 0.448500000000055 loss_dqn tensor(3.3243e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30411258339881897
dqn reward tensor(-223.2500, device='cuda:0') e 0.448000000000055 loss_dqn tensor(2.6753e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09846312552690506
dqn reward tensor(-297.7500, device='cuda:0') e 0.447500000000055 loss_dqn tensor(3.7032e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08578665554523468
dqn reward tensor(-338.7500, device='cuda:0') e 0.447000000000055 loss_dqn tensor(2.1699e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14695154130458832
dqn reward tensor(-148.8750, device='cuda:0') e 0.446500000000055 loss_dqn tensor(5.6445e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14125970005989075
dqn reward tensor(-352.8750, device='cuda:0') e 0.446000000000055 loss_dqn tensor(3.6838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12869268655776978
dqn reward tensor(-338.1250, device='cuda:0') e 0.445500000000055 loss_dqn tensor(4.1757e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03812666982412338
dqn reward tensor(-191.4375, device='cuda:0') e 0.445000000000055 loss_dqn tensor(3.5182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029077816754579544
dqn reward tensor(-263.1250, device='cuda:0') e 0.444500000000055 loss_dqn tensor(2.4701e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11836627870798111
dqn reward tensor(-248.7500, device='cuda:0') e 0.444000000000055 loss_dqn tensor(4.7397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12378524243831635
dqn reward tensor(-234.6250, device='cuda:0') e 0.443500000000055 loss_dqn tensor(2.7258e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15369556844234467
dqn reward tensor(-310.8750, device='cuda:0') e 0.443000000000055 loss_dqn tensor(4.5790e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08210410177707672
dqn reward tensor(-327.1250, device='cuda:0') e 0.442500000000055 loss_dqn tensor(2.7208e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13559800386428833
dqn reward tensor(-196., device='cuda:0') e 0.442000000000055 loss_dqn tensor(4.0453e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14163236320018768
dqn reward tensor(-228.6250, device='cuda:0') e 0.441500000000055 loss_dqn tensor(2.4563e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20152902603149414
dqn reward tensor(-243.1250, device='cuda:0') e 0.441000000000055 loss_dqn tensor(4.5938e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11378300189971924
dqn reward tensor(-154., device='cuda:0') e 0.440500000000055 loss_dqn tensor(3.7595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08578681945800781
dqn reward tensor(-275., device='cuda:0') e 0.440000000000055 loss_dqn tensor(3.4004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11690355092287064
dqn reward tensor(-273.5000, device='cuda:0') e 0.439500000000055 loss_dqn tensor(4.8476e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15586233139038086
dqn reward tensor(-316.8750, device='cuda:0') e 0.439000000000055 loss_dqn tensor(2.9075e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08203032612800598
dqn reward tensor(-276.2500, device='cuda:0') e 0.438500000000055 loss_dqn tensor(4.2919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2808224558830261
dqn reward tensor(-386.1875, device='cuda:0') e 0.438000000000055 loss_dqn tensor(3.0137e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08877569437026978
dqn reward tensor(-237.1250, device='cuda:0') e 0.437500000000055 loss_dqn tensor(2.9400e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31645023822784424
dqn reward tensor(-294.1250, device='cuda:0') e 0.437000000000055 loss_dqn tensor(2.7157e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10246579349040985
dqn reward tensor(-424.5000, device='cuda:0') e 0.436500000000055 loss_dqn tensor(3.4445e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20295721292495728
dqn reward tensor(-335.5000, device='cuda:0') e 0.436000000000055 loss_dqn tensor(5.5513e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16967391967773438
dqn reward tensor(-361.5000, device='cuda:0') e 0.435500000000055 loss_dqn tensor(3.2690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1317436397075653
dqn reward tensor(-377.8750, device='cuda:0') e 0.435000000000055 loss_dqn tensor(2.9803e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20678210258483887
dqn reward tensor(-327.8125, device='cuda:0') e 0.434500000000055 loss_dqn tensor(4.0457e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05087181180715561
dqn reward tensor(-316., device='cuda:0') e 0.434000000000055 loss_dqn tensor(4.1779e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14815206825733185
dqn reward tensor(-427.5000, device='cuda:0') e 0.433500000000055 loss_dqn tensor(3.6943e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13745924830436707
dqn reward tensor(-300.6250, device='cuda:0') e 0.433000000000055 loss_dqn tensor(3.9302e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3041715621948242
dqn reward tensor(-326.1250, device='cuda:0') e 0.432500000000055 loss_dqn tensor(3.8054e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0719374418258667
dqn reward tensor(-550.2500, device='cuda:0') e 0.432000000000055 loss_dqn tensor(3.4900e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16866928339004517
dqn reward tensor(-378.5625, device='cuda:0') e 0.431500000000055 loss_dqn tensor(4.2159e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06865669041872025
dqn reward tensor(-425., device='cuda:0') e 0.431000000000055 loss_dqn tensor(2.6814e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15586581826210022
dqn reward tensor(-273.2500, device='cuda:0') e 0.430500000000055 loss_dqn tensor(3.2812e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07510600984096527
dqn reward tensor(-415., device='cuda:0') e 0.430000000000055 loss_dqn tensor(3.0447e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21150964498519897
dqn reward tensor(-456.1250, device='cuda:0') e 0.429500000000055 loss_dqn tensor(2.7720e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38238057494163513
dqn reward tensor(-319.8750, device='cuda:0') e 0.429000000000055 loss_dqn tensor(3.1658e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17187972366809845
dqn reward tensor(-400.6250, device='cuda:0') e 0.428500000000055 loss_dqn tensor(2.9785e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2287924885749817
dqn reward tensor(-248.6250, device='cuda:0') e 0.428000000000055 loss_dqn tensor(5.3854e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15361258387565613
dqn reward tensor(-427.5000, device='cuda:0') e 0.427500000000055 loss_dqn tensor(3.5216e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1605129837989807
dqn reward tensor(-455.6250, device='cuda:0') e 0.427000000000055 loss_dqn tensor(5.3690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1257232129573822
dqn reward tensor(-350.6250, device='cuda:0') e 0.426500000000055 loss_dqn tensor(5.3131e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11221387982368469
dqn reward tensor(-324., device='cuda:0') e 0.426000000000055 loss_dqn tensor(2.7481e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1668538898229599
dqn reward tensor(-347.1250, device='cuda:0') e 0.425500000000055 loss_dqn tensor(3.9659e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2023792564868927
dqn reward tensor(-447.3750, device='cuda:0') e 0.425000000000055 loss_dqn tensor(3.9504e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09836728870868683
dqn reward tensor(-530.7500, device='cuda:0') e 0.424500000000055 loss_dqn tensor(2.6341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27521809935569763
dqn reward tensor(-467.3750, device='cuda:0') e 0.424000000000055 loss_dqn tensor(2.1856e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10300328582525253
dqn reward tensor(-386.7500, device='cuda:0') e 0.423500000000055 loss_dqn tensor(2.8447e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20493179559707642
dqn reward tensor(-347.2500, device='cuda:0') e 0.423000000000055 loss_dqn tensor(3.9703e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20365695655345917
dqn reward tensor(-461.3750, device='cuda:0') e 0.422500000000055 loss_dqn tensor(2.4365e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2145088016986847
dqn reward tensor(-319.6250, device='cuda:0') e 0.422000000000055 loss_dqn tensor(2.5142e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603616178035736
dqn reward tensor(-480., device='cuda:0') e 0.421500000000055 loss_dqn tensor(2.5575e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04933997988700867
dqn reward tensor(-381.3750, device='cuda:0') e 0.421000000000055 loss_dqn tensor(3.4247e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18818160891532898
dqn reward tensor(-316., device='cuda:0') e 0.420500000000055 loss_dqn tensor(3.5236e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09095632284879684
dqn reward tensor(-178.3750, device='cuda:0') e 0.420000000000055 loss_dqn tensor(3.8760e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1816994547843933
dqn reward tensor(-455.0625, device='cuda:0') e 0.419500000000055 loss_dqn tensor(3.6496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19118613004684448
dqn reward tensor(-267.7500, device='cuda:0') e 0.419000000000055 loss_dqn tensor(3.0870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11991217732429504
dqn reward tensor(-487.5000, device='cuda:0') e 0.418500000000055 loss_dqn tensor(3.2187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.156902015209198
dqn reward tensor(-520.6250, device='cuda:0') e 0.418000000000055 loss_dqn tensor(3.0838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13347917795181274
dqn reward tensor(-400.5000, device='cuda:0') e 0.417500000000055 loss_dqn tensor(3.1775e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03582630306482315
dqn reward tensor(-480.6250, device='cuda:0') e 0.417000000000055 loss_dqn tensor(2.3481e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19599881768226624
dqn reward tensor(-518., device='cuda:0') e 0.416500000000055 loss_dqn tensor(3.4533e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30378127098083496
dqn reward tensor(-520.8125, device='cuda:0') e 0.416000000000055 loss_dqn tensor(2.6235e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21443141996860504
dqn reward tensor(-424.1250, device='cuda:0') e 0.415500000000055 loss_dqn tensor(4.6198e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19675709307193756
dqn reward tensor(-427.1875, device='cuda:0') e 0.415000000000055 loss_dqn tensor(4.2974e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22363775968551636
dqn reward tensor(-412.8750, device='cuda:0') e 0.414500000000055 loss_dqn tensor(4.0084e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1357245147228241
dqn reward tensor(-460.3750, device='cuda:0') e 0.414000000000055 loss_dqn tensor(3.0053e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18289296329021454
dqn reward tensor(-412.2500, device='cuda:0') e 0.413500000000055 loss_dqn tensor(2.1701e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18521402776241302
dqn reward tensor(-371.1250, device='cuda:0') e 0.413000000000055 loss_dqn tensor(2.5616e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11728173494338989
dqn reward tensor(-446.6250, device='cuda:0') e 0.412500000000055 loss_dqn tensor(2.6334e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19402706623077393
dqn reward tensor(-509.5000, device='cuda:0') e 0.412000000000055 loss_dqn tensor(2.9443e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1403852254152298
dqn reward tensor(-533.6250, device='cuda:0') e 0.411500000000055 loss_dqn tensor(3.2977e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11322605609893799
dqn reward tensor(-332.6250, device='cuda:0') e 0.411000000000055 loss_dqn tensor(1.9932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21024997532367706
dqn reward tensor(-445.6875, device='cuda:0') e 0.410500000000055 loss_dqn tensor(2.1617e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07902399450540543
dqn reward tensor(-296.5000, device='cuda:0') e 0.410000000000055 loss_dqn tensor(3.5825e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1417933851480484
dqn reward tensor(-382.7500, device='cuda:0') e 0.409500000000055 loss_dqn tensor(2.4302e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13361507654190063
dqn reward tensor(-480.6250, device='cuda:0') e 0.409000000000055 loss_dqn tensor(2.5636e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07451244443655014
dqn reward tensor(-360.5000, device='cuda:0') e 0.408500000000055 loss_dqn tensor(2.1711e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2043769657611847
dqn reward tensor(-501.4375, device='cuda:0') e 0.408000000000055 loss_dqn tensor(2.6718e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08811059594154358
dqn reward tensor(-402.8750, device='cuda:0') e 0.407500000000055 loss_dqn tensor(1.8919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1629284769296646
dqn reward tensor(-313.3750, device='cuda:0') e 0.407000000000055 loss_dqn tensor(2.6853e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2663272023200989
dqn reward tensor(-432., device='cuda:0') e 0.406500000000055 loss_dqn tensor(2.0558e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21057333052158356
dqn reward tensor(-385.5000, device='cuda:0') e 0.406000000000055 loss_dqn tensor(3.1311e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1581278294324875
dqn reward tensor(-378.8125, device='cuda:0') e 0.405500000000055 loss_dqn tensor(2.6263e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1174478679895401
dqn reward tensor(-321.8750, device='cuda:0') e 0.405000000000055 loss_dqn tensor(1.8265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18785884976387024
dqn reward tensor(-431.3750, device='cuda:0') e 0.404500000000055 loss_dqn tensor(2.5513e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.102382592856884
dqn reward tensor(-287.3750, device='cuda:0') e 0.404000000000055 loss_dqn tensor(2.4618e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23074713349342346
dqn reward tensor(-297.5000, device='cuda:0') e 0.403500000000055 loss_dqn tensor(1.7219e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11686411499977112
dqn reward tensor(-394.3750, device='cuda:0') e 0.403000000000055 loss_dqn tensor(2.4068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26479682326316833
dqn reward tensor(-398.2500, device='cuda:0') e 0.402500000000055 loss_dqn tensor(1.4357e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206333875656128
dqn reward tensor(-492.6250, device='cuda:0') e 0.402000000000055 loss_dqn tensor(2.0068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09869194030761719
dqn reward tensor(-341.6875, device='cuda:0') e 0.401500000000055 loss_dqn tensor(1.4633e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08044160902500153
dqn reward tensor(-387.8750, device='cuda:0') e 0.401000000000055 loss_dqn tensor(1.5712e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22731418907642365
dqn reward tensor(-306.7500, device='cuda:0') e 0.400500000000055 loss_dqn tensor(1.1004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13112527132034302
dqn reward tensor(-416.7500, device='cuda:0') e 0.400000000000055 loss_dqn tensor(1.2119e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10834760963916779
dqn reward tensor(-334., device='cuda:0') e 0.399500000000055 loss_dqn tensor(1.3072e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23294560611248016
dqn reward tensor(-217.1250, device='cuda:0') e 0.399000000000055 loss_dqn tensor(1.4214e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13163700699806213
dqn reward tensor(-306., device='cuda:0') e 0.398500000000055 loss_dqn tensor(1.5501e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2285316288471222
dqn reward tensor(-344.2500, device='cuda:0') e 0.398000000000055 loss_dqn tensor(1.5435e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03873082995414734
dqn reward tensor(-401.2500, device='cuda:0') e 0.397500000000055 loss_dqn tensor(1.7265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.212213933467865
dqn reward tensor(-515.8750, device='cuda:0') e 0.397000000000055 loss_dqn tensor(1.2307e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24071678519248962
dqn reward tensor(-360.0625, device='cuda:0') e 0.396500000000055 loss_dqn tensor(1.5795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03706677630543709
dqn reward tensor(-395.2500, device='cuda:0') e 0.396000000000055 loss_dqn tensor(1.5625e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1428680419921875
dqn reward tensor(-440.1250, device='cuda:0') e 0.395500000000055 loss_dqn tensor(1.3981e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.160506933927536
dqn reward tensor(-282.7500, device='cuda:0') e 0.395000000000055 loss_dqn tensor(1.6450e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1051824688911438
dqn reward tensor(-347.8750, device='cuda:0') e 0.394500000000055 loss_dqn tensor(1.1110e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09398701786994934
dqn reward tensor(-363.2500, device='cuda:0') e 0.394000000000055 loss_dqn tensor(1.7673e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28771188855171204
dqn reward tensor(-287.6250, device='cuda:0') e 0.393500000000055 loss_dqn tensor(1.6110e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15961001813411713
dqn reward tensor(-337.8750, device='cuda:0') e 0.39300000000005497 loss_dqn tensor(1.6267e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1427607536315918
dqn reward tensor(-222.5000, device='cuda:0') e 0.39250000000005497 loss_dqn tensor(2.4486e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0699339509010315
dqn reward tensor(-393.5000, device='cuda:0') e 0.39200000000005497 loss_dqn tensor(1.6038e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11114825308322906
dqn reward tensor(-397.1250, device='cuda:0') e 0.39150000000005497 loss_dqn tensor(2.3602e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2094198316335678
dqn reward tensor(-470.7500, device='cuda:0') e 0.39100000000005497 loss_dqn tensor(1.8659e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15352469682693481
dqn reward tensor(-242.7500, device='cuda:0') e 0.39050000000005497 loss_dqn tensor(2.8013e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.249813050031662
dqn reward tensor(-342.2500, device='cuda:0') e 0.39000000000005497 loss_dqn tensor(2.8506e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2394746094942093
dqn reward tensor(-432.3750, device='cuda:0') e 0.38950000000005497 loss_dqn tensor(2.8444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07942503690719604
dqn reward tensor(-313.1250, device='cuda:0') e 0.38900000000005497 loss_dqn tensor(1.8047e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28735435009002686
dqn reward tensor(-359., device='cuda:0') e 0.38850000000005497 loss_dqn tensor(2.8223e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0619739294052124
dqn reward tensor(-244.2500, device='cuda:0') e 0.38800000000005497 loss_dqn tensor(4.1144e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10847645252943039
dqn reward tensor(-233.3750, device='cuda:0') e 0.38750000000005497 loss_dqn tensor(2.9467e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19300177693367004
dqn reward tensor(-371.4375, device='cuda:0') e 0.38700000000005497 loss_dqn tensor(3.8032e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03432133048772812
dqn reward tensor(-317.1250, device='cuda:0') e 0.38650000000005497 loss_dqn tensor(3.1403e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09448885917663574
dqn reward tensor(-231.6250, device='cuda:0') e 0.38600000000005497 loss_dqn tensor(1.7849e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06797908991575241
dqn reward tensor(-388.8750, device='cuda:0') e 0.38550000000005497 loss_dqn tensor(1.7942e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1402253657579422
dqn reward tensor(-397.1250, device='cuda:0') e 0.38500000000005496 loss_dqn tensor(3.4444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11167047917842865
dqn reward tensor(-98.5000, device='cuda:0') e 0.38450000000005496 loss_dqn tensor(2.8408e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4338487684726715
dqn reward tensor(-387.5000, device='cuda:0') e 0.38400000000005496 loss_dqn tensor(2.1658e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13936640322208405
dqn reward tensor(-123.5000, device='cuda:0') e 0.38350000000005496 loss_dqn tensor(2.5265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0848274901509285
dqn reward tensor(-261.1250, device='cuda:0') e 0.38300000000005496 loss_dqn tensor(4.0794e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2155815064907074
dqn reward tensor(-384.3750, device='cuda:0') e 0.38250000000005496 loss_dqn tensor(3.9695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4130799472332001
dqn reward tensor(-598.2500, device='cuda:0') e 0.38200000000005496 loss_dqn tensor(3.3872e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2143242359161377
dqn reward tensor(-314.8750, device='cuda:0') e 0.38150000000005496 loss_dqn tensor(2.5802e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2686707377433777
dqn reward tensor(-182.3125, device='cuda:0') e 0.38100000000005496 loss_dqn tensor(3.6323e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17680484056472778
dqn reward tensor(-295.8750, device='cuda:0') e 0.38050000000005496 loss_dqn tensor(2.3673e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1686418652534485
dqn reward tensor(-208.7500, device='cuda:0') e 0.38000000000005496 loss_dqn tensor(2.2246e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2218264490365982
dqn reward tensor(-315.6250, device='cuda:0') e 0.37950000000005496 loss_dqn tensor(2.5613e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09328289330005646
dqn reward tensor(-313.5000, device='cuda:0') e 0.37900000000005496 loss_dqn tensor(4.6429e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17196960747241974
dqn reward tensor(-370.7500, device='cuda:0') e 0.37850000000005496 loss_dqn tensor(3.0761e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2146490514278412
dqn reward tensor(-254.5000, device='cuda:0') e 0.37800000000005496 loss_dqn tensor(2.8796e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17496445775032043
dqn reward tensor(-265.3750, device='cuda:0') e 0.37750000000005496 loss_dqn tensor(4.8141e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10899288952350616
dqn reward tensor(-274.1875, device='cuda:0') e 0.37700000000005496 loss_dqn tensor(4.0714e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18019133806228638
dqn reward tensor(-377.6250, device='cuda:0') e 0.37650000000005496 loss_dqn tensor(4.3801e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01923844777047634
dqn reward tensor(-283.2500, device='cuda:0') e 0.37600000000005496 loss_dqn tensor(4.7933e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07610557228326797
dqn reward tensor(-162.1250, device='cuda:0') e 0.37550000000005496 loss_dqn tensor(3.6386e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014634652063250542
dqn reward tensor(-549.3750, device='cuda:0') e 0.37500000000005496 loss_dqn tensor(3.3969e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10311602056026459
dqn reward tensor(-459.7500, device='cuda:0') e 0.37450000000005496 loss_dqn tensor(2.3984e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23997104167938232
dqn reward tensor(-377.5625, device='cuda:0') e 0.37400000000005496 loss_dqn tensor(2.3287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07174291461706161
dqn reward tensor(-228.1250, device='cuda:0') e 0.37350000000005495 loss_dqn tensor(4.7310e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15821191668510437
dqn reward tensor(-426.7500, device='cuda:0') e 0.37300000000005495 loss_dqn tensor(4.6695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1560058444738388
dqn reward tensor(-259.2500, device='cuda:0') e 0.37250000000005495 loss_dqn tensor(4.6520e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329253613948822
dqn reward tensor(-352.1875, device='cuda:0') e 0.37200000000005495 loss_dqn tensor(2.9621e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09631779044866562
dqn reward tensor(-346.5000, device='cuda:0') e 0.37150000000005495 loss_dqn tensor(2.7054e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20359757542610168
dqn reward tensor(-217.1250, device='cuda:0') e 0.37100000000005495 loss_dqn tensor(3.9002e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11996296048164368
dqn reward tensor(-321.2500, device='cuda:0') e 0.37050000000005495 loss_dqn tensor(2.6829e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10295286774635315
dqn reward tensor(-280.2500, device='cuda:0') e 0.37000000000005495 loss_dqn tensor(4.1651e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24011541903018951
dqn reward tensor(-504.7500, device='cuda:0') e 0.36950000000005495 loss_dqn tensor(3.4294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09605729579925537
dqn reward tensor(-363.8125, device='cuda:0') e 0.36900000000005495 loss_dqn tensor(2.9404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941969394683838
dqn reward tensor(-337.2500, device='cuda:0') e 0.36850000000005495 loss_dqn tensor(4.4108e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30396413803100586
dqn reward tensor(-258.8750, device='cuda:0') e 0.36800000000005495 loss_dqn tensor(4.3012e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06428740918636322
dqn reward tensor(-240.7500, device='cuda:0') e 0.36750000000005495 loss_dqn tensor(4.5362e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15026310086250305
dqn reward tensor(-138., device='cuda:0') e 0.36700000000005495 loss_dqn tensor(3.3095e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365372359752655
dqn reward tensor(-352.1875, device='cuda:0') e 0.36650000000005495 loss_dqn tensor(4.8393e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28063225746154785
dqn reward tensor(-203.7500, device='cuda:0') e 0.36600000000005495 loss_dqn tensor(2.9292e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16359460353851318
dqn reward tensor(-212., device='cuda:0') e 0.36550000000005495 loss_dqn tensor(2.5101e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04651184752583504
dqn reward tensor(-269.3750, device='cuda:0') e 0.36500000000005495 loss_dqn tensor(3.8574e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19476920366287231
dqn reward tensor(-224.2500, device='cuda:0') e 0.36450000000005495 loss_dqn tensor(4.2982e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09278717637062073
dqn reward tensor(-380., device='cuda:0') e 0.36400000000005495 loss_dqn tensor(3.6234e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08934522420167923
dqn reward tensor(-319.5000, device='cuda:0') e 0.36350000000005495 loss_dqn tensor(4.3027e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022959858179092407
dqn reward tensor(-45.7500, device='cuda:0') e 0.36300000000005495 loss_dqn tensor(4.3235e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2355937957763672
dqn reward tensor(-221.6250, device='cuda:0') e 0.36250000000005494 loss_dqn tensor(4.0420e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.142904132604599
dqn reward tensor(-237.3750, device='cuda:0') e 0.36200000000005494 loss_dqn tensor(4.6468e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14176468551158905
dqn reward tensor(-191.8750, device='cuda:0') e 0.36150000000005494 loss_dqn tensor(4.5659e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14654996991157532
dqn reward tensor(-373.1250, device='cuda:0') e 0.36100000000005494 loss_dqn tensor(3.3515e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17501410841941833
dqn reward tensor(-162.7500, device='cuda:0') e 0.36050000000005494 loss_dqn tensor(3.8299e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23798474669456482
dqn reward tensor(-313., device='cuda:0') e 0.36000000000005494 loss_dqn tensor(3.0652e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2631545066833496
dqn reward tensor(-253.2500, device='cuda:0') e 0.35950000000005494 loss_dqn tensor(4.2354e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13996446132659912
dqn reward tensor(-146.1250, device='cuda:0') e 0.35900000000005494 loss_dqn tensor(4.7409e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16581863164901733
dqn reward tensor(-282.1250, device='cuda:0') e 0.35850000000005494 loss_dqn tensor(3.6823e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22166116535663605
dqn reward tensor(-259.1875, device='cuda:0') e 0.35800000000005494 loss_dqn tensor(2.9377e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14625513553619385
dqn reward tensor(-369., device='cuda:0') e 0.35750000000005494 loss_dqn tensor(2.9826e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13662612438201904
dqn reward tensor(-315.8750, device='cuda:0') e 0.35700000000005494 loss_dqn tensor(4.4813e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1495516449213028
dqn reward tensor(-184.7500, device='cuda:0') e 0.35650000000005494 loss_dqn tensor(4.5343e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22073468565940857
dqn reward tensor(-311.3125, device='cuda:0') e 0.35600000000005494 loss_dqn tensor(2.9896e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2576054334640503
dqn reward tensor(-262.2500, device='cuda:0') e 0.35550000000005494 loss_dqn tensor(5.2457e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16503095626831055
dqn reward tensor(-300.3750, device='cuda:0') e 0.35500000000005494 loss_dqn tensor(3.4741e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08123631030321121
dqn reward tensor(-425., device='cuda:0') e 0.35450000000005494 loss_dqn tensor(3.6747e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603882759809494
dqn reward tensor(-250.2500, device='cuda:0') e 0.35400000000005494 loss_dqn tensor(3.4274e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17296233773231506
dqn reward tensor(-281.1875, device='cuda:0') e 0.35350000000005494 loss_dqn tensor(3.3559e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07732346653938293
dqn reward tensor(-207.6250, device='cuda:0') e 0.35300000000005494 loss_dqn tensor(3.3128e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11874184757471085
dqn reward tensor(-79.8750, device='cuda:0') e 0.35250000000005494 loss_dqn tensor(3.2419e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16498856246471405
dqn reward tensor(-156.6250, device='cuda:0') e 0.35200000000005494 loss_dqn tensor(4.5962e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13695421814918518
dqn reward tensor(6.7500, device='cuda:0') e 0.35150000000005494 loss_dqn tensor(6.1895e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13065865635871887
dqn reward tensor(-335., device='cuda:0') e 0.35100000000005493 loss_dqn tensor(3.7085e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06218252331018448
dqn reward tensor(-354.3125, device='cuda:0') e 0.35050000000005493 loss_dqn tensor(4.4761e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24651393294334412
dqn reward tensor(-159.1250, device='cuda:0') e 0.35000000000005493 loss_dqn tensor(3.2961e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19252626597881317
dqn reward tensor(-262.1250, device='cuda:0') e 0.34950000000005493 loss_dqn tensor(4.3604e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06796245276927948
dqn reward tensor(-482.8750, device='cuda:0') e 0.34900000000005493 loss_dqn tensor(4.3264e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029927270486950874
dqn reward tensor(-272.8125, device='cuda:0') e 0.34850000000005493 loss_dqn tensor(3.0610e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14951454102993011
dqn reward tensor(-113.5000, device='cuda:0') e 0.34800000000005493 loss_dqn tensor(4.5832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27047640085220337
dqn reward tensor(-314.1250, device='cuda:0') e 0.34750000000005493 loss_dqn tensor(3.7873e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23875924944877625
dqn reward tensor(-268.7500, device='cuda:0') e 0.34700000000005493 loss_dqn tensor(3.7642e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23339033126831055
dqn reward tensor(-307.3125, device='cuda:0') e 0.34650000000005493 loss_dqn tensor(4.0832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14496047794818878
dqn reward tensor(-218.3125, device='cuda:0') e 0.34600000000005493 loss_dqn tensor(3.3107e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20870700478553772
dqn reward tensor(-244.1250, device='cuda:0') e 0.34550000000005493 loss_dqn tensor(4.0728e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15986846387386322
dqn reward tensor(-379., device='cuda:0') e 0.34500000000005493 loss_dqn tensor(3.3250e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08804646879434586
dqn reward tensor(-361.6250, device='cuda:0') e 0.34450000000005493 loss_dqn tensor(3.3580e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05707523971796036
dqn reward tensor(-334.3125, device='cuda:0') e 0.34400000000005493 loss_dqn tensor(3.6409e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27948734164237976
dqn reward tensor(-199.5000, device='cuda:0') e 0.34350000000005493 loss_dqn tensor(3.9031e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15220116078853607
dqn reward tensor(-319.3750, device='cuda:0') e 0.3430000000000549 loss_dqn tensor(5.1932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34758567810058594
dqn reward tensor(-211.7500, device='cuda:0') e 0.3425000000000549 loss_dqn tensor(4.0387e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12062050402164459
dqn reward tensor(-111.3750, device='cuda:0') e 0.3420000000000549 loss_dqn tensor(4.6309e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17542201280593872
dqn reward tensor(-321.6250, device='cuda:0') e 0.3415000000000549 loss_dqn tensor(3.8210e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18317967653274536
dqn reward tensor(-157.7500, device='cuda:0') e 0.3410000000000549 loss_dqn tensor(5.5184e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08704125881195068
dqn reward tensor(-363.6875, device='cuda:0') e 0.3405000000000549 loss_dqn tensor(4.2051e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04859533905982971
dqn reward tensor(-273.8125, device='cuda:0') e 0.3400000000000549 loss_dqn tensor(3.5227e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14402201771736145
dqn reward tensor(-247.7500, device='cuda:0') e 0.3395000000000549 loss_dqn tensor(2.9799e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13417519629001617
dqn reward tensor(-424.8750, device='cuda:0') e 0.3390000000000549 loss_dqn tensor(3.7102e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3194422721862793
dqn reward tensor(-331.4375, device='cuda:0') e 0.3385000000000549 loss_dqn tensor(3.6868e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18416441977024078
dqn reward tensor(-274.1250, device='cuda:0') e 0.3380000000000549 loss_dqn tensor(3.0647e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03772487863898277
dqn reward tensor(-207.1250, device='cuda:0') e 0.3375000000000549 loss_dqn tensor(3.5777e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14590057730674744
dqn reward tensor(-431.1250, device='cuda:0') e 0.3370000000000549 loss_dqn tensor(4.4381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12936778366565704
dqn reward tensor(-229.6250, device='cuda:0') e 0.3365000000000549 loss_dqn tensor(3.0410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13353048264980316
dqn reward tensor(-291.9375, device='cuda:0') e 0.3360000000000549 loss_dqn tensor(4.4956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05285872146487236
dqn reward tensor(-239.6250, device='cuda:0') e 0.3355000000000549 loss_dqn tensor(3.6119e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.39160263538360596
dqn reward tensor(-284., device='cuda:0') e 0.3350000000000549 loss_dqn tensor(5.3310e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12130951136350632
dqn reward tensor(-318.7500, device='cuda:0') e 0.3345000000000549 loss_dqn tensor(3.6975e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16564413905143738
dqn reward tensor(-147.7500, device='cuda:0') e 0.3340000000000549 loss_dqn tensor(4.4205e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21795761585235596
dqn reward tensor(-362.1250, device='cuda:0') e 0.3335000000000549 loss_dqn tensor(4.4643e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15513120591640472
dqn reward tensor(-257.2500, device='cuda:0') e 0.3330000000000549 loss_dqn tensor(3.5167e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09064633399248123
dqn reward tensor(-168.1250, device='cuda:0') e 0.3325000000000549 loss_dqn tensor(5.8783e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06373581290245056
dqn reward tensor(-211.2500, device='cuda:0') e 0.3320000000000549 loss_dqn tensor(3.9966e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1027146726846695
dqn reward tensor(-129.2500, device='cuda:0') e 0.3315000000000549 loss_dqn tensor(6.0705e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08414118736982346
dqn reward tensor(-387.8750, device='cuda:0') e 0.3310000000000549 loss_dqn tensor(3.2626e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2701093554496765
dqn reward tensor(-236., device='cuda:0') e 0.3305000000000549 loss_dqn tensor(3.7102e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03359439969062805
dqn reward tensor(-237.6875, device='cuda:0') e 0.3300000000000549 loss_dqn tensor(3.2182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11331609636545181
dqn reward tensor(-279.8750, device='cuda:0') e 0.3295000000000549 loss_dqn tensor(3.4287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.079625703394413
dqn reward tensor(-299.1250, device='cuda:0') e 0.3290000000000549 loss_dqn tensor(5.1838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13015498220920563
dqn reward tensor(-162.6250, device='cuda:0') e 0.3285000000000549 loss_dqn tensor(4.6639e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06965567916631699
dqn reward tensor(-49.1250, device='cuda:0') e 0.3280000000000549 loss_dqn tensor(4.0047e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16921129822731018
dqn reward tensor(-100.3750, device='cuda:0') e 0.3275000000000549 loss_dqn tensor(3.8698e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16189585626125336
dqn reward tensor(-173.6250, device='cuda:0') e 0.3270000000000549 loss_dqn tensor(4.3430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07400226593017578
dqn reward tensor(-216.7500, device='cuda:0') e 0.3265000000000549 loss_dqn tensor(4.6283e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1433270275592804
dqn reward tensor(-284.8125, device='cuda:0') e 0.3260000000000549 loss_dqn tensor(4.0871e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38071486353874207
dqn reward tensor(-226.7500, device='cuda:0') e 0.3255000000000549 loss_dqn tensor(3.2033e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10349994897842407
dqn reward tensor(-265.6250, device='cuda:0') e 0.3250000000000549 loss_dqn tensor(3.1748e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040805235505104065
dqn reward tensor(-299.7500, device='cuda:0') e 0.3245000000000549 loss_dqn tensor(4.7093e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051155250519514084
dqn reward tensor(-292., device='cuda:0') e 0.3240000000000549 loss_dqn tensor(3.4650e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0934733897447586
dqn reward tensor(-335.3750, device='cuda:0') e 0.3235000000000549 loss_dqn tensor(4.2976e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2132100760936737
dqn reward tensor(-303.8750, device='cuda:0') e 0.3230000000000549 loss_dqn tensor(3.5610e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053769636899232864
dqn reward tensor(-252.6250, device='cuda:0') e 0.3225000000000549 loss_dqn tensor(2.8670e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19548386335372925
dqn reward tensor(-143.2500, device='cuda:0') e 0.3220000000000549 loss_dqn tensor(4.3450e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17066408693790436
dqn reward tensor(-220.2500, device='cuda:0') e 0.3215000000000549 loss_dqn tensor(3.6793e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22134801745414734
dqn reward tensor(-218.2500, device='cuda:0') e 0.3210000000000549 loss_dqn tensor(5.0761e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18851271271705627
dqn reward tensor(-391.3750, device='cuda:0') e 0.3205000000000549 loss_dqn tensor(5.1008e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13828045129776
dqn reward tensor(-282., device='cuda:0') e 0.3200000000000549 loss_dqn tensor(3.6285e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10969381034374237
dqn reward tensor(-296.3750, device='cuda:0') e 0.3195000000000549 loss_dqn tensor(3.9372e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023108743131160736
dqn reward tensor(-296.5625, device='cuda:0') e 0.3190000000000549 loss_dqn tensor(3.4888e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029212575405836105
dqn reward tensor(-43.3125, device='cuda:0') e 0.3185000000000549 loss_dqn tensor(4.9703e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02884836494922638
dqn reward tensor(-196.7500, device='cuda:0') e 0.3180000000000549 loss_dqn tensor(3.4179e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04034387320280075
dqn reward tensor(-290.7500, device='cuda:0') e 0.3175000000000549 loss_dqn tensor(3.3350e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05864045396447182
dqn reward tensor(-9.6250, device='cuda:0') e 0.3170000000000549 loss_dqn tensor(5.3226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2810038924217224
dqn reward tensor(-285.2500, device='cuda:0') e 0.3165000000000549 loss_dqn tensor(3.2710e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20342257618904114
dqn reward tensor(-374.5000, device='cuda:0') e 0.3160000000000549 loss_dqn tensor(5.5800e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17615507543087006
dqn reward tensor(-142., device='cuda:0') e 0.3155000000000549 loss_dqn tensor(4.8397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056804679334163666
dqn reward tensor(-282.5000, device='cuda:0') e 0.3150000000000549 loss_dqn tensor(4.7862e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19214074313640594
dqn reward tensor(-316.1250, device='cuda:0') e 0.3145000000000549 loss_dqn tensor(3.4614e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03689155727624893
dqn reward tensor(-202.6250, device='cuda:0') e 0.3140000000000549 loss_dqn tensor(4.3101e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2096726894378662
dqn reward tensor(-245., device='cuda:0') e 0.3135000000000549 loss_dqn tensor(4.7250e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15241532027721405
dqn reward tensor(-264.7500, device='cuda:0') e 0.3130000000000549 loss_dqn tensor(4.0957e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2697281837463379
dqn reward tensor(-369.7500, device='cuda:0') e 0.3125000000000549 loss_dqn tensor(3.7979e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13005216419696808
dqn reward tensor(-294.8750, device='cuda:0') e 0.3120000000000549 loss_dqn tensor(4.6901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046250686049461365
dqn reward tensor(-354.2500, device='cuda:0') e 0.3115000000000549 loss_dqn tensor(4.4153e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09797555208206177
dqn reward tensor(-247.7500, device='cuda:0') e 0.3110000000000549 loss_dqn tensor(4.8449e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2516637146472931
dqn reward tensor(-381.3750, device='cuda:0') e 0.3105000000000549 loss_dqn tensor(3.7607e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11105945706367493
dqn reward tensor(-303.2500, device='cuda:0') e 0.3100000000000549 loss_dqn tensor(4.1088e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14272984862327576
dqn reward tensor(-315.7500, device='cuda:0') e 0.3095000000000549 loss_dqn tensor(3.7573e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19229131937026978
dqn reward tensor(-330.8750, device='cuda:0') e 0.3090000000000549 loss_dqn tensor(5.1668e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13675272464752197
dqn reward tensor(-283.1250, device='cuda:0') e 0.3085000000000549 loss_dqn tensor(5.5062e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19047732651233673
dqn reward tensor(-191.7500, device='cuda:0') e 0.3080000000000549 loss_dqn tensor(3.1519e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11046729236841202
dqn reward tensor(-363.2500, device='cuda:0') e 0.3075000000000549 loss_dqn tensor(4.7004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15742947161197662
dqn reward tensor(-183., device='cuda:0') e 0.3070000000000549 loss_dqn tensor(3.8045e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03291825205087662
dqn reward tensor(-253.2500, device='cuda:0') e 0.3065000000000549 loss_dqn tensor(5.8030e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2274741232395172
dqn reward tensor(-278.5000, device='cuda:0') e 0.3060000000000549 loss_dqn tensor(4.8496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0999513640999794
dqn reward tensor(-334., device='cuda:0') e 0.3055000000000549 loss_dqn tensor(3.3322e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14074701070785522
dqn reward tensor(-217.3750, device='cuda:0') e 0.3050000000000549 loss_dqn tensor(4.7111e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03773113340139389
dqn reward tensor(-354.7500, device='cuda:0') e 0.3045000000000549 loss_dqn tensor(5.0030e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06099111586809158
dqn reward tensor(-324.1250, device='cuda:0') e 0.3040000000000549 loss_dqn tensor(4.5404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1504644751548767
dqn reward tensor(-267.7500, device='cuda:0') e 0.3035000000000549 loss_dqn tensor(4.6075e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11884776502847672
dqn reward tensor(-308.7500, device='cuda:0') e 0.3030000000000549 loss_dqn tensor(4.4205e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.116298608481884
dqn reward tensor(-364.1250, device='cuda:0') e 0.3025000000000549 loss_dqn tensor(5.1387e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159725159406662
dqn reward tensor(-139.0625, device='cuda:0') e 0.3020000000000549 loss_dqn tensor(3.8684e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3780236840248108
dqn reward tensor(-14.2500, device='cuda:0') e 0.3015000000000549 loss_dqn tensor(4.7851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18837250769138336
dqn reward tensor(-354.2500, device='cuda:0') e 0.3010000000000549 loss_dqn tensor(4.2224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14197495579719543
dqn reward tensor(-167.1250, device='cuda:0') e 0.3005000000000549 loss_dqn tensor(3.8337e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09860774874687195
dqn reward tensor(-260.8750, device='cuda:0') e 0.3000000000000549 loss_dqn tensor(4.1645e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05430042743682861
dqn reward tensor(-383., device='cuda:0') e 0.2995000000000549 loss_dqn tensor(3.7795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1853477954864502
dqn reward tensor(-355.2500, device='cuda:0') e 0.2990000000000549 loss_dqn tensor(4.7447e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24368542432785034
dqn reward tensor(-244.2500, device='cuda:0') e 0.2985000000000549 loss_dqn tensor(5.3256e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10795725882053375
dqn reward tensor(-345.3750, device='cuda:0') e 0.2980000000000549 loss_dqn tensor(4.2761e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0599076971411705
dqn reward tensor(-324.6250, device='cuda:0') e 0.2975000000000549 loss_dqn tensor(4.1851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15603163838386536
dqn reward tensor(-237.3750, device='cuda:0') e 0.2970000000000549 loss_dqn tensor(5.1953e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17375773191452026
dqn reward tensor(-264.1250, device='cuda:0') e 0.2965000000000549 loss_dqn tensor(3.9355e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1136043518781662
dqn reward tensor(-234.1250, device='cuda:0') e 0.2960000000000549 loss_dqn tensor(3.2419e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15133264660835266
dqn reward tensor(-253.8750, device='cuda:0') e 0.2955000000000549 loss_dqn tensor(5.2213e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23803240060806274
dqn reward tensor(-271.5000, device='cuda:0') e 0.2950000000000549 loss_dqn tensor(3.8810e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02842843532562256
dqn reward tensor(-382.3750, device='cuda:0') e 0.2945000000000549 loss_dqn tensor(3.6289e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08117644488811493
dqn reward tensor(-408.1250, device='cuda:0') e 0.2940000000000549 loss_dqn tensor(4.5950e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15112310647964478
dqn reward tensor(-312.5625, device='cuda:0') e 0.2935000000000549 loss_dqn tensor(4.5665e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16915984451770782
dqn reward tensor(-180.2500, device='cuda:0') e 0.2930000000000549 loss_dqn tensor(4.9396e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2289569228887558
dqn reward tensor(-227.8750, device='cuda:0') e 0.2925000000000549 loss_dqn tensor(3.7127e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15992388129234314
dqn reward tensor(-202.8750, device='cuda:0') e 0.2920000000000549 loss_dqn tensor(5.1639e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3138599395751953
dqn reward tensor(-314.8750, device='cuda:0') e 0.2915000000000549 loss_dqn tensor(4.1741e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2459488809108734
dqn reward tensor(-253.6250, device='cuda:0') e 0.2910000000000549 loss_dqn tensor(4.3499e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12796080112457275
dqn reward tensor(-372.1250, device='cuda:0') e 0.2905000000000549 loss_dqn tensor(3.8591e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1483595073223114
dqn reward tensor(-186.3750, device='cuda:0') e 0.2900000000000549 loss_dqn tensor(4.1366e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20108452439308167
dqn reward tensor(-222., device='cuda:0') e 0.2895000000000549 loss_dqn tensor(3.4230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20109304785728455
dqn reward tensor(-205.7500, device='cuda:0') e 0.2890000000000549 loss_dqn tensor(3.2840e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1667822003364563
dqn reward tensor(-254.3750, device='cuda:0') e 0.2885000000000549 loss_dqn tensor(3.6317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13698694109916687
dqn reward tensor(-286.6250, device='cuda:0') e 0.2880000000000549 loss_dqn tensor(3.5296e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1013650968670845
dqn reward tensor(-295.9375, device='cuda:0') e 0.2875000000000549 loss_dqn tensor(2.7170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19893693923950195
dqn reward tensor(-167.5000, device='cuda:0') e 0.2870000000000549 loss_dqn tensor(3.5434e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15970344841480255
dqn reward tensor(-386.1250, device='cuda:0') e 0.2865000000000549 loss_dqn tensor(2.4468e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18766754865646362
dqn reward tensor(-305.1250, device='cuda:0') e 0.2860000000000549 loss_dqn tensor(2.2994e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09836447238922119
dqn reward tensor(-382.4375, device='cuda:0') e 0.2855000000000549 loss_dqn tensor(1.8293e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14363625645637512
dqn reward tensor(-259.5000, device='cuda:0') e 0.2850000000000549 loss_dqn tensor(2.0932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13887757062911987
dqn reward tensor(-391.8750, device='cuda:0') e 0.2845000000000549 loss_dqn tensor(2.6163e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10569034516811371
dqn reward tensor(-375.8750, device='cuda:0') e 0.2840000000000549 loss_dqn tensor(2.2414e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07346997410058975
dqn reward tensor(-375.5000, device='cuda:0') e 0.2835000000000549 loss_dqn tensor(2.1502e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3275926113128662
dqn reward tensor(-236.5000, device='cuda:0') e 0.2830000000000549 loss_dqn tensor(1.9783e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1339149922132492
dqn reward tensor(-317.3750, device='cuda:0') e 0.2825000000000549 loss_dqn tensor(1.4040e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06381401419639587
dqn reward tensor(-325.7500, device='cuda:0') e 0.2820000000000549 loss_dqn tensor(1.8175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12367445975542068
dqn reward tensor(-339.1250, device='cuda:0') e 0.2815000000000549 loss_dqn tensor(1.5142e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08624918013811111
dqn reward tensor(-317.8750, device='cuda:0') e 0.2810000000000549 loss_dqn tensor(1.5430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2581983804702759
dqn reward tensor(-202.8750, device='cuda:0') e 0.28050000000005487 loss_dqn tensor(2.2149e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13670271635055542
dqn reward tensor(-392.2500, device='cuda:0') e 0.28000000000005487 loss_dqn tensor(1.7746e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1542987823486328
dqn reward tensor(-384.8750, device='cuda:0') e 0.27950000000005487 loss_dqn tensor(2.2068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22554904222488403
dqn reward tensor(-470., device='cuda:0') e 0.27900000000005487 loss_dqn tensor(1.5313e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18571576476097107
dqn reward tensor(-277.0625, device='cuda:0') e 0.27850000000005487 loss_dqn tensor(1.5358e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11033172160387039
dqn reward tensor(-296.9375, device='cuda:0') e 0.27800000000005487 loss_dqn tensor(1.8327e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08630847930908203
dqn reward tensor(-153.9375, device='cuda:0') e 0.27750000000005487 loss_dqn tensor(2.1967e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1567966639995575
dqn reward tensor(-294.6250, device='cuda:0') e 0.27700000000005487 loss_dqn tensor(1.8114e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11081025004386902
dqn reward tensor(-358.5000, device='cuda:0') e 0.27650000000005487 loss_dqn tensor(1.9198e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17258583009243011
dqn reward tensor(-332.3750, device='cuda:0') e 0.27600000000005487 loss_dqn tensor(2.2671e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028186462819576263
dqn reward tensor(-311.1875, device='cuda:0') e 0.27550000000005487 loss_dqn tensor(2.1407e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13221590220928192
dqn reward tensor(-242.6250, device='cuda:0') e 0.27500000000005487 loss_dqn tensor(2.1514e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20184919238090515
dqn reward tensor(-171.3750, device='cuda:0') e 0.27450000000005487 loss_dqn tensor(2.0494e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02011522650718689
dqn reward tensor(-199.1875, device='cuda:0') e 0.27400000000005487 loss_dqn tensor(2.3913e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12463614344596863
dqn reward tensor(-252., device='cuda:0') e 0.27350000000005487 loss_dqn tensor(2.6515e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.272285521030426
dqn reward tensor(-310.2500, device='cuda:0') e 0.27300000000005487 loss_dqn tensor(3.1363e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013339174911379814
dqn reward tensor(-411.3750, device='cuda:0') e 0.27250000000005487 loss_dqn tensor(2.6085e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3438684344291687
dqn reward tensor(-265.6250, device='cuda:0') e 0.27200000000005486 loss_dqn tensor(2.5348e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13487309217453003
dqn reward tensor(-335.1250, device='cuda:0') e 0.27150000000005486 loss_dqn tensor(2.9838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13655701279640198
dqn reward tensor(-262.3750, device='cuda:0') e 0.27100000000005486 loss_dqn tensor(2.2300e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032935015857219696
dqn reward tensor(-332.7500, device='cuda:0') e 0.27050000000005486 loss_dqn tensor(4.1214e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16008144617080688
dqn reward tensor(-275., device='cuda:0') e 0.27000000000005486 loss_dqn tensor(3.6194e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25247055292129517
dqn reward tensor(-281.8750, device='cuda:0') e 0.26950000000005486 loss_dqn tensor(3.3337e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14299029111862183
dqn reward tensor(-389.1875, device='cuda:0') e 0.26900000000005486 loss_dqn tensor(3.2341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13818776607513428
dqn reward tensor(-211.3750, device='cuda:0') e 0.26850000000005486 loss_dqn tensor(3.5821e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1406286209821701
dqn reward tensor(-321.0625, device='cuda:0') e 0.26800000000005486 loss_dqn tensor(3.8814e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1903952956199646
dqn reward tensor(-29., device='cuda:0') e 0.26750000000005486 loss_dqn tensor(4.3110e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2861984968185425
dqn reward tensor(-270.1250, device='cuda:0') e 0.26700000000005486 loss_dqn tensor(4.4832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11924272030591965
dqn reward tensor(-347.6875, device='cuda:0') e 0.26650000000005486 loss_dqn tensor(3.7965e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19304513931274414
dqn reward tensor(-319.2500, device='cuda:0') e 0.26600000000005486 loss_dqn tensor(3.3998e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11373596638441086
dqn reward tensor(-105.2500, device='cuda:0') e 0.26550000000005486 loss_dqn tensor(5.8216e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10120517015457153
dqn reward tensor(-273.2500, device='cuda:0') e 0.26500000000005486 loss_dqn tensor(6.2749e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.179365873336792
dqn reward tensor(-330.3750, device='cuda:0') e 0.26450000000005486 loss_dqn tensor(5.8474e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3327793478965759
dqn reward tensor(-278.8750, device='cuda:0') e 0.26400000000005486 loss_dqn tensor(5.1620e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1047530323266983
dqn reward tensor(-225.5000, device='cuda:0') e 0.26350000000005486 loss_dqn tensor(4.8517e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20963159203529358
dqn reward tensor(-324.6250, device='cuda:0') e 0.26300000000005486 loss_dqn tensor(5.9478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13581183552742004
dqn reward tensor(-114.2500, device='cuda:0') e 0.26250000000005486 loss_dqn tensor(6.7584e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11001509428024292
dqn reward tensor(-203.2500, device='cuda:0') e 0.26200000000005486 loss_dqn tensor(5.9172e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10938449949026108
dqn reward tensor(-349., device='cuda:0') e 0.26150000000005486 loss_dqn tensor(5.4425e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06788738071918488
dqn reward tensor(-443.8750, device='cuda:0') e 0.26100000000005485 loss_dqn tensor(6.2212e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376609206199646
dqn reward tensor(-176.6250, device='cuda:0') e 0.26050000000005485 loss_dqn tensor(5.7199e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19942493736743927
dqn reward tensor(-181.9375, device='cuda:0') e 0.26000000000005485 loss_dqn tensor(5.2836e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14427511394023895
dqn reward tensor(-176.8750, device='cuda:0') e 0.25950000000005485 loss_dqn tensor(6.1377e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11247776448726654
dqn reward tensor(-307.1250, device='cuda:0') e 0.25900000000005485 loss_dqn tensor(4.5278e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18710599839687347
dqn reward tensor(-250.1875, device='cuda:0') e 0.25850000000005485 loss_dqn tensor(5.3322e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19837096333503723
dqn reward tensor(-298.3750, device='cuda:0') e 0.25800000000005485 loss_dqn tensor(5.9523e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07041424512863159
dqn reward tensor(-320.5000, device='cuda:0') e 0.25750000000005485 loss_dqn tensor(5.2153e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.267037034034729
dqn reward tensor(-153.8750, device='cuda:0') e 0.25700000000005485 loss_dqn tensor(4.2636e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19645129144191742
dqn reward tensor(-368.6250, device='cuda:0') e 0.25650000000005485 loss_dqn tensor(4.2780e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16057509183883667
dqn reward tensor(-412.2500, device='cuda:0') e 0.25600000000005485 loss_dqn tensor(4.6984e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0928945243358612
dqn reward tensor(-338.6250, device='cuda:0') e 0.25550000000005485 loss_dqn tensor(4.7279e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08730453997850418
dqn reward tensor(-204.5000, device='cuda:0') e 0.25500000000005485 loss_dqn tensor(5.5690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05875222384929657
dqn reward tensor(-260., device='cuda:0') e 0.25450000000005485 loss_dqn tensor(4.4896e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039398543536663055
dqn reward tensor(-310.1250, device='cuda:0') e 0.25400000000005485 loss_dqn tensor(4.2176e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1303415596485138
dqn reward tensor(-175.8750, device='cuda:0') e 0.25350000000005485 loss_dqn tensor(4.4629e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040117230266332626
dqn reward tensor(-326.8750, device='cuda:0') e 0.25300000000005485 loss_dqn tensor(4.6327e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09226217120885849
dqn reward tensor(-86.2500, device='cuda:0') e 0.25250000000005485 loss_dqn tensor(4.5423e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25681063532829285
dqn reward tensor(-61.5000, device='cuda:0') e 0.25200000000005485 loss_dqn tensor(3.8956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.00845317356288433
dqn reward tensor(-130.8750, device='cuda:0') e 0.25150000000005485 loss_dqn tensor(5.2521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2214299589395523
dqn reward tensor(-315.6875, device='cuda:0') e 0.25100000000005485 loss_dqn tensor(4.5771e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26143208146095276
dqn reward tensor(-230.5000, device='cuda:0') e 0.25050000000005485 loss_dqn tensor(4.6997e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09367454051971436
dqn reward tensor(-315.7500, device='cuda:0') e 0.25000000000005485 loss_dqn tensor(4.5195e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09293776750564575
dqn reward tensor(-243.5000, device='cuda:0') e 0.24950000000005484 loss_dqn tensor(3.8525e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051092728972435
dqn reward tensor(-176.1250, device='cuda:0') e 0.24900000000005484 loss_dqn tensor(4.6536e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09574715793132782
dqn reward tensor(-172.1250, device='cuda:0') e 0.24850000000005484 loss_dqn tensor(4.5740e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016276217997074127
dqn reward tensor(-262.7500, device='cuda:0') e 0.24800000000005484 loss_dqn tensor(4.5879e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020821433514356613
dqn reward tensor(-349.6250, device='cuda:0') e 0.24750000000005484 loss_dqn tensor(3.4372e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20225493609905243
dqn reward tensor(-259.2500, device='cuda:0') e 0.24700000000005484 loss_dqn tensor(4.0997e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028153223916888237
dqn reward tensor(-195.2500, device='cuda:0') e 0.24650000000005484 loss_dqn tensor(4.3294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3332550525665283
dqn reward tensor(-248., device='cuda:0') e 0.24600000000005484 loss_dqn tensor(4.4502e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07859081029891968
dqn reward tensor(-276., device='cuda:0') e 0.24550000000005484 loss_dqn tensor(3.8655e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08974135667085648
dqn reward tensor(-196.8750, device='cuda:0') e 0.24500000000005484 loss_dqn tensor(3.7052e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1491464078426361
dqn reward tensor(-297.5625, device='cuda:0') e 0.24450000000005484 loss_dqn tensor(4.1873e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09431973099708557
dqn reward tensor(-372.3750, device='cuda:0') e 0.24400000000005484 loss_dqn tensor(4.2364e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2555949091911316
dqn reward tensor(-231., device='cuda:0') e 0.24350000000005484 loss_dqn tensor(3.9940e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08719849586486816
dqn reward tensor(-116.3750, device='cuda:0') e 0.24300000000005484 loss_dqn tensor(3.8224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11477549374103546
dqn reward tensor(-192.3750, device='cuda:0') e 0.24250000000005484 loss_dqn tensor(3.8453e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24276213347911835
dqn reward tensor(-279., device='cuda:0') e 0.24200000000005484 loss_dqn tensor(3.9397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28441059589385986
dqn reward tensor(-299., device='cuda:0') e 0.24150000000005484 loss_dqn tensor(4.5234e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290280967950821
dqn reward tensor(-323.2500, device='cuda:0') e 0.24100000000005484 loss_dqn tensor(4.2507e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12826848030090332
dqn reward tensor(-160.6250, device='cuda:0') e 0.24050000000005484 loss_dqn tensor(3.4052e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11347664147615433
dqn reward tensor(-319.7500, device='cuda:0') e 0.24000000000005484 loss_dqn tensor(3.9648e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10519850999116898
dqn reward tensor(-288.1250, device='cuda:0') e 0.23950000000005484 loss_dqn tensor(4.1147e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15210047364234924
dqn reward tensor(-266.3750, device='cuda:0') e 0.23900000000005484 loss_dqn tensor(4.6282e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23018896579742432
dqn reward tensor(-112., device='cuda:0') e 0.23850000000005483 loss_dqn tensor(4.5723e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028522878885269165
dqn reward tensor(-187.0625, device='cuda:0') e 0.23800000000005483 loss_dqn tensor(4.4226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17712412774562836
dqn reward tensor(-338.5000, device='cuda:0') e 0.23750000000005483 loss_dqn tensor(4.4448e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22121286392211914
dqn reward tensor(-179.9375, device='cuda:0') e 0.23700000000005483 loss_dqn tensor(4.7627e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06416134536266327
dqn reward tensor(-266.3750, device='cuda:0') e 0.23650000000005483 loss_dqn tensor(5.1527e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08359916508197784
dqn reward tensor(-224., device='cuda:0') e 0.23600000000005483 loss_dqn tensor(4.9332e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024749308824539185
dqn reward tensor(-348., device='cuda:0') e 0.23550000000005483 loss_dqn tensor(3.8963e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038265205919742584
dqn reward tensor(-213., device='cuda:0') e 0.23500000000005483 loss_dqn tensor(4.4247e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08705548942089081
dqn reward tensor(-282., device='cuda:0') e 0.23450000000005483 loss_dqn tensor(3.6557e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2062234878540039
dqn reward tensor(-325., device='cuda:0') e 0.23400000000005483 loss_dqn tensor(4.1039e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21218186616897583
dqn reward tensor(-164., device='cuda:0') e 0.23350000000005483 loss_dqn tensor(4.2830e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14511650800704956
dqn reward tensor(-232.2500, device='cuda:0') e 0.23300000000005483 loss_dqn tensor(3.8317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08278471976518631
dqn reward tensor(-279.1250, device='cuda:0') e 0.23250000000005483 loss_dqn tensor(4.8331e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10804499685764313
dqn reward tensor(-189.5000, device='cuda:0') e 0.23200000000005483 loss_dqn tensor(4.8514e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2134656012058258
dqn reward tensor(-485.2500, device='cuda:0') e 0.23150000000005483 loss_dqn tensor(3.8695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2472187876701355
dqn reward tensor(-173.3750, device='cuda:0') e 0.23100000000005483 loss_dqn tensor(3.8477e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053842172026634216
dqn reward tensor(-218., device='cuda:0') e 0.23050000000005483 loss_dqn tensor(4.4939e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16533492505550385
dqn reward tensor(-340.8750, device='cuda:0') e 0.23000000000005483 loss_dqn tensor(4.5916e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043377552181482315
dqn reward tensor(-370.3750, device='cuda:0') e 0.22950000000005483 loss_dqn tensor(3.7336e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04121914505958557
dqn reward tensor(-264.6250, device='cuda:0') e 0.22900000000005483 loss_dqn tensor(4.3759e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025856636464595795
dqn reward tensor(-359.6250, device='cuda:0') e 0.22850000000005483 loss_dqn tensor(3.8199e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09281565994024277
dqn reward tensor(-94.5000, device='cuda:0') e 0.22800000000005483 loss_dqn tensor(4.1166e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07796713709831238
dqn reward tensor(-1.7500, device='cuda:0') e 0.22750000000005483 loss_dqn tensor(4.0847e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013614426366984844
Evaluating...
Train: {'rocauc': 0.7229345173677565} -4.468120098114014
=====Epoch 4=====
Training...
dqn reward tensor(-445.3750, device='cuda:0') e 0.22700000000005482 loss_dqn tensor(3.6430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22558453679084778
dqn reward tensor(-192.1250, device='cuda:0') e 0.22650000000005482 loss_dqn tensor(4.2981e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15236665308475494
dqn reward tensor(-271.5000, device='cuda:0') e 0.22600000000005482 loss_dqn tensor(3.9521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11991365253925323
dqn reward tensor(-328.1250, device='cuda:0') e 0.22550000000005482 loss_dqn tensor(4.0697e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22055251896381378
dqn reward tensor(-202.7500, device='cuda:0') e 0.22500000000005482 loss_dqn tensor(3.2553e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14041322469711304
dqn reward tensor(-378., device='cuda:0') e 0.22450000000005482 loss_dqn tensor(3.8658e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1537901908159256
dqn reward tensor(-171., device='cuda:0') e 0.22400000000005482 loss_dqn tensor(3.5313e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19887113571166992
dqn reward tensor(-379.2500, device='cuda:0') e 0.22350000000005482 loss_dqn tensor(4.2835e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08715061843395233
dqn reward tensor(-339.6875, device='cuda:0') e 0.22300000000005482 loss_dqn tensor(3.5252e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1877894401550293
dqn reward tensor(-266.6250, device='cuda:0') e 0.22250000000005482 loss_dqn tensor(3.0093e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1422276794910431
dqn reward tensor(-432.6250, device='cuda:0') e 0.22200000000005482 loss_dqn tensor(4.2013e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3098907470703125
dqn reward tensor(-200.8750, device='cuda:0') e 0.22150000000005482 loss_dqn tensor(2.7551e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07837706804275513
dqn reward tensor(-283.8750, device='cuda:0') e 0.22100000000005482 loss_dqn tensor(3.6226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06509287655353546
dqn reward tensor(-322.4375, device='cuda:0') e 0.22050000000005482 loss_dqn tensor(3.1381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2004043161869049
dqn reward tensor(-377.5000, device='cuda:0') e 0.22000000000005482 loss_dqn tensor(2.8706e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15263760089874268
dqn reward tensor(-156.2500, device='cuda:0') e 0.21950000000005482 loss_dqn tensor(3.6168e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1553964763879776
dqn reward tensor(-157.2500, device='cuda:0') e 0.21900000000005482 loss_dqn tensor(4.0392e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1002870574593544
dqn reward tensor(-262.7500, device='cuda:0') e 0.21850000000005482 loss_dqn tensor(3.5901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15845398604869843
dqn reward tensor(-302., device='cuda:0') e 0.21800000000005482 loss_dqn tensor(3.0051e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13866031169891357
dqn reward tensor(-500.5000, device='cuda:0') e 0.21750000000005482 loss_dqn tensor(3.8478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27048459649086
dqn reward tensor(-239.8750, device='cuda:0') e 0.21700000000005482 loss_dqn tensor(4.2695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2875399589538574
dqn reward tensor(-239., device='cuda:0') e 0.21650000000005482 loss_dqn tensor(4.0834e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051689401268959045
dqn reward tensor(-282., device='cuda:0') e 0.21600000000005481 loss_dqn tensor(3.4987e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17238357663154602
dqn reward tensor(-192.1250, device='cuda:0') e 0.21550000000005481 loss_dqn tensor(3.7932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08359731733798981
dqn reward tensor(-323.3750, device='cuda:0') e 0.21500000000005481 loss_dqn tensor(4.0303e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13189084827899933
dqn reward tensor(-264.6250, device='cuda:0') e 0.2145000000000548 loss_dqn tensor(4.6730e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10757048428058624
dqn reward tensor(-225.7500, device='cuda:0') e 0.2140000000000548 loss_dqn tensor(3.9086e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13113339245319366
dqn reward tensor(-262.6250, device='cuda:0') e 0.2135000000000548 loss_dqn tensor(4.3873e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12857481837272644
dqn reward tensor(-170.1250, device='cuda:0') e 0.2130000000000548 loss_dqn tensor(5.0466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07375398278236389
dqn reward tensor(-245.8750, device='cuda:0') e 0.2125000000000548 loss_dqn tensor(3.6617e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13194739818572998
dqn reward tensor(-337.6250, device='cuda:0') e 0.2120000000000548 loss_dqn tensor(4.4179e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25706955790519714
dqn reward tensor(-219.1250, device='cuda:0') e 0.2115000000000548 loss_dqn tensor(3.8290e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670905351638794
dqn reward tensor(-157.6250, device='cuda:0') e 0.2110000000000548 loss_dqn tensor(3.9274e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435520350933075
dqn reward tensor(-314., device='cuda:0') e 0.2105000000000548 loss_dqn tensor(4.0068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048934824764728546
dqn reward tensor(-243.6250, device='cuda:0') e 0.2100000000000548 loss_dqn tensor(4.6081e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13425980508327484
dqn reward tensor(-260.2500, device='cuda:0') e 0.2095000000000548 loss_dqn tensor(3.2803e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12892937660217285
dqn reward tensor(-164.0625, device='cuda:0') e 0.2090000000000548 loss_dqn tensor(4.5595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16024743020534515
dqn reward tensor(-187.1250, device='cuda:0') e 0.2085000000000548 loss_dqn tensor(4.3467e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15257328748703003
dqn reward tensor(-386.7500, device='cuda:0') e 0.2080000000000548 loss_dqn tensor(3.3344e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21009057760238647
dqn reward tensor(-67.8750, device='cuda:0') e 0.2075000000000548 loss_dqn tensor(4.4647e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1631837636232376
dqn reward tensor(-244., device='cuda:0') e 0.2070000000000548 loss_dqn tensor(4.4756e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20523197948932648
dqn reward tensor(-380.6875, device='cuda:0') e 0.2065000000000548 loss_dqn tensor(4.5109e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13786375522613525
dqn reward tensor(-17.1250, device='cuda:0') e 0.2060000000000548 loss_dqn tensor(4.6238e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13082678616046906
dqn reward tensor(-295.6875, device='cuda:0') e 0.2055000000000548 loss_dqn tensor(3.8869e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09179334342479706
dqn reward tensor(-176.5000, device='cuda:0') e 0.2050000000000548 loss_dqn tensor(3.3156e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11262191832065582
dqn reward tensor(-285.1250, device='cuda:0') e 0.2045000000000548 loss_dqn tensor(2.7769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2882343530654907
dqn reward tensor(-147.5000, device='cuda:0') e 0.2040000000000548 loss_dqn tensor(3.8494e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14529919624328613
dqn reward tensor(-195., device='cuda:0') e 0.2035000000000548 loss_dqn tensor(2.3655e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033211175352334976
dqn reward tensor(-240.5000, device='cuda:0') e 0.2030000000000548 loss_dqn tensor(2.4222e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09689156711101532
dqn reward tensor(-280.2500, device='cuda:0') e 0.2025000000000548 loss_dqn tensor(2.2293e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21787400543689728
dqn reward tensor(-168.0625, device='cuda:0') e 0.2020000000000548 loss_dqn tensor(2.6253e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06336644291877747
dqn reward tensor(-281.1250, device='cuda:0') e 0.2015000000000548 loss_dqn tensor(2.6216e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20776358246803284
dqn reward tensor(-260.8750, device='cuda:0') e 0.2010000000000548 loss_dqn tensor(2.0453e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835374966263771
dqn reward tensor(-142.3750, device='cuda:0') e 0.2005000000000548 loss_dqn tensor(2.3312e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1426692008972168
dqn reward tensor(-318.3750, device='cuda:0') e 0.2000000000000548 loss_dqn tensor(2.1656e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16730313003063202
dqn reward tensor(-255.6250, device='cuda:0') e 0.1995000000000548 loss_dqn tensor(1.9732e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05408145859837532
dqn reward tensor(-241.7500, device='cuda:0') e 0.1990000000000548 loss_dqn tensor(2.4532e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14736643433570862
dqn reward tensor(-272.7500, device='cuda:0') e 0.1985000000000548 loss_dqn tensor(1.7362e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02840937301516533
dqn reward tensor(-275.3750, device='cuda:0') e 0.1980000000000548 loss_dqn tensor(1.3929e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09210681170225143
dqn reward tensor(-367.5625, device='cuda:0') e 0.1975000000000548 loss_dqn tensor(1.6795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16764166951179504
dqn reward tensor(-372.6250, device='cuda:0') e 0.1970000000000548 loss_dqn tensor(1.4595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2736032009124756
dqn reward tensor(-317.7500, device='cuda:0') e 0.1965000000000548 loss_dqn tensor(1.7369e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19403928518295288
dqn reward tensor(-314.5000, device='cuda:0') e 0.1960000000000548 loss_dqn tensor(1.7193e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15656232833862305
dqn reward tensor(-226.3750, device='cuda:0') e 0.1955000000000548 loss_dqn tensor(1.5676e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21774394810199738
dqn reward tensor(-235.9375, device='cuda:0') e 0.1950000000000548 loss_dqn tensor(1.7315e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14632974565029144
dqn reward tensor(-373.5000, device='cuda:0') e 0.1945000000000548 loss_dqn tensor(1.2818e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05531962588429451
dqn reward tensor(-130.6875, device='cuda:0') e 0.1940000000000548 loss_dqn tensor(1.8378e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11174570769071579
dqn reward tensor(-309.1250, device='cuda:0') e 0.1935000000000548 loss_dqn tensor(1.4143e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2116745412349701
dqn reward tensor(-200.3750, device='cuda:0') e 0.1930000000000548 loss_dqn tensor(1.6390e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19737502932548523
dqn reward tensor(-293.3750, device='cuda:0') e 0.1925000000000548 loss_dqn tensor(1.4027e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1737697869539261
dqn reward tensor(-185., device='cuda:0') e 0.1920000000000548 loss_dqn tensor(1.1773e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12221918255090714
dqn reward tensor(-401.3750, device='cuda:0') e 0.1915000000000548 loss_dqn tensor(1.1360e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17375023663043976
dqn reward tensor(-246., device='cuda:0') e 0.1910000000000548 loss_dqn tensor(1.1316e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08314207196235657
dqn reward tensor(-296.2500, device='cuda:0') e 0.1905000000000548 loss_dqn tensor(1.0480e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08824266493320465
dqn reward tensor(-365.1250, device='cuda:0') e 0.1900000000000548 loss_dqn tensor(97849464., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021922308951616287
dqn reward tensor(-308.7500, device='cuda:0') e 0.1895000000000548 loss_dqn tensor(1.3143e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22852754592895508
dqn reward tensor(-140.2500, device='cuda:0') e 0.1890000000000548 loss_dqn tensor(1.3285e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432669460773468
dqn reward tensor(-374.4375, device='cuda:0') e 0.1885000000000548 loss_dqn tensor(1.1325e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835246816277504
dqn reward tensor(-297.1250, device='cuda:0') e 0.1880000000000548 loss_dqn tensor(98633152., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0927908718585968
dqn reward tensor(-301.8750, device='cuda:0') e 0.1875000000000548 loss_dqn tensor(1.0677e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018545247614383698
dqn reward tensor(-328.9375, device='cuda:0') e 0.1870000000000548 loss_dqn tensor(1.0380e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2313692420721054
dqn reward tensor(-434.3750, device='cuda:0') e 0.1865000000000548 loss_dqn tensor(1.1009e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1768612265586853
dqn reward tensor(-426.1250, device='cuda:0') e 0.1860000000000548 loss_dqn tensor(1.6105e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08985685557126999
dqn reward tensor(-281.5000, device='cuda:0') e 0.1855000000000548 loss_dqn tensor(96565776., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0609908401966095
dqn reward tensor(-301.6250, device='cuda:0') e 0.1850000000000548 loss_dqn tensor(1.1278e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25677865743637085
dqn reward tensor(-280.3750, device='cuda:0') e 0.1845000000000548 loss_dqn tensor(1.0787e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3007398843765259
dqn reward tensor(-230.3750, device='cuda:0') e 0.1840000000000548 loss_dqn tensor(1.4607e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09472182393074036
dqn reward tensor(-306.2500, device='cuda:0') e 0.18350000000005479 loss_dqn tensor(1.2690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08763754367828369
dqn reward tensor(-330.3750, device='cuda:0') e 0.18300000000005479 loss_dqn tensor(1.2666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1952883005142212
dqn reward tensor(-357.6250, device='cuda:0') e 0.18250000000005479 loss_dqn tensor(1.2359e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.145878404378891
dqn reward tensor(-206.1250, device='cuda:0') e 0.18200000000005478 loss_dqn tensor(1.0929e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20366758108139038
dqn reward tensor(-330.7500, device='cuda:0') e 0.18150000000005478 loss_dqn tensor(1.6104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15047484636306763
dqn reward tensor(-244.6250, device='cuda:0') e 0.18100000000005478 loss_dqn tensor(1.3110e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06626306474208832
dqn reward tensor(-377.3750, device='cuda:0') e 0.18050000000005478 loss_dqn tensor(1.1724e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0882401317358017
dqn reward tensor(-288.8750, device='cuda:0') e 0.18000000000005478 loss_dqn tensor(1.2923e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3654431104660034
dqn reward tensor(-360.3750, device='cuda:0') e 0.17950000000005478 loss_dqn tensor(1.1446e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19537673890590668
dqn reward tensor(-328.8750, device='cuda:0') e 0.17900000000005478 loss_dqn tensor(1.6604e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12614934146404266
dqn reward tensor(-331.3750, device='cuda:0') e 0.17850000000005478 loss_dqn tensor(1.1068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11005625873804092
dqn reward tensor(-297.9375, device='cuda:0') e 0.17800000000005478 loss_dqn tensor(1.7811e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3999654948711395
dqn reward tensor(-363.8750, device='cuda:0') e 0.17750000000005478 loss_dqn tensor(1.3202e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09995850175619125
dqn reward tensor(-129.7500, device='cuda:0') e 0.17700000000005478 loss_dqn tensor(1.3753e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1398823857307434
dqn reward tensor(-97.8125, device='cuda:0') e 0.17650000000005478 loss_dqn tensor(1.6838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.40261709690093994
dqn reward tensor(-328.6250, device='cuda:0') e 0.17600000000005478 loss_dqn tensor(1.1493e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21238352358341217
dqn reward tensor(-255.7500, device='cuda:0') e 0.17550000000005478 loss_dqn tensor(1.2972e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12993134558200836
dqn reward tensor(-303.6250, device='cuda:0') e 0.17500000000005478 loss_dqn tensor(1.1840e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14398248493671417
dqn reward tensor(-184.3750, device='cuda:0') e 0.17450000000005478 loss_dqn tensor(1.3832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17972922325134277
dqn reward tensor(-339.3750, device='cuda:0') e 0.17400000000005478 loss_dqn tensor(1.1371e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16313114762306213
dqn reward tensor(-333.4375, device='cuda:0') e 0.17350000000005478 loss_dqn tensor(1.1315e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05661151185631752
dqn reward tensor(-276.1250, device='cuda:0') e 0.17300000000005478 loss_dqn tensor(1.2508e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18671655654907227
dqn reward tensor(-383.5000, device='cuda:0') e 0.17250000000005478 loss_dqn tensor(1.1257e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047918468713760376
dqn reward tensor(-258.8750, device='cuda:0') e 0.17200000000005478 loss_dqn tensor(1.4338e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03488197922706604
dqn reward tensor(-393.6875, device='cuda:0') e 0.17150000000005478 loss_dqn tensor(1.4523e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25547531247138977
dqn reward tensor(-320., device='cuda:0') e 0.17100000000005477 loss_dqn tensor(1.4188e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04700813069939613
dqn reward tensor(-221.3750, device='cuda:0') e 0.17050000000005477 loss_dqn tensor(1.2208e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21626868844032288
dqn reward tensor(-332.8750, device='cuda:0') e 0.17000000000005477 loss_dqn tensor(1.6224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2625536024570465
dqn reward tensor(-334.3750, device='cuda:0') e 0.16950000000005477 loss_dqn tensor(1.1394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09150731563568115
dqn reward tensor(-276.7500, device='cuda:0') e 0.16900000000005477 loss_dqn tensor(1.1969e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16734974086284637
dqn reward tensor(-318.1250, device='cuda:0') e 0.16850000000005477 loss_dqn tensor(1.4988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14835163950920105
dqn reward tensor(-340.6250, device='cuda:0') e 0.16800000000005477 loss_dqn tensor(1.2956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04985244572162628
dqn reward tensor(-179., device='cuda:0') e 0.16750000000005477 loss_dqn tensor(1.9446e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1676153689622879
dqn reward tensor(-224.5000, device='cuda:0') e 0.16700000000005477 loss_dqn tensor(1.5278e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15707477927207947
dqn reward tensor(-200.4375, device='cuda:0') e 0.16650000000005477 loss_dqn tensor(1.4456e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11646787822246552
dqn reward tensor(-201.7500, device='cuda:0') e 0.16600000000005477 loss_dqn tensor(1.7019e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14515960216522217
dqn reward tensor(-351.5000, device='cuda:0') e 0.16550000000005477 loss_dqn tensor(1.3261e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11430100351572037
dqn reward tensor(-265.3125, device='cuda:0') e 0.16500000000005477 loss_dqn tensor(1.4136e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08498135209083557
dqn reward tensor(-248.1250, device='cuda:0') e 0.16450000000005477 loss_dqn tensor(1.3377e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20732413232326508
dqn reward tensor(-319.1250, device='cuda:0') e 0.16400000000005477 loss_dqn tensor(1.5190e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12250843644142151
dqn reward tensor(-197.6250, device='cuda:0') e 0.16350000000005477 loss_dqn tensor(2.0205e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0982150137424469
dqn reward tensor(-282.7500, device='cuda:0') e 0.16300000000005477 loss_dqn tensor(2.0027e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09175422787666321
dqn reward tensor(-212., device='cuda:0') e 0.16250000000005477 loss_dqn tensor(1.5267e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05892835557460785
dqn reward tensor(-230.5000, device='cuda:0') e 0.16200000000005477 loss_dqn tensor(1.3761e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10981718450784683
dqn reward tensor(-89.8750, device='cuda:0') e 0.16150000000005477 loss_dqn tensor(1.8064e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18055284023284912
dqn reward tensor(-121.1250, device='cuda:0') e 0.16100000000005477 loss_dqn tensor(1.5587e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02834876999258995
dqn reward tensor(-86.8750, device='cuda:0') e 0.16050000000005477 loss_dqn tensor(2.0298e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2041502296924591
dqn reward tensor(-178.8750, device='cuda:0') e 0.16000000000005477 loss_dqn tensor(1.4659e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17367562651634216
dqn reward tensor(-192.7500, device='cuda:0') e 0.15950000000005476 loss_dqn tensor(1.4092e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3960953950881958
dqn reward tensor(-83.9375, device='cuda:0') e 0.15900000000005476 loss_dqn tensor(1.4641e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1981365829706192
dqn reward tensor(-277.8750, device='cuda:0') e 0.15850000000005476 loss_dqn tensor(1.7130e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11887475103139877
dqn reward tensor(-180.3750, device='cuda:0') e 0.15800000000005476 loss_dqn tensor(1.5472e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10268113762140274
dqn reward tensor(-231.6250, device='cuda:0') e 0.15750000000005476 loss_dqn tensor(1.7234e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11335855722427368
dqn reward tensor(-168.6250, device='cuda:0') e 0.15700000000005476 loss_dqn tensor(1.8061e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10792301595211029
dqn reward tensor(-11.1250, device='cuda:0') e 0.15650000000005476 loss_dqn tensor(1.5258e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.203232079744339
dqn reward tensor(83.3750, device='cuda:0') e 0.15600000000005476 loss_dqn tensor(1.5728e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1369430422782898
dqn reward tensor(-127.5625, device='cuda:0') e 0.15550000000005476 loss_dqn tensor(1.6194e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12912806868553162
dqn reward tensor(-40., device='cuda:0') e 0.15500000000005476 loss_dqn tensor(1.5342e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22227761149406433
dqn reward tensor(-146.2500, device='cuda:0') e 0.15450000000005476 loss_dqn tensor(1.8595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16536906361579895
dqn reward tensor(-101.8750, device='cuda:0') e 0.15400000000005476 loss_dqn tensor(1.4596e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12692025303840637
dqn reward tensor(131.1875, device='cuda:0') e 0.15350000000005476 loss_dqn tensor(1.4168e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03607071563601494
dqn reward tensor(-38.1250, device='cuda:0') e 0.15300000000005476 loss_dqn tensor(1.5164e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19927620887756348
dqn reward tensor(0.8750, device='cuda:0') e 0.15250000000005476 loss_dqn tensor(1.2602e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18926891684532166
dqn reward tensor(-190.5000, device='cuda:0') e 0.15200000000005476 loss_dqn tensor(1.2866e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08107510209083557
dqn reward tensor(-142.8750, device='cuda:0') e 0.15150000000005476 loss_dqn tensor(1.3235e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07649059593677521
dqn reward tensor(-86.8750, device='cuda:0') e 0.15100000000005476 loss_dqn tensor(1.3141e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09579621255397797
dqn reward tensor(-217.5000, device='cuda:0') e 0.15050000000005476 loss_dqn tensor(1.3337e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15162727236747742
dqn reward tensor(-41.2500, device='cuda:0') e 0.15000000000005476 loss_dqn tensor(1.7179e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1446586549282074
dqn reward tensor(146.0625, device='cuda:0') e 0.14950000000005476 loss_dqn tensor(1.3140e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2383885383605957
dqn reward tensor(88.1250, device='cuda:0') e 0.14900000000005476 loss_dqn tensor(1.5889e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1357187032699585
dqn reward tensor(-191.7500, device='cuda:0') e 0.14850000000005475 loss_dqn tensor(1.3475e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0552012100815773
dqn reward tensor(-155., device='cuda:0') e 0.14800000000005475 loss_dqn tensor(1.3893e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06281687319278717
dqn reward tensor(-184.1250, device='cuda:0') e 0.14750000000005475 loss_dqn tensor(1.3892e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07529020309448242
dqn reward tensor(74.7500, device='cuda:0') e 0.14700000000005475 loss_dqn tensor(1.4059e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0689312294125557
dqn reward tensor(-56.6250, device='cuda:0') e 0.14650000000005475 loss_dqn tensor(1.4268e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18988542258739471
dqn reward tensor(-5.5000, device='cuda:0') e 0.14600000000005475 loss_dqn tensor(1.2875e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031205644831061363
dqn reward tensor(-74.2500, device='cuda:0') e 0.14550000000005475 loss_dqn tensor(1.5048e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2577495276927948
dqn reward tensor(-83.2500, device='cuda:0') e 0.14500000000005475 loss_dqn tensor(1.3107e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17562644183635712
dqn reward tensor(-215.1250, device='cuda:0') e 0.14450000000005475 loss_dqn tensor(1.4342e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16934695839881897
dqn reward tensor(-122.3750, device='cuda:0') e 0.14400000000005475 loss_dqn tensor(1.2580e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344442069530487
dqn reward tensor(-65.6250, device='cuda:0') e 0.14350000000005475 loss_dqn tensor(1.2740e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25931885838508606
dqn reward tensor(84.7500, device='cuda:0') e 0.14300000000005475 loss_dqn tensor(1.0081e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21613052487373352
dqn reward tensor(-150.2500, device='cuda:0') e 0.14250000000005475 loss_dqn tensor(1.2995e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07230494171380997
dqn reward tensor(-107., device='cuda:0') e 0.14200000000005475 loss_dqn tensor(1.1222e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1650092899799347
dqn reward tensor(-127.6250, device='cuda:0') e 0.14150000000005475 loss_dqn tensor(1.1870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12574976682662964
dqn reward tensor(-129.3125, device='cuda:0') e 0.14100000000005475 loss_dqn tensor(1.1927e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10713794082403183
dqn reward tensor(-219.8750, device='cuda:0') e 0.14050000000005475 loss_dqn tensor(97482464., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07267001271247864
dqn reward tensor(-231.1875, device='cuda:0') e 0.14000000000005475 loss_dqn tensor(89355216., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16588565707206726
dqn reward tensor(-260.5000, device='cuda:0') e 0.13950000000005475 loss_dqn tensor(96703456., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015100340358912945
dqn reward tensor(-236.5000, device='cuda:0') e 0.13900000000005475 loss_dqn tensor(1.0949e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10236234962940216
dqn reward tensor(-131.5000, device='cuda:0') e 0.13850000000005475 loss_dqn tensor(97796416., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24545645713806152
dqn reward tensor(-271.6250, device='cuda:0') e 0.13800000000005475 loss_dqn tensor(1.2615e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07834507524967194
dqn reward tensor(-246.5000, device='cuda:0') e 0.13750000000005475 loss_dqn tensor(1.1979e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.081008180975914
dqn reward tensor(-189.2500, device='cuda:0') e 0.13700000000005474 loss_dqn tensor(1.2559e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31758031249046326
dqn reward tensor(-191.2500, device='cuda:0') e 0.13650000000005474 loss_dqn tensor(1.1592e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3672986328601837
dqn reward tensor(-201.2500, device='cuda:0') e 0.13600000000005474 loss_dqn tensor(1.2843e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13644614815711975
dqn reward tensor(-170.6250, device='cuda:0') e 0.13550000000005474 loss_dqn tensor(1.2187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06869146972894669
dqn reward tensor(-192.3750, device='cuda:0') e 0.13500000000005474 loss_dqn tensor(1.0789e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666426956653595
dqn reward tensor(-186.3125, device='cuda:0') e 0.13450000000005474 loss_dqn tensor(1.3404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18694493174552917
dqn reward tensor(-287., device='cuda:0') e 0.13400000000005474 loss_dqn tensor(1.3618e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19018054008483887
dqn reward tensor(29.7500, device='cuda:0') e 0.13350000000005474 loss_dqn tensor(1.1821e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13904082775115967
dqn reward tensor(-97.8750, device='cuda:0') e 0.13300000000005474 loss_dqn tensor(1.3305e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24592414498329163
dqn reward tensor(36., device='cuda:0') e 0.13250000000005474 loss_dqn tensor(1.2921e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15275560319423676
dqn reward tensor(-85.5000, device='cuda:0') e 0.13200000000005474 loss_dqn tensor(1.5578e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09129909425973892
dqn reward tensor(-117.2500, device='cuda:0') e 0.13150000000005474 loss_dqn tensor(1.4024e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20498836040496826
dqn reward tensor(-195.5000, device='cuda:0') e 0.13100000000005474 loss_dqn tensor(1.7505e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09732671827077866
dqn reward tensor(-90.1250, device='cuda:0') e 0.13050000000005474 loss_dqn tensor(1.5053e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1798023283481598
dqn reward tensor(-235.8125, device='cuda:0') e 0.13000000000005474 loss_dqn tensor(1.4091e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08581256866455078
dqn reward tensor(-118.2500, device='cuda:0') e 0.12950000000005474 loss_dqn tensor(2.0309e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23635198175907135
dqn reward tensor(-29.6250, device='cuda:0') e 0.12900000000005474 loss_dqn tensor(1.9816e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0839044600725174
dqn reward tensor(-107.1250, device='cuda:0') e 0.12850000000005474 loss_dqn tensor(1.4277e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07863680273294449
dqn reward tensor(-211.1250, device='cuda:0') e 0.12800000000005474 loss_dqn tensor(1.5452e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16839033365249634
dqn reward tensor(-54.3750, device='cuda:0') e 0.12750000000005474 loss_dqn tensor(1.9076e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17020422220230103
dqn reward tensor(-56.6250, device='cuda:0') e 0.12700000000005474 loss_dqn tensor(1.9106e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16748003661632538
dqn reward tensor(-266.7500, device='cuda:0') e 0.12650000000005474 loss_dqn tensor(1.8385e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0867495909333229
dqn reward tensor(45.8750, device='cuda:0') e 0.12600000000005473 loss_dqn tensor(2.6310e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2029692530632019
dqn reward tensor(-94.6250, device='cuda:0') e 0.12550000000005473 loss_dqn tensor(2.2490e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14024169743061066
dqn reward tensor(-176., device='cuda:0') e 0.12500000000005473 loss_dqn tensor(2.2950e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0907091498374939
dqn reward tensor(-76.8750, device='cuda:0') e 0.12450000000005473 loss_dqn tensor(2.9852e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791255533695221
dqn reward tensor(-80.1250, device='cuda:0') e 0.12400000000005473 loss_dqn tensor(2.6182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15867571532726288
dqn reward tensor(-194.2500, device='cuda:0') e 0.12350000000005473 loss_dqn tensor(3.0326e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08430435508489609
dqn reward tensor(124.5000, device='cuda:0') e 0.12300000000005473 loss_dqn tensor(3.1666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14429925382137299
dqn reward tensor(126., device='cuda:0') e 0.12250000000005473 loss_dqn tensor(3.0742e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18409669399261475
dqn reward tensor(-37.6875, device='cuda:0') e 0.12200000000005473 loss_dqn tensor(3.3772e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0906132236123085
dqn reward tensor(-102.7500, device='cuda:0') e 0.12150000000005473 loss_dqn tensor(3.3932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2692134976387024
dqn reward tensor(-3.6250, device='cuda:0') e 0.12100000000005473 loss_dqn tensor(3.9190e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08746369928121567
dqn reward tensor(37., device='cuda:0') e 0.12050000000005473 loss_dqn tensor(3.9021e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12793897092342377
dqn reward tensor(-52.6250, device='cuda:0') e 0.12000000000005473 loss_dqn tensor(3.5607e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23858122527599335
dqn reward tensor(-113.2500, device='cuda:0') e 0.11950000000005473 loss_dqn tensor(4.2182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22818514704704285
dqn reward tensor(-60.3750, device='cuda:0') e 0.11900000000005473 loss_dqn tensor(4.0201e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26596054434776306
dqn reward tensor(6.3750, device='cuda:0') e 0.11850000000005473 loss_dqn tensor(3.9293e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16449111700057983
dqn reward tensor(72.8750, device='cuda:0') e 0.11800000000005473 loss_dqn tensor(4.6625e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673540472984314
dqn reward tensor(-73., device='cuda:0') e 0.11750000000005473 loss_dqn tensor(4.5089e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2042524367570877
dqn reward tensor(-96.1250, device='cuda:0') e 0.11700000000005473 loss_dqn tensor(4.6510e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09153354912996292
dqn reward tensor(-190.5000, device='cuda:0') e 0.11650000000005473 loss_dqn tensor(4.4681e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06251273304224014
dqn reward tensor(24.6250, device='cuda:0') e 0.11600000000005473 loss_dqn tensor(4.2630e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04859255999326706
dqn reward tensor(-176.6250, device='cuda:0') e 0.11550000000005473 loss_dqn tensor(4.8475e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18692809343338013
dqn reward tensor(-136.8750, device='cuda:0') e 0.11500000000005473 loss_dqn tensor(4.2142e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09706048667430878
dqn reward tensor(-101.4375, device='cuda:0') e 0.11450000000005472 loss_dqn tensor(4.8771e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15098069608211517
dqn reward tensor(-44.3750, device='cuda:0') e 0.11400000000005472 loss_dqn tensor(4.7456e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08323998749256134
dqn reward tensor(-123.5000, device='cuda:0') e 0.11350000000005472 loss_dqn tensor(4.5993e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11868474632501602
dqn reward tensor(37.6250, device='cuda:0') e 0.11300000000005472 loss_dqn tensor(5.0186e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16136205196380615
dqn reward tensor(-200.7500, device='cuda:0') e 0.11250000000005472 loss_dqn tensor(5.1650e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13878823816776276
dqn reward tensor(-25.6250, device='cuda:0') e 0.11200000000005472 loss_dqn tensor(5.0230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10679836571216583
dqn reward tensor(-50.0625, device='cuda:0') e 0.11150000000005472 loss_dqn tensor(4.9926e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24996066093444824
dqn reward tensor(-70., device='cuda:0') e 0.11100000000005472 loss_dqn tensor(4.3199e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2826853096485138
dqn reward tensor(68.7500, device='cuda:0') e 0.11050000000005472 loss_dqn tensor(4.0684e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21778042614459991
dqn reward tensor(-81., device='cuda:0') e 0.11000000000005472 loss_dqn tensor(4.2307e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943501889705658
dqn reward tensor(-19.3750, device='cuda:0') e 0.10950000000005472 loss_dqn tensor(3.9328e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18492671847343445
dqn reward tensor(76.5000, device='cuda:0') e 0.10900000000005472 loss_dqn tensor(3.8587e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15417557954788208
dqn reward tensor(89.7500, device='cuda:0') e 0.10850000000005472 loss_dqn tensor(3.4131e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19035124778747559
dqn reward tensor(107.6250, device='cuda:0') e 0.10800000000005472 loss_dqn tensor(2.8295e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19583997130393982
dqn reward tensor(210.2500, device='cuda:0') e 0.10750000000005472 loss_dqn tensor(2.9478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488972306251526
dqn reward tensor(115.7500, device='cuda:0') e 0.10700000000005472 loss_dqn tensor(3.3301e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643727719783783
dqn reward tensor(-0.7500, device='cuda:0') e 0.10650000000005472 loss_dqn tensor(3.2811e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04446684569120407
dqn reward tensor(185.7500, device='cuda:0') e 0.10600000000005472 loss_dqn tensor(3.5631e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09055870026350021
dqn reward tensor(183.2500, device='cuda:0') e 0.10550000000005472 loss_dqn tensor(3.5710e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1962066888809204
dqn reward tensor(161.1250, device='cuda:0') e 0.10500000000005472 loss_dqn tensor(3.1721e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17456474900245667
dqn reward tensor(104.1250, device='cuda:0') e 0.10450000000005472 loss_dqn tensor(3.5612e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04368419200181961
dqn reward tensor(93.5000, device='cuda:0') e 0.10400000000005472 loss_dqn tensor(3.3954e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1336798220872879
dqn reward tensor(89.1250, device='cuda:0') e 0.10350000000005471 loss_dqn tensor(3.4513e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06678907573223114
dqn reward tensor(-17.6250, device='cuda:0') e 0.10300000000005471 loss_dqn tensor(2.9350e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13672246038913727
dqn reward tensor(33.2500, device='cuda:0') e 0.10250000000005471 loss_dqn tensor(3.5200e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01906691864132881
dqn reward tensor(-31., device='cuda:0') e 0.10200000000005471 loss_dqn tensor(3.5246e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24578005075454712
dqn reward tensor(95.3750, device='cuda:0') e 0.10150000000005471 loss_dqn tensor(3.5695e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2049221247434616
dqn reward tensor(-129.5000, device='cuda:0') e 0.10100000000005471 loss_dqn tensor(3.8133e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019030317664146423
dqn reward tensor(21.5000, device='cuda:0') e 0.10050000000005471 loss_dqn tensor(3.7474e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26819053292274475
dqn reward tensor(-21.6250, device='cuda:0') e 0.10000000000005471 loss_dqn tensor(3.4996e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15177330374717712
dqn reward tensor(9.4375, device='cuda:0') e 0.09950000000005471 loss_dqn tensor(3.2698e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18308231234550476
dqn reward tensor(28.7500, device='cuda:0') e 0.09900000000005471 loss_dqn tensor(3.7316e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15058857202529907
dqn reward tensor(68.2500, device='cuda:0') e 0.09850000000005471 loss_dqn tensor(3.8988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1293577253818512
dqn reward tensor(241.4375, device='cuda:0') e 0.09800000000005471 loss_dqn tensor(3.9846e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22651000320911407
dqn reward tensor(160.2500, device='cuda:0') e 0.09750000000005471 loss_dqn tensor(4.1082e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12454831600189209
dqn reward tensor(245.7500, device='cuda:0') e 0.09700000000005471 loss_dqn tensor(3.8327e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07082632184028625
dqn reward tensor(122.2500, device='cuda:0') e 0.09650000000005471 loss_dqn tensor(4.0387e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24075935781002045
dqn reward tensor(152.7500, device='cuda:0') e 0.09600000000005471 loss_dqn tensor(3.6966e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17710760235786438
dqn reward tensor(-42.5000, device='cuda:0') e 0.09550000000005471 loss_dqn tensor(3.9231e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10323984920978546
dqn reward tensor(167.2500, device='cuda:0') e 0.09500000000005471 loss_dqn tensor(4.1244e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2819693684577942
dqn reward tensor(56.8750, device='cuda:0') e 0.0945000000000547 loss_dqn tensor(3.5754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09743612259626389
dqn reward tensor(129.6250, device='cuda:0') e 0.0940000000000547 loss_dqn tensor(3.9884e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04717206582427025
dqn reward tensor(189., device='cuda:0') e 0.0935000000000547 loss_dqn tensor(4.1834e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1765218824148178
dqn reward tensor(182., device='cuda:0') e 0.0930000000000547 loss_dqn tensor(4.0747e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08287113159894943
dqn reward tensor(92.1250, device='cuda:0') e 0.0925000000000547 loss_dqn tensor(3.8586e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17213240265846252
dqn reward tensor(-13., device='cuda:0') e 0.0920000000000547 loss_dqn tensor(3.9798e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03164219111204147
dqn reward tensor(5.5000, device='cuda:0') e 0.0915000000000547 loss_dqn tensor(4.1849e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03175957128405571
dqn reward tensor(-228., device='cuda:0') e 0.0910000000000547 loss_dqn tensor(3.5482e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11435782164335251
dqn reward tensor(-18.1250, device='cuda:0') e 0.0905000000000547 loss_dqn tensor(3.8062e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06544367223978043
dqn reward tensor(-170.1250, device='cuda:0') e 0.0900000000000547 loss_dqn tensor(3.8973e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11210165917873383
dqn reward tensor(48.5000, device='cuda:0') e 0.0895000000000547 loss_dqn tensor(4.0766e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2639578878879547
dqn reward tensor(-97.8125, device='cuda:0') e 0.0890000000000547 loss_dqn tensor(4.0288e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01293208822607994
dqn reward tensor(15.6250, device='cuda:0') e 0.0885000000000547 loss_dqn tensor(3.4933e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08945795893669128
dqn reward tensor(55.8750, device='cuda:0') e 0.0880000000000547 loss_dqn tensor(4.0155e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3335725963115692
dqn reward tensor(109.5625, device='cuda:0') e 0.0875000000000547 loss_dqn tensor(3.7392e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.157942995429039
dqn reward tensor(92.5000, device='cuda:0') e 0.0870000000000547 loss_dqn tensor(3.9863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019085915759205818
dqn reward tensor(22.5000, device='cuda:0') e 0.0865000000000547 loss_dqn tensor(3.9633e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20691360533237457
dqn reward tensor(10.6250, device='cuda:0') e 0.0860000000000547 loss_dqn tensor(3.7416e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038234204053878784
dqn reward tensor(232.5000, device='cuda:0') e 0.0855000000000547 loss_dqn tensor(4.1004e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029320605099201202
dqn reward tensor(-69., device='cuda:0') e 0.0850000000000547 loss_dqn tensor(3.7406e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1096692830324173
dqn reward tensor(25.7500, device='cuda:0') e 0.0845000000000547 loss_dqn tensor(4.2677e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1059567928314209
dqn reward tensor(33.8750, device='cuda:0') e 0.0840000000000547 loss_dqn tensor(4.0979e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14895367622375488
dqn reward tensor(-61.6875, device='cuda:0') e 0.0835000000000547 loss_dqn tensor(3.3951e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08763088285923004
dqn reward tensor(152.2500, device='cuda:0') e 0.0830000000000547 loss_dqn tensor(3.6721e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.237237349152565
dqn reward tensor(-46.3750, device='cuda:0') e 0.0825000000000547 loss_dqn tensor(4.0032e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17700430750846863
dqn reward tensor(43.2500, device='cuda:0') e 0.0820000000000547 loss_dqn tensor(3.7540e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05170518159866333
dqn reward tensor(90.2500, device='cuda:0') e 0.0815000000000547 loss_dqn tensor(4.0661e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11504056304693222
dqn reward tensor(207.1250, device='cuda:0') e 0.0810000000000547 loss_dqn tensor(4.5016e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027240272611379623
dqn reward tensor(203.7500, device='cuda:0') e 0.0805000000000547 loss_dqn tensor(4.4346e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13462111353874207
dqn reward tensor(-87.5000, device='cuda:0') e 0.0800000000000547 loss_dqn tensor(4.1595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02938910946249962
dqn reward tensor(48.1250, device='cuda:0') e 0.0795000000000547 loss_dqn tensor(4.3486e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1487133949995041
dqn reward tensor(277.5000, device='cuda:0') e 0.0790000000000547 loss_dqn tensor(4.1900e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1801871657371521
dqn reward tensor(-52.7500, device='cuda:0') e 0.07850000000005469 loss_dqn tensor(3.7454e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20056791603565216
dqn reward tensor(-88.1250, device='cuda:0') e 0.07800000000005469 loss_dqn tensor(3.1921e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17140190303325653
dqn reward tensor(121.6250, device='cuda:0') e 0.07750000000005469 loss_dqn tensor(2.9531e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11562937498092651
dqn reward tensor(32.6250, device='cuda:0') e 0.07700000000005469 loss_dqn tensor(2.8424e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15231426060199738
dqn reward tensor(70.8750, device='cuda:0') e 0.07650000000005469 loss_dqn tensor(2.7515e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2319149374961853
dqn reward tensor(186.1250, device='cuda:0') e 0.07600000000005469 loss_dqn tensor(2.6327e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31901273131370544
dqn reward tensor(103.2500, device='cuda:0') e 0.07550000000005469 loss_dqn tensor(2.4039e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16103461384773254
dqn reward tensor(33.0625, device='cuda:0') e 0.07500000000005469 loss_dqn tensor(2.0716e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09511476755142212
dqn reward tensor(81.8750, device='cuda:0') e 0.07450000000005469 loss_dqn tensor(1.9826e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16782499849796295
dqn reward tensor(273., device='cuda:0') e 0.07400000000005469 loss_dqn tensor(2.4187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08589550852775574
dqn reward tensor(42.5625, device='cuda:0') e 0.07350000000005469 loss_dqn tensor(1.9952e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12501922249794006
dqn reward tensor(277.1250, device='cuda:0') e 0.07300000000005469 loss_dqn tensor(2.0976e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149178609251976
dqn reward tensor(183.3750, device='cuda:0') e 0.07250000000005469 loss_dqn tensor(2.1170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15190748870372772
dqn reward tensor(57.7500, device='cuda:0') e 0.07200000000005469 loss_dqn tensor(1.7846e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0955723226070404
dqn reward tensor(80.2500, device='cuda:0') e 0.07150000000005469 loss_dqn tensor(2.0156e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12753355503082275
dqn reward tensor(266.3125, device='cuda:0') e 0.07100000000005469 loss_dqn tensor(2.0349e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22433680295944214
dqn reward tensor(94.8750, device='cuda:0') e 0.07050000000005469 loss_dqn tensor(1.8471e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021718578413128853
dqn reward tensor(-58.0625, device='cuda:0') e 0.07000000000005469 loss_dqn tensor(1.8438e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07099135220050812
dqn reward tensor(169.6250, device='cuda:0') e 0.06950000000005468 loss_dqn tensor(1.7506e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10980254411697388
dqn reward tensor(173.7500, device='cuda:0') e 0.06900000000005468 loss_dqn tensor(1.7469e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13923779129981995
dqn reward tensor(103.6250, device='cuda:0') e 0.06850000000005468 loss_dqn tensor(1.7545e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2645101249217987
dqn reward tensor(44.8750, device='cuda:0') e 0.06800000000005468 loss_dqn tensor(1.8838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07813169807195663
dqn reward tensor(183.1250, device='cuda:0') e 0.06750000000005468 loss_dqn tensor(1.5939e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12796808779239655
dqn reward tensor(114.7500, device='cuda:0') e 0.06700000000005468 loss_dqn tensor(1.8430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3516909182071686
dqn reward tensor(201.2500, device='cuda:0') e 0.06650000000005468 loss_dqn tensor(1.9419e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2090696394443512
dqn reward tensor(173.1250, device='cuda:0') e 0.06600000000005468 loss_dqn tensor(1.9798e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2954275608062744
dqn reward tensor(74.3750, device='cuda:0') e 0.06550000000005468 loss_dqn tensor(1.8404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09070681780576706
dqn reward tensor(269.7500, device='cuda:0') e 0.06500000000005468 loss_dqn tensor(2.0234e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0883423388004303
dqn reward tensor(-2., device='cuda:0') e 0.06450000000005468 loss_dqn tensor(1.8883e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22781574726104736
dqn reward tensor(117.3750, device='cuda:0') e 0.06400000000005468 loss_dqn tensor(1.9606e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17139463126659393
dqn reward tensor(17.1875, device='cuda:0') e 0.06350000000005468 loss_dqn tensor(1.7003e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07443597912788391
dqn reward tensor(304.6250, device='cuda:0') e 0.06300000000005468 loss_dqn tensor(2.1475e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09651928395032883
dqn reward tensor(123.1250, device='cuda:0') e 0.06250000000005468 loss_dqn tensor(1.7669e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19722974300384521
dqn reward tensor(170.3750, device='cuda:0') e 0.06200000000005468 loss_dqn tensor(1.8696e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17417797446250916
dqn reward tensor(254.7500, device='cuda:0') e 0.06150000000005468 loss_dqn tensor(2.0948e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24502047896385193
dqn reward tensor(16.5625, device='cuda:0') e 0.06100000000005468 loss_dqn tensor(2.0740e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13687914609909058
dqn reward tensor(38.6250, device='cuda:0') e 0.06050000000005468 loss_dqn tensor(1.9220e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1829642951488495
dqn reward tensor(-13.3750, device='cuda:0') e 0.060000000000054676 loss_dqn tensor(1.9432e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07108353823423386
dqn reward tensor(74.8750, device='cuda:0') e 0.059500000000054676 loss_dqn tensor(2.1252e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07511714845895767
dqn reward tensor(114., device='cuda:0') e 0.059000000000054675 loss_dqn tensor(2.0676e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030512426048517227
dqn reward tensor(-31.3750, device='cuda:0') e 0.058500000000054675 loss_dqn tensor(2.1271e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25078660249710083
dqn reward tensor(103.6250, device='cuda:0') e 0.058000000000054674 loss_dqn tensor(2.0285e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2590787410736084
dqn reward tensor(-61.5000, device='cuda:0') e 0.057500000000054674 loss_dqn tensor(1.8963e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22961090505123138
dqn reward tensor(91.5000, device='cuda:0') e 0.057000000000054674 loss_dqn tensor(2.0046e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1502169966697693
dqn reward tensor(-8.6875, device='cuda:0') e 0.05650000000005467 loss_dqn tensor(1.9789e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04024159535765648
dqn reward tensor(6.8750, device='cuda:0') e 0.05600000000005467 loss_dqn tensor(1.7932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1782667338848114
dqn reward tensor(62.3750, device='cuda:0') e 0.05550000000005467 loss_dqn tensor(2.0946e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23660175502300262
dqn reward tensor(-26.3750, device='cuda:0') e 0.05500000000005467 loss_dqn tensor(1.8900e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.180131196975708
dqn reward tensor(73., device='cuda:0') e 0.05450000000005467 loss_dqn tensor(1.7558e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11164472997188568
dqn reward tensor(29.1250, device='cuda:0') e 0.05400000000005467 loss_dqn tensor(1.8739e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1865413784980774
dqn reward tensor(42.1250, device='cuda:0') e 0.05350000000005467 loss_dqn tensor(1.8810e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21212515234947205
dqn reward tensor(33.3750, device='cuda:0') e 0.05300000000005467 loss_dqn tensor(2.0509e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28997835516929626
dqn reward tensor(83., device='cuda:0') e 0.05250000000005467 loss_dqn tensor(1.7484e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17794765532016754
dqn reward tensor(17., device='cuda:0') e 0.05200000000005467 loss_dqn tensor(1.8312e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09407362341880798
dqn reward tensor(-16.7500, device='cuda:0') e 0.05150000000005467 loss_dqn tensor(1.9165e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09900595247745514
dqn reward tensor(3.8750, device='cuda:0') e 0.05100000000005467 loss_dqn tensor(1.8476e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23512282967567444
dqn reward tensor(-29.1250, device='cuda:0') e 0.05050000000005467 loss_dqn tensor(1.8045e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08545169234275818
dqn reward tensor(36.3750, device='cuda:0') e 0.05000000000005467 loss_dqn tensor(1.6149e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0843227207660675
dqn reward tensor(276.1250, device='cuda:0') e 0.04950000000005467 loss_dqn tensor(1.7137e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2844785451889038
dqn reward tensor(-32.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6195e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023908136412501335
dqn reward tensor(68.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6061e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2497408390045166
dqn reward tensor(157.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5347e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05264926701784134
dqn reward tensor(-44.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6096e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26763802766799927
dqn reward tensor(118.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4981e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22144559025764465
dqn reward tensor(149.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5811e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11187754571437836
dqn reward tensor(4.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7573e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06899838149547577
dqn reward tensor(69., device='cuda:0') e 0.05 loss_dqn tensor(1.5756e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10575030744075775
dqn reward tensor(78.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7066e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15462660789489746
dqn reward tensor(90.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2284100353717804
dqn reward tensor(178.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3528e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11820130050182343
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09031577408313751
dqn reward tensor(145.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3724e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14030027389526367
dqn reward tensor(-53., device='cuda:0') e 0.05 loss_dqn tensor(1.3678e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1250971257686615
dqn reward tensor(-70.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4433e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1889650672674179
dqn reward tensor(-65.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4532e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09076597541570663
dqn reward tensor(12.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4609e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12679216265678406
dqn reward tensor(109.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3113e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20221522450447083
dqn reward tensor(72.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3965e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15893703699111938
dqn reward tensor(-225.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5487e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1515536606311798
dqn reward tensor(-100.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3620e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10471412539482117
dqn reward tensor(11.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3156e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20795178413391113
dqn reward tensor(-152.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3876e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03041941672563553
dqn reward tensor(-99.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3241e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08496361970901489
dqn reward tensor(127.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2818e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20042604207992554
dqn reward tensor(-121.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3451e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1578676700592041
dqn reward tensor(85.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3344e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24818718433380127
dqn reward tensor(96., device='cuda:0') e 0.05 loss_dqn tensor(1.3507e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04641567915678024
dqn reward tensor(84.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2258e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12683770060539246
dqn reward tensor(68.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1378e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042140379548072815
dqn reward tensor(-38.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3700e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1334809511899948
dqn reward tensor(-97., device='cuda:0') e 0.05 loss_dqn tensor(1.1886e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09567081928253174
dqn reward tensor(-54.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3249e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12862330675125122
dqn reward tensor(-224.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3421e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14443671703338623
dqn reward tensor(-217.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2068e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1792774796485901
dqn reward tensor(-94.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0944e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19668106734752655
dqn reward tensor(-82.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09961950033903122
dqn reward tensor(-289.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2696e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10967698693275452
dqn reward tensor(-243., device='cuda:0') e 0.05 loss_dqn tensor(1.2630e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17610345780849457
dqn reward tensor(-248.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1493e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10776138305664062
dqn reward tensor(-261.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3479e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16173055768013
dqn reward tensor(-253.1250, device='cuda:0') e 0.05 loss_dqn tensor(97667632., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14979718625545502
dqn reward tensor(-393.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6516e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.178674578666687
dqn reward tensor(-238.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13640013337135315
dqn reward tensor(-302.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3359e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28065037727355957
dqn reward tensor(-351., device='cuda:0') e 0.05 loss_dqn tensor(1.4257e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18434832990169525
dqn reward tensor(-246.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0663e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08869887888431549
dqn reward tensor(-326.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3650e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10791236907243729
dqn reward tensor(-272.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3366e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0415850467979908
dqn reward tensor(-319., device='cuda:0') e 0.05 loss_dqn tensor(1.2189e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07161987572908401
dqn reward tensor(-286.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4437e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.339144229888916
dqn reward tensor(-220.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3544e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06026570498943329
dqn reward tensor(-358.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1228e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08751432597637177
dqn reward tensor(-359.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2697e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2728254497051239
dqn reward tensor(-266.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3581e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22921711206436157
dqn reward tensor(-359.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0652e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07310225814580917
dqn reward tensor(-215.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6356e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19506427645683289
dqn reward tensor(-318.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3424e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15304258465766907
dqn reward tensor(-220.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4134e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09862674027681351
dqn reward tensor(-456.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2281e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12245583534240723
dqn reward tensor(-390.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.074172243475914
dqn reward tensor(-396.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14360196888446808
dqn reward tensor(-332.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2500e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09899106621742249
dqn reward tensor(-326.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06934039294719696
dqn reward tensor(-295., device='cuda:0') e 0.05 loss_dqn tensor(1.2347e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344001591205597
dqn reward tensor(-201.7500, device='cuda:0') e 0.05 loss_dqn tensor(92176520., device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07428379356861115
dqn reward tensor(-339.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3162e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13304403424263
dqn reward tensor(-157.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4154e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16171368956565857
dqn reward tensor(-248.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3644e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.190298393368721
dqn reward tensor(-329.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2586e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2620043158531189
dqn reward tensor(-272.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2414e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414022445678711
dqn reward tensor(-278.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1296e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15780198574066162
dqn reward tensor(-185.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2021e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10320553183555603
dqn reward tensor(-228.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0967e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10130162537097931
dqn reward tensor(-355.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2133e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24120695888996124
dqn reward tensor(-315.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21295762062072754
dqn reward tensor(-322.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3948e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050576820969581604
dqn reward tensor(-186.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24383027851581573
dqn reward tensor(-293.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1475236713886261
dqn reward tensor(-170., device='cuda:0') e 0.05 loss_dqn tensor(1.3238e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3217729926109314
dqn reward tensor(-262.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3188e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07905377447605133
dqn reward tensor(-237.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3485e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2926621437072754
dqn reward tensor(-338.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3705e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08926399797201157
dqn reward tensor(-327., device='cuda:0') e 0.05 loss_dqn tensor(1.5399e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14404945075511932
dqn reward tensor(-441.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1549e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11232449114322662
dqn reward tensor(-271.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4048e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16779018938541412
dqn reward tensor(-178.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0052e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05219337344169617
dqn reward tensor(-270., device='cuda:0') e 0.05 loss_dqn tensor(1.3639e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15177112817764282
dqn reward tensor(-246.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3360e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1971619427204132
dqn reward tensor(-392.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6314e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3161477744579315
dqn reward tensor(-443., device='cuda:0') e 0.05 loss_dqn tensor(1.5883e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0940864086151123
dqn reward tensor(-301.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8119e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1446542888879776
dqn reward tensor(-255.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4142e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07308530807495117
dqn reward tensor(-317.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5312e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2102237492799759
dqn reward tensor(-297.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2252e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04866568371653557
dqn reward tensor(-197.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6497e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14907941222190857
dqn reward tensor(-332.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5203e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2124345302581787
dqn reward tensor(-333.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6474e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20411443710327148
dqn reward tensor(-378.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4080e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09173841774463654
dqn reward tensor(-375.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09423142671585083
dqn reward tensor(-197.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6210e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19108694791793823
dqn reward tensor(-274.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4746e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23043309152126312
dqn reward tensor(-244.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23861205577850342
dqn reward tensor(-214.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8336e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1490740329027176
dqn reward tensor(-274.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5423e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16549018025398254
dqn reward tensor(-302.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15737435221672058
dqn reward tensor(-312.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8546e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2286657989025116
dqn reward tensor(-214.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6079e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062442462891340256
dqn reward tensor(-149.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7632e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06521056592464447
dqn reward tensor(-340.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6859e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08902077376842499
dqn reward tensor(-296.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3273e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0318453386425972
dqn reward tensor(-241., device='cuda:0') e 0.05 loss_dqn tensor(1.4751e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08790135383605957
dqn reward tensor(-356.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6498e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1295948624610901
dqn reward tensor(-172.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6568e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1937377154827118
dqn reward tensor(-301.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7807e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09751168638467789
dqn reward tensor(-267., device='cuda:0') e 0.05 loss_dqn tensor(1.6572e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11099766194820404
dqn reward tensor(-246.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7908e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11535066366195679
dqn reward tensor(-316.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4361e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0217884574085474
dqn reward tensor(-394.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7171e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12355531752109528
dqn reward tensor(-364.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7457e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10327562689781189
dqn reward tensor(-278.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3981e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08086098730564117
dqn reward tensor(-197.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5866e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28255605697631836
dqn reward tensor(-305., device='cuda:0') e 0.05 loss_dqn tensor(1.2438e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38316333293914795
dqn reward tensor(-372.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8163e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13512027263641357
dqn reward tensor(-289.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5701e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22105585038661957
dqn reward tensor(-287.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5045e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05856369063258171
dqn reward tensor(-290.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7299e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18948808312416077
dqn reward tensor(-235.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6699e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07331270724534988
dqn reward tensor(-246.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.7973e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22238583862781525
dqn reward tensor(-339.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5067e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18220971524715424
dqn reward tensor(-276.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6473e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1855441927909851
dqn reward tensor(-294.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9545e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07792162150144577
dqn reward tensor(-190.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.7609e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14222267270088196
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8518e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033325519412755966
dqn reward tensor(-216.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8295e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22734184563159943
dqn reward tensor(-249.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0376e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10745872557163239
dqn reward tensor(-269.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4852e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05381893366575241
dqn reward tensor(-320.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1080e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017384525388479233
dqn reward tensor(-429.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1418e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19302037358283997
dqn reward tensor(-400.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2231e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18448078632354736
dqn reward tensor(-313.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9064e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01375336479395628
dqn reward tensor(-272., device='cuda:0') e 0.05 loss_dqn tensor(1.9696e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1442284882068634
dqn reward tensor(-223., device='cuda:0') e 0.05 loss_dqn tensor(1.9973e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13598856329917908
dqn reward tensor(-282.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0735e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1584278643131256
dqn reward tensor(-241.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2471235990524292
dqn reward tensor(-410.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1569e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.155359148979187
dqn reward tensor(-264.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3547e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1751507818698883
dqn reward tensor(-349., device='cuda:0') e 0.05 loss_dqn tensor(2.1699e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23152416944503784
dqn reward tensor(-379.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7926e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039298079907894135
dqn reward tensor(-102.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1447e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048007067292928696
dqn reward tensor(-80.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2572e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1323632001876831
dqn reward tensor(-247.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9005e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17466728389263153
dqn reward tensor(-361.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2284e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10596836358308792
dqn reward tensor(-298.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11650965362787247
dqn reward tensor(-287.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3263e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07374002784490585
dqn reward tensor(-361.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6706e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16360196471214294
dqn reward tensor(-382.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9613e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0823500007390976
dqn reward tensor(-294.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.3804e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19237375259399414
dqn reward tensor(-53.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0756e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030510256066918373
Evaluating...
Train: {'rocauc': 0.7340560364041528} -4.557553768157959
=====Epoch 5=====
Training...
dqn reward tensor(-301.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8458e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2116248905658722
dqn reward tensor(-311., device='cuda:0') e 0.05 loss_dqn tensor(2.7959e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15611985325813293
dqn reward tensor(-230.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8016e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16036356985569
dqn reward tensor(-257.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8971e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27520567178726196
dqn reward tensor(-401.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7333e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09529943764209747
dqn reward tensor(-284.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0103e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09320899844169617
dqn reward tensor(-178.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2295e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021595437079668045
dqn reward tensor(-274.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9681e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14268003404140472
dqn reward tensor(-398.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8283e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02195356786251068
dqn reward tensor(-413.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7826e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0762333944439888
dqn reward tensor(-410.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8348e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16859683394432068
dqn reward tensor(-446.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3330e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2627981901168823
dqn reward tensor(-182.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8892e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17584137618541718
dqn reward tensor(-387.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3502e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1033191829919815
dqn reward tensor(-464.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3607e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14111876487731934
dqn reward tensor(-354.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9029e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2292335331439972
dqn reward tensor(-420.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.0571e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11584850400686264
dqn reward tensor(-429.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0217e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2245217114686966
dqn reward tensor(-406., device='cuda:0') e 0.05 loss_dqn tensor(2.3348e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12022407352924347
dqn reward tensor(-252.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09581038355827332
dqn reward tensor(-422.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1424e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13856783509254456
dqn reward tensor(-481.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3071e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1468908041715622
dqn reward tensor(-354.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1477e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04181260988116264
dqn reward tensor(-197.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3433e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17825987935066223
dqn reward tensor(-192.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4574e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11389236152172089
dqn reward tensor(-248.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13392427563667297
dqn reward tensor(-316.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8106e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06847555935382843
dqn reward tensor(-240.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6767e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07362037897109985
dqn reward tensor(-346.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9148e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23320633172988892
dqn reward tensor(-428.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.4845e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1478131115436554
dqn reward tensor(-379.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6813e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2801538407802582
dqn reward tensor(-310., device='cuda:0') e 0.05 loss_dqn tensor(2.9838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0830916091799736
dqn reward tensor(-460.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.6900e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06858767569065094
dqn reward tensor(-411.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9076e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08307123184204102
dqn reward tensor(-219.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6891e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15527722239494324
dqn reward tensor(-331.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6906e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07791049778461456
dqn reward tensor(-395.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0784e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1295889914035797
dqn reward tensor(-444.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24381716549396515
dqn reward tensor(-174.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030936356633901596
dqn reward tensor(-309.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6554e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2696255147457123
dqn reward tensor(-283.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2472e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11097102612257004
dqn reward tensor(-251.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9250e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20460796356201172
dqn reward tensor(-277.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0809e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12288005650043488
dqn reward tensor(-354.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5211e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15670651197433472
dqn reward tensor(-378.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8828e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07524796575307846
dqn reward tensor(-355.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8225e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14335429668426514
dqn reward tensor(-274.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5391e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15412917733192444
dqn reward tensor(-313.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9483e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07595080137252808
dqn reward tensor(-287.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5861e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13167646527290344
dqn reward tensor(-260.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5043e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14819350838661194
dqn reward tensor(-260.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0837e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09234737604856491
dqn reward tensor(-434.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4157e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2413187325000763
dqn reward tensor(-263.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2770e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17689894139766693
dqn reward tensor(-384.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2766e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20500360429286957
dqn reward tensor(-61.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1755e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12251564115285873
dqn reward tensor(-180., device='cuda:0') e 0.05 loss_dqn tensor(3.0739e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33221861720085144
dqn reward tensor(-151., device='cuda:0') e 0.05 loss_dqn tensor(3.1669e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22020480036735535
dqn reward tensor(-185.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1379e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048424262553453445
dqn reward tensor(-166., device='cuda:0') e 0.05 loss_dqn tensor(3.0891e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22031700611114502
dqn reward tensor(-291.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7597e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1266280561685562
dqn reward tensor(-218.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1192e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22365443408489227
dqn reward tensor(-311., device='cuda:0') e 0.05 loss_dqn tensor(2.7251e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0689457505941391
dqn reward tensor(-175.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3982e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10986921191215515
dqn reward tensor(-300.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0132e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17999914288520813
dqn reward tensor(-56.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0153e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08565040677785873
dqn reward tensor(-216.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17972253262996674
dqn reward tensor(-141.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8017e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2375583052635193
dqn reward tensor(-7.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.8569e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14991876482963562
dqn reward tensor(-194.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3930e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19426657259464264
dqn reward tensor(-302.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2303e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028245670720934868
dqn reward tensor(-250.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3040e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08989286422729492
dqn reward tensor(-233., device='cuda:0') e 0.05 loss_dqn tensor(3.2235e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.242034912109375
dqn reward tensor(-305.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0336e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1464880108833313
dqn reward tensor(-339.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9525e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03439819812774658
dqn reward tensor(-230.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4286e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2314290702342987
dqn reward tensor(-218.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2045e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1754532754421234
dqn reward tensor(-94.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2457e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06642503291368484
dqn reward tensor(-255.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3332e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24322697520256042
dqn reward tensor(-270.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8041e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04312572255730629
dqn reward tensor(-225.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03537805378437042
dqn reward tensor(-339.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8930e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03606467694044113
dqn reward tensor(-271.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1528378576040268
dqn reward tensor(-269.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1080479621887207
dqn reward tensor(-320.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.7764e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20128914713859558
dqn reward tensor(-219.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1241e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0956561267375946
dqn reward tensor(-236.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.2542e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06599396467208862
dqn reward tensor(-239.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5041e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10936927795410156
dqn reward tensor(-194.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1933014690876007
dqn reward tensor(-163.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9875e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16067107021808624
dqn reward tensor(-200.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6966e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17573173344135284
dqn reward tensor(-252.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4025e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25713014602661133
dqn reward tensor(-228.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08357618749141693
dqn reward tensor(-253., device='cuda:0') e 0.05 loss_dqn tensor(2.4170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18356549739837646
dqn reward tensor(-310.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5814e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25521549582481384
dqn reward tensor(-169.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08240996301174164
dqn reward tensor(-396., device='cuda:0') e 0.05 loss_dqn tensor(1.8933e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1557071954011917
dqn reward tensor(-213.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9882e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21144959330558777
dqn reward tensor(-18.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2167e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14527389407157898
dqn reward tensor(-327., device='cuda:0') e 0.05 loss_dqn tensor(2.4120e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09500554949045181
dqn reward tensor(-261.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2157e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04263778403401375
dqn reward tensor(-242.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2921e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19331638514995575
dqn reward tensor(-163.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2884e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054002176970243454
dqn reward tensor(-173., device='cuda:0') e 0.05 loss_dqn tensor(2.3755e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16047924757003784
dqn reward tensor(-258., device='cuda:0') e 0.05 loss_dqn tensor(2.4478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12180676311254501
dqn reward tensor(-354.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3742e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15279535949230194
dqn reward tensor(-103., device='cuda:0') e 0.05 loss_dqn tensor(2.5749e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12744714319705963
dqn reward tensor(-283., device='cuda:0') e 0.05 loss_dqn tensor(2.4882e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34380340576171875
dqn reward tensor(-394.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5694e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17407460510730743
dqn reward tensor(-349.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2002e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04463295266032219
dqn reward tensor(-221., device='cuda:0') e 0.05 loss_dqn tensor(2.5785e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12160040438175201
dqn reward tensor(-226.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6822e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13782580196857452
dqn reward tensor(-83., device='cuda:0') e 0.05 loss_dqn tensor(2.4439e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045375287532806396
dqn reward tensor(-163.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6772e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09311437606811523
dqn reward tensor(-297.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.3867e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09943590313196182
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7753e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14871746301651
dqn reward tensor(-298.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8830e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2490944117307663
dqn reward tensor(-349.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4778e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2452976256608963
dqn reward tensor(-305.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4413e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3596499562263489
dqn reward tensor(-198.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5847e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12809313833713531
dqn reward tensor(-286.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3464e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09728753566741943
dqn reward tensor(-286.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1564810425043106
dqn reward tensor(-256.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.5898e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20632189512252808
dqn reward tensor(-279.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6525e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23533895611763
dqn reward tensor(-299.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5782e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11182389408349991
dqn reward tensor(-281.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7095e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1658252775669098
dqn reward tensor(-226.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.6549e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056746773421764374
dqn reward tensor(-275.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6079e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19385290145874023
dqn reward tensor(-307.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6451e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15694588422775269
dqn reward tensor(-307.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7052e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08353202044963837
dqn reward tensor(-284.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6706e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18236327171325684
dqn reward tensor(-106.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6262e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08556104451417923
dqn reward tensor(-396.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.6512e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015701981261372566
dqn reward tensor(-201.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5506e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07716608792543411
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7572e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2235482633113861
dqn reward tensor(-179.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19387803971767426
dqn reward tensor(-310.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.8658e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18007507920265198
dqn reward tensor(-341.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7366e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30854982137680054
dqn reward tensor(-200.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7947e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31983688473701477
dqn reward tensor(-230.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4703e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14074929058551788
dqn reward tensor(-176.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5880e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3414187729358673
dqn reward tensor(-234.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3614e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11237502098083496
dqn reward tensor(-280.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2874e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19734546542167664
dqn reward tensor(-214.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6620e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1310425102710724
dqn reward tensor(-225.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6483e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0766160637140274
dqn reward tensor(-315.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8192e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19973275065422058
dqn reward tensor(-254.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17701146006584167
dqn reward tensor(-228.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8773e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13546335697174072
dqn reward tensor(-246.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7387e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11868001520633698
dqn reward tensor(-243.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6073e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22846664488315582
dqn reward tensor(-308.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8668e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840768992900848
dqn reward tensor(-325.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9027e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20412905514240265
dqn reward tensor(-326.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1442e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015584368258714676
dqn reward tensor(-246., device='cuda:0') e 0.05 loss_dqn tensor(2.9344e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15254074335098267
dqn reward tensor(-226.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1382e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.188196063041687
dqn reward tensor(-269.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9131e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2382776439189911
dqn reward tensor(-288.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9516e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04933901131153107
dqn reward tensor(-303.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1622e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3317639231681824
dqn reward tensor(-336.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0764e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04896813631057739
dqn reward tensor(-359.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2476e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25041618943214417
dqn reward tensor(-272.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2000e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05395779013633728
dqn reward tensor(-287.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3549e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2510736286640167
dqn reward tensor(-208.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1084e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08281523734331131
dqn reward tensor(-445.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9660e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07046230137348175
dqn reward tensor(-293.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0399e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09254256635904312
dqn reward tensor(-274.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8360e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07058325409889221
dqn reward tensor(-280.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.8178e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07367388904094696
dqn reward tensor(-290.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9757e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1491023600101471
dqn reward tensor(-383.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1480e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21369147300720215
dqn reward tensor(-147., device='cuda:0') e 0.05 loss_dqn tensor(3.1385e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2362857609987259
dqn reward tensor(-158.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9254e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06897734105587006
dqn reward tensor(-320.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0148e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2108362317085266
dqn reward tensor(-302.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9033e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07277857512235641
dqn reward tensor(-280.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5173e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11434440314769745
dqn reward tensor(-320.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4189e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.298278272151947
dqn reward tensor(-193.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18947574496269226
dqn reward tensor(-340.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2067e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03150263801217079
dqn reward tensor(-214.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3126e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14043548703193665
dqn reward tensor(-283.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3053e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15413901209831238
dqn reward tensor(-242., device='cuda:0') e 0.05 loss_dqn tensor(3.5118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03630056232213974
dqn reward tensor(-246.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1311e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042158741503953934
dqn reward tensor(-380.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07558754831552505
dqn reward tensor(-310.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2776e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10168411582708359
dqn reward tensor(-334.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5501e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05245261266827583
dqn reward tensor(-285.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4777e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04412872716784477
dqn reward tensor(-203.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.5236e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17535743117332458
dqn reward tensor(-263.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3312e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21118101477622986
dqn reward tensor(-206.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4830e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059801310300827026
dqn reward tensor(-265.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2037e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015799768269062042
dqn reward tensor(-207.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1020e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058600232005119324
dqn reward tensor(-237.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0800420269370079
dqn reward tensor(-272.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.0854e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22394931316375732
dqn reward tensor(-178.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7011e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2455141544342041
dqn reward tensor(-190.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.6616e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32167354226112366
dqn reward tensor(-241.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3957e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13063116371631622
dqn reward tensor(-286.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6518e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03785226494073868
dqn reward tensor(-336.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5229e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10075622797012329
dqn reward tensor(-291.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4296e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22024831175804138
dqn reward tensor(-280.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7079e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20207127928733826
dqn reward tensor(-309.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8908e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.068715400993824
dqn reward tensor(-379.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.2262e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10159454494714737
dqn reward tensor(-229.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.8341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06656481325626373
dqn reward tensor(-239., device='cuda:0') e 0.05 loss_dqn tensor(3.1275e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2412138730287552
dqn reward tensor(-235.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6289e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22004741430282593
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4522e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022278331220149994
dqn reward tensor(-279.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0958e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14151570200920105
dqn reward tensor(-203.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9361e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032665885984897614
dqn reward tensor(-326.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7867e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15308478474617004
dqn reward tensor(-332.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6040e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15352347493171692
dqn reward tensor(-351.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.1746e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23005038499832153
dqn reward tensor(-222., device='cuda:0') e 0.05 loss_dqn tensor(3.6230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06741926074028015
dqn reward tensor(-368., device='cuda:0') e 0.05 loss_dqn tensor(4.0112e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038591619580984116
dqn reward tensor(-231.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1712e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044404421001672745
dqn reward tensor(-178.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5490e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3233689069747925
dqn reward tensor(-345.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0996e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25768548250198364
dqn reward tensor(-251.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2284e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07904849201440811
dqn reward tensor(-302.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9603e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1968313455581665
dqn reward tensor(-320.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2337e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19654476642608643
dqn reward tensor(-268.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.2566e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15574273467063904
dqn reward tensor(-250.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2691e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1945836991071701
dqn reward tensor(-320.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1580202430486679
dqn reward tensor(-319.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.7631e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07052649557590485
dqn reward tensor(-276.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.2554e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26817792654037476
dqn reward tensor(-196., device='cuda:0') e 0.05 loss_dqn tensor(4.2321e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059866610914468765
dqn reward tensor(-329., device='cuda:0') e 0.05 loss_dqn tensor(3.6901e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20663592219352722
dqn reward tensor(-318.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9529e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13462387025356293
dqn reward tensor(-252., device='cuda:0') e 0.05 loss_dqn tensor(3.8106e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15038424730300903
dqn reward tensor(-294.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6285e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09086913615465164
dqn reward tensor(-223.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.4515e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14122508466243744
dqn reward tensor(-207., device='cuda:0') e 0.05 loss_dqn tensor(3.6101e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2824156880378723
dqn reward tensor(-285.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4174e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09880707412958145
dqn reward tensor(-213.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1005e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2019837498664856
dqn reward tensor(-298.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0750e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13580042123794556
dqn reward tensor(-201.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20860858261585236
dqn reward tensor(-252.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1829e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10222996771335602
dqn reward tensor(-298.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5193e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12921100854873657
dqn reward tensor(-357.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6060e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07362790405750275
dqn reward tensor(-163.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7276e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14945054054260254
dqn reward tensor(-393.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4617e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10508842766284943
dqn reward tensor(-243.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6541e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2567148804664612
dqn reward tensor(-297.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0461e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09989921748638153
dqn reward tensor(-306., device='cuda:0') e 0.05 loss_dqn tensor(4.6652e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17370867729187012
dqn reward tensor(-230.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7867e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13108545541763306
dqn reward tensor(-215.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06677950173616409
dqn reward tensor(-240., device='cuda:0') e 0.05 loss_dqn tensor(4.6537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16770035028457642
dqn reward tensor(-235.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6591e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08211289346218109
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9236e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11940907686948776
dqn reward tensor(-271.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7345e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07032136619091034
dqn reward tensor(-255.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2342e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0361277237534523
dqn reward tensor(-307.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.6376e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28991439938545227
dqn reward tensor(-132.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7953e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10996392369270325
dqn reward tensor(-315.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9543e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052251122891902924
dqn reward tensor(-251.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9338e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23983879387378693
dqn reward tensor(-268.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5768e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1551181972026825
dqn reward tensor(-170.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9869e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13402120769023895
dqn reward tensor(-236.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0758e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09276513755321503
dqn reward tensor(-284.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8718e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10680598020553589
dqn reward tensor(-176.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8099e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0342249870300293
dqn reward tensor(-305.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.9109e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07771800458431244
dqn reward tensor(-86.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9231e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0776442438364029
dqn reward tensor(-236.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0353e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.150068998336792
dqn reward tensor(-110.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0140e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16075806319713593
dqn reward tensor(-304.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7703e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37280064821243286
dqn reward tensor(-246.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043234825134277344
dqn reward tensor(-302.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8830e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04874636232852936
dqn reward tensor(-225.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5650e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06738314777612686
dqn reward tensor(-308.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9307e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1102856993675232
dqn reward tensor(-289.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9229e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07476469874382019
dqn reward tensor(-184.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3500e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019218074157834053
dqn reward tensor(-240.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4361e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08571243286132812
dqn reward tensor(-279.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9772e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15290169417858124
dqn reward tensor(-137.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2758e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206253558397293
dqn reward tensor(-376.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0188e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12586168944835663
dqn reward tensor(-206.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0392e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1453978568315506
dqn reward tensor(-329.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7566e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12104051560163498
dqn reward tensor(-243.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.0294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2649971544742584
dqn reward tensor(-344.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3476e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18987056612968445
dqn reward tensor(-249.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4636e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09234195947647095
dqn reward tensor(-332.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3565e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04989239573478699
dqn reward tensor(-259.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5832e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04296004772186279
dqn reward tensor(-274., device='cuda:0') e 0.05 loss_dqn tensor(5.3560e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13247764110565186
dqn reward tensor(-202.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3542e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09111863374710083
dqn reward tensor(-221.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1571e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21535328030586243
dqn reward tensor(-197.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4431e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11558613181114197
dqn reward tensor(-290.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7515e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16460497677326202
dqn reward tensor(-263.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2552257776260376
dqn reward tensor(-181.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21568061411380768
dqn reward tensor(-292.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1625e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13492384552955627
dqn reward tensor(-247.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15099455416202545
dqn reward tensor(-274.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2716e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12334632128477097
dqn reward tensor(-321.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3605e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13875725865364075
dqn reward tensor(-160., device='cuda:0') e 0.05 loss_dqn tensor(4.9064e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08233648538589478
dqn reward tensor(-248.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5696e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1793229728937149
dqn reward tensor(-387.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11216408014297485
dqn reward tensor(-350.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7388e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1938897669315338
dqn reward tensor(-192.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.2390e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25259193778038025
dqn reward tensor(-317.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0236e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11030712723731995
dqn reward tensor(-318.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8166e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08587762713432312
dqn reward tensor(-274., device='cuda:0') e 0.05 loss_dqn tensor(4.0537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036080554127693176
dqn reward tensor(-251., device='cuda:0') e 0.05 loss_dqn tensor(4.9228e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35019588470458984
dqn reward tensor(-211.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2036e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06249143183231354
dqn reward tensor(-145.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7922e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1572672575712204
dqn reward tensor(-257.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9786e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08483072370290756
dqn reward tensor(-328.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7596e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15649794042110443
dqn reward tensor(-266.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7980e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03405294567346573
dqn reward tensor(-190.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13922639191150665
dqn reward tensor(-378.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2520e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04077043756842613
dqn reward tensor(-269., device='cuda:0') e 0.05 loss_dqn tensor(4.5126e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19260382652282715
dqn reward tensor(-193.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.1518e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13184884190559387
dqn reward tensor(-181.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9221e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16322708129882812
dqn reward tensor(-374.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2855e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024610593914985657
dqn reward tensor(-312.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5386e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1306169033050537
dqn reward tensor(-346.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3507e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13633087277412415
dqn reward tensor(-192.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2970e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12548668682575226
dqn reward tensor(-243.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4861e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20995840430259705
dqn reward tensor(-380.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.3838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08106885850429535
dqn reward tensor(-299., device='cuda:0') e 0.05 loss_dqn tensor(4.4463e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15734314918518066
dqn reward tensor(-339.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9609e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16795992851257324
dqn reward tensor(-242.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3843e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10574693232774734
dqn reward tensor(-154.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2506e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10021021217107773
dqn reward tensor(-381.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4449e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08306796103715897
dqn reward tensor(-291.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7109e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12640824913978577
dqn reward tensor(-362.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4148e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062019526958465576
dqn reward tensor(-240.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5537e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18005234003067017
dqn reward tensor(-236.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.3989e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19985562562942505
dqn reward tensor(-312.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5661e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034328870475292206
dqn reward tensor(-425.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3154e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057067058980464935
dqn reward tensor(-281.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3335e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21039257943630219
dqn reward tensor(-327., device='cuda:0') e 0.05 loss_dqn tensor(4.4230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20119941234588623
dqn reward tensor(-268.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6833e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13306501507759094
dqn reward tensor(-214.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3255e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19189292192459106
dqn reward tensor(-349., device='cuda:0') e 0.05 loss_dqn tensor(4.1917e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037139005959033966
dqn reward tensor(-338., device='cuda:0') e 0.05 loss_dqn tensor(4.4452e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0900101363658905
dqn reward tensor(-296.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5393e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1651982069015503
dqn reward tensor(-192.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5350e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18081696331501007
dqn reward tensor(-315.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0101e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21280363202095032
dqn reward tensor(-215.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4814e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37466901540756226
dqn reward tensor(-173.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2058e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1074376180768013
dqn reward tensor(-316.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9536e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33100831508636475
dqn reward tensor(-349.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5416e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1604597121477127
dqn reward tensor(-301.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7391e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24100378155708313
dqn reward tensor(-121.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.4449e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376395970582962
dqn reward tensor(-297.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4227e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17034006118774414
dqn reward tensor(-199.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9344e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11311061680316925
dqn reward tensor(-156.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1553e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21032002568244934
dqn reward tensor(-352.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4720e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1429472267627716
dqn reward tensor(-239., device='cuda:0') e 0.05 loss_dqn tensor(4.8775e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10716450214385986
dqn reward tensor(-136.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0616e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.084974005818367
dqn reward tensor(-253.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8388e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22487270832061768
dqn reward tensor(-286.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8150e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15098333358764648
dqn reward tensor(-391.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8879e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12918776273727417
dqn reward tensor(-221.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6633e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35364270210266113
dqn reward tensor(-200.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02672591805458069
dqn reward tensor(-375.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9102e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18031790852546692
dqn reward tensor(-267.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8755e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2834113538265228
dqn reward tensor(-266.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9367e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14508047699928284
dqn reward tensor(-341.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1828906387090683
dqn reward tensor(-284.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0336e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19810941815376282
dqn reward tensor(-304.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0952e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1926060914993286
dqn reward tensor(-295.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.8074e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15411591529846191
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9627e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10706320405006409
dqn reward tensor(-298.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8450e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0985880196094513
dqn reward tensor(-316., device='cuda:0') e 0.05 loss_dqn tensor(4.9028e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050656456500291824
dqn reward tensor(-170.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9277e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0470273531973362
dqn reward tensor(-283.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9561e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20698684453964233
dqn reward tensor(-145.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2373e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14324888586997986
dqn reward tensor(-274.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5420e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.103421151638031
dqn reward tensor(-279.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0906e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3039349317550659
dqn reward tensor(-314.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6276e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28088802099227905
dqn reward tensor(-280.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9461e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15189671516418457
dqn reward tensor(-313.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9448e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15476828813552856
dqn reward tensor(-419.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2524e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02694106660783291
dqn reward tensor(-288.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2975e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09282160550355911
dqn reward tensor(-302.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7797e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0869758203625679
dqn reward tensor(-333.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4949e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16469496488571167
dqn reward tensor(-388.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3069e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02854154258966446
dqn reward tensor(-262.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7048e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1509697288274765
dqn reward tensor(-324.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2043e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049875251948833466
dqn reward tensor(-275.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4525e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.268566757440567
dqn reward tensor(-302.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8203e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15617236495018005
dqn reward tensor(-209.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5417e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1547321081161499
dqn reward tensor(-305.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5645e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06968605518341064
dqn reward tensor(-309.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4385e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16941240429878235
dqn reward tensor(-330.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.6778e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2447521984577179
dqn reward tensor(-221.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3789e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13363656401634216
dqn reward tensor(-261.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3699e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2406463325023651
dqn reward tensor(-355.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.0714e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17460328340530396
dqn reward tensor(-231.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5611e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15552754700183868
dqn reward tensor(-392.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8474e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09473715722560883
dqn reward tensor(-295.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1248e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12479418516159058
dqn reward tensor(-236.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8744e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12327657639980316
dqn reward tensor(-320.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3588e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13761205971240997
dqn reward tensor(-387.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.5210e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0499822236597538
dqn reward tensor(-218.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5056e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21622005105018616
dqn reward tensor(-214.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8705e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18339964747428894
dqn reward tensor(-189.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8736e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0396132692694664
dqn reward tensor(-292.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9243e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08406111598014832
dqn reward tensor(-160.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8264e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20416128635406494
dqn reward tensor(-235.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9772e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11379505693912506
dqn reward tensor(-144.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6323e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09769965708255768
dqn reward tensor(-317.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1127239391207695
dqn reward tensor(-220.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3663e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020765628665685654
dqn reward tensor(-287.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8615e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0891733467578888
dqn reward tensor(-224.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0489e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20400364696979523
dqn reward tensor(-160.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6340e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2520146667957306
dqn reward tensor(-227.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6062e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15059928596019745
dqn reward tensor(-282.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7776e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13719885051250458
dqn reward tensor(-253., device='cuda:0') e 0.05 loss_dqn tensor(6.1263e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1337304264307022
dqn reward tensor(-244.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9918e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14670881628990173
dqn reward tensor(-343.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7049e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20115983486175537
dqn reward tensor(-189.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8762e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15645775198936462
dqn reward tensor(-170.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0025e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26450788974761963
dqn reward tensor(-315.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9683e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19662398099899292
dqn reward tensor(-275.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7697e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19353601336479187
dqn reward tensor(-122.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8167e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19592049717903137
dqn reward tensor(-234.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2671e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1958773136138916
dqn reward tensor(-235.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8778e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07428904622793198
dqn reward tensor(-283.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0414e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2523697018623352
dqn reward tensor(-283.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.7410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16961769759655
dqn reward tensor(-244.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2642e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20616433024406433
dqn reward tensor(-366.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4541e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2100650817155838
dqn reward tensor(-310.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1009e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376095861196518
dqn reward tensor(-355.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1788e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13370507955551147
dqn reward tensor(-306.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0853e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03744130581617355
dqn reward tensor(-411.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16926546394824982
dqn reward tensor(-286.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1861e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06439770758152008
dqn reward tensor(-209.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2953e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12559695541858673
dqn reward tensor(-372.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3451e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4495479464530945
dqn reward tensor(-328.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4296e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07311014086008072
dqn reward tensor(-254.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2257e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17415404319763184
dqn reward tensor(-170.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029558107256889343
dqn reward tensor(-257.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0446e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13124746084213257
dqn reward tensor(-271., device='cuda:0') e 0.05 loss_dqn tensor(6.4657e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2195592075586319
dqn reward tensor(-348., device='cuda:0') e 0.05 loss_dqn tensor(5.3214e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13176952302455902
dqn reward tensor(-279.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0443e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08078169822692871
dqn reward tensor(-249.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4766e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.085151307284832
dqn reward tensor(-367.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19230365753173828
dqn reward tensor(-348., device='cuda:0') e 0.05 loss_dqn tensor(6.3851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13593629002571106
dqn reward tensor(-207.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2541775703430176
dqn reward tensor(-332.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0438714325428009
dqn reward tensor(-299.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7328e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1900751292705536
dqn reward tensor(-317.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1668e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2774072587490082
dqn reward tensor(-366.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08940063416957855
dqn reward tensor(-245.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.3098e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19642379879951477
dqn reward tensor(-353.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8077e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0973602682352066
dqn reward tensor(-247., device='cuda:0') e 0.05 loss_dqn tensor(7.1354e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0921495258808136
dqn reward tensor(-318.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6808e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09091174602508545
dqn reward tensor(-334.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1121e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1273651421070099
dqn reward tensor(-317.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5447e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0818723812699318
dqn reward tensor(-239.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5803e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21293477714061737
dqn reward tensor(-309.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8702e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024270568042993546
dqn reward tensor(-233.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7763e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15047785639762878
dqn reward tensor(-448., device='cuda:0') e 0.05 loss_dqn tensor(6.2963e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016711270436644554
dqn reward tensor(-248., device='cuda:0') e 0.05 loss_dqn tensor(7.5987e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08837848156690598
dqn reward tensor(-345.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6225e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2632027566432953
dqn reward tensor(-319.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7190e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08296281099319458
dqn reward tensor(-295.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.5916e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2881562113761902
dqn reward tensor(-350.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7653e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1108127236366272
dqn reward tensor(-210.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0382e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1722317337989807
dqn reward tensor(-374., device='cuda:0') e 0.05 loss_dqn tensor(7.5437e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12397319823503494
dqn reward tensor(-257.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1445e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2342773675918579
dqn reward tensor(-310.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5941e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15112563967704773
dqn reward tensor(-281.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6435e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15471608936786652
dqn reward tensor(-283.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7050e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2169961929321289
dqn reward tensor(-289.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.0820e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23659878969192505
dqn reward tensor(-286.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6620e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13402779400348663
dqn reward tensor(-304.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2556692063808441
dqn reward tensor(-308.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4753e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20988449454307556
dqn reward tensor(-306.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09686800837516785
dqn reward tensor(-336.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4787e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.129486545920372
dqn reward tensor(-260.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.5741e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0967155322432518
dqn reward tensor(-410.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0230e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13120797276496887
dqn reward tensor(-311.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5897e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0311578381806612
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3568e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2008129358291626
dqn reward tensor(-210., device='cuda:0') e 0.05 loss_dqn tensor(8.0349e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25277987122535706
dqn reward tensor(-292.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13717395067214966
dqn reward tensor(-250.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4574e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15302921831607819
dqn reward tensor(-310.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6079e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13049253821372986
dqn reward tensor(-351.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8989e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05862512066960335
dqn reward tensor(-227.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0143e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07595586031675339
dqn reward tensor(-296.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3005e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06603933870792389
dqn reward tensor(-276.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2779e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13990509510040283
dqn reward tensor(-362.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4998e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017604531720280647
dqn reward tensor(-373.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3364e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.010847020894289017
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4868e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13839180767536163
dqn reward tensor(-317.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1963e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25802773237228394
dqn reward tensor(-301.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6793e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0752808153629303
dqn reward tensor(-270.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7535e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16816404461860657
dqn reward tensor(-308.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6911e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07196374982595444
dqn reward tensor(-473.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7093e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11323734372854233
dqn reward tensor(-251.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4556e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2264348864555359
dqn reward tensor(-205., device='cuda:0') e 0.05 loss_dqn tensor(8.0551e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0622844472527504
dqn reward tensor(-378.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7617e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1758168488740921
dqn reward tensor(-420.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0248e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13990075886249542
dqn reward tensor(-418.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4868e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0909874215722084
dqn reward tensor(-163.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6783e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16448798775672913
dqn reward tensor(-360.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4756e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09657886624336243
dqn reward tensor(-266.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7592e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09964872896671295
dqn reward tensor(-232.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9728e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11330834776163101
dqn reward tensor(-318.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5174e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17816181480884552
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1151e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14865922927856445
dqn reward tensor(-376.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7332e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18904709815979004
dqn reward tensor(-443.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5293e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14306692779064178
dqn reward tensor(-244., device='cuda:0') e 0.05 loss_dqn tensor(7.7058e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11645716428756714
dqn reward tensor(-333.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8571e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1123906746506691
dqn reward tensor(-315.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6710e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14765579998493195
dqn reward tensor(-335.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3220e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14038628339767456
dqn reward tensor(-370.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4319e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18207448720932007
dqn reward tensor(-210.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0610e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11518394947052002
dqn reward tensor(-335.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8276e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06504496186971664
dqn reward tensor(-312.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8141e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0964275524020195
dqn reward tensor(-331., device='cuda:0') e 0.05 loss_dqn tensor(6.7118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025735225528478622
dqn reward tensor(-332.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3520e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09448020905256271
dqn reward tensor(-276.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1063e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26956894993782043
dqn reward tensor(-343.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0572e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08952319622039795
dqn reward tensor(-16.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6319e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024065300822257996
Evaluating...
Train: {'rocauc': 0.7384470961401477} -4.432368755340576
=====Epoch 6=====
Training...
dqn reward tensor(-248., device='cuda:0') e 0.05 loss_dqn tensor(7.4694e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2769797444343567
dqn reward tensor(-258.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5256e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1480340212583542
dqn reward tensor(-348.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7323e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3684355616569519
dqn reward tensor(-304.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3664e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0256529301404953
dqn reward tensor(-283.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5557e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2287903130054474
dqn reward tensor(-212.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1207e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10860320925712585
dqn reward tensor(-429.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08311384916305542
dqn reward tensor(-190.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7251e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05323648452758789
dqn reward tensor(-259.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0996e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08723768591880798
dqn reward tensor(-180., device='cuda:0') e 0.05 loss_dqn tensor(7.5173e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13312825560569763
dqn reward tensor(-122.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2033e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19552510976791382
dqn reward tensor(-279.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0876e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17260077595710754
dqn reward tensor(-384.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23796877264976501
dqn reward tensor(-329.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4280e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22476732730865479
dqn reward tensor(-272.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8167e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1730169653892517
dqn reward tensor(-298.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2213e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24780020117759705
dqn reward tensor(-269., device='cuda:0') e 0.05 loss_dqn tensor(7.9440e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2178223729133606
dqn reward tensor(-328.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2946e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16770868003368378
dqn reward tensor(-289., device='cuda:0') e 0.05 loss_dqn tensor(7.7202e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17011424899101257
dqn reward tensor(-283., device='cuda:0') e 0.05 loss_dqn tensor(8.1955e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20400427281856537
dqn reward tensor(-423., device='cuda:0') e 0.05 loss_dqn tensor(7.6563e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10374389588832855
dqn reward tensor(-296., device='cuda:0') e 0.05 loss_dqn tensor(8.6237e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15180927515029907
dqn reward tensor(-281.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8938e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12118389457464218
dqn reward tensor(-332.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1831e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1769939363002777
dqn reward tensor(-206.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5801e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028637133538722992
dqn reward tensor(-311.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9463e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15285807847976685
dqn reward tensor(-175.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905839592218399
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5274e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2515207529067993
dqn reward tensor(-270.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.2448e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13593757152557373
dqn reward tensor(-300.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8248e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1985546350479126
dqn reward tensor(-338.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1437e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11136853694915771
dqn reward tensor(-280.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5495e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12126106023788452
dqn reward tensor(-298.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6775e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0838363915681839
dqn reward tensor(-151.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3131e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10956015437841415
dqn reward tensor(-146.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7261e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21080487966537476
dqn reward tensor(-251.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3097e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10392054915428162
dqn reward tensor(-269.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.2905e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11354471743106842
dqn reward tensor(-269.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4988e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07746357470750809
dqn reward tensor(-286.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6687e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09906120598316193
dqn reward tensor(-258.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0021e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20203520357608795
dqn reward tensor(-289.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2529e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11025509983301163
dqn reward tensor(-115.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9485e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03029530867934227
dqn reward tensor(-351.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.7164e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21731726825237274
dqn reward tensor(-48.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1396e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17282353341579437
dqn reward tensor(-197.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0721e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21848246455192566
dqn reward tensor(-271.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8314e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2184583842754364
dqn reward tensor(-231.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0226e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1938810795545578
dqn reward tensor(-259., device='cuda:0') e 0.05 loss_dqn tensor(8.7181e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07015903294086456
dqn reward tensor(-218.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0361e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0813697800040245
dqn reward tensor(-207.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0259e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14967767894268036
dqn reward tensor(-232.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0299e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049623847007751465
dqn reward tensor(-295.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0185e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07089453935623169
dqn reward tensor(-137.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0378e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20334531366825104
dqn reward tensor(-318., device='cuda:0') e 0.05 loss_dqn tensor(1.1370e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11211784929037094
dqn reward tensor(-211.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0835e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07534103840589523
dqn reward tensor(-307.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1090e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13036464154720306
dqn reward tensor(-360.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0901e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19894751906394958
dqn reward tensor(-223., device='cuda:0') e 0.05 loss_dqn tensor(8.5845e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17188110947608948
dqn reward tensor(-328.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8532e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13873687386512756
dqn reward tensor(-145.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.2023e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10392618179321289
dqn reward tensor(-290.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0902e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2560204863548279
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1083e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1723087877035141
dqn reward tensor(-281.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0627e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11520315706729889
dqn reward tensor(-296., device='cuda:0') e 0.05 loss_dqn tensor(1.0993e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1144975796341896
dqn reward tensor(-229., device='cuda:0') e 0.05 loss_dqn tensor(1.1702e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13826638460159302
dqn reward tensor(-303., device='cuda:0') e 0.05 loss_dqn tensor(1.0418e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07419168949127197
dqn reward tensor(-75.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0598e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09084539115428925
dqn reward tensor(-274.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9628e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04103314131498337
dqn reward tensor(-288.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0390e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0642625167965889
dqn reward tensor(-206.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6143e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19726435840129852
dqn reward tensor(-294.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1836e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01564096100628376
dqn reward tensor(-90.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1453e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638156622648239
dqn reward tensor(-276.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1781e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07204826921224594
dqn reward tensor(-192.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1409e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04680314287543297
dqn reward tensor(-233.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1687e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0683971643447876
dqn reward tensor(-214.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1916e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12622231245040894
dqn reward tensor(-245.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1457e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11965576559305191
dqn reward tensor(-341.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1862e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20980212092399597
dqn reward tensor(-170.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1601e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0917377695441246
dqn reward tensor(-222.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0606e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01970316842198372
dqn reward tensor(-159.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2247e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0917697623372078
dqn reward tensor(-230., device='cuda:0') e 0.05 loss_dqn tensor(1.1763e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08265634626150131
dqn reward tensor(-260.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1564e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06816456466913223
dqn reward tensor(-186.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0824e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057598620653152466
dqn reward tensor(-429.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2290e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01944618485867977
dqn reward tensor(-141.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0446e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20025910437107086
dqn reward tensor(-291.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0673e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09974346309900284
dqn reward tensor(-296.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2400e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15418988466262817
dqn reward tensor(-166.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0941e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10302229225635529
dqn reward tensor(-210.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2883e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08477146178483963
dqn reward tensor(-316.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2626e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1732518970966339
dqn reward tensor(-290.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2772e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.157557412981987
dqn reward tensor(-217.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2468e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0289215836673975
dqn reward tensor(-245.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1120e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11743126809597015
dqn reward tensor(-299.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2844e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030261438339948654
dqn reward tensor(-277.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2076e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19702520966529846
dqn reward tensor(-216.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2758e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2519300580024719
dqn reward tensor(-383.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3454e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.138018399477005
dqn reward tensor(-322.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1554e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04144631326198578
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3442e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02931961417198181
dqn reward tensor(-256.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3775e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1009364053606987
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2912e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09347538650035858
dqn reward tensor(-233.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3343e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07999049127101898
dqn reward tensor(-308.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0900062620639801
dqn reward tensor(-160., device='cuda:0') e 0.05 loss_dqn tensor(1.3283e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.48102664947509766
dqn reward tensor(-215.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2298e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1224241554737091
dqn reward tensor(-347.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3718e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0860866829752922
dqn reward tensor(-282.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1463e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2767398953437805
dqn reward tensor(-205.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1999e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2404090166091919
dqn reward tensor(-299.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3815e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2642613351345062
dqn reward tensor(-268.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1824e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15698577463626862
dqn reward tensor(-291.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4098e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12751753628253937
dqn reward tensor(-349., device='cuda:0') e 0.05 loss_dqn tensor(1.3836e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16934943199157715
dqn reward tensor(-300.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2750e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08098363876342773
dqn reward tensor(-284.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2397e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19789612293243408
dqn reward tensor(-218.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3354e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1145942211151123
dqn reward tensor(-252.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4305e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14663207530975342
dqn reward tensor(-235.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0848e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1902199685573578
dqn reward tensor(-262.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4010e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1039620041847229
dqn reward tensor(-225.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3813e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17890866100788116
dqn reward tensor(-349.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3978e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1263427585363388
dqn reward tensor(-259.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2176e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.063836008310318
dqn reward tensor(-235.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2937e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049394965171813965
dqn reward tensor(-180.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3705e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10088992118835449
dqn reward tensor(-341.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3846e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2724877595901489
dqn reward tensor(-115.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3442e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2756897211074829
dqn reward tensor(-340., device='cuda:0') e 0.05 loss_dqn tensor(1.4171e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2714495062828064
dqn reward tensor(-160.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3379e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1398811936378479
dqn reward tensor(-272.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3857e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11903000622987747
dqn reward tensor(-133.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3636e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2164294272661209
dqn reward tensor(-149.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1377e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14915962517261505
dqn reward tensor(-178.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2305e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10184133052825928
dqn reward tensor(-228.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3729e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17595385015010834
dqn reward tensor(-221.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2775e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12800875306129456
dqn reward tensor(-252., device='cuda:0') e 0.05 loss_dqn tensor(1.4109e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12973904609680176
dqn reward tensor(-204.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2911e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12690961360931396
dqn reward tensor(-232.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1652e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05259142071008682
dqn reward tensor(-285.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3047e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08436018973588943
dqn reward tensor(-233.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4488e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09726078063249588
dqn reward tensor(-234.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4488e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17119097709655762
dqn reward tensor(-292.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4303e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2195502072572708
dqn reward tensor(-212.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3024e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1028885692358017
dqn reward tensor(-258.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3449e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.133198082447052
dqn reward tensor(-359.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4623e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06679624319076538
dqn reward tensor(-233., device='cuda:0') e 0.05 loss_dqn tensor(1.2278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09274539351463318
dqn reward tensor(-291.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2072e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08408255875110626
dqn reward tensor(-197., device='cuda:0') e 0.05 loss_dqn tensor(1.2406e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18899282813072205
dqn reward tensor(-296.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4515e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04610859602689743
dqn reward tensor(-275.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.2433e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1421336978673935
dqn reward tensor(-272., device='cuda:0') e 0.05 loss_dqn tensor(1.3822e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07388874143362045
dqn reward tensor(-255.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2280e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0879494920372963
dqn reward tensor(-380., device='cuda:0') e 0.05 loss_dqn tensor(1.3375e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08652351796627045
dqn reward tensor(-242.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4897e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07745687663555145
dqn reward tensor(-269.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.4755e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12899912893772125
dqn reward tensor(-356.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3569e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11663048714399338
dqn reward tensor(-189.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3901e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24372631311416626
dqn reward tensor(-331.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4491e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08380578458309174
dqn reward tensor(-203.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2980e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21145714819431305
dqn reward tensor(-182.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2089e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14648506045341492
dqn reward tensor(-161.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4506e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08311766386032104
dqn reward tensor(-143.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4250e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08301421999931335
dqn reward tensor(-291., device='cuda:0') e 0.05 loss_dqn tensor(1.5880e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0984952300786972
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3633e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1876373589038849
dqn reward tensor(-311.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2546e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18664944171905518
dqn reward tensor(-300.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4899e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2824087142944336
dqn reward tensor(-330.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5571e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060686737298965454
dqn reward tensor(-351.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6335e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15312905609607697
dqn reward tensor(-226.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5193e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3629767894744873
dqn reward tensor(-198.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.5218e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4646366536617279
dqn reward tensor(-256.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3676e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08194131404161453
dqn reward tensor(-274.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.5624e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11166001856327057
dqn reward tensor(-174.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4517e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16649633646011353
dqn reward tensor(-148.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4790e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2427302747964859
dqn reward tensor(-194.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4872e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1531509906053543
dqn reward tensor(-277., device='cuda:0') e 0.05 loss_dqn tensor(1.4632e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15027019381523132
dqn reward tensor(-209.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5151e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25041675567626953
dqn reward tensor(-293.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5306e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07290889322757721
dqn reward tensor(-169.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16352614760398865
dqn reward tensor(-193.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5054e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08192229270935059
dqn reward tensor(-120.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6068e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08290969580411911
dqn reward tensor(-189.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4361e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07402797043323517
dqn reward tensor(-199., device='cuda:0') e 0.05 loss_dqn tensor(1.6406e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021264158189296722
dqn reward tensor(-200.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5000e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2823685109615326
dqn reward tensor(-260.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5593e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2619417905807495
dqn reward tensor(-301.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6528e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015220338478684425
dqn reward tensor(-255.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4054e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07289139926433563
dqn reward tensor(-260.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7362e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020444035530090332
dqn reward tensor(-151.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4640e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07972380518913269
dqn reward tensor(-229.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5822e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0187210813164711
dqn reward tensor(-297.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7428e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015948336571455002
dqn reward tensor(-222.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3678e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08570002019405365
dqn reward tensor(-183.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4093e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20367573201656342
dqn reward tensor(-167.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6955e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08673569560050964
dqn reward tensor(-362.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3823e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07097852230072021
dqn reward tensor(-126.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4046e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09651102870702744
dqn reward tensor(-153.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6554e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21098750829696655
dqn reward tensor(-165.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4680e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15517115592956543
dqn reward tensor(-167.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7730e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07824961841106415
dqn reward tensor(-211.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7008e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16840815544128418
dqn reward tensor(-174.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7474e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28428953886032104
dqn reward tensor(-172.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4903e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16441622376441956
dqn reward tensor(-166.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5709e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2561371326446533
dqn reward tensor(-144.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6284e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05992211028933525
dqn reward tensor(-273.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6912e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17003671824932098
dqn reward tensor(-302.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4822e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18350356817245483
dqn reward tensor(-291.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8013e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29600536823272705
dqn reward tensor(-157.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7275e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16186495125293732
dqn reward tensor(-298.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6229e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07307848334312439
dqn reward tensor(-154.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19728460907936096
dqn reward tensor(-235.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7961e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13391482830047607
dqn reward tensor(-323.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18774907290935516
dqn reward tensor(-207.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.6274e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19989803433418274
dqn reward tensor(-255.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7944e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13701921701431274
dqn reward tensor(-237.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8297e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04525255039334297
dqn reward tensor(-282.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9068e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13782024383544922
dqn reward tensor(-239.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8094e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14328862726688385
dqn reward tensor(-271.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9077e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10212335735559464
dqn reward tensor(-209.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6806e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12678970396518707
dqn reward tensor(-337.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6720e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08169738203287125
dqn reward tensor(-184.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8946e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2807686924934387
dqn reward tensor(-206.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9284e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06228284537792206
dqn reward tensor(-233.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8933e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1625957190990448
dqn reward tensor(-302.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8098e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19730393588542938
dqn reward tensor(-265.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.9022e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15329501032829285
dqn reward tensor(-244.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8378e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1922895759344101
dqn reward tensor(-180.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9378e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2451888918876648
dqn reward tensor(-333., device='cuda:0') e 0.05 loss_dqn tensor(1.9216e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17196226119995117
dqn reward tensor(-178.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6909e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1312752366065979
dqn reward tensor(-257.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6738e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17426995933055878
dqn reward tensor(-220.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6969e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11733023077249527
dqn reward tensor(-202.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8869e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0765160396695137
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7737e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1850770115852356
dqn reward tensor(-243.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6942e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13597372174263
dqn reward tensor(-250.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.9237e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062198616564273834
dqn reward tensor(-261.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9448e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.237867534160614
dqn reward tensor(-216.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6313e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10099148750305176
dqn reward tensor(-260., device='cuda:0') e 0.05 loss_dqn tensor(1.6776e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16531236469745636
dqn reward tensor(-261.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9606e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1011178120970726
dqn reward tensor(-168.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9605e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2092040777206421
dqn reward tensor(-349.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9653e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1124579906463623
dqn reward tensor(-355.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.8658e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33853957056999207
dqn reward tensor(-219.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8181e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21835383772850037
dqn reward tensor(-277.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9199e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20460693538188934
dqn reward tensor(-161.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9979e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19144970178604126
dqn reward tensor(-242.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9739e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19932308793067932
dqn reward tensor(-208.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.8969e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05445264279842377
dqn reward tensor(-281.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8873e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1377352774143219
dqn reward tensor(-241.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8214e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0688311904668808
dqn reward tensor(-128.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4994e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14890219271183014
dqn reward tensor(-237.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4986e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14112065732479095
dqn reward tensor(-149.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5891e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08427777886390686
dqn reward tensor(-220., device='cuda:0') e 0.05 loss_dqn tensor(1.6550e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040965735912323
dqn reward tensor(27.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5805e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07782086730003357
dqn reward tensor(-177.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.4560e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1386844515800476
dqn reward tensor(-167.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5588e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16836899518966675
dqn reward tensor(-48.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5698e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04600122198462486
dqn reward tensor(-33.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6931e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21137338876724243
dqn reward tensor(-6., device='cuda:0') e 0.05 loss_dqn tensor(1.5894e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.008130347356200218
dqn reward tensor(-225., device='cuda:0') e 0.05 loss_dqn tensor(1.4947e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1601370871067047
dqn reward tensor(-3.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4323e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14915737509727478
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3352e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1640196293592453
dqn reward tensor(-219.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5488e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18872331082820892
dqn reward tensor(-79.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5450e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3008629381656647
dqn reward tensor(-143.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5024e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0471072681248188
dqn reward tensor(-193.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5033e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16034817695617676
dqn reward tensor(-170.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3715e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12293048202991486
dqn reward tensor(9.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5835e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09221415221691132
dqn reward tensor(-118., device='cuda:0') e 0.05 loss_dqn tensor(1.5995e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16799578070640564
dqn reward tensor(-159.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4959e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17089121043682098
dqn reward tensor(-109.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5274e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15550421178340912
dqn reward tensor(-104.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4907e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13976144790649414
dqn reward tensor(-230., device='cuda:0') e 0.05 loss_dqn tensor(1.4137e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12884649634361267
dqn reward tensor(-154.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5137e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06655368953943253
dqn reward tensor(-194.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3707e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11416174471378326
dqn reward tensor(-157.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6132e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0916338786482811
dqn reward tensor(-143.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4716e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07005807757377625
dqn reward tensor(-224.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5721e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.5181818008422852
dqn reward tensor(-10.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2846e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14070124924182892
dqn reward tensor(-11.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5691e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12212792783975601
dqn reward tensor(-92.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5656e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13127681612968445
dqn reward tensor(-211.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4818e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07052621245384216
dqn reward tensor(-35.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4754e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13177627325057983
dqn reward tensor(-98.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4672e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09158511459827423
dqn reward tensor(-64., device='cuda:0') e 0.05 loss_dqn tensor(1.2541e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21019630134105682
dqn reward tensor(-133.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4942e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13980908691883087
dqn reward tensor(-97.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2789e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06316746026277542
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4680e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17387311160564423
dqn reward tensor(-102.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3543e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09154728055000305
dqn reward tensor(-139.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1618e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18820182979106903
dqn reward tensor(7.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4359e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051179107278585434
dqn reward tensor(-247., device='cuda:0') e 0.05 loss_dqn tensor(1.4273e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13803640007972717
dqn reward tensor(-59., device='cuda:0') e 0.05 loss_dqn tensor(1.2122e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06857934594154358
dqn reward tensor(-103.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3924e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34138110280036926
dqn reward tensor(-88.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3271e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17507421970367432
dqn reward tensor(-172.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4427e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17662495374679565
dqn reward tensor(-61.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2793e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24125152826309204
dqn reward tensor(-121.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3328e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12814009189605713
dqn reward tensor(-37.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0966e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2843291461467743
dqn reward tensor(-47.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1930e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22392861545085907
dqn reward tensor(-176., device='cuda:0') e 0.05 loss_dqn tensor(1.3201e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05779354274272919
dqn reward tensor(-111.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3020e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12376755475997925
dqn reward tensor(-68.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2358e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1888001710176468
dqn reward tensor(-73.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4540e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15464608371257782
dqn reward tensor(-123.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3315e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11998802423477173
dqn reward tensor(-78., device='cuda:0') e 0.05 loss_dqn tensor(1.3674e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08167554438114166
dqn reward tensor(-199.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3581e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16017284989356995
dqn reward tensor(-123.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1123e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1493530571460724
dqn reward tensor(-135., device='cuda:0') e 0.05 loss_dqn tensor(1.3922e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19674469530582428
dqn reward tensor(-72.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3076e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061484575271606445
dqn reward tensor(-28.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4537e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07185136526823044
dqn reward tensor(-87.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3297e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12287776172161102
dqn reward tensor(-162.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1944e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06501676142215729
dqn reward tensor(-81.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3800e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04531858488917351
dqn reward tensor(79.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2753e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13691991567611694
dqn reward tensor(-208.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0133e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02028070017695427
dqn reward tensor(-76.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2209e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060339491814374924
dqn reward tensor(-48.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3044e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08897356688976288
dqn reward tensor(-133.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1799e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19341403245925903
dqn reward tensor(-8.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2605e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20733535289764404
dqn reward tensor(-174.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3144e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524348258972168
dqn reward tensor(-133.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0346e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025208745151758194
dqn reward tensor(21.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0254e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1335616111755371
dqn reward tensor(-163.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3024e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2494077980518341
dqn reward tensor(11.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2293e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05063755065202713
dqn reward tensor(-71., device='cuda:0') e 0.05 loss_dqn tensor(1.1973e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1331024169921875
dqn reward tensor(-63.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2585e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1426512897014618
dqn reward tensor(-236.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0500e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13004565238952637
dqn reward tensor(-185.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1662e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20420168340206146
dqn reward tensor(-173.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3275e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1321137696504593
dqn reward tensor(-220.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2253e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04622416943311691
dqn reward tensor(-183., device='cuda:0') e 0.05 loss_dqn tensor(1.2035e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14926275610923767
dqn reward tensor(-129.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3927e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10054058581590652
dqn reward tensor(-310.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4312e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14611263573169708
dqn reward tensor(-247.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4330e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022672049701213837
dqn reward tensor(-173.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1796e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22660456597805023
dqn reward tensor(-156.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5337e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23153799772262573
dqn reward tensor(-179.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2370e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027496926486492157
dqn reward tensor(-106.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1701e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16475790739059448
dqn reward tensor(-68.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3733e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2506403923034668
dqn reward tensor(-224.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2950e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02054300159215927
dqn reward tensor(-276.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4503e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02479991689324379
dqn reward tensor(-170.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4642e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20620489120483398
dqn reward tensor(-210.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4170e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24971911311149597
dqn reward tensor(-231.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4368e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044447485357522964
dqn reward tensor(-237.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4191e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12292661517858505
dqn reward tensor(-245.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3857e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05825287103652954
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2755e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1343640387058258
dqn reward tensor(-159.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8544e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14206549525260925
dqn reward tensor(-302.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1865e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09732162952423096
dqn reward tensor(-209.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6169e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07147644460201263
dqn reward tensor(-199.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0839e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11659665405750275
dqn reward tensor(-243.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1584e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20730160176753998
dqn reward tensor(-344.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0268e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0966590940952301
dqn reward tensor(-330.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0200e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18948452174663544
dqn reward tensor(-232.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1026e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20817361772060394
dqn reward tensor(-217.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0641e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.185220867395401
dqn reward tensor(-283.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5898e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14399397373199463
dqn reward tensor(-234., device='cuda:0') e 0.05 loss_dqn tensor(9.1829e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17449721693992615
dqn reward tensor(-315.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4266e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22006021440029144
dqn reward tensor(-236.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3024e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17777088284492493
dqn reward tensor(-313.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0156e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26126915216445923
dqn reward tensor(-265.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5726e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18736478686332703
dqn reward tensor(-203.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4109e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12819458544254303
dqn reward tensor(-159.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5858e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21903793513774872
dqn reward tensor(-191.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05194304138422012
dqn reward tensor(-296.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0112e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11775502562522888
dqn reward tensor(-181.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0587e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10701769590377808
dqn reward tensor(-249.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7174e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08843345195055008
dqn reward tensor(-212.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8543e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1506699025630951
dqn reward tensor(-243.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3018749952316284
dqn reward tensor(-197.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0263e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2858523428440094
dqn reward tensor(-304.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0980e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23504501581192017
dqn reward tensor(-221., device='cuda:0') e 0.05 loss_dqn tensor(9.0673e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14447751641273499
dqn reward tensor(-204.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1865e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12796910107135773
dqn reward tensor(-106.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8837e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.142257958650589
dqn reward tensor(-309.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4415e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09032024443149567
dqn reward tensor(-227.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8977e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11810861527919769
dqn reward tensor(-202.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3676e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18194639682769775
dqn reward tensor(-102.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2509e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07310712337493896
dqn reward tensor(-245.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09836385399103165
dqn reward tensor(-160.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3128e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28015583753585815
dqn reward tensor(-168.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2725e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15508879721164703
dqn reward tensor(-123.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14472123980522156
dqn reward tensor(-261.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3330e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06518185883760452
dqn reward tensor(-151.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6029e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022684194147586823
dqn reward tensor(-148.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.8920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09058661758899689
dqn reward tensor(-267.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4509e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23939844965934753
dqn reward tensor(-136., device='cuda:0') e 0.05 loss_dqn tensor(7.4062e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08185673505067825
dqn reward tensor(-261.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1009e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09735982120037079
dqn reward tensor(-234., device='cuda:0') e 0.05 loss_dqn tensor(6.8909e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14611956477165222
dqn reward tensor(-182.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3075e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04434145987033844
dqn reward tensor(-277.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.8723e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14263054728507996
dqn reward tensor(-199.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5211e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0696226954460144
dqn reward tensor(-170.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8556e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1891639083623886
dqn reward tensor(-195.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3165e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04870093613862991
dqn reward tensor(-221.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7176e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2904665172100067
dqn reward tensor(-218.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8291e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1350477933883667
dqn reward tensor(-177.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0803e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058981120586395264
dqn reward tensor(-137.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1760e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1270751953125
dqn reward tensor(-66.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1338e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0417773500084877
dqn reward tensor(-387.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0495e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1413930207490921
dqn reward tensor(-169.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.3286e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1590304970741272
dqn reward tensor(-228.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7291e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499992161989212
dqn reward tensor(-124.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7954e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1890888661146164
dqn reward tensor(-184.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3918e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08779541403055191
dqn reward tensor(-299.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1477e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02918454259634018
dqn reward tensor(-24.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5470e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15502187609672546
dqn reward tensor(-244.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5581e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1606924831867218
dqn reward tensor(-199.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4259e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19790400564670563
dqn reward tensor(-163.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7649e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20548686385154724
dqn reward tensor(-286.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1092e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0857018530368805
dqn reward tensor(-212.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2906152307987213
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9690e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14893154799938202
dqn reward tensor(-223.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2733e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18491092324256897
dqn reward tensor(-93.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.7509e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12778708338737488
dqn reward tensor(-324.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2495e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09608227014541626
dqn reward tensor(-206.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3282e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07547532021999359
dqn reward tensor(-261.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0074e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2054874300956726
dqn reward tensor(-173.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0142e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22430270910263062
dqn reward tensor(-279.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4019e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10575710237026215
dqn reward tensor(-233.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.5591e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18927887082099915
dqn reward tensor(-229.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2790e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11990715563297272
dqn reward tensor(-125.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0614e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0442076250910759
dqn reward tensor(-175.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8811e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17409129440784454
dqn reward tensor(-353.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2173e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09219828248023987
dqn reward tensor(-271.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4015e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17522910237312317
dqn reward tensor(-204.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2742e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1477222591638565
dqn reward tensor(-242.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7749e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1988302320241928
dqn reward tensor(-141.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1895e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2165279984474182
dqn reward tensor(-175.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0722e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07760384678840637
dqn reward tensor(-237.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1212e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20137271285057068
dqn reward tensor(-222.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0790e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13294491171836853
dqn reward tensor(-337.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0025e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29815196990966797
dqn reward tensor(-283., device='cuda:0') e 0.05 loss_dqn tensor(1.1558e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08245986700057983
dqn reward tensor(-346.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1922e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06401878595352173
dqn reward tensor(-166.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0716e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28287824988365173
dqn reward tensor(-306.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0900e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11265020072460175
dqn reward tensor(-175.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0306e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14938180148601532
dqn reward tensor(-68.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1049e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17971521615982056
dqn reward tensor(-221.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4827e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24498504400253296
dqn reward tensor(-193.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0824e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1550598442554474
dqn reward tensor(-217.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0703e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1533074527978897
dqn reward tensor(-136.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1528e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11250302940607071
dqn reward tensor(-287.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1417e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07429077476263046
dqn reward tensor(-227.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1862e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17801803350448608
dqn reward tensor(-317.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2013e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06629598140716553
dqn reward tensor(-268.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1217e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07982094585895538
dqn reward tensor(-199.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0124e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3529793918132782
dqn reward tensor(-297.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0666e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1267678141593933
dqn reward tensor(-155.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2747e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02485339716076851
dqn reward tensor(-306.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1640e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06908409297466278
dqn reward tensor(-398.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1597e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14587074518203735
dqn reward tensor(-138.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0876e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06749745458364487
dqn reward tensor(-264.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1648e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14417186379432678
dqn reward tensor(-334.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8751e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1898578554391861
dqn reward tensor(-163.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2248e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07412336766719818
dqn reward tensor(-247.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0855e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08487853407859802
dqn reward tensor(-197.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2568e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11919514834880829
dqn reward tensor(-200.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2172e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03862391784787178
dqn reward tensor(-263.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1509e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07951430231332779
dqn reward tensor(-227.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1649e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11114966124296188
dqn reward tensor(-268.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0951e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02170977182686329
dqn reward tensor(-162.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1786e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19031310081481934
dqn reward tensor(-130.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1781e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0479116216301918
dqn reward tensor(-133., device='cuda:0') e 0.05 loss_dqn tensor(8.0752e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15771551430225372
dqn reward tensor(-264.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2868e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23608532547950745
dqn reward tensor(-253.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0436e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2971714437007904
dqn reward tensor(-324.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1971e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11718478053808212
dqn reward tensor(-218., device='cuda:0') e 0.05 loss_dqn tensor(9.7899e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12899574637413025
dqn reward tensor(-160.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0860e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2404681295156479
dqn reward tensor(-325.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2063e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19797873497009277
dqn reward tensor(-238.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0350e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14331915974617004
dqn reward tensor(-142.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2409e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23357127606868744
dqn reward tensor(-176.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9402e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2145075500011444
dqn reward tensor(-196.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12450909614562988
dqn reward tensor(-206.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0930e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11690579354763031
dqn reward tensor(-145.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1869e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.138400137424469
dqn reward tensor(-177.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0669e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09242909401655197
dqn reward tensor(-136.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1028e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840876281261444
dqn reward tensor(-159.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.2281e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028897810727357864
dqn reward tensor(-241., device='cuda:0') e 0.05 loss_dqn tensor(1.0067e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2700704038143158
dqn reward tensor(-263.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1852e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07947484403848648
dqn reward tensor(-154.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0025e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10146352648735046
dqn reward tensor(-109.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1319e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13215962052345276
dqn reward tensor(-233.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9762e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15398229658603668
dqn reward tensor(-224.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2489e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2801126539707184
dqn reward tensor(-166.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3014e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18216566741466522
dqn reward tensor(-122.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2696e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05658629536628723
dqn reward tensor(-192.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1334e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07765354961156845
dqn reward tensor(-170.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.2273e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21321797370910645
dqn reward tensor(-119.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2020e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24115920066833496
dqn reward tensor(-289.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0449e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17641979455947876
dqn reward tensor(-184.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2098e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09818609058856964
dqn reward tensor(-283.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0695e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06972118467092514
dqn reward tensor(-236.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5114e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1390099823474884
dqn reward tensor(-70.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0398e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2110351026058197
dqn reward tensor(-174.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0158e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13142859935760498
dqn reward tensor(-251.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1615e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1787821650505066
dqn reward tensor(-175.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1346e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1859193593263626
dqn reward tensor(-172.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0430e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12509159743785858
dqn reward tensor(-259.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1387e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2752254903316498
dqn reward tensor(-358.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1553e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07246682047843933
dqn reward tensor(-283.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0722e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02787027694284916
dqn reward tensor(-259.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8595e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16195721924304962
dqn reward tensor(-192.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0926e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23769959807395935
dqn reward tensor(-276.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8032e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20744721591472626
dqn reward tensor(-215.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9174e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2910608947277069
dqn reward tensor(-239.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0463e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1058054193854332
dqn reward tensor(-266., device='cuda:0') e 0.05 loss_dqn tensor(8.1357e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16932086646556854
dqn reward tensor(-235.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0907e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2178463637828827
dqn reward tensor(-275.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0582e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24460859596729279
dqn reward tensor(-287., device='cuda:0') e 0.05 loss_dqn tensor(1.0971e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22002772986888885
dqn reward tensor(-236.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.5933e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17766547203063965
dqn reward tensor(-272.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0443e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11994363367557526
dqn reward tensor(-49., device='cuda:0') e 0.05 loss_dqn tensor(1.0377e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04548630863428116
Evaluating...
Train: {'rocauc': 0.7506298664390963} -3.7950873374938965
=====Epoch 7=====
Training...
dqn reward tensor(-223.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0015e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21211187541484833
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1385e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17776170372962952
dqn reward tensor(-319.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0745e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12897373735904694
dqn reward tensor(-357.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0866e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1417098343372345
dqn reward tensor(-292.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9575e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1092909649014473
dqn reward tensor(-268.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.0521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01635046675801277
dqn reward tensor(-290.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1189e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.148680180311203
dqn reward tensor(-188.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1226e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2893819808959961
dqn reward tensor(-99.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0780e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1395946741104126
dqn reward tensor(-225.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6642e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0842365175485611
dqn reward tensor(-329.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0797e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018863260746002197
dqn reward tensor(-208.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0905e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08052441477775574
dqn reward tensor(-308., device='cuda:0') e 0.05 loss_dqn tensor(1.0387e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24824738502502441
dqn reward tensor(-157.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3207e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3178287446498871
dqn reward tensor(-298.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2179e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13580608367919922
dqn reward tensor(-201.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1874e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17295745015144348
dqn reward tensor(-228.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3941e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1084488183259964
dqn reward tensor(-290., device='cuda:0') e 0.05 loss_dqn tensor(1.3613e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10960441827774048
dqn reward tensor(-198.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3955e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11974801123142242
dqn reward tensor(-218.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3335e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10160916298627853
dqn reward tensor(-259., device='cuda:0') e 0.05 loss_dqn tensor(1.3864e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1394481062889099
dqn reward tensor(-225.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0938e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09541363269090652
dqn reward tensor(-103.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1786e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14320509135723114
dqn reward tensor(-333.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5886e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09675925970077515
dqn reward tensor(-178.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2441e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16257399320602417
dqn reward tensor(-193.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2579e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3274146318435669
dqn reward tensor(-184.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3465e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09426818042993546
dqn reward tensor(-243.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5792e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621813029050827
dqn reward tensor(-277.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4520e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14425233006477356
dqn reward tensor(-253.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3974e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11783533543348312
dqn reward tensor(-300., device='cuda:0') e 0.05 loss_dqn tensor(1.5483e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18465480208396912
dqn reward tensor(-233.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4128e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12570467591285706
dqn reward tensor(-279.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4141e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1761723756790161
dqn reward tensor(-185.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3101e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049429092556238174
dqn reward tensor(-197.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1172e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09292502701282501
dqn reward tensor(-244.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3044e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1581079065799713
dqn reward tensor(-206.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5314e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13886414468288422
dqn reward tensor(-334.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0376e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08240841329097748
dqn reward tensor(-299.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3068e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03411538153886795
dqn reward tensor(-313.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2155e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032877061516046524
dqn reward tensor(-185.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0466e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06950339674949646
dqn reward tensor(-167.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4050e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.165008544921875
dqn reward tensor(-336.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4548e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08775416761636734
dqn reward tensor(-366.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5423e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11560072004795074
dqn reward tensor(-287.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4022e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10525311529636383
dqn reward tensor(-307.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5532e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2433742880821228
dqn reward tensor(-329.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3785e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06382942199707031
dqn reward tensor(-313.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2604e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18110081553459167
dqn reward tensor(-261.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3292e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06572209298610687
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5649e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1516827493906021
dqn reward tensor(-303.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2662e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1677890568971634
dqn reward tensor(-304.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1502e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08266645669937134
dqn reward tensor(-336., device='cuda:0') e 0.05 loss_dqn tensor(1.4398e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24615854024887085
dqn reward tensor(-471.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0546e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15235793590545654
dqn reward tensor(-250.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4491e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09493997693061829
dqn reward tensor(-316.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5332e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05468691512942314
dqn reward tensor(-333., device='cuda:0') e 0.05 loss_dqn tensor(1.1587e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041682709008455276
dqn reward tensor(-113.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0997e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10517250001430511
dqn reward tensor(-332.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2630e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12213670462369919
dqn reward tensor(-187.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5217e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09742600470781326
dqn reward tensor(-102.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4883e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14678896963596344
dqn reward tensor(-247.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4323e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12948794662952423
dqn reward tensor(-335.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3473e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2444697916507721
dqn reward tensor(-269.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4562e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16778519749641418
dqn reward tensor(-281.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2847e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13873717188835144
dqn reward tensor(-185.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4572e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2341664433479309
dqn reward tensor(-324.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2365e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1312178373336792
dqn reward tensor(-299.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4169e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025214355438947678
dqn reward tensor(-325.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7242e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2345365285873413
dqn reward tensor(-324.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5886e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1607368290424347
dqn reward tensor(-371.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6031e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06425803154706955
dqn reward tensor(-407.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3151e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18462596833705902
dqn reward tensor(-181.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2733e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2537197470664978
dqn reward tensor(-260.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5147e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16667211055755615
dqn reward tensor(-301.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2073e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12627092003822327
dqn reward tensor(-301.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2134e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1653534471988678
dqn reward tensor(-137.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6013e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14663638174533844
dqn reward tensor(-217.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4901e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11634287983179092
dqn reward tensor(-229.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6711e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24411486089229584
dqn reward tensor(-243.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6449e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2072640210390091
dqn reward tensor(-390.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3301e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08004899322986603
dqn reward tensor(-290.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7415e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20752684772014618
dqn reward tensor(-314.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5492e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16211576759815216
dqn reward tensor(-233.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8737e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.379329651594162
dqn reward tensor(-322.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8084e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16312448680400848
dqn reward tensor(-335.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5930e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23805095255374908
dqn reward tensor(-303.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4838e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09255392849445343
dqn reward tensor(-268., device='cuda:0') e 0.05 loss_dqn tensor(1.3880e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1320512294769287
dqn reward tensor(-384.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9328e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23896652460098267
dqn reward tensor(-190.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7861e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1188223734498024
dqn reward tensor(-366.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6818e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06324206292629242
dqn reward tensor(-192., device='cuda:0') e 0.05 loss_dqn tensor(1.6731e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16459020972251892
dqn reward tensor(-269.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7171e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21671423316001892
dqn reward tensor(-344.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7254e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15918368101119995
dqn reward tensor(-215.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6224e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18794649839401245
dqn reward tensor(-378.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6932e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22368615865707397
dqn reward tensor(-285.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6703e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12394310534000397
dqn reward tensor(-246.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4141e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1722586750984192
dqn reward tensor(-393.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1167e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.158280611038208
dqn reward tensor(-270., device='cuda:0') e 0.05 loss_dqn tensor(9.8814e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038071732968091965
dqn reward tensor(-277.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.7127e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09990651905536652
dqn reward tensor(-294.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8713e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407562494277954
dqn reward tensor(-130., device='cuda:0') e 0.05 loss_dqn tensor(7.3802e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09580902755260468
dqn reward tensor(-304.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.6841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08927930146455765
dqn reward tensor(-315., device='cuda:0') e 0.05 loss_dqn tensor(6.2912e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05467679351568222
dqn reward tensor(-347.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9765e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03502617031335831
dqn reward tensor(-349.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.8343e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019920947030186653
dqn reward tensor(-247.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2756e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22494223713874817
dqn reward tensor(-190.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1978e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14938457310199738
dqn reward tensor(-175., device='cuda:0') e 0.05 loss_dqn tensor(6.7680e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04489579796791077
dqn reward tensor(-443.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.0021e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.141495943069458
dqn reward tensor(-238.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08927778899669647
dqn reward tensor(-253., device='cuda:0') e 0.05 loss_dqn tensor(8.3465e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13442668318748474
dqn reward tensor(-223.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6637e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26266926527023315
dqn reward tensor(-239.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0624e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08130085468292236
dqn reward tensor(-307.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6224e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1660635769367218
dqn reward tensor(-389.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9160e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3095167875289917
dqn reward tensor(-236.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2165e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14227378368377686
dqn reward tensor(-300., device='cuda:0') e 0.05 loss_dqn tensor(6.0432e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047174643725156784
dqn reward tensor(-305.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06072092801332474
dqn reward tensor(-373., device='cuda:0') e 0.05 loss_dqn tensor(6.8685e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13880637288093567
dqn reward tensor(-398.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.0496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1546347439289093
dqn reward tensor(-200.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1909e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09522400796413422
dqn reward tensor(-266.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04200177267193794
dqn reward tensor(-267.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.6127e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08556337654590607
dqn reward tensor(-76.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1877e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15918248891830444
dqn reward tensor(-192.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4053e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020894652232527733
dqn reward tensor(-267.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8934e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09055645018815994
dqn reward tensor(-171.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2556e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048197370022535324
dqn reward tensor(-283.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.9195e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0261337012052536
dqn reward tensor(-293.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.0422e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1572325974702835
dqn reward tensor(-169.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9588e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.011514219455420971
dqn reward tensor(-392.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.4809e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17713451385498047
dqn reward tensor(-293.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6976e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.007957085967063904
dqn reward tensor(-374.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2469e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15463998913764954
dqn reward tensor(-313.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3367e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08548997342586517
dqn reward tensor(-329.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06009512022137642
dqn reward tensor(-211., device='cuda:0') e 0.05 loss_dqn tensor(6.0795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22032207250595093
dqn reward tensor(-299.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17175054550170898
dqn reward tensor(-330.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012224694713950157
dqn reward tensor(-274.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9139e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1582106202840805
dqn reward tensor(-105.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2058e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10449352860450745
dqn reward tensor(-293.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4207e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16671282052993774
dqn reward tensor(-290.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0001e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023795830085873604
dqn reward tensor(-262.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1881e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08566799014806747
dqn reward tensor(-219., device='cuda:0') e 0.05 loss_dqn tensor(6.3117e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07562689483165741
dqn reward tensor(-319.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9820e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13667848706245422
dqn reward tensor(-280.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3092e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22281816601753235
dqn reward tensor(-200.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.6858e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1789175271987915
dqn reward tensor(-312.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2329e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14237333834171295
dqn reward tensor(-149., device='cuda:0') e 0.05 loss_dqn tensor(7.1569e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12190041691064835
dqn reward tensor(-150.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9550e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17492906749248505
dqn reward tensor(-169.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.7262e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3069370687007904
dqn reward tensor(-274.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5291e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20187467336654663
dqn reward tensor(-398.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0194e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10518314689397812
dqn reward tensor(-260.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3183e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1379050463438034
dqn reward tensor(-338.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6599e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1903248131275177
dqn reward tensor(-355.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22237974405288696
dqn reward tensor(-307.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06450194120407104
dqn reward tensor(-229.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9357e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10601122677326202
dqn reward tensor(-227.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1002e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09533119201660156
dqn reward tensor(-166.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1087e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17244650423526764
dqn reward tensor(-329.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6671e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029709573835134506
dqn reward tensor(-330.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.2993e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08423268795013428
dqn reward tensor(-323.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0136e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04481473192572594
dqn reward tensor(-243.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7646e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015535504557192326
dqn reward tensor(-297.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8242e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12350335717201233
dqn reward tensor(-180.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9924e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23807722330093384
dqn reward tensor(-239., device='cuda:0') e 0.05 loss_dqn tensor(7.7114e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2114507257938385
dqn reward tensor(-264.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3576e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016419706866145134
dqn reward tensor(-328.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9676e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13540591299533844
dqn reward tensor(-394.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1696e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08185295015573502
dqn reward tensor(-314., device='cuda:0') e 0.05 loss_dqn tensor(8.2318e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0796324834227562
dqn reward tensor(-272.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.6352e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18546734750270844
dqn reward tensor(-309.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1510e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0934082567691803
dqn reward tensor(-285., device='cuda:0') e 0.05 loss_dqn tensor(8.2740e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2238190770149231
dqn reward tensor(-262.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.6819e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13062222301959991
dqn reward tensor(-159.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4563e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025994250550866127
dqn reward tensor(-234.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0723e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06564011424779892
dqn reward tensor(-289.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0021e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10073159635066986
dqn reward tensor(-198.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1921887993812561
dqn reward tensor(-256.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9788e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19824162125587463
dqn reward tensor(-393.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0177e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10131976008415222
dqn reward tensor(-262.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4184e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06135019659996033
dqn reward tensor(-288., device='cuda:0') e 0.05 loss_dqn tensor(1.2698e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24735556542873383
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2294e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11473363637924194
dqn reward tensor(-290., device='cuda:0') e 0.05 loss_dqn tensor(1.0180e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15357442200183868
dqn reward tensor(-220.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4061e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15593326091766357
dqn reward tensor(-284.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2846696972846985
dqn reward tensor(-313.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0436e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2479502260684967
dqn reward tensor(-218.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6399e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13224071264266968
dqn reward tensor(-313.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2358e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2241237610578537
dqn reward tensor(-342.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2354e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30997180938720703
dqn reward tensor(-254.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0367e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2575317621231079
dqn reward tensor(-491.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0794e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13542209565639496
dqn reward tensor(-363.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3298e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10891887545585632
dqn reward tensor(-305., device='cuda:0') e 0.05 loss_dqn tensor(1.0813e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11857135593891144
dqn reward tensor(-309.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0771e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08248584717512131
dqn reward tensor(-296.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4807e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05614732205867767
dqn reward tensor(-172.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0597e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15103843808174133
dqn reward tensor(-317.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1159e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17417195439338684
dqn reward tensor(-301.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7267e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20193707942962646
dqn reward tensor(-303.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0890e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1390087902545929
dqn reward tensor(-187.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0842e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13984587788581848
dqn reward tensor(-306.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5482e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14823530614376068
dqn reward tensor(-251.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0620e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10986784845590591
dqn reward tensor(-211.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3555e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2801561951637268
dqn reward tensor(-125.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1540e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1327504813671112
dqn reward tensor(-314.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2458e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09971540421247482
dqn reward tensor(-314.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1103e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09951703250408173
dqn reward tensor(-214.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1811e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04876730591058731
dqn reward tensor(-383., device='cuda:0') e 0.05 loss_dqn tensor(1.0975e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17907851934432983
dqn reward tensor(-333.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5675e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03739379346370697
dqn reward tensor(-280.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4587e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1876106858253479
dqn reward tensor(-314.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1219e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1257772147655487
dqn reward tensor(-238.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0491e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15364232659339905
dqn reward tensor(-344.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0950e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1463470757007599
dqn reward tensor(-155., device='cuda:0') e 0.05 loss_dqn tensor(1.0951e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21931171417236328
dqn reward tensor(-264.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1118e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12852442264556885
dqn reward tensor(-313.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1140e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11135257035493851
dqn reward tensor(-275.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0790e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0958905816078186
dqn reward tensor(-342.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0843e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09369279444217682
dqn reward tensor(-259.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0295e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03593504801392555
dqn reward tensor(-327.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1141e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13248547911643982
dqn reward tensor(-198., device='cuda:0') e 0.05 loss_dqn tensor(1.2593e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21993637084960938
dqn reward tensor(-250., device='cuda:0') e 0.05 loss_dqn tensor(1.0391e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2044784128665924
dqn reward tensor(-192.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9884e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20526060461997986
dqn reward tensor(-258.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0001e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15759599208831787
dqn reward tensor(-217.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3164e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17758417129516602
dqn reward tensor(-281.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0388e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054096926003694534
dqn reward tensor(-302.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6846e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05607244744896889
dqn reward tensor(-305.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3310e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08705020695924759
dqn reward tensor(-223., device='cuda:0') e 0.05 loss_dqn tensor(1.2563e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0656595453619957
dqn reward tensor(-256.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4176e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09573175013065338
dqn reward tensor(-214.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4026e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18523448705673218
dqn reward tensor(-251.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1495e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1308717578649521
dqn reward tensor(-276.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.4529e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.256440669298172
dqn reward tensor(-297.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1077e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11407837271690369
dqn reward tensor(-351.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4322e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1726956069469452
dqn reward tensor(-350.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4365e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20688581466674805
dqn reward tensor(-370.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3771e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07892176508903503
dqn reward tensor(-212., device='cuda:0') e 0.05 loss_dqn tensor(1.6093e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08654169738292694
dqn reward tensor(-383.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8266e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17269915342330933
dqn reward tensor(-248.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2531e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17543980479240417
dqn reward tensor(-334.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5134e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24786508083343506
dqn reward tensor(-310., device='cuda:0') e 0.05 loss_dqn tensor(1.5413e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22375372052192688
dqn reward tensor(-291.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2153e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08168069273233414
dqn reward tensor(-339.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2059e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08953151106834412
dqn reward tensor(-281.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2888e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18438026309013367
dqn reward tensor(-425.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2321e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.311662882566452
dqn reward tensor(-307.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2940e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3127577006816864
dqn reward tensor(-263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8375e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09195780754089355
dqn reward tensor(-235.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5323e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22379601001739502
dqn reward tensor(-311.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4987e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10673210769891739
dqn reward tensor(-241.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3288e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15992817282676697
dqn reward tensor(-317.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2908e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10441215336322784
dqn reward tensor(-221.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8307e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06645775586366653
dqn reward tensor(-371., device='cuda:0') e 0.05 loss_dqn tensor(1.5126e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07357451319694519
dqn reward tensor(-298.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7250e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13423340022563934
dqn reward tensor(-297.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5923e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08506318181753159
dqn reward tensor(-266.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2612e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22001886367797852
dqn reward tensor(-479.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3582e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22456896305084229
dqn reward tensor(-371.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7219e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17897547781467438
dqn reward tensor(-323.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3400e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4523366689682007
dqn reward tensor(-282.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3613e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06351137906312943
dqn reward tensor(-388.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3388e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11260510236024857
dqn reward tensor(-385.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3021e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12345743179321289
dqn reward tensor(-172.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2709e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08015772700309753
dqn reward tensor(-332.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5271e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03432340919971466
dqn reward tensor(-202.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3493e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.147390678524971
dqn reward tensor(-388.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7174e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2637348175048828
dqn reward tensor(-321.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2526e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08276985585689545
dqn reward tensor(-255.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5610e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03865712136030197
dqn reward tensor(-252.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6864e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047137852758169174
dqn reward tensor(-234.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3629e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20830193161964417
dqn reward tensor(-256.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1656e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22972901165485382
dqn reward tensor(-272.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5497e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09414611756801605
dqn reward tensor(-216.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1345e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16966620087623596
dqn reward tensor(-271.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6526e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1786869615316391
dqn reward tensor(-217.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2381e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1791960895061493
dqn reward tensor(-239.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1294e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1337406486272812
dqn reward tensor(-308.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4437e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16567783057689667
dqn reward tensor(-259.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3796e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16247746348381042
dqn reward tensor(-201.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1469e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14670036733150482
dqn reward tensor(-297.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7965e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20821228623390198
dqn reward tensor(-206.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1697e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19163814187049866
dqn reward tensor(-221.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3736e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18773290514945984
dqn reward tensor(-263., device='cuda:0') e 0.05 loss_dqn tensor(1.1604e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04866266995668411
dqn reward tensor(-179.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1772e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055883683264255524
dqn reward tensor(-334.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2137e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11139749735593796
dqn reward tensor(-171.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1625e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16046826541423798
dqn reward tensor(-259.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2130e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1466015726327896
dqn reward tensor(-198.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2425e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2459092140197754
dqn reward tensor(-229.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2549e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03825848549604416
dqn reward tensor(-188., device='cuda:0') e 0.05 loss_dqn tensor(1.1981e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15271921455860138
dqn reward tensor(-230.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6464e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14362144470214844
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2548e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09792589396238327
dqn reward tensor(-331.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1957e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02794838137924671
dqn reward tensor(-339.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4575e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11410732567310333
dqn reward tensor(-286.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5397e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21046851575374603
dqn reward tensor(-336.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7895e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14008790254592896
dqn reward tensor(-137.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4469e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13900160789489746
dqn reward tensor(-270.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2541e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07993101328611374
dqn reward tensor(-151.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6460e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13348323106765747
dqn reward tensor(-296.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2752e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2084382176399231
dqn reward tensor(-229.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2946e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09247195720672607
dqn reward tensor(-201.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2948e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11840807646512985
dqn reward tensor(-200.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2271e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1275462806224823
dqn reward tensor(-218.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3145e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2433575540781021
dqn reward tensor(-339.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7835e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09297004342079163
dqn reward tensor(-329.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4702e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21492460370063782
dqn reward tensor(-116.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2428e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09307345002889633
dqn reward tensor(-278.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2292e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18708860874176025
dqn reward tensor(-269.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2293e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24636109173297882
dqn reward tensor(-218.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2995e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10690778493881226
dqn reward tensor(-473.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2606e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1198873221874237
dqn reward tensor(-296.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3877e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09796956181526184
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2099e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13970573246479034
dqn reward tensor(-271.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2326e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13838356733322144
dqn reward tensor(-259.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6357e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1584322154521942
dqn reward tensor(-342.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2096e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07655222713947296
dqn reward tensor(-312.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2258e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1415153443813324
dqn reward tensor(-213.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2596e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33567237854003906
dqn reward tensor(-283.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3696e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12575335800647736
dqn reward tensor(-313.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3734e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2212039828300476
dqn reward tensor(-336.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2953e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24852308630943298
dqn reward tensor(-343.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2796e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09923423081636429
dqn reward tensor(-353.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5185e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19353538751602173
dqn reward tensor(-221.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.2117e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18893682956695557
dqn reward tensor(-193.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2681e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10948671400547028
dqn reward tensor(-390.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6192e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11368447542190552
dqn reward tensor(-287.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3041e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056830935180187225
dqn reward tensor(-438.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2795e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12043362110853195
dqn reward tensor(-422.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7735e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1087656244635582
dqn reward tensor(-307.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4506e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1536969393491745
dqn reward tensor(-286.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6083e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21249210834503174
dqn reward tensor(-265.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0116e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15298566222190857
dqn reward tensor(-209.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3063e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2838764488697052
dqn reward tensor(-264.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6606e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1664062738418579
dqn reward tensor(-350.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3776e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11938197165727615
dqn reward tensor(-248.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5380e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02478436753153801
dqn reward tensor(-338.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5163e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19067582488059998
dqn reward tensor(-329.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9657e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08640672266483307
dqn reward tensor(-251.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5567e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11519281566143036
dqn reward tensor(-238.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3846e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12414638698101044
dqn reward tensor(-291.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5721e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11713229864835739
dqn reward tensor(-311.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5445e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17543888092041016
dqn reward tensor(-341.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.0302e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15973332524299622
dqn reward tensor(-400.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4656e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1882036179304123
dqn reward tensor(-264.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3661e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11791543662548065
dqn reward tensor(-247.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2635e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15052282810211182
dqn reward tensor(-295.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3178e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23935450613498688
dqn reward tensor(-295.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4424e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06104561686515808
dqn reward tensor(-287., device='cuda:0') e 0.05 loss_dqn tensor(1.4871e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2590811252593994
dqn reward tensor(-323.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7243e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07128442823886871
dqn reward tensor(-170.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4873e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14054904878139496
dqn reward tensor(-351.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5940e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19853216409683228
dqn reward tensor(-267.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5542e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22180315852165222
dqn reward tensor(-410.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2139e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1589047908782959
dqn reward tensor(-310.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5439e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28490838408470154
dqn reward tensor(-386.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5469e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0559285432100296
dqn reward tensor(-300.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1861e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407594084739685
dqn reward tensor(-282., device='cuda:0') e 0.05 loss_dqn tensor(1.2512e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19161772727966309
dqn reward tensor(-290.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1691e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16994278132915497
dqn reward tensor(-314.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2021e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04119320213794708
dqn reward tensor(-402.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2067e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0736011192202568
dqn reward tensor(-172., device='cuda:0') e 0.05 loss_dqn tensor(1.2386e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11690880358219147
dqn reward tensor(-209.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2652e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2962469756603241
dqn reward tensor(-340.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0738e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12984998524188995
dqn reward tensor(-344.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0600e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1384388506412506
dqn reward tensor(-399.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4051e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16549766063690186
dqn reward tensor(-294.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4909e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20503057539463043
dqn reward tensor(-333.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0772e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029384858906269073
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4607e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04308025911450386
dqn reward tensor(-245.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0522e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07324601709842682
dqn reward tensor(-444.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2562e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09389710426330566
dqn reward tensor(-243., device='cuda:0') e 0.05 loss_dqn tensor(1.0287e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08528304100036621
dqn reward tensor(-244.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0053e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1742120087146759
dqn reward tensor(-309.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3191e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016812585294246674
dqn reward tensor(-378.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0093e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01475790236145258
dqn reward tensor(-240.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0650e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11793752759695053
dqn reward tensor(-291.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5819e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26253339648246765
dqn reward tensor(-230.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3318e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1333940327167511
dqn reward tensor(-364., device='cuda:0') e 0.05 loss_dqn tensor(9.5090e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21231818199157715
dqn reward tensor(-384.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2107e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13872981071472168
dqn reward tensor(-241.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1897e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2090381234884262
dqn reward tensor(-391.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3667e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08904311060905457
dqn reward tensor(-462.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0474e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13551437854766846
dqn reward tensor(-237.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09966004639863968
dqn reward tensor(-272.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2559e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2557944059371948
dqn reward tensor(-282.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1642e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1780141443014145
dqn reward tensor(-226.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8431e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23119613528251648
dqn reward tensor(-316.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7603e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11362333595752716
dqn reward tensor(-218.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8783e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21833722293376923
dqn reward tensor(-201.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1914e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.194611594080925
dqn reward tensor(-270.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9462e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13191989064216614
dqn reward tensor(-285.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2030e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10631978511810303
dqn reward tensor(-281.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9417e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12040619552135468
dqn reward tensor(-372.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2264e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1336323767900467
dqn reward tensor(-328.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8420e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1044817864894867
dqn reward tensor(-287., device='cuda:0') e 0.05 loss_dqn tensor(9.5681e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09996644407510757
dqn reward tensor(-223.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4260e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12946432828903198
dqn reward tensor(-280.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0987e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07952391356229782
dqn reward tensor(-310.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4803e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22923070192337036
dqn reward tensor(-266.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2654e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08018089830875397
dqn reward tensor(-354.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0031e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2234918177127838
dqn reward tensor(-300.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.8317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01568182371556759
dqn reward tensor(-381., device='cuda:0') e 0.05 loss_dqn tensor(1.2930e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19217178225517273
dqn reward tensor(-417., device='cuda:0') e 0.05 loss_dqn tensor(1.3591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19778338074684143
dqn reward tensor(-213.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.0218e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21091891825199127
dqn reward tensor(-258.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7359e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15087354183197021
dqn reward tensor(-219.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1497e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09375812113285065
dqn reward tensor(-293.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0371e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17881956696510315
dqn reward tensor(-358.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0850e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21072284877300262
dqn reward tensor(-312.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0016e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13981863856315613
dqn reward tensor(-257.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18877995014190674
dqn reward tensor(-292.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8390e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1252700388431549
dqn reward tensor(-188., device='cuda:0') e 0.05 loss_dqn tensor(8.5020e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10094980895519257
dqn reward tensor(-294.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0166e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.267846941947937
dqn reward tensor(-381.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0497e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12646658718585968
dqn reward tensor(-177.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3779e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08898270130157471
dqn reward tensor(-262.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.9944e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08545048534870148
dqn reward tensor(-229.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5365e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16914881765842438
dqn reward tensor(-309.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5547e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1854882836341858
dqn reward tensor(-314.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8458e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11307762563228607
dqn reward tensor(-398.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0620e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04337561875581741
dqn reward tensor(-288.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1124e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27107030153274536
dqn reward tensor(-374.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0923e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13298815488815308
dqn reward tensor(-332.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9456e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11097243428230286
dqn reward tensor(-219.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8350e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1431955248117447
dqn reward tensor(-342.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1201e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14832311868667603
dqn reward tensor(-292.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6876e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2129134088754654
dqn reward tensor(-325.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1911e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30743375420570374
dqn reward tensor(-215.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0333e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15152834355831146
dqn reward tensor(-262.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2390e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0905592292547226
dqn reward tensor(-318.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0375e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1371336281299591
dqn reward tensor(-311.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0209e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27957597374916077
dqn reward tensor(-301.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2611e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10730967670679092
dqn reward tensor(-278.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5573e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15876148641109467
dqn reward tensor(-268.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1426e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10845472663640976
dqn reward tensor(-195.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5635e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24690508842468262
dqn reward tensor(-332.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5460e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15722589194774628
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1693e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11644171178340912
dqn reward tensor(-350.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2446e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06602556258440018
dqn reward tensor(-251.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0220e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13378466665744781
dqn reward tensor(-299.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2378e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1327022761106491
dqn reward tensor(-239.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0742e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1656111776828766
dqn reward tensor(-296.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7415e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17384424805641174
dqn reward tensor(-151.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0552e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08875847607851028
dqn reward tensor(-247.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5778e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20584021508693695
dqn reward tensor(-288.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7063e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10357234627008438
dqn reward tensor(-322.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.5279e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03653140738606453
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3429e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10099145770072937
dqn reward tensor(-255.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8949e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32611218094825745
dqn reward tensor(-213.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9762e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15097637474536896
dqn reward tensor(-225.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6613e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09669843316078186
dqn reward tensor(-165.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8334e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04003988206386566
dqn reward tensor(-249.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0371e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1413983851671219
dqn reward tensor(-235.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0165e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17474204301834106
dqn reward tensor(-217., device='cuda:0') e 0.05 loss_dqn tensor(7.6104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31957802176475525
dqn reward tensor(-293.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8897e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12755203247070312
dqn reward tensor(-189.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3284e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09597356617450714
dqn reward tensor(-203.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9430e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032560594379901886
dqn reward tensor(-271.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5834e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1268976330757141
dqn reward tensor(-264.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2054e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1370067596435547
dqn reward tensor(-210.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9421e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14631959795951843
dqn reward tensor(-201.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0512e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1284647136926651
dqn reward tensor(-388.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1391e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08036532998085022
dqn reward tensor(-188.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3520e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13534656167030334
dqn reward tensor(-280.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8797e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17012570798397064
dqn reward tensor(-308.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5851e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17721763253211975
dqn reward tensor(-268.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8687e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13064917922019958
dqn reward tensor(-238.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8545e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09617315977811813
dqn reward tensor(-195.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7252e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2460203468799591
dqn reward tensor(-342.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1130e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053430311381816864
dqn reward tensor(-308.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1638e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2312777042388916
dqn reward tensor(-318.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9847e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1664763242006302
dqn reward tensor(-307.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8905e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09067937731742859
dqn reward tensor(-440., device='cuda:0') e 0.05 loss_dqn tensor(7.9692e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16379405558109283
dqn reward tensor(-348.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1769e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1797279715538025
dqn reward tensor(-208.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8096e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10868175327777863
dqn reward tensor(-348.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1214e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055313169956207275
dqn reward tensor(-436., device='cuda:0') e 0.05 loss_dqn tensor(6.5735e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.176065593957901
dqn reward tensor(-201.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1128e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11471793800592422
dqn reward tensor(-380.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7825e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07240985333919525
dqn reward tensor(-396.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6791e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10496759414672852
dqn reward tensor(-399.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2729e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06310538947582245
dqn reward tensor(-241.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9414e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13804031908512115
dqn reward tensor(-454.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7549e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09388042986392975
dqn reward tensor(-312.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0366e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3212481141090393
dqn reward tensor(-294.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2895e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09581328183412552
dqn reward tensor(-313.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8235e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13127431273460388
dqn reward tensor(-272.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2467e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14755785465240479
dqn reward tensor(-366., device='cuda:0') e 0.05 loss_dqn tensor(6.4285e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10673383623361588
dqn reward tensor(-348.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8280e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11394049972295761
dqn reward tensor(-354.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23380164802074432
dqn reward tensor(-362.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0496e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03804922103881836
dqn reward tensor(-272.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8099e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11068173497915268
dqn reward tensor(-247.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.9837e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13988588750362396
dqn reward tensor(-315.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2626e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08755375444889069
dqn reward tensor(-265.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1063e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1985262632369995
dqn reward tensor(-328.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060084909200668335
dqn reward tensor(-336.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4818e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06637442857027054
dqn reward tensor(-366.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0353e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16003045439720154
dqn reward tensor(-274.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7513e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02922656573355198
dqn reward tensor(-332.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4478e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08454569429159164
dqn reward tensor(-315.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2175e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553616493940353
dqn reward tensor(-372.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8601e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02149832248687744
dqn reward tensor(-396.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7054e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07992444932460785
dqn reward tensor(-412.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.0297e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12453576177358627
dqn reward tensor(-274.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09582427144050598
dqn reward tensor(-313.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0403e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10107503086328506
dqn reward tensor(-292.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2875e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013119176030158997
dqn reward tensor(-327.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4116e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1609259396791458
dqn reward tensor(-12.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4897e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.009518267586827278
Evaluating...
Train: {'rocauc': 0.7546489525583829} -5.372951030731201
=====Epoch 8=====
Training...
dqn reward tensor(-378.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07365642488002777
dqn reward tensor(-309.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6887e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18036286532878876
dqn reward tensor(-338.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.1838e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08950214087963104
dqn reward tensor(-492.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5992e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2373007833957672
dqn reward tensor(-222., device='cuda:0') e 0.05 loss_dqn tensor(5.8641e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2101966291666031
dqn reward tensor(-296.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8364e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06548774987459183
dqn reward tensor(-301.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6784e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033293966203927994
dqn reward tensor(-325., device='cuda:0') e 0.05 loss_dqn tensor(6.2808e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11243471503257751
dqn reward tensor(-358.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0889e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04818986728787422
dqn reward tensor(-241.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1188e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21074849367141724
dqn reward tensor(-225.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3545e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17560771107673645
dqn reward tensor(-328.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5244e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06574638187885284
dqn reward tensor(-360.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6328e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10918793082237244
dqn reward tensor(-466., device='cuda:0') e 0.05 loss_dqn tensor(5.6398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1486225575208664
dqn reward tensor(-301.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3586e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12061608582735062
dqn reward tensor(-390.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0001e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06951849907636642
dqn reward tensor(-401.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9451e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17741909623146057
dqn reward tensor(-300.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2280e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12670543789863586
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7955e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.077901192009449
dqn reward tensor(-292., device='cuda:0') e 0.05 loss_dqn tensor(6.7022e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21169646084308624
dqn reward tensor(-187.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7090e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14792323112487793
dqn reward tensor(-346.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.2203e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08730398118495941
dqn reward tensor(-544.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1122e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15796610713005066
dqn reward tensor(-396.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6067e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05403744429349899
dqn reward tensor(-294.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9353e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06258109956979752
dqn reward tensor(-314.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9008e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18727055191993713
dqn reward tensor(-333.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3929e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08324909955263138
dqn reward tensor(-323.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8094e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031476013362407684
dqn reward tensor(-278.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4301e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2541608512401581
dqn reward tensor(-349.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.4909e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014972218312323093
dqn reward tensor(-333.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8246e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11543872952461243
dqn reward tensor(-487.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4152e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06054725497961044
dqn reward tensor(-410.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.3778e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08722484111785889
dqn reward tensor(-384.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.8053e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11001820862293243
dqn reward tensor(-422.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.5176e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14018884301185608
dqn reward tensor(-309., device='cuda:0') e 0.05 loss_dqn tensor(5.4400e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014126472175121307
dqn reward tensor(-331.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8646e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1964206099510193
dqn reward tensor(-389.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5862e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08745531737804413
dqn reward tensor(-300.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9245e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3223532438278198
dqn reward tensor(-289.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8970e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03112711012363434
dqn reward tensor(-284.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8140e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09985240548849106
dqn reward tensor(-441., device='cuda:0') e 0.05 loss_dqn tensor(6.2555e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07526639848947525
dqn reward tensor(-354., device='cuda:0') e 0.05 loss_dqn tensor(7.0473e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0719570517539978
dqn reward tensor(-376.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1164e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047076012939214706
dqn reward tensor(-285.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6667e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06653034687042236
dqn reward tensor(-413., device='cuda:0') e 0.05 loss_dqn tensor(7.4319e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07173716276884079
dqn reward tensor(-311.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038846611976623535
dqn reward tensor(-330.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.2342e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017997799441218376
dqn reward tensor(-387.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3757e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07882460951805115
dqn reward tensor(-416.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4319e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.150473952293396
dqn reward tensor(-332.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7481e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06864316016435623
dqn reward tensor(-374.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015231914818286896
dqn reward tensor(-454.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3438e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24580064415931702
dqn reward tensor(-436.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.1320e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23763829469680786
dqn reward tensor(-255.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20572054386138916
dqn reward tensor(-280.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2029e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3579561710357666
dqn reward tensor(-301.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4215e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07352329790592194
dqn reward tensor(-219.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2267e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.247728630900383
dqn reward tensor(-424.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8615e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07266470044851303
dqn reward tensor(-387., device='cuda:0') e 0.05 loss_dqn tensor(5.1725e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33123207092285156
dqn reward tensor(-362.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5716e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524210274219513
dqn reward tensor(-231.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3062e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2320631444454193
dqn reward tensor(-239.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4017e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.274140328168869
dqn reward tensor(-332.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8096e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20821428298950195
dqn reward tensor(-247.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6416e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13112539052963257
dqn reward tensor(-303.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0853e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12054011225700378
dqn reward tensor(-155.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7487e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19672203063964844
dqn reward tensor(-341.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22629298269748688
dqn reward tensor(-273.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7384e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13507550954818726
dqn reward tensor(-439.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0748e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15378163754940033
dqn reward tensor(-368.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14903084933757782
dqn reward tensor(-334.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21964973211288452
dqn reward tensor(-382.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5018e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01807474158704281
dqn reward tensor(-343.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8145e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08124569058418274
dqn reward tensor(-492.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6465e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0848272293806076
dqn reward tensor(-362., device='cuda:0') e 0.05 loss_dqn tensor(5.1388e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0481281615793705
dqn reward tensor(-338.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8719e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014515656977891922
dqn reward tensor(-327.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8146e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1823597103357315
dqn reward tensor(-358.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5006e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040993355214595795
dqn reward tensor(-374.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0118e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012167446315288544
dqn reward tensor(-249.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5410e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18650057911872864
dqn reward tensor(-327.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2622e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07508961111307144
dqn reward tensor(-347.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.0989e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14184337854385376
dqn reward tensor(-373.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6223e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1056622639298439
dqn reward tensor(-430.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5544e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1587323546409607
dqn reward tensor(-312.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.9614e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07572785019874573
dqn reward tensor(-290.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4566e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03500047326087952
dqn reward tensor(-348.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3774e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025869887322187424
dqn reward tensor(-218.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9259e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2264007031917572
dqn reward tensor(-340., device='cuda:0') e 0.05 loss_dqn tensor(5.5012e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10022790729999542
dqn reward tensor(-448.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8894e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031284645199775696
dqn reward tensor(-413.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2058e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1473141610622406
dqn reward tensor(-466.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1291e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14812490344047546
dqn reward tensor(-327.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9119e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06348623335361481
dqn reward tensor(-349.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3824e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09033235162496567
dqn reward tensor(-345.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5246e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16634708642959595
dqn reward tensor(-310.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2091e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11215047538280487
dqn reward tensor(-361.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02438163384795189
dqn reward tensor(-352.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8445e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11054975539445877
dqn reward tensor(-258.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8255e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13614854216575623
dqn reward tensor(-345.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6131e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15531450510025024
dqn reward tensor(-369.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.4381e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04045093432068825
dqn reward tensor(-371.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8912e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05992499738931656
dqn reward tensor(-380.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7129e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20317062735557556
dqn reward tensor(-489., device='cuda:0') e 0.05 loss_dqn tensor(5.3675e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08459266275167465
dqn reward tensor(-295.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0942e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15158945322036743
dqn reward tensor(-282.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8210e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2710387408733368
dqn reward tensor(-416.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2400e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12498865276575089
dqn reward tensor(-313.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6616e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1478371024131775
dqn reward tensor(-356.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9860e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05261754244565964
dqn reward tensor(-312.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3920e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1753455102443695
dqn reward tensor(-360.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4373e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06551015377044678
dqn reward tensor(-418.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6582e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10633688420057297
dqn reward tensor(-459.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2876e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10938084125518799
dqn reward tensor(-476.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.7770e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14690279960632324
dqn reward tensor(-237.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8848e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06235884502530098
dqn reward tensor(-388.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0539e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2549004554748535
dqn reward tensor(-556.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6619e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12777607142925262
dqn reward tensor(-396.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7451e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2853202223777771
dqn reward tensor(-391.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5680e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12006611377000809
dqn reward tensor(-457.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9281e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2089279294013977
dqn reward tensor(-360., device='cuda:0') e 0.05 loss_dqn tensor(4.5788e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19789624214172363
dqn reward tensor(-345.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2086e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047650180757045746
dqn reward tensor(-495.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.6253e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1619572937488556
dqn reward tensor(-578.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8408e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32793673872947693
dqn reward tensor(-427.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10654446482658386
dqn reward tensor(-440.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3767e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09959736466407776
dqn reward tensor(-506.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.5663e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1533738374710083
dqn reward tensor(-386.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5820e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2682613730430603
dqn reward tensor(-392.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7978e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1701522320508957
dqn reward tensor(-357.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7974e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20228052139282227
dqn reward tensor(-405., device='cuda:0') e 0.05 loss_dqn tensor(4.7534e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16030193865299225
dqn reward tensor(-403., device='cuda:0') e 0.05 loss_dqn tensor(4.3964e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07298219203948975
dqn reward tensor(-419.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8148e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16076448559761047
dqn reward tensor(-433., device='cuda:0') e 0.05 loss_dqn tensor(4.6626e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09817282855510712
dqn reward tensor(-505.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0122e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1972290277481079
dqn reward tensor(-437.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9157e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04851289093494415
dqn reward tensor(-467., device='cuda:0') e 0.05 loss_dqn tensor(4.9774e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06445738673210144
dqn reward tensor(-426.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7411e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18531687557697296
dqn reward tensor(-506.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2225e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3034988045692444
dqn reward tensor(-569.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1861e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14832918345928192
dqn reward tensor(-352.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6211e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023758061230182648
dqn reward tensor(-444.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2367e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06385116279125214
dqn reward tensor(-401.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13134479522705078
dqn reward tensor(-544., device='cuda:0') e 0.05 loss_dqn tensor(4.0839e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02090115286409855
dqn reward tensor(-498.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7494e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21966922283172607
dqn reward tensor(-374.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6354e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17366744577884674
dqn reward tensor(-532.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5643e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030249109491705894
dqn reward tensor(-424., device='cuda:0') e 0.05 loss_dqn tensor(4.5982e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03475788235664368
dqn reward tensor(-460.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7777e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08092422038316727
dqn reward tensor(-369.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8127e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1457848846912384
dqn reward tensor(-455.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2594e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0876961201429367
dqn reward tensor(-353.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8852e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24282369017601013
dqn reward tensor(-389.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4862e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.100516177713871
dqn reward tensor(-411.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9807e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05120565742254257
dqn reward tensor(-346.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8069e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13410621881484985
dqn reward tensor(-536.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9860e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07608946412801743
dqn reward tensor(-407.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4668e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.151463583111763
dqn reward tensor(-442.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6989e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21261252462863922
dqn reward tensor(-490.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7581e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23823517560958862
dqn reward tensor(-469.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4862e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061643391847610474
dqn reward tensor(-526.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4647e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19570063054561615
dqn reward tensor(-495.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6221e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1852092742919922
dqn reward tensor(-503.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3691e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13699406385421753
dqn reward tensor(-441., device='cuda:0') e 0.05 loss_dqn tensor(5.0685e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23771381378173828
dqn reward tensor(-401.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6740e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16010147333145142
dqn reward tensor(-345.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8366e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14368586242198944
dqn reward tensor(-422.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3654e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07233763486146927
dqn reward tensor(-417.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9386e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07375113666057587
dqn reward tensor(-330.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5104e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19763323664665222
dqn reward tensor(-413.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5279e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20172639191150665
dqn reward tensor(-359.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2604e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10064642876386642
dqn reward tensor(-440.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6135e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12446118146181107
dqn reward tensor(-409.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5626e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29003217816352844
dqn reward tensor(-412.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5444e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19358663260936737
dqn reward tensor(-476.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8860e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12141752243041992
dqn reward tensor(-432.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5117e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15438780188560486
dqn reward tensor(-459., device='cuda:0') e 0.05 loss_dqn tensor(4.5540e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1931898593902588
dqn reward tensor(-319.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3890e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047157615423202515
dqn reward tensor(-531.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1014e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20929187536239624
dqn reward tensor(-417.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2810e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19716638326644897
dqn reward tensor(-376.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0837e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16395704448223114
dqn reward tensor(-352.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5437e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20535515248775482
dqn reward tensor(-456.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4159e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0948282927274704
dqn reward tensor(-354.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9277e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0987328439950943
dqn reward tensor(-471.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7448e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18838539719581604
dqn reward tensor(-516.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.5372e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25523290038108826
dqn reward tensor(-484.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6186e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14858415722846985
dqn reward tensor(-342.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4095e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1331782042980194
dqn reward tensor(-483.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2202e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033823587000370026
dqn reward tensor(-369.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2309e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1534828543663025
dqn reward tensor(-503.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2495e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13123658299446106
dqn reward tensor(-476.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5041e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02648758888244629
dqn reward tensor(-374.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2590e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18499314785003662
dqn reward tensor(-332.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5330e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23651030659675598
dqn reward tensor(-520.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.5481e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1196393072605133
dqn reward tensor(-351.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2333e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029124103486537933
dqn reward tensor(-474.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0521e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10558715462684631
dqn reward tensor(-395.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2745e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07956960797309875
dqn reward tensor(-380.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5936e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04768743738532066
dqn reward tensor(-395.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4438e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1489054262638092
dqn reward tensor(-508.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.5079e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13365048170089722
dqn reward tensor(-393.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0044e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1973273903131485
dqn reward tensor(-471., device='cuda:0') e 0.05 loss_dqn tensor(4.3775e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2572909891605377
dqn reward tensor(-385.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5396e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13587355613708496
dqn reward tensor(-386.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1755e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06911526620388031
dqn reward tensor(-327.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6609e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09388255327939987
dqn reward tensor(-398.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9802e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09030836075544357
dqn reward tensor(-348., device='cuda:0') e 0.05 loss_dqn tensor(4.2507e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08733073621988297
dqn reward tensor(-417.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9887e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16125066578388214
dqn reward tensor(-420.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2677e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11649539321660995
dqn reward tensor(-460.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3737e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09650258719921112
dqn reward tensor(-448.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2592e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10022582113742828
dqn reward tensor(-335.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8554e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10013837367296219
dqn reward tensor(-470.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2910e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2455117404460907
dqn reward tensor(-438.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2997e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14248353242874146
dqn reward tensor(-427.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13004128634929657
dqn reward tensor(-371.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3794e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11813439428806305
dqn reward tensor(-406.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.4781e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2182009518146515
dqn reward tensor(-337., device='cuda:0') e 0.05 loss_dqn tensor(5.3406e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432180404663086
dqn reward tensor(-453.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7086e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055389758199453354
dqn reward tensor(-328.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5621e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2322600781917572
dqn reward tensor(-397.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4019e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19333995878696442
dqn reward tensor(-333.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3928e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03437964618206024
dqn reward tensor(-287.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4194e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16450676321983337
dqn reward tensor(-392.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7576e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08119916915893555
dqn reward tensor(-478.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4113e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16299086809158325
dqn reward tensor(-519.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9657e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2240491807460785
dqn reward tensor(-319., device='cuda:0') e 0.05 loss_dqn tensor(6.7749e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1854732483625412
dqn reward tensor(-287.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0341e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13461834192276
dqn reward tensor(-357.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3549e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23319903016090393
dqn reward tensor(-323.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0579e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32043272256851196
dqn reward tensor(-372.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2850e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05445342883467674
dqn reward tensor(-356.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17971211671829224
dqn reward tensor(-372.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7466e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14583756029605865
dqn reward tensor(-423.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0476e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14962013065814972
dqn reward tensor(-374.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4256e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10063499957323074
dqn reward tensor(-425.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9096e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11429904401302338
dqn reward tensor(-346.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.4318e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12066702544689178
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1259e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07096469402313232
dqn reward tensor(-261.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.0919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23096564412117004
dqn reward tensor(-178.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1746436357498169
dqn reward tensor(-244.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7752e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1080625057220459
dqn reward tensor(-310.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2133e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20384404063224792
dqn reward tensor(-444.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6677e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15692836046218872
dqn reward tensor(-359.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18912751972675323
dqn reward tensor(-373.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3890e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13828110694885254
dqn reward tensor(-434.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6738e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2101227045059204
dqn reward tensor(-458.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1162e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03990558534860611
dqn reward tensor(-405.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2192e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05860445648431778
dqn reward tensor(-405.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3598e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20331233739852905
dqn reward tensor(-412.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9526e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1608208864927292
dqn reward tensor(-423.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2718040943145752
dqn reward tensor(-498.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1199e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10588395595550537
dqn reward tensor(-279.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.7720e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1110161617398262
dqn reward tensor(-391.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8719e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21218028664588928
dqn reward tensor(-404.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3799e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10826893150806427
dqn reward tensor(-454.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3170e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14334961771965027
dqn reward tensor(-234.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0467e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056978270411491394
dqn reward tensor(-428.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0316e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11185863614082336
dqn reward tensor(-379.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.9016e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12245692312717438
dqn reward tensor(-435.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.9586e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054984115064144135
dqn reward tensor(-394.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6189e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14805762469768524
dqn reward tensor(-406.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1723e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12624697387218475
dqn reward tensor(-355.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3540e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056101761758327484
dqn reward tensor(-335.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7815e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11882967501878738
dqn reward tensor(-219., device='cuda:0') e 0.05 loss_dqn tensor(6.8280e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29195109009742737
dqn reward tensor(-213.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1812e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2256823629140854
dqn reward tensor(-318.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6790e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19650350511074066
dqn reward tensor(-393., device='cuda:0') e 0.05 loss_dqn tensor(6.3715e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33950185775756836
dqn reward tensor(-337.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0744e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04527226835489273
dqn reward tensor(-284.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.6765e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2529163658618927
dqn reward tensor(-277.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2816e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16470924019813538
dqn reward tensor(-296., device='cuda:0') e 0.05 loss_dqn tensor(6.7034e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2166886329650879
dqn reward tensor(-391.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7666e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22907204926013947
dqn reward tensor(-398.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2117e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11051846295595169
dqn reward tensor(-216.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.3728e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19177976250648499
dqn reward tensor(-349.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5790e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16981667280197144
dqn reward tensor(-265.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5748e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943419486284256
dqn reward tensor(-203.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4003e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1347639560699463
dqn reward tensor(-305., device='cuda:0') e 0.05 loss_dqn tensor(7.5788e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048357121646404266
dqn reward tensor(-267.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36062249541282654
dqn reward tensor(-229., device='cuda:0') e 0.05 loss_dqn tensor(7.2492e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12361779063940048
dqn reward tensor(-304.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0306e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15925142168998718
dqn reward tensor(-402.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0229e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10725641250610352
dqn reward tensor(-279., device='cuda:0') e 0.05 loss_dqn tensor(6.9237e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14272665977478027
dqn reward tensor(-348., device='cuda:0') e 0.05 loss_dqn tensor(6.8754e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29493269324302673
dqn reward tensor(-274.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21802997589111328
dqn reward tensor(-251.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2065e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17344717681407928
dqn reward tensor(-311.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6805e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07631771266460419
dqn reward tensor(-290.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10017774999141693
dqn reward tensor(-271.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3439e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08993730694055557
dqn reward tensor(-295., device='cuda:0') e 0.05 loss_dqn tensor(6.1725e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22138366103172302
dqn reward tensor(-398.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1371e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1389029622077942
dqn reward tensor(-393.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6456e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06469881534576416
dqn reward tensor(-453.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1858e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0773901492357254
dqn reward tensor(-433.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1098e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12985453009605408
dqn reward tensor(-306.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11971777677536011
dqn reward tensor(-299.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4949e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0659387856721878
dqn reward tensor(-441.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8472e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09889724105596542
dqn reward tensor(-379.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9363e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14529845118522644
dqn reward tensor(-400.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0821e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19371271133422852
dqn reward tensor(-264.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6353e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013695837929844856
dqn reward tensor(-294.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3307e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12906013429164886
dqn reward tensor(-217.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4166e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28641337156295776
dqn reward tensor(-406.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8245e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1640731245279312
dqn reward tensor(-464., device='cuda:0') e 0.05 loss_dqn tensor(5.9709e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15110978484153748
dqn reward tensor(-327.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1420333981513977
dqn reward tensor(-210.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8243e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05476435646414757
dqn reward tensor(-264.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2108e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26092728972435
dqn reward tensor(-252., device='cuda:0') e 0.05 loss_dqn tensor(6.7016e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19012007117271423
dqn reward tensor(-226.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5168e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3076147437095642
dqn reward tensor(-369.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9193e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13525457680225372
dqn reward tensor(-361.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.4751e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08589117228984833
dqn reward tensor(-170.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9486e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3040394186973572
dqn reward tensor(-369.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6951e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22455070912837982
dqn reward tensor(-417.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7176e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24540039896965027
dqn reward tensor(-301.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1775e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1844949722290039
dqn reward tensor(-282.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.7792e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2689061164855957
dqn reward tensor(-335.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0976e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24994759261608124
dqn reward tensor(-375., device='cuda:0') e 0.05 loss_dqn tensor(7.0576e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18594466149806976
dqn reward tensor(-312.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3374e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13942524790763855
dqn reward tensor(-446.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5275e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19235645234584808
dqn reward tensor(-267.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0056e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11290372163057327
dqn reward tensor(-403., device='cuda:0') e 0.05 loss_dqn tensor(6.0642e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12745144963264465
dqn reward tensor(-345.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1965e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19416403770446777
dqn reward tensor(-394.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5067e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27235159277915955
dqn reward tensor(-390.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.1679e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13860389590263367
dqn reward tensor(-390.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.2870e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3089471757411957
dqn reward tensor(-363.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9047e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16363327205181122
dqn reward tensor(-229.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7057e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017401088029146194
dqn reward tensor(-238.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5484e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22714734077453613
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6473e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09070216864347458
dqn reward tensor(-230.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3829e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019652094691991806
dqn reward tensor(-268.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5598e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1580236554145813
dqn reward tensor(-360.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4165e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15096847712993622
dqn reward tensor(-408.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3956e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1831645965576172
dqn reward tensor(-215.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5120e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12639060616493225
dqn reward tensor(-351.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1208e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1169886663556099
dqn reward tensor(-188.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.8420e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18442611396312714
dqn reward tensor(-257., device='cuda:0') e 0.05 loss_dqn tensor(8.7914e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1951860785484314
dqn reward tensor(-398., device='cuda:0') e 0.05 loss_dqn tensor(9.0192e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15097668766975403
dqn reward tensor(-365.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2493e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12417199462652206
dqn reward tensor(-365.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0272e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07127435505390167
dqn reward tensor(-228.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1335e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17748938500881195
dqn reward tensor(-224.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6272e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1635744273662567
dqn reward tensor(-303.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0870421975851059
dqn reward tensor(-265.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6315e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17655789852142334
dqn reward tensor(-244.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0446e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16765761375427246
dqn reward tensor(-398.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.5846e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18684864044189453
dqn reward tensor(-276., device='cuda:0') e 0.05 loss_dqn tensor(1.0956e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2726803719997406
dqn reward tensor(-359., device='cuda:0') e 0.05 loss_dqn tensor(9.4226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414523720741272
dqn reward tensor(-273.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0119e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08192745596170425
dqn reward tensor(-293.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2628e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14535290002822876
dqn reward tensor(-232.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0586e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0898669958114624
dqn reward tensor(-306.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4419e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0888168066740036
dqn reward tensor(-176.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0324e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11228425055742264
dqn reward tensor(-415.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1757e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20973551273345947
dqn reward tensor(-394.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0987e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2525443434715271
dqn reward tensor(-195.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0089e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14558957517147064
dqn reward tensor(-250., device='cuda:0') e 0.05 loss_dqn tensor(9.1783e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031234541907906532
dqn reward tensor(-355.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9408e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08859679102897644
dqn reward tensor(-302.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0287e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1377858817577362
dqn reward tensor(-300.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8847e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13059157133102417
dqn reward tensor(-388.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0878e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17062141001224518
dqn reward tensor(-279.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0955e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08886414021253586
dqn reward tensor(-418.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0365e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12618033587932587
dqn reward tensor(-360.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0079e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07912180572748184
dqn reward tensor(-280.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0008e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13177397847175598
dqn reward tensor(-281.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1327e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1739540994167328
dqn reward tensor(-165.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9016e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15044623613357544
dqn reward tensor(-367.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0857e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434749811887741
dqn reward tensor(-349.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0263e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08929826319217682
dqn reward tensor(-219., device='cuda:0') e 0.05 loss_dqn tensor(8.9772e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1294122189283371
dqn reward tensor(-215.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0499e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08798904716968536
dqn reward tensor(-371.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1201e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11243859678506851
dqn reward tensor(-366.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0247e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06069738790392876
dqn reward tensor(-325.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4389e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028335493057966232
dqn reward tensor(-239.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2404e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0356505885720253
dqn reward tensor(-288.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0167e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0811375305056572
dqn reward tensor(-321.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0146e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08493760973215103
dqn reward tensor(-329.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0963e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20902952551841736
dqn reward tensor(-182.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0845e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09689702093601227
dqn reward tensor(-346.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1354e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18606239557266235
dqn reward tensor(-260.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8089e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09508822858333588
dqn reward tensor(-376., device='cuda:0') e 0.05 loss_dqn tensor(8.7201e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15272879600524902
dqn reward tensor(-249.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0027e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14103011786937714
dqn reward tensor(-250.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0275e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1730777770280838
dqn reward tensor(-404.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1153e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19421538710594177
dqn reward tensor(-255.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8818e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06680158525705338
dqn reward tensor(-241.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3681e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028801798820495605
dqn reward tensor(-257.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4578e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16957908868789673
dqn reward tensor(-347.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0095e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1865117847919464
dqn reward tensor(-339.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0316e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13997513055801392
dqn reward tensor(-183.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6701e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1566670835018158
dqn reward tensor(-257.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1318e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09679477661848068
dqn reward tensor(-242.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8938e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1011916920542717
dqn reward tensor(-316.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.6317e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08432097733020782
dqn reward tensor(-274.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4802e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28437304496765137
dqn reward tensor(-282., device='cuda:0') e 0.05 loss_dqn tensor(9.0810e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10862039774656296
dqn reward tensor(-329.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6597e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23309150338172913
dqn reward tensor(-302.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8288e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08026683330535889
dqn reward tensor(-407.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4548e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13108418881893158
dqn reward tensor(-292., device='cuda:0') e 0.05 loss_dqn tensor(6.2253e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11939907819032669
dqn reward tensor(-310.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5201e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13330192863941193
dqn reward tensor(-293.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3865e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05800507962703705
dqn reward tensor(-290.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5983e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11687781661748886
dqn reward tensor(-346.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0182e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2665678560733795
dqn reward tensor(-355.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.1133e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18647758662700653
dqn reward tensor(-386.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4173e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10978467762470245
dqn reward tensor(-364.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4849e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09731178730726242
dqn reward tensor(-247., device='cuda:0') e 0.05 loss_dqn tensor(5.4352e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1430407613515854
dqn reward tensor(-450.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8270e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1438571661710739
dqn reward tensor(-400., device='cuda:0') e 0.05 loss_dqn tensor(5.5919e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03663672134280205
dqn reward tensor(-456.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2810e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031128348782658577
dqn reward tensor(-336.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9270e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02746361866593361
dqn reward tensor(-332.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4677e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07503761351108551
dqn reward tensor(-317.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8998e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06458419561386108
dqn reward tensor(-398.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7555e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08899112790822983
dqn reward tensor(-435.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5954e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2654015123844147
dqn reward tensor(-331.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5930e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018809519708156586
dqn reward tensor(-378.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0886e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17811888456344604
dqn reward tensor(-398.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5464e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13032230734825134
dqn reward tensor(-315.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1652e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13830342888832092
dqn reward tensor(-502.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9711e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21019011735916138
dqn reward tensor(-229.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2570e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09395068138837814
dqn reward tensor(-383.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2840e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07779363542795181
dqn reward tensor(-346.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2021e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16950981318950653
dqn reward tensor(-446.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6566e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21713298559188843
dqn reward tensor(-381.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2325e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02809053286910057
dqn reward tensor(-346.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1752e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22175082564353943
dqn reward tensor(-426.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1853e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16876785457134247
dqn reward tensor(-320.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6911e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36650222539901733
dqn reward tensor(-395.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1415e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1964404284954071
dqn reward tensor(-476.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7971e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11258715391159058
dqn reward tensor(-518.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8691e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13821040093898773
dqn reward tensor(-489.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.5025e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1838417798280716
dqn reward tensor(-391.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4485e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13289068639278412
dqn reward tensor(-339.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.6669e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16067758202552795
dqn reward tensor(-301.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0629e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1522151231765747
dqn reward tensor(-406.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6032e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06696867942810059
dqn reward tensor(-446.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8904e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329210102558136
dqn reward tensor(-484.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8576e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08730045706033707
dqn reward tensor(-470.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3055e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1441274732351303
dqn reward tensor(-397.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6007e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21012723445892334
dqn reward tensor(-408.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8490e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11721432954072952
dqn reward tensor(-447., device='cuda:0') e 0.05 loss_dqn tensor(5.6902e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3168380558490753
dqn reward tensor(-415.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8946e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025298284366726875
dqn reward tensor(-380.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3356e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09253481775522232
dqn reward tensor(-423.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.9773e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18660080432891846
dqn reward tensor(-322.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3226e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1409616768360138
dqn reward tensor(-468., device='cuda:0') e 0.05 loss_dqn tensor(6.3564e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09420593827962875
dqn reward tensor(-323.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5345e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05272694304585457
dqn reward tensor(-437.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1897e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10178978741168976
dqn reward tensor(-438.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1128e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10057248175144196
dqn reward tensor(-289.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2943e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09382811188697815
dqn reward tensor(-374.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5654e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06801506876945496
dqn reward tensor(-411.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8263e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08246513456106186
dqn reward tensor(-266.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4274e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10912816971540451
dqn reward tensor(-404.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9284e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1874905526638031
dqn reward tensor(-401.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0185e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23825739324092865
dqn reward tensor(-387.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9938e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03458081930875778
dqn reward tensor(-473.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0484e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15393522381782532
dqn reward tensor(-387.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7066e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1198631227016449
dqn reward tensor(-435.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3833e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031567975878715515
dqn reward tensor(-416.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5511e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0710189938545227
dqn reward tensor(-334.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4809e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03196222335100174
dqn reward tensor(-306.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1453e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16055792570114136
dqn reward tensor(-406.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6512e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2261020392179489
dqn reward tensor(-365.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9711e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17974328994750977
dqn reward tensor(-340.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2594e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052716027945280075
dqn reward tensor(-311.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2256e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07086889445781708
dqn reward tensor(-422.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4448e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.185616135597229
dqn reward tensor(-416.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6398e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13771000504493713
dqn reward tensor(-284.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7848e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17632494866847992
dqn reward tensor(-370.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9721e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20539124310016632
dqn reward tensor(-373.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3590e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21451164782047272
dqn reward tensor(-434.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7570e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14641277492046356
dqn reward tensor(-405.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6732e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2193843424320221
dqn reward tensor(-484.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18203121423721313
dqn reward tensor(-320.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5263e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22028551995754242
dqn reward tensor(-277.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7996e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.151280015707016
dqn reward tensor(-418.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9225e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09645818173885345
dqn reward tensor(-471.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6816e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1147860437631607
dqn reward tensor(-337.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0912e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.239554300904274
dqn reward tensor(-460.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4365e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18536019325256348
dqn reward tensor(-509.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9902e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11894883215427399
dqn reward tensor(-469.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0698e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07604758441448212
dqn reward tensor(-484.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3380e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18460650742053986
dqn reward tensor(-447.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1124e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12296973168849945
dqn reward tensor(-346., device='cuda:0') e 0.05 loss_dqn tensor(5.6957e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670478880405426
dqn reward tensor(-305., device='cuda:0') e 0.05 loss_dqn tensor(6.2632e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28916609287261963
dqn reward tensor(-386.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2672e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08459989726543427
dqn reward tensor(-404.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1132e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13562564551830292
dqn reward tensor(-315.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.2196e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14945530891418457
dqn reward tensor(-306.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4774e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10780739039182663
dqn reward tensor(-402., device='cuda:0') e 0.05 loss_dqn tensor(6.3217e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1993958055973053
dqn reward tensor(-345., device='cuda:0') e 0.05 loss_dqn tensor(6.2867e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06934110820293427
dqn reward tensor(-308.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1052e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2054862529039383
dqn reward tensor(-401.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3970e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13320055603981018
dqn reward tensor(-441.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3488e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22136452794075012
dqn reward tensor(-356.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5522e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.139295756816864
dqn reward tensor(-468.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1858e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16697031259536743
dqn reward tensor(-338.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0884e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25332391262054443
dqn reward tensor(-325.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7562e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13507124781608582
dqn reward tensor(-360.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0139e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20304276049137115
dqn reward tensor(-334.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3960e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1310112327337265
dqn reward tensor(-475.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9566e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0509117916226387
dqn reward tensor(-301.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.3934e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08123134076595306
dqn reward tensor(-342.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.0250e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23556481301784515
dqn reward tensor(-316.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2745e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06853750348091125
dqn reward tensor(-367.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8632e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18450117111206055
dqn reward tensor(-426.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3346e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13230350613594055
dqn reward tensor(-458.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4018e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16403183341026306
dqn reward tensor(-15., device='cuda:0') e 0.05 loss_dqn tensor(5.4938e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22749200463294983
Evaluating...
Train: {'rocauc': 0.7440672989030609} -6.3695549964904785
=====Epoch 9=====
Training...
dqn reward tensor(-484.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8624e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30318665504455566
dqn reward tensor(-472.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1211e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05171745643019676
dqn reward tensor(-518.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3189e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09021130204200745
dqn reward tensor(-442.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1971e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.164559006690979
dqn reward tensor(-436.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8553e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06398334354162216
dqn reward tensor(-370.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.5397e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13605749607086182
dqn reward tensor(-474.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.7594e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0932542011141777
dqn reward tensor(-424.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4795e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1213981881737709
dqn reward tensor(-405.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7100e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14464859664440155
dqn reward tensor(-558.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1841e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1667083352804184
dqn reward tensor(-283.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3953e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08564895391464233
dqn reward tensor(-432.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4423e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18550272285938263
dqn reward tensor(-443.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1189e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16896355152130127
dqn reward tensor(-474., device='cuda:0') e 0.05 loss_dqn tensor(5.9163e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1421787142753601
dqn reward tensor(-515., device='cuda:0') e 0.05 loss_dqn tensor(5.5746e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1479397863149643
dqn reward tensor(-434.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7618e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2723275423049927
dqn reward tensor(-401., device='cuda:0') e 0.05 loss_dqn tensor(6.1287e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21369734406471252
dqn reward tensor(-466.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1452e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11820008605718613
dqn reward tensor(-398.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5038e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13175499439239502
dqn reward tensor(-277.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2821e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16052421927452087
dqn reward tensor(-367.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.6948e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08525779843330383
dqn reward tensor(-577.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.5013e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22037199139595032
dqn reward tensor(-466., device='cuda:0') e 0.05 loss_dqn tensor(5.9202e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10983310639858246
dqn reward tensor(-524.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2394e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15285924077033997
dqn reward tensor(-451.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5163e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24153167009353638
dqn reward tensor(-505., device='cuda:0') e 0.05 loss_dqn tensor(6.2294e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042921099811792374
dqn reward tensor(-476.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2422e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18020567297935486
dqn reward tensor(-457.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7385e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.196212500333786
dqn reward tensor(-484., device='cuda:0') e 0.05 loss_dqn tensor(6.1110e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28486165404319763
dqn reward tensor(-382.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6648e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24050718545913696
dqn reward tensor(-339.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3178e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2024897038936615
dqn reward tensor(-361.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8329e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14609920978546143
dqn reward tensor(-360.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3146e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14942073822021484
dqn reward tensor(-383.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2463e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042061395943164825
dqn reward tensor(-381.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.4892e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21587815880775452
dqn reward tensor(-364.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1850e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1250881552696228
dqn reward tensor(-445.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2680e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22481989860534668
dqn reward tensor(-411.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3116e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19255276024341583
dqn reward tensor(-502.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8473e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22487923502922058
dqn reward tensor(-482.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3172e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1679026186466217
dqn reward tensor(-341.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4338e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0727742463350296
dqn reward tensor(-482.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9983e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12695135176181793
dqn reward tensor(-379.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7807e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06836889684200287
dqn reward tensor(-452.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2346e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09230345487594604
dqn reward tensor(-533.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7830e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08112066984176636
dqn reward tensor(-435.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8115e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08552692830562592
dqn reward tensor(-490.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3315e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26411688327789307
dqn reward tensor(-427., device='cuda:0') e 0.05 loss_dqn tensor(5.2730e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020681537687778473
dqn reward tensor(-560.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7041e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2098255157470703
dqn reward tensor(-430.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2718e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03155716136097908
dqn reward tensor(-488.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4522e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1339063197374344
dqn reward tensor(-368.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.3299e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11159994453191757
dqn reward tensor(-457.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9516e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16521745920181274
dqn reward tensor(-382.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8826e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27191609144210815
dqn reward tensor(-423., device='cuda:0') e 0.05 loss_dqn tensor(5.0265e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14988452196121216
dqn reward tensor(-518., device='cuda:0') e 0.05 loss_dqn tensor(5.4358e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14111310243606567
dqn reward tensor(-292.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4114e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0669078677892685
dqn reward tensor(-485.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1431e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09342160820960999
dqn reward tensor(-413., device='cuda:0') e 0.05 loss_dqn tensor(5.8470e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05770781636238098
dqn reward tensor(-426.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7967e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17119307816028595
dqn reward tensor(-443.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8339e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14515402913093567
dqn reward tensor(-419.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2793e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29377424716949463
dqn reward tensor(-427.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8459e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12954747676849365
dqn reward tensor(-423., device='cuda:0') e 0.05 loss_dqn tensor(7.3422e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1043974757194519
dqn reward tensor(-459.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5306e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10336223989725113
dqn reward tensor(-376.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9924e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09542682766914368
dqn reward tensor(-529., device='cuda:0') e 0.05 loss_dqn tensor(5.9187e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254359781742096
dqn reward tensor(-446.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9940e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06212623417377472
dqn reward tensor(-374.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.7555e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09064392745494843
dqn reward tensor(-351.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4959e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03326717019081116
dqn reward tensor(-198.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3134e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1320195496082306
dqn reward tensor(-408.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0321e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15092042088508606
dqn reward tensor(-411.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5542e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021504495292901993
dqn reward tensor(-423.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3932e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061090633273124695
dqn reward tensor(-391.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5568e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07766123116016388
dqn reward tensor(-429.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3323e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08687862008810043
dqn reward tensor(-359.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4601e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18096178770065308
dqn reward tensor(-223.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5890e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14855648577213287
dqn reward tensor(-348.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2777e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13530629873275757
dqn reward tensor(-378.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2022e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19703856110572815
dqn reward tensor(-453., device='cuda:0') e 0.05 loss_dqn tensor(6.7693e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27111297845840454
dqn reward tensor(-373.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.0742e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01988278701901436
dqn reward tensor(-490.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6022e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03380227088928223
dqn reward tensor(-451.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3790e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13268649578094482
dqn reward tensor(-309., device='cuda:0') e 0.05 loss_dqn tensor(6.5535e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07864035665988922
dqn reward tensor(-269.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.2472e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12465071678161621
dqn reward tensor(-474.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9729e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23536145687103271
dqn reward tensor(-399.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1863e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09782158583402634
dqn reward tensor(-333.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4846e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21663618087768555
dqn reward tensor(-416.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6384e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14013099670410156
dqn reward tensor(-267.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6453e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1674407422542572
dqn reward tensor(-410.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8837e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18372611701488495
dqn reward tensor(-373.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3933e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23759526014328003
dqn reward tensor(-354.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.0597e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2202039510011673
dqn reward tensor(-388.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0993e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19043317437171936
dqn reward tensor(-458.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4342e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15800350904464722
dqn reward tensor(-434.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5277e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07265295088291168
dqn reward tensor(-398.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.0899e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08693571388721466
dqn reward tensor(-344.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5159e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20654617249965668
dqn reward tensor(-468.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4822e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1413278430700302
dqn reward tensor(-284.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5033e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017999939620494843
dqn reward tensor(-364.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7709e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.077882319688797
dqn reward tensor(-277.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9474e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08334871381521225
dqn reward tensor(-280.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.1152e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28580963611602783
dqn reward tensor(-434.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7493e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3542211651802063
dqn reward tensor(-286.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3849e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07828434556722641
dqn reward tensor(-377.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4744e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1674191653728485
dqn reward tensor(-348.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1111e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09104909002780914
dqn reward tensor(-298.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1345e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061507657170295715
dqn reward tensor(-320.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1103e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12280864268541336
dqn reward tensor(-377.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5507e+08, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12388177216053009
dqn reward tensor(-125.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0892e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27634674310684204
dqn reward tensor(-326.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1947e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029139991849660873
dqn reward tensor(-221.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1576e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09462850540876389
dqn reward tensor(-341.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0323e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12895748019218445
dqn reward tensor(-296.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3565e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1426628828048706
dqn reward tensor(-414.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3025e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07784045487642288
dqn reward tensor(-306.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3892e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10163033753633499
dqn reward tensor(-280.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1379e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26997262239456177
dqn reward tensor(-200.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4149e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22065556049346924
dqn reward tensor(-261.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6687e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14587265253067017
dqn reward tensor(-397.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5826e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17108820378780365
dqn reward tensor(-159.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3087e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10687434673309326
dqn reward tensor(-277.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2626e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11888111382722855
dqn reward tensor(-375.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5854e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04327359423041344
dqn reward tensor(-253.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4309e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2166673243045807
dqn reward tensor(-352.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4770e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08916449546813965
dqn reward tensor(-276.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5353e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08724243193864822
dqn reward tensor(-382.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0450e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14067137241363525
dqn reward tensor(-375.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5989e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18850991129875183
dqn reward tensor(-318., device='cuda:0') e 0.05 loss_dqn tensor(1.9269e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07762401551008224
dqn reward tensor(-224.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5836e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10745204985141754
dqn reward tensor(-362.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6832e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11933416873216629
dqn reward tensor(-267.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2908e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018823694437742233
dqn reward tensor(-260.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7935e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16792263090610504
dqn reward tensor(-359.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1369e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0645827054977417
dqn reward tensor(-329.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0432e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20129160583019257
dqn reward tensor(-338., device='cuda:0') e 0.05 loss_dqn tensor(1.7099e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08758598566055298
dqn reward tensor(-285.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9746e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2754615545272827
dqn reward tensor(-208.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6362e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13908541202545166
dqn reward tensor(-304.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2339e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1157597154378891
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0741e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11934424936771393
dqn reward tensor(-334.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1950e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04697515815496445
dqn reward tensor(-382.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1961e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06218456104397774
dqn reward tensor(-395., device='cuda:0') e 0.05 loss_dqn tensor(2.8912e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14670489728450775
dqn reward tensor(-292.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3759e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12611642479896545
dqn reward tensor(-371.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1744e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1593949794769287
dqn reward tensor(-389.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5754e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03240857273340225
dqn reward tensor(-228.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5690e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1511048674583435
dqn reward tensor(-305.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7376e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12847885489463806
dqn reward tensor(-362.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4412e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08479903638362885
dqn reward tensor(-253.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5420e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3035266697406769
dqn reward tensor(-181., device='cuda:0') e 0.05 loss_dqn tensor(4.2587e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2344023436307907
dqn reward tensor(-216.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9632e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18801668286323547
dqn reward tensor(-358.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7605e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10565957427024841
dqn reward tensor(-376.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7686e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15492278337478638
dqn reward tensor(-272.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1899e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05877871438860893
dqn reward tensor(-259.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5060e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07767463475465775
dqn reward tensor(-331.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0032e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06752213835716248
dqn reward tensor(-326., device='cuda:0') e 0.05 loss_dqn tensor(3.1425e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09935791045427322
dqn reward tensor(-259.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1247e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03479708731174469
dqn reward tensor(-341., device='cuda:0') e 0.05 loss_dqn tensor(3.1883e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10247150808572769
dqn reward tensor(-349.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2699e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22647488117218018
dqn reward tensor(-287.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2681e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059874486178159714
dqn reward tensor(-372.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.7419e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12806011736392975
dqn reward tensor(-324.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7331e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.011949099600315094
dqn reward tensor(-352.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4519e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07762806862592697
dqn reward tensor(-363.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.0519e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15057919919490814
dqn reward tensor(-307.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6803e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16043607890605927
dqn reward tensor(-320.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7502e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1459028124809265
dqn reward tensor(-338.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0578e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29216957092285156
dqn reward tensor(-363.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2332e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09340466558933258
dqn reward tensor(-295., device='cuda:0') e 0.05 loss_dqn tensor(5.2545e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682324856519699
dqn reward tensor(-300.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5388e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059769660234451294
dqn reward tensor(-353.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8198e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12791961431503296
dqn reward tensor(-381.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9783e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022466029971837997
dqn reward tensor(-189., device='cuda:0') e 0.05 loss_dqn tensor(5.4922e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20499330759048462
dqn reward tensor(-291.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6389e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16962401568889618
dqn reward tensor(-319.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1133e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19069454073905945
dqn reward tensor(-101.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3286e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19085392355918884
dqn reward tensor(-371.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3318e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13630883395671844
dqn reward tensor(-283., device='cuda:0') e 0.05 loss_dqn tensor(7.1298e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13838931918144226
dqn reward tensor(-307.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2265e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12007351219654083
dqn reward tensor(-287., device='cuda:0') e 0.05 loss_dqn tensor(4.3505e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11389173567295074
dqn reward tensor(-334.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2959e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15144868195056915
dqn reward tensor(-387.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5425e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05776564031839371
dqn reward tensor(-297.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6167e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20296832919120789
dqn reward tensor(-274.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3140e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23871192336082458
dqn reward tensor(-283.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7483e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020210212096571922
dqn reward tensor(-387.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8920e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021087413653731346
dqn reward tensor(-414.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9290e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2912325859069824
dqn reward tensor(-242.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8337e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13951782882213593
dqn reward tensor(-359.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9217e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11513517796993256
dqn reward tensor(-327.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2615e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08006083965301514
dqn reward tensor(-276.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9477e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14088775217533112
dqn reward tensor(-463.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6543e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022333873435854912
dqn reward tensor(-277.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5349e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16769568622112274
dqn reward tensor(-319.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.3426e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04011666029691696
dqn reward tensor(-249.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1556e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11822984367609024
dqn reward tensor(-295., device='cuda:0') e 0.05 loss_dqn tensor(9.2524e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07452301681041718
dqn reward tensor(-443., device='cuda:0') e 0.05 loss_dqn tensor(5.5796e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11498360335826874
dqn reward tensor(-286., device='cuda:0') e 0.05 loss_dqn tensor(5.7613e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15619982779026031
dqn reward tensor(-335.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.4202e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21980682015419006
dqn reward tensor(-341.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4070e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08014246821403503
dqn reward tensor(-246.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4394e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09484702348709106
dqn reward tensor(-240.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9479e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04295601695775986
dqn reward tensor(-322.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.2801e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20362290740013123
dqn reward tensor(-344.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8506e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2344897985458374
dqn reward tensor(-353.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5787e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15507259964942932
dqn reward tensor(-369.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9446e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09199079871177673
dqn reward tensor(-308.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9136e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2834501266479492
dqn reward tensor(-350.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1025e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0892903059720993
dqn reward tensor(-315.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3290e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08234573900699615
dqn reward tensor(-405., device='cuda:0') e 0.05 loss_dqn tensor(5.5480e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17218473553657532
dqn reward tensor(-339.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7692e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.288493275642395
dqn reward tensor(-337.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.7572e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08595651388168335
dqn reward tensor(-263.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2500e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22699031233787537
dqn reward tensor(-108.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7144e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18603399395942688
dqn reward tensor(-252.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8119e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09362979233264923
dqn reward tensor(-312.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6212e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14396324753761292
dqn reward tensor(-355.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9628e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1956874579191208
dqn reward tensor(-221.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0601e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15600840747356415
dqn reward tensor(-310.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0436e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17540551722049713
dqn reward tensor(-366.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4877e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17022688686847687
dqn reward tensor(-361.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5696e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18106138706207275
dqn reward tensor(-206.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6493e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04541729390621185
dqn reward tensor(-238.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4236e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031687501817941666
dqn reward tensor(-307.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3860e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0635775774717331
dqn reward tensor(-365.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2905e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08526602387428284
dqn reward tensor(-399.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1430e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425672173500061
dqn reward tensor(-369.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7362e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1678481101989746
dqn reward tensor(-331.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1589e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2466573417186737
dqn reward tensor(-266.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23904921114444733
dqn reward tensor(-321.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9534e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17869143187999725
dqn reward tensor(-317.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0519e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11640207469463348
dqn reward tensor(-384.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1891e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09575360268354416
dqn reward tensor(-321.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.4997e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15581509470939636
dqn reward tensor(-269.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6418e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14018064737319946
dqn reward tensor(-321.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5955e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08919943869113922
dqn reward tensor(-316.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2619e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10067310929298401
dqn reward tensor(-226.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3789e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477473795413971
dqn reward tensor(-339.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7839e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25627681612968445
dqn reward tensor(-358.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7966e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24780242145061493
dqn reward tensor(-404.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8506e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07979622483253479
dqn reward tensor(-292., device='cuda:0') e 0.05 loss_dqn tensor(7.0698e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07217511534690857
dqn reward tensor(-348.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6976e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11896543204784393
dqn reward tensor(-270.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4639e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18255360424518585
dqn reward tensor(-300.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9533e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15088218450546265
dqn reward tensor(-456.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8083e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2972414195537567
dqn reward tensor(-233.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9123e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10413658618927002
dqn reward tensor(-398.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2780e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1697433590888977
dqn reward tensor(-209.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07408149540424347
dqn reward tensor(-302.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07538571208715439
dqn reward tensor(-292.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3564e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25447598099708557
dqn reward tensor(-299.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3196e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10927504301071167
dqn reward tensor(-291.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14203810691833496
dqn reward tensor(-413.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1701e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17382188141345978
dqn reward tensor(-293.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2884e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05413134768605232
dqn reward tensor(-393.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1289e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2546481192111969
dqn reward tensor(-338.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1159e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09159604460000992
dqn reward tensor(-219.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.5899e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05668461695313454
dqn reward tensor(-358.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09369631856679916
dqn reward tensor(-217.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5624e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15653355419635773
dqn reward tensor(-333.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0497e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08410093188285828
dqn reward tensor(-216., device='cuda:0') e 0.05 loss_dqn tensor(8.8907e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14551052451133728
dqn reward tensor(-327.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9979e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0724712610244751
dqn reward tensor(-231.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2705e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1566414088010788
dqn reward tensor(-337.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1843e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13152748346328735
dqn reward tensor(-304.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08154360949993134
dqn reward tensor(-249.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1678e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1691236048936844
dqn reward tensor(-295.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.3613e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022490855306386948
dqn reward tensor(-369.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0619e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38805752992630005
dqn reward tensor(-279.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.6731e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10283971577882767
dqn reward tensor(-370.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2479e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1380210816860199
dqn reward tensor(-287.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1026e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15098169445991516
dqn reward tensor(-386., device='cuda:0') e 0.05 loss_dqn tensor(1.3776e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.193811297416687
dqn reward tensor(-268.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1351e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16659632325172424
dqn reward tensor(-214.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8378e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1244962215423584
dqn reward tensor(-314.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11659732460975647
dqn reward tensor(-357.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9867e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17554402351379395
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19185611605644226
dqn reward tensor(-310.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043553419411182404
dqn reward tensor(-434.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17690512537956238
dqn reward tensor(-285., device='cuda:0') e 0.05 loss_dqn tensor(1.1418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12381595373153687
dqn reward tensor(-344.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3148e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09514421224594116
dqn reward tensor(-251.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10266533493995667
dqn reward tensor(-311.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14713989198207855
dqn reward tensor(-346.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1424e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12565340101718903
dqn reward tensor(-150.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4619e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31020528078079224
dqn reward tensor(-334.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2669285535812378
dqn reward tensor(-265.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3784e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13045033812522888
dqn reward tensor(-249.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1705e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046731602400541306
dqn reward tensor(-292., device='cuda:0') e 0.05 loss_dqn tensor(1.7276e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24182727932929993
dqn reward tensor(-336.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13928411900997162
dqn reward tensor(-323.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21951289474964142
dqn reward tensor(-291.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19955623149871826
dqn reward tensor(-266.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1877e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05684281513094902
dqn reward tensor(-329.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2110e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1899481564760208
dqn reward tensor(-414.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10098008811473846
dqn reward tensor(-244.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4413e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1662818342447281
dqn reward tensor(-373.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9988e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12022926658391953
dqn reward tensor(-346., device='cuda:0') e 0.05 loss_dqn tensor(1.1830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.151157945394516
dqn reward tensor(-283.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05150506645441055
dqn reward tensor(-363.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2764e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3038586974143982
dqn reward tensor(-405.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11097311228513718
dqn reward tensor(-292.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8394e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2474866658449173
dqn reward tensor(-224.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5534e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02426062524318695
dqn reward tensor(-299.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1937e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24036604166030884
dqn reward tensor(-327.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16806820034980774
dqn reward tensor(-348., device='cuda:0') e 0.05 loss_dqn tensor(1.8833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05966869369149208
dqn reward tensor(-354.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.155781090259552
dqn reward tensor(-346.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040509458631277084
dqn reward tensor(-287.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11756420880556107
dqn reward tensor(-329.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4184e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039254285395145416
dqn reward tensor(-212.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3864e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1194489598274231
dqn reward tensor(-191., device='cuda:0') e 0.05 loss_dqn tensor(1.3996e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10331826657056808
dqn reward tensor(-279.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1830541491508484
dqn reward tensor(-349.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13186991214752197
dqn reward tensor(-371.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.212233304977417
dqn reward tensor(-350.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24272286891937256
dqn reward tensor(-248.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4715e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15759463608264923
dqn reward tensor(-438.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9991e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1127813458442688
dqn reward tensor(-211.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33393189311027527
dqn reward tensor(-484.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4963e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18090903759002686
dqn reward tensor(-406., device='cuda:0') e 0.05 loss_dqn tensor(1.5472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13857820630073547
dqn reward tensor(-369., device='cuda:0') e 0.05 loss_dqn tensor(1.4653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12863251566886902
dqn reward tensor(-368.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1565e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10133295506238937
dqn reward tensor(-374.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15497435629367828
dqn reward tensor(-366.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0234e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15447448194026947
dqn reward tensor(-359.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4585e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06413432955741882
dqn reward tensor(-231.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15546724200248718
dqn reward tensor(-330.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5534e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10490837693214417
dqn reward tensor(-351.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025079216808080673
dqn reward tensor(-326.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19014912843704224
dqn reward tensor(-355.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0837811678647995
dqn reward tensor(-403.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5740e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10152292251586914
dqn reward tensor(-350.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013528712093830109
dqn reward tensor(-240.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8239e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1956024318933487
dqn reward tensor(-296.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6533e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1605132520198822
dqn reward tensor(-260.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5494e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19856704771518707
dqn reward tensor(-299.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11273372173309326
dqn reward tensor(-215.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1837e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09399552643299103
dqn reward tensor(-350., device='cuda:0') e 0.05 loss_dqn tensor(1.7292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0817871019244194
dqn reward tensor(-350.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0629e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06790599226951599
dqn reward tensor(-395.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7694e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09258057177066803
dqn reward tensor(-330.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21886970102787018
dqn reward tensor(-269.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3046388328075409
dqn reward tensor(-326.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0965576171875
dqn reward tensor(-424.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1905e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30503278970718384
dqn reward tensor(-495.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09382066130638123
dqn reward tensor(-311., device='cuda:0') e 0.05 loss_dqn tensor(1.9581e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13566018640995026
dqn reward tensor(-341.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2134e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16558793187141418
dqn reward tensor(-436.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3084535002708435
dqn reward tensor(-412.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6877e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12783388793468475
dqn reward tensor(-211.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10122610628604889
dqn reward tensor(-256.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9766e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15465980768203735
dqn reward tensor(-247.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.4738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06997950375080109
dqn reward tensor(-268.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9200e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10803353786468506
dqn reward tensor(-292.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09841945767402649
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6615e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031109806150197983
dqn reward tensor(-344.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0077e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05884525924921036
dqn reward tensor(-391.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09064837545156479
dqn reward tensor(-388.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10233889520168304
dqn reward tensor(-219.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10710299760103226
dqn reward tensor(-500.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12505069375038147
dqn reward tensor(-277.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5141e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16845299303531647
dqn reward tensor(-406.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6750e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2993437647819519
dqn reward tensor(-345., device='cuda:0') e 0.05 loss_dqn tensor(2.5029e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07120892405509949
dqn reward tensor(-373.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0764043927192688
dqn reward tensor(-354.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7584e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12009957432746887
dqn reward tensor(-228.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9955e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1554388850927353
dqn reward tensor(-479.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20980393886566162
dqn reward tensor(-361.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02727305144071579
dqn reward tensor(-399.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7430e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09739398211240768
dqn reward tensor(-414.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.6686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08427556604146957
dqn reward tensor(-337.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0587e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18508711457252502
dqn reward tensor(-271., device='cuda:0') e 0.05 loss_dqn tensor(2.1630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13184627890586853
dqn reward tensor(-363.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07280254364013672
dqn reward tensor(-318.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2104e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14419540762901306
dqn reward tensor(-243.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8514e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041020721197128296
dqn reward tensor(-302., device='cuda:0') e 0.05 loss_dqn tensor(2.4726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1296733021736145
dqn reward tensor(-345., device='cuda:0') e 0.05 loss_dqn tensor(1.8162e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10061632841825485
dqn reward tensor(-358.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5263e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23731304705142975
dqn reward tensor(-258.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7364e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21954749524593353
dqn reward tensor(-340., device='cuda:0') e 0.05 loss_dqn tensor(1.8497e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4125783443450928
dqn reward tensor(-237.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08208005875349045
dqn reward tensor(-424., device='cuda:0') e 0.05 loss_dqn tensor(2.6533e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1437840759754181
dqn reward tensor(-380.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8180e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1421322077512741
dqn reward tensor(-340.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0741899237036705
dqn reward tensor(-254.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3848e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24701257050037384
dqn reward tensor(-290.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8809e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17628343403339386
dqn reward tensor(-464.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2879910171031952
dqn reward tensor(-379.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19213367998600006
dqn reward tensor(-310.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0729389488697052
dqn reward tensor(-395.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25678151845932007
dqn reward tensor(-383.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12065796554088593
dqn reward tensor(-374., device='cuda:0') e 0.05 loss_dqn tensor(2.9055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12251880764961243
dqn reward tensor(-430.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18482394516468048
dqn reward tensor(-279.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15470992028713226
dqn reward tensor(-259.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11492232978343964
dqn reward tensor(-249.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7448e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.116120345890522
dqn reward tensor(-398.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14934664964675903
dqn reward tensor(-319.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1671280562877655
dqn reward tensor(-342.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8764e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2581762373447418
dqn reward tensor(-323.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4101e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16940274834632874
dqn reward tensor(-270.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8596e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029881447553634644
dqn reward tensor(-229.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7663e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019861161708831787
dqn reward tensor(-387.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6644e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07629852741956711
dqn reward tensor(-228.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13181918859481812
dqn reward tensor(-280.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10150597244501114
dqn reward tensor(-343.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0701771154999733
dqn reward tensor(-300.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0790586993098259
dqn reward tensor(-234., device='cuda:0') e 0.05 loss_dqn tensor(1.9126e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023175248876214027
dqn reward tensor(-292.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7673e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0735553652048111
dqn reward tensor(-351.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8920e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06858928501605988
dqn reward tensor(-360.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7114e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17215639352798462
dqn reward tensor(-287.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24534563720226288
dqn reward tensor(-258.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9318e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16499722003936768
dqn reward tensor(-284.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08295579999685287
dqn reward tensor(-292.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16290554404258728
dqn reward tensor(-377., device='cuda:0') e 0.05 loss_dqn tensor(1.7932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13065986335277557
dqn reward tensor(-441.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03801339119672775
dqn reward tensor(-173.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8144e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0793832316994667
dqn reward tensor(-271.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2715e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03316878154873848
dqn reward tensor(-244.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12178321927785873
dqn reward tensor(-373.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14088571071624756
dqn reward tensor(-302.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1923e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05800441652536392
dqn reward tensor(-254., device='cuda:0') e 0.05 loss_dqn tensor(1.9197e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025302276015281677
dqn reward tensor(-199.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19100923836231232
dqn reward tensor(-313., device='cuda:0') e 0.05 loss_dqn tensor(1.7261e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13558107614517212
dqn reward tensor(-362.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2309e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17767351865768433
dqn reward tensor(-452., device='cuda:0') e 0.05 loss_dqn tensor(1.7558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22975534200668335
dqn reward tensor(-347.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.299138605594635
dqn reward tensor(-196.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.7314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07339434325695038
dqn reward tensor(-317.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2065e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053178831934928894
dqn reward tensor(-364.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2477e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17508207261562347
dqn reward tensor(-201.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4121e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10314269363880157
dqn reward tensor(-420.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03817933797836304
dqn reward tensor(-249., device='cuda:0') e 0.05 loss_dqn tensor(2.3261e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2586386799812317
dqn reward tensor(-221.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2034458965063095
dqn reward tensor(-130.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09586148709058762
dqn reward tensor(-286.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9015e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13511529564857483
dqn reward tensor(-274.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13732337951660156
dqn reward tensor(-285.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9068e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13295099139213562
dqn reward tensor(-321.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8920e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14330552518367767
dqn reward tensor(-409., device='cuda:0') e 0.05 loss_dqn tensor(1.9224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14342395961284637
dqn reward tensor(-368.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8621e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08428110927343369
dqn reward tensor(-278.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0232e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1455208659172058
dqn reward tensor(-451.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2494082748889923
dqn reward tensor(-348.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8957e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09197096526622772
dqn reward tensor(-215.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17837809026241302
dqn reward tensor(-245.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9850e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3342447876930237
dqn reward tensor(-264.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3041675090789795
dqn reward tensor(-199.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8767e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26031413674354553
dqn reward tensor(-362.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8935e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10738770663738251
dqn reward tensor(-313.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1404e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10592973977327347
dqn reward tensor(-506., device='cuda:0') e 0.05 loss_dqn tensor(2.1076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1345495581626892
dqn reward tensor(-425.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0607e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10209448635578156
dqn reward tensor(-256.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12802007794380188
dqn reward tensor(-257.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8523e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25253424048423767
dqn reward tensor(-411.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29571980237960815
dqn reward tensor(-413.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8234e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13571226596832275
dqn reward tensor(-341.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13974176347255707
dqn reward tensor(-388.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4521e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12293016910552979
dqn reward tensor(-407.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9092e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.278421014547348
dqn reward tensor(-279.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9276e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1984017938375473
dqn reward tensor(-284.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.098526731133461
dqn reward tensor(-355.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.3949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08635281026363373
dqn reward tensor(-349.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12314553558826447
dqn reward tensor(-381.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8610e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0609331913292408
dqn reward tensor(-306., device='cuda:0') e 0.05 loss_dqn tensor(1.8030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27999794483184814
dqn reward tensor(-366.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0574e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10873764008283615
dqn reward tensor(-273.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9349e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2083088457584381
dqn reward tensor(-350., device='cuda:0') e 0.05 loss_dqn tensor(2.3485e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22699177265167236
dqn reward tensor(-429.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8251e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03366668149828911
dqn reward tensor(-466.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04401778802275658
dqn reward tensor(-313., device='cuda:0') e 0.05 loss_dqn tensor(2.2108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19747090339660645
dqn reward tensor(-274.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04946116358041763
dqn reward tensor(-202.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03162981942296028
dqn reward tensor(-297.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3202129602432251
dqn reward tensor(-200.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9696e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024436835199594498
dqn reward tensor(-361.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9100e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026333991438150406
dqn reward tensor(-197., device='cuda:0') e 0.05 loss_dqn tensor(2.2428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15411695837974548
dqn reward tensor(-335.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9305e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02001519314944744
dqn reward tensor(-358.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8180e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05753757059574127
dqn reward tensor(-324.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27235573530197144
dqn reward tensor(-284.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07506154477596283
dqn reward tensor(-400.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3286e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14366835355758667
dqn reward tensor(-303.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0151e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15370243787765503
dqn reward tensor(-296.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9033e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15386566519737244
dqn reward tensor(-310.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2009468376636505
dqn reward tensor(-266., device='cuda:0') e 0.05 loss_dqn tensor(1.9028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14783324301242828
dqn reward tensor(-374.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14336742460727692
dqn reward tensor(-339.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3063209056854248
dqn reward tensor(-304.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3289e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0390845388174057
dqn reward tensor(-283.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13346201181411743
dqn reward tensor(-323.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10297232866287231
dqn reward tensor(-293.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27162066102027893
dqn reward tensor(-300.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254722774028778
dqn reward tensor(-405.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10859043151140213
dqn reward tensor(-304.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8367e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1645275205373764
dqn reward tensor(-365.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11109891533851624
dqn reward tensor(-292.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6750e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17301449179649353
dqn reward tensor(-481.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8662e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13649161159992218
dqn reward tensor(-308.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0528e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15174834430217743
dqn reward tensor(-279.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9658e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2029980570077896
dqn reward tensor(-279.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1940e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15050657093524933
dqn reward tensor(-306.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0967e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2668023407459259
dqn reward tensor(-255.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1405e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06121666356921196
dqn reward tensor(-308.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9948e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22541049122810364
dqn reward tensor(-300.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0641e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031558308750391006
dqn reward tensor(-242.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1139e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09192515909671783
dqn reward tensor(-294.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.9366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032335907220840454
dqn reward tensor(-290.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1151e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425705999135971
dqn reward tensor(-38.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3878e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028978897258639336
Evaluating...
Train: {'rocauc': 0.7583050869525814} -5.030195236206055
=====Epoch 10=====
Training...
dqn reward tensor(-305.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1265040636062622
dqn reward tensor(-230.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9191e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24438922107219696
dqn reward tensor(-393.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12899984419345856
dqn reward tensor(-342.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.8797e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21602517366409302
dqn reward tensor(-393.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12638697028160095
dqn reward tensor(-338., device='cuda:0') e 0.05 loss_dqn tensor(2.0634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02275516465306282
dqn reward tensor(-346.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1158466786146164
dqn reward tensor(-313.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1314343810081482
dqn reward tensor(-346.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9504e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058861128985881805
dqn reward tensor(-271.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13457363843917847
dqn reward tensor(-177.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9241e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29161518812179565
dqn reward tensor(-397., device='cuda:0') e 0.05 loss_dqn tensor(2.0473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07594865560531616
dqn reward tensor(-362.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2431e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14093929529190063
dqn reward tensor(-377.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10047023743391037
dqn reward tensor(-383.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.198790043592453
dqn reward tensor(-294.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07194966077804565
dqn reward tensor(-247.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9075e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23395173251628876
dqn reward tensor(-235.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0077e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22064819931983948
dqn reward tensor(-378.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.5396e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1307145357131958
dqn reward tensor(-215.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0318e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10844697058200836
dqn reward tensor(-335.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09157513827085495
dqn reward tensor(-470.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1649690866470337
dqn reward tensor(-348.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0933835506439209
dqn reward tensor(-307.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0851e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21615147590637207
dqn reward tensor(-260., device='cuda:0') e 0.05 loss_dqn tensor(2.6036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1717497706413269
dqn reward tensor(-440.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7732e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04758867621421814
dqn reward tensor(-353.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10598544031381607
dqn reward tensor(-413.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9936e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21804189682006836
dqn reward tensor(-229.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17523565888404846
dqn reward tensor(-264.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7983e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030452895909547806
dqn reward tensor(-316.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4337e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1869664490222931
dqn reward tensor(-336.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.9302e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02193445898592472
dqn reward tensor(-346.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0775e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10508038103580475
dqn reward tensor(-326.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052752282470464706
dqn reward tensor(-221.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9828e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22599920630455017
dqn reward tensor(-313.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.198465034365654
dqn reward tensor(-404.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0486e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18050386011600494
dqn reward tensor(-305.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8095e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12367585301399231
dqn reward tensor(-350.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0401e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1313416063785553
dqn reward tensor(-344.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8859e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22111032903194427
dqn reward tensor(-308.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2073e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14948776364326477
dqn reward tensor(-396.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1721321940422058
dqn reward tensor(-255., device='cuda:0') e 0.05 loss_dqn tensor(1.9025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1826542466878891
dqn reward tensor(-336.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.7967e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2199392318725586
dqn reward tensor(-158.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12857383489608765
dqn reward tensor(-381.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.0692e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14096452295780182
dqn reward tensor(-325., device='cuda:0') e 0.05 loss_dqn tensor(2.4708e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11321815103292465
dqn reward tensor(-373.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17020954191684723
dqn reward tensor(-310.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07427886128425598
dqn reward tensor(-382.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0338e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19176357984542847
dqn reward tensor(-356., device='cuda:0') e 0.05 loss_dqn tensor(2.1414e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13426195085048676
dqn reward tensor(-342.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1989153027534485
dqn reward tensor(-442.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.4154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18853217363357544
dqn reward tensor(-372., device='cuda:0') e 0.05 loss_dqn tensor(2.0737e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13650336861610413
dqn reward tensor(-202.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5564e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09852580726146698
dqn reward tensor(-338.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0440e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0772753357887268
dqn reward tensor(-437.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8136e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23434650897979736
dqn reward tensor(-391.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16466093063354492
dqn reward tensor(-395.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13528120517730713
dqn reward tensor(-308., device='cuda:0') e 0.05 loss_dqn tensor(1.8726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20223981142044067
dqn reward tensor(-420.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0422586053609848
dqn reward tensor(-338.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9553e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04940304160118103
dqn reward tensor(-341.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0556e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1370062232017517
dqn reward tensor(-328.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.4582e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08290313184261322
dqn reward tensor(-299.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12688688933849335
dqn reward tensor(-299., device='cuda:0') e 0.05 loss_dqn tensor(3.5725e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1439739465713501
dqn reward tensor(-373.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07176203280687332
dqn reward tensor(-413.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1504080891609192
dqn reward tensor(-258.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22229859232902527
dqn reward tensor(-372.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0124e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17248426377773285
dqn reward tensor(-368.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09003201872110367
dqn reward tensor(-400.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9420e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10558051615953445
dqn reward tensor(-389.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9847e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023456934839487076
dqn reward tensor(-322.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0463e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15422049164772034
dqn reward tensor(-289.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1891e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11547691375017166
dqn reward tensor(-193.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4173e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15395860373973846
dqn reward tensor(-303.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7290e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19483205676078796
dqn reward tensor(-281.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36597931385040283
dqn reward tensor(-345.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8041e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15297649800777435
dqn reward tensor(-375.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6781e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07672317326068878
dqn reward tensor(-332.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2350e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14267030358314514
dqn reward tensor(-356., device='cuda:0') e 0.05 loss_dqn tensor(2.1127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08997536450624466
dqn reward tensor(-335.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13515709340572357
dqn reward tensor(-467.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12495636940002441
dqn reward tensor(-404.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09926667809486389
dqn reward tensor(-265.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0010e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05489427596330643
dqn reward tensor(-319.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15249018371105194
dqn reward tensor(-286.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9766e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08604592084884644
dqn reward tensor(-288., device='cuda:0') e 0.05 loss_dqn tensor(1.9590e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17122480273246765
dqn reward tensor(-297.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0096e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08001383394002914
dqn reward tensor(-365.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029476845636963844
dqn reward tensor(-234.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.0126e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20619583129882812
dqn reward tensor(-392.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4047e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08873551338911057
dqn reward tensor(-356.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6126e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10502979159355164
dqn reward tensor(-384.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3251e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09473249316215515
dqn reward tensor(-317.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.0583e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16501885652542114
dqn reward tensor(-313., device='cuda:0') e 0.05 loss_dqn tensor(2.0589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29678910970687866
dqn reward tensor(-352.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5666e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22295212745666504
dqn reward tensor(-414.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4324e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13434875011444092
dqn reward tensor(-442.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8883e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052503298968076706
dqn reward tensor(-361.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06639352440834045
dqn reward tensor(-402.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1941e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12946859002113342
dqn reward tensor(-335., device='cuda:0') e 0.05 loss_dqn tensor(1.8660e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3030983805656433
dqn reward tensor(-356., device='cuda:0') e 0.05 loss_dqn tensor(2.0108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24908697605133057
dqn reward tensor(-308., device='cuda:0') e 0.05 loss_dqn tensor(2.0868e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15393748879432678
dqn reward tensor(-411.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11252783238887787
dqn reward tensor(-218., device='cuda:0') e 0.05 loss_dqn tensor(1.9160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14497825503349304
dqn reward tensor(-311.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13829204440116882
dqn reward tensor(-304.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.9900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19286012649536133
dqn reward tensor(-228.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21533897519111633
dqn reward tensor(-422.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9552e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07437075674533844
dqn reward tensor(-339., device='cuda:0') e 0.05 loss_dqn tensor(2.1643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28093600273132324
dqn reward tensor(-209.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2059e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0834275409579277
dqn reward tensor(-330.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8326e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0812905877828598
dqn reward tensor(-285.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3500e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1041116863489151
dqn reward tensor(-305.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8880e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0967201516032219
dqn reward tensor(-406.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9110e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07921458035707474
dqn reward tensor(-219.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025358673185110092
dqn reward tensor(-341.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9118e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016064655035734177
dqn reward tensor(-287.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012107694521546364
dqn reward tensor(-377.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08138608932495117
dqn reward tensor(-345.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1864e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2320898324251175
dqn reward tensor(-386.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16280131042003632
dqn reward tensor(-434.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6432e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058723390102386475
dqn reward tensor(-323.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03481650352478027
dqn reward tensor(-425.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18348652124404907
dqn reward tensor(-323.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.9284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04391377046704292
dqn reward tensor(-288.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5013e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14627355337142944
dqn reward tensor(-377.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4927e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1355212926864624
dqn reward tensor(-349.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4521e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01391619723290205
dqn reward tensor(-203.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21649570763111115
dqn reward tensor(-242.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9886e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08538763970136642
dqn reward tensor(-426.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16326399147510529
dqn reward tensor(-492.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8433e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1599525511264801
dqn reward tensor(-303.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9499e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12954877316951752
dqn reward tensor(-274.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.4075e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14307871460914612
dqn reward tensor(-437.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028202494606375694
dqn reward tensor(-380.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2506614029407501
dqn reward tensor(-387.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2348383069038391
dqn reward tensor(-277.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20317110419273376
dqn reward tensor(-337.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3148189187049866
dqn reward tensor(-461.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.6423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13870249688625336
dqn reward tensor(-240.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12141586095094681
dqn reward tensor(-337.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3675e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1333221197128296
dqn reward tensor(-309.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10294020175933838
dqn reward tensor(-351.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3027e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15262064337730408
dqn reward tensor(-295.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1806e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1654999554157257
dqn reward tensor(-394.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1389893889427185
dqn reward tensor(-219.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18189850449562073
dqn reward tensor(-256.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7609e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14074528217315674
dqn reward tensor(-315.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5859e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13207757472991943
dqn reward tensor(-409.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4485e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036943137645721436
dqn reward tensor(-367.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16291648149490356
dqn reward tensor(-411., device='cuda:0') e 0.05 loss_dqn tensor(2.0925e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23413531482219696
dqn reward tensor(-311., device='cuda:0') e 0.05 loss_dqn tensor(2.0970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15251503884792328
dqn reward tensor(-346.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3448988199234009
dqn reward tensor(-253.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0186e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13572783768177032
dqn reward tensor(-247.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.1048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2768787145614624
dqn reward tensor(-200.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18048258125782013
dqn reward tensor(-347., device='cuda:0') e 0.05 loss_dqn tensor(2.9515e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20250295102596283
dqn reward tensor(-362.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6542e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10929784923791885
dqn reward tensor(-301.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13035863637924194
dqn reward tensor(-325.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12288708984851837
dqn reward tensor(-405.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11853761225938797
dqn reward tensor(-445.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1345e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0811549723148346
dqn reward tensor(-432.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2082005739212036
dqn reward tensor(-301.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3158165216445923
dqn reward tensor(-328.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3758e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15644657611846924
dqn reward tensor(-275.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06789162755012512
dqn reward tensor(-329., device='cuda:0') e 0.05 loss_dqn tensor(2.3105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22130468487739563
dqn reward tensor(-381.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1262e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13123074173927307
dqn reward tensor(-410.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24084806442260742
dqn reward tensor(-354.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0447e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07536401599645615
dqn reward tensor(-431.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20663821697235107
dqn reward tensor(-482.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1998e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1960275024175644
dqn reward tensor(-404.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.6288e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13510450720787048
dqn reward tensor(-425., device='cuda:0') e 0.05 loss_dqn tensor(2.2273e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13463017344474792
dqn reward tensor(-385.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1665135622024536
dqn reward tensor(-396., device='cuda:0') e 0.05 loss_dqn tensor(3.3454e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22987008094787598
dqn reward tensor(-348.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16243429481983185
dqn reward tensor(-437.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5329e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09870200604200363
dqn reward tensor(-365., device='cuda:0') e 0.05 loss_dqn tensor(3.1838e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2587776184082031
dqn reward tensor(-341.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9504e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14109228551387787
dqn reward tensor(-299.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5109e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06428169459104538
dqn reward tensor(-184.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0471e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19815745949745178
dqn reward tensor(-225.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08138919621706009
dqn reward tensor(-407.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12034475058317184
dqn reward tensor(-245.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15004093945026398
dqn reward tensor(-205.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6676e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20882979035377502
dqn reward tensor(-319.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.1199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254759132862091
dqn reward tensor(-374.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12800461053848267
dqn reward tensor(-231.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9610e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09727776050567627
dqn reward tensor(-274.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1882e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026734236627817154
dqn reward tensor(-325.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.098023422062397
dqn reward tensor(-263.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02211066521704197
dqn reward tensor(-285.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13674330711364746
dqn reward tensor(-324.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1505e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1792633980512619
dqn reward tensor(-418.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05255391448736191
dqn reward tensor(-354.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1945e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18156881630420685
dqn reward tensor(-250.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17029570043087006
dqn reward tensor(-312.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11914914846420288
dqn reward tensor(-358.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4169e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11235049366950989
dqn reward tensor(-252.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9732e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2208913415670395
dqn reward tensor(-302.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1071e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1365533471107483
dqn reward tensor(-428.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0195e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17934316396713257
dqn reward tensor(-258.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9869e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04872448742389679
dqn reward tensor(-437.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0111e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1738424003124237
dqn reward tensor(-428.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1336e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03454059362411499
dqn reward tensor(-238.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23114924132823944
dqn reward tensor(-283.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2249305695295334
dqn reward tensor(-324.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7652e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23340485990047455
dqn reward tensor(-319.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13904541730880737
dqn reward tensor(-249.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2896e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09994326531887054
dqn reward tensor(-405.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14976589381694794
dqn reward tensor(-344., device='cuda:0') e 0.05 loss_dqn tensor(1.9751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11147469282150269
dqn reward tensor(-363.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045805517584085464
dqn reward tensor(-349.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17288324236869812
dqn reward tensor(-326.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05233268067240715
dqn reward tensor(-338.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04484403878450394
dqn reward tensor(-398.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1838197112083435
dqn reward tensor(-311.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0179e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0726328119635582
dqn reward tensor(-380.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20984230935573578
dqn reward tensor(-369.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9353e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11927692592144012
dqn reward tensor(-227.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6610e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11236638575792313
dqn reward tensor(-366.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08710239082574844
dqn reward tensor(-372.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5912e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2568309009075165
dqn reward tensor(-223.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1053e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09454365819692612
dqn reward tensor(-336.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20750662684440613
dqn reward tensor(-408.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13561059534549713
dqn reward tensor(-329.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6582e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08596430718898773
dqn reward tensor(-253.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10333602875471115
dqn reward tensor(-271.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9223e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20884695649147034
dqn reward tensor(-350.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08633573353290558
dqn reward tensor(-345.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5283e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2359335720539093
dqn reward tensor(-420.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7219e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10324427485466003
dqn reward tensor(-328., device='cuda:0') e 0.05 loss_dqn tensor(1.7647e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11313670873641968
dqn reward tensor(-386.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.5530e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12361476570367813
dqn reward tensor(-343.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5887e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21126757562160492
dqn reward tensor(-398.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.3128e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19889487326145172
dqn reward tensor(-383., device='cuda:0') e 0.05 loss_dqn tensor(1.8054e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09634242951869965
dqn reward tensor(-296.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07485993206501007
dqn reward tensor(-407., device='cuda:0') e 0.05 loss_dqn tensor(1.8832e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08611228317022324
dqn reward tensor(-339.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1166953295469284
dqn reward tensor(-403.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8310e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17359575629234314
dqn reward tensor(-332.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8193e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23054233193397522
dqn reward tensor(-308.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2204e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19558775424957275
dqn reward tensor(-372.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9439e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025570901110768318
dqn reward tensor(-413.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26104408502578735
dqn reward tensor(-337.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14464432001113892
dqn reward tensor(-366.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5736e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12515650689601898
dqn reward tensor(-372.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8144e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10936404764652252
dqn reward tensor(-317., device='cuda:0') e 0.05 loss_dqn tensor(1.8067e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07292911410331726
dqn reward tensor(-367., device='cuda:0') e 0.05 loss_dqn tensor(1.7760e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038118090480566025
dqn reward tensor(-321.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.084092877805233
dqn reward tensor(-467.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.7514e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26206541061401367
dqn reward tensor(-293.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22640477120876312
dqn reward tensor(-348.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7161e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477648884057999
dqn reward tensor(-314.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22458142042160034
dqn reward tensor(-296., device='cuda:0') e 0.05 loss_dqn tensor(3.2268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1715097725391388
dqn reward tensor(-384., device='cuda:0') e 0.05 loss_dqn tensor(1.9456e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24612167477607727
dqn reward tensor(-327.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.7538e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09154503047466278
dqn reward tensor(-403.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3336e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09959922730922699
dqn reward tensor(-308.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2082e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11042682826519012
dqn reward tensor(-325.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28332093358039856
dqn reward tensor(-336.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14987656474113464
dqn reward tensor(-242.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1692807674407959
dqn reward tensor(-360.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8053e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18011057376861572
dqn reward tensor(-360.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16301795840263367
dqn reward tensor(-432.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2886e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1263379454612732
dqn reward tensor(-338.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7246e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1679058074951172
dqn reward tensor(-397.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.7225e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21247699856758118
dqn reward tensor(-340.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3454e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2256978452205658
dqn reward tensor(-372.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29182714223861694
dqn reward tensor(-295.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1295427680015564
dqn reward tensor(-273., device='cuda:0') e 0.05 loss_dqn tensor(2.5780e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11634403467178345
dqn reward tensor(-336.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8149e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10668513178825378
dqn reward tensor(-388.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06927661597728729
dqn reward tensor(-396.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9942e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03867349028587341
dqn reward tensor(-381.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9844e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1093757152557373
dqn reward tensor(-212.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.8866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376926600933075
dqn reward tensor(-309.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6640e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18021170794963837
dqn reward tensor(-380.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7358e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02917291969060898
dqn reward tensor(-278.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1278173327445984
dqn reward tensor(-303.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7776e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06281471252441406
dqn reward tensor(-393.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2076934278011322
dqn reward tensor(-335.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7874e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1772729903459549
dqn reward tensor(-378.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5177e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11678887903690338
dqn reward tensor(-257.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1462e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14120721817016602
dqn reward tensor(-294.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0820e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08066678047180176
dqn reward tensor(-411.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4359e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13447891175746918
dqn reward tensor(-356.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0092e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15511028468608856
dqn reward tensor(-330.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7571e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21656955778598785
dqn reward tensor(-299.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11522775143384933
dqn reward tensor(-298., device='cuda:0') e 0.05 loss_dqn tensor(2.2030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028775295242667198
dqn reward tensor(-299., device='cuda:0') e 0.05 loss_dqn tensor(2.0485e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09829306602478027
dqn reward tensor(-318.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1897e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03576849400997162
dqn reward tensor(-306.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09864090383052826
dqn reward tensor(-438.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904885470867157
dqn reward tensor(-257.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06312985718250275
dqn reward tensor(-336.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16925884783267975
dqn reward tensor(-393.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0819e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08379596471786499
dqn reward tensor(-381.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6082e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09299172461032867
dqn reward tensor(-306., device='cuda:0') e 0.05 loss_dqn tensor(1.5136e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12550203502178192
dqn reward tensor(-239.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9421e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053342849016189575
dqn reward tensor(-477.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5662e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10159763693809509
dqn reward tensor(-361., device='cuda:0') e 0.05 loss_dqn tensor(1.5932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17982891201972961
dqn reward tensor(-406.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9437e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10012820363044739
dqn reward tensor(-438.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03343730419874191
dqn reward tensor(-278.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6153e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13585565984249115
dqn reward tensor(-391.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9674e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06788645684719086
dqn reward tensor(-341.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6210e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21577522158622742
dqn reward tensor(-354.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4232e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.275790274143219
dqn reward tensor(-200.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1849355250597
dqn reward tensor(-181.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8372e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13817936182022095
dqn reward tensor(-348.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18047954142093658
dqn reward tensor(-305.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4861e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09638988971710205
dqn reward tensor(-393.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17317230999469757
dqn reward tensor(-373.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5893e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06969641149044037
dqn reward tensor(-284.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1074475347995758
dqn reward tensor(-258.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1115e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2103012055158615
dqn reward tensor(-255.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.7466e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19014817476272583
dqn reward tensor(-370.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13841694593429565
dqn reward tensor(-418.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2739051878452301
dqn reward tensor(-289.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04458152502775192
dqn reward tensor(-250.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021636169403791428
dqn reward tensor(-278.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7651e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07824980467557907
dqn reward tensor(-392.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9701e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1221671849489212
dqn reward tensor(-309.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13905692100524902
dqn reward tensor(-372.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2304e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25792863965034485
dqn reward tensor(-345.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.5248e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22042419016361237
dqn reward tensor(-301.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5060e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09853784739971161
dqn reward tensor(-291.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3380e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16733892261981964
dqn reward tensor(-448.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18680720031261444
dqn reward tensor(-382.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3397e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17947238683700562
dqn reward tensor(-263.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5425e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10349476337432861
dqn reward tensor(-308.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9217e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12277831137180328
dqn reward tensor(-301.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20742985606193542
dqn reward tensor(-355.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5976e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10277102142572403
dqn reward tensor(-340.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670854687690735
dqn reward tensor(-319.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17046162486076355
dqn reward tensor(-261.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3447e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09576015919446945
dqn reward tensor(-248.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5012e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17490097880363464
dqn reward tensor(-343.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5135e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03951387107372284
dqn reward tensor(-332.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6310e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254572570323944
dqn reward tensor(-352.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21877369284629822
dqn reward tensor(-415., device='cuda:0') e 0.05 loss_dqn tensor(1.5455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1987258791923523
dqn reward tensor(-341.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5825e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22071430087089539
dqn reward tensor(-281.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5397e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15236149728298187
dqn reward tensor(-336.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2112748920917511
dqn reward tensor(-456.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5615e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060006074607372284
dqn reward tensor(-310.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943383276462555
dqn reward tensor(-251., device='cuda:0') e 0.05 loss_dqn tensor(2.0227e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24106094241142273
dqn reward tensor(-339.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21572653949260712
dqn reward tensor(-276.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7249e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12963345646858215
dqn reward tensor(-313.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5785e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14240944385528564
dqn reward tensor(-374.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7221e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18434478342533112
dqn reward tensor(-389., device='cuda:0') e 0.05 loss_dqn tensor(1.4892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16182538866996765
dqn reward tensor(-298.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6164e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2380518913269043
dqn reward tensor(-414.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15124918520450592
dqn reward tensor(-320.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5636e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12889660894870758
dqn reward tensor(-351.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14214147627353668
dqn reward tensor(-301.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051866330206394196
dqn reward tensor(-368.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.154791459441185
dqn reward tensor(-439.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1389613151550293
dqn reward tensor(-335.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20126953721046448
dqn reward tensor(-446.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11560562252998352
dqn reward tensor(-317.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10291703045368195
dqn reward tensor(-423., device='cuda:0') e 0.05 loss_dqn tensor(1.9341e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1683512032032013
dqn reward tensor(-313.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4927e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08182668685913086
dqn reward tensor(-524.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036048538982868195
dqn reward tensor(-319.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6309e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11907356977462769
dqn reward tensor(-316.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5010e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1428855061531067
dqn reward tensor(-363.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034243471920490265
dqn reward tensor(-339.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5539e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07508182525634766
dqn reward tensor(-294.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7801e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06489194184541702
dqn reward tensor(-305.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4397e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18592385947704315
dqn reward tensor(-309.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4571e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08062595874071121
dqn reward tensor(-363.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02122950553894043
dqn reward tensor(-261.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4788e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015086567960679531
dqn reward tensor(-350.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2227e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01300753653049469
dqn reward tensor(-362.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.8089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11048789322376251
dqn reward tensor(-399.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4512e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3733936548233032
dqn reward tensor(-464.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4128e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2851046025753021
dqn reward tensor(-293.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4929e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11464367806911469
dqn reward tensor(-258.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5372e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06535176187753677
dqn reward tensor(-324.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.284616082906723
dqn reward tensor(-348.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0802316665649414
dqn reward tensor(-369.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09050028026103973
dqn reward tensor(-308.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.6718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0482611358165741
dqn reward tensor(-335., device='cuda:0') e 0.05 loss_dqn tensor(2.0086e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07911483943462372
dqn reward tensor(-333.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07502268254756927
dqn reward tensor(-234.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17110072076320648
dqn reward tensor(-326.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4983e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18071870505809784
dqn reward tensor(-322.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6545e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08763917535543442
dqn reward tensor(-194.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07144438475370407
dqn reward tensor(-374.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13896749913692474
dqn reward tensor(-201.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1723e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15726259350776672
dqn reward tensor(-312.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4707e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20372557640075684
dqn reward tensor(-285.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03969822824001312
dqn reward tensor(-311.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20393279194831848
dqn reward tensor(-430.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3446e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12202946841716766
dqn reward tensor(-318.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0484e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09963002055883408
dqn reward tensor(-362.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3942e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1274021863937378
dqn reward tensor(-341.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5165e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09403089433908463
dqn reward tensor(-462.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07547261565923691
dqn reward tensor(-286.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4705e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09580501914024353
dqn reward tensor(-261., device='cuda:0') e 0.05 loss_dqn tensor(2.1029e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20395922660827637
dqn reward tensor(-382., device='cuda:0') e 0.05 loss_dqn tensor(1.7293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18428394198417664
dqn reward tensor(-416.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4440e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12259763479232788
dqn reward tensor(-351.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18127591907978058
dqn reward tensor(-352.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9142e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0297524593770504
dqn reward tensor(-391.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09151209145784378
dqn reward tensor(-278.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18189239501953125
dqn reward tensor(-302.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4249e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10442261397838593
dqn reward tensor(-353.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9169e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055524349212646484
dqn reward tensor(-302., device='cuda:0') e 0.05 loss_dqn tensor(1.5097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10426995158195496
dqn reward tensor(-376., device='cuda:0') e 0.05 loss_dqn tensor(1.5190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11808153241872787
dqn reward tensor(-252.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5183e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10074546933174133
dqn reward tensor(-251.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2285815179347992
dqn reward tensor(-347.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2427e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08214865624904633
dqn reward tensor(-362.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22258774936199188
dqn reward tensor(-371.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4665e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2376335710287094
dqn reward tensor(-382.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1174e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07311387360095978
dqn reward tensor(-395.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09949863702058792
dqn reward tensor(-270.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4218e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12993045151233673
dqn reward tensor(-360.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03810952976346016
dqn reward tensor(-297.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254687905311584
dqn reward tensor(-411.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4846e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0447501577436924
dqn reward tensor(-292.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3998e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10903418064117432
dqn reward tensor(-241.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4585e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09410111606121063
dqn reward tensor(-376.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.3940e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017768505960702896
dqn reward tensor(-349.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1158e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13799253106117249
dqn reward tensor(-203.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0688e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15057748556137085
dqn reward tensor(-182.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10815639793872833
dqn reward tensor(-312.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14758475124835968
dqn reward tensor(-299.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.6613e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08013457804918289
dqn reward tensor(-395.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4649e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17775630950927734
dqn reward tensor(-324.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0901651680469513
dqn reward tensor(-317.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01723756641149521
dqn reward tensor(-156.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08679533004760742
dqn reward tensor(-367., device='cuda:0') e 0.05 loss_dqn tensor(1.5887e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16968032717704773
dqn reward tensor(-447.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3928e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012033733539283276
dqn reward tensor(-231.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4873e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11655578762292862
dqn reward tensor(-401.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4100e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1503182351589203
dqn reward tensor(-166.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09296086430549622
dqn reward tensor(-376.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2163e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3132982850074768
dqn reward tensor(-200.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6807e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027440175414085388
dqn reward tensor(-421.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3966e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32618820667266846
dqn reward tensor(-304.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050838448107242584
dqn reward tensor(-297.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7909e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38101282715797424
dqn reward tensor(-230.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5012e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1210760623216629
dqn reward tensor(-437.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3512e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14777398109436035
dqn reward tensor(-309.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3864e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1920122653245926
dqn reward tensor(-351.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3599e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11730998754501343
dqn reward tensor(-267.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17275165021419525
dqn reward tensor(-414.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18148501217365265
dqn reward tensor(-342.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4411e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07562392950057983
dqn reward tensor(-312.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3978e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035380348563194275
dqn reward tensor(-388.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09018568694591522
dqn reward tensor(-259.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09685499221086502
dqn reward tensor(-161.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9533e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1789969801902771
dqn reward tensor(-342.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5313e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12249268591403961
dqn reward tensor(-331.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2006e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08731888979673386
dqn reward tensor(-359., device='cuda:0') e 0.05 loss_dqn tensor(1.3989e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2949836850166321
dqn reward tensor(-284.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012859190814197063
dqn reward tensor(-237.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8150e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.011832503601908684
dqn reward tensor(-336.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4214e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07492086291313171
dqn reward tensor(-390., device='cuda:0') e 0.05 loss_dqn tensor(1.6642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21781229972839355
dqn reward tensor(-335.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5815e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05234807729721069
dqn reward tensor(-301.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8820e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16659905016422272
dqn reward tensor(-298.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37509849667549133
dqn reward tensor(-218., device='cuda:0') e 0.05 loss_dqn tensor(1.3464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2084931880235672
dqn reward tensor(-417., device='cuda:0') e 0.05 loss_dqn tensor(1.3463e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1460125893354416
dqn reward tensor(-420.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5349e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19835369288921356
dqn reward tensor(-239., device='cuda:0') e 0.05 loss_dqn tensor(1.3383e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18533644080162048
dqn reward tensor(-285.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3615e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10830320417881012
dqn reward tensor(-278.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5740e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10382736474275589
dqn reward tensor(-359., device='cuda:0') e 0.05 loss_dqn tensor(1.6857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21737074851989746
dqn reward tensor(-284.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3276e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0830029845237732
dqn reward tensor(-53.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2616e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09508365392684937
dqn reward tensor(-160., device='cuda:0') e 0.05 loss_dqn tensor(1.2850e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1212998628616333
dqn reward tensor(-339.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.3094e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1627904772758484
dqn reward tensor(-214.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2401e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.089657723903656
dqn reward tensor(-511.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2856e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33239609003067017
dqn reward tensor(-289.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7660e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22171831130981445
dqn reward tensor(-323.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18476401269435883
dqn reward tensor(-201.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040570080280303955
dqn reward tensor(-112.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8329e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09760164469480515
dqn reward tensor(-369.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7559e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22438406944274902
dqn reward tensor(-279.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2895e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039639148861169815
dqn reward tensor(-137.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4273e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15682707726955414
dqn reward tensor(-364., device='cuda:0') e 0.05 loss_dqn tensor(2.2264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12852203845977783
dqn reward tensor(-425.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.126783087849617
dqn reward tensor(-197., device='cuda:0') e 0.05 loss_dqn tensor(1.5901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25502318143844604
dqn reward tensor(-191.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8783e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17631369829177856
dqn reward tensor(-233.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08942309767007828
dqn reward tensor(-281., device='cuda:0') e 0.05 loss_dqn tensor(1.2752e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20054945349693298
dqn reward tensor(-371.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8666e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15355294942855835
dqn reward tensor(-270.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6934e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0377219095826149
dqn reward tensor(-266.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2521e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11146920919418335
dqn reward tensor(-399.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17245015501976013
dqn reward tensor(-162.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19520092010498047
dqn reward tensor(-311., device='cuda:0') e 0.05 loss_dqn tensor(1.7203e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11954586207866669
dqn reward tensor(-177.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08915887773036957
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3056e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0335722491145134
dqn reward tensor(-262., device='cuda:0') e 0.05 loss_dqn tensor(1.3131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10101889073848724
dqn reward tensor(-272.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5514e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022440044209361076
dqn reward tensor(-213.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11379360407590866
dqn reward tensor(-304.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9555e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07548043131828308
dqn reward tensor(-256.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2306e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08116724342107773
dqn reward tensor(-349.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04773243889212608
dqn reward tensor(-356.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05595434457063675
dqn reward tensor(-236., device='cuda:0') e 0.05 loss_dqn tensor(1.2346e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0936405137181282
dqn reward tensor(-266., device='cuda:0') e 0.05 loss_dqn tensor(1.2648e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33316275477409363
dqn reward tensor(-37.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.00921940989792347
Evaluating...
Train: {'rocauc': 0.7584265108490296} -4.2126688957214355
=====Epoch 11=====
Training...
dqn reward tensor(-263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2248e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0790041908621788
dqn reward tensor(-375.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13857504725456238
dqn reward tensor(-331.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5813e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08179250359535217
dqn reward tensor(-327.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6347e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0834466814994812
dqn reward tensor(-270.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643744707107544
dqn reward tensor(-228.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10393728315830231
dqn reward tensor(-247.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2114e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3408036231994629
dqn reward tensor(-251.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14926186203956604
dqn reward tensor(-199.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050505511462688446
dqn reward tensor(-323.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10271577537059784
dqn reward tensor(-184.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2074e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10112270712852478
dqn reward tensor(-338.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1194240152835846
dqn reward tensor(-124.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2147e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10866568982601166
dqn reward tensor(-258.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10412812232971191
dqn reward tensor(-262.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1761e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0665147602558136
dqn reward tensor(-262.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.178323894739151
dqn reward tensor(-301., device='cuda:0') e 0.05 loss_dqn tensor(1.5754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12438411265611649
dqn reward tensor(-304.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11159010976552963
dqn reward tensor(-224.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3687e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09920148551464081
dqn reward tensor(-332.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1837e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15370218455791473
dqn reward tensor(-214.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08165626227855682
dqn reward tensor(-140.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17677107453346252
dqn reward tensor(-246.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5988e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15294526517391205
dqn reward tensor(-97.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2008e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22100380063056946
dqn reward tensor(-271.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2006e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13930892944335938
dqn reward tensor(-350., device='cuda:0') e 0.05 loss_dqn tensor(1.1834e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11109421402215958
dqn reward tensor(-341.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2972e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18716512620449066
dqn reward tensor(-203., device='cuda:0') e 0.05 loss_dqn tensor(1.1322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17370212078094482
dqn reward tensor(-303., device='cuda:0') e 0.05 loss_dqn tensor(1.3450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10909250378608704
dqn reward tensor(-374.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13561901450157166
dqn reward tensor(-223.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26102307438850403
dqn reward tensor(-269.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1663e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0875914990901947
dqn reward tensor(-186.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056931689381599426
dqn reward tensor(-314.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5929e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11172039806842804
dqn reward tensor(-165.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08979012817144394
dqn reward tensor(-139.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21248087286949158
dqn reward tensor(-216.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7470e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13276509940624237
dqn reward tensor(-205.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6070e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07915286719799042
dqn reward tensor(-224.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15227091312408447
dqn reward tensor(-393.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19069349765777588
dqn reward tensor(-254.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1940e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07910697162151337
dqn reward tensor(-282.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23346111178398132
dqn reward tensor(-263.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2101e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10151039063930511
dqn reward tensor(-164.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15498261153697968
dqn reward tensor(-154.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040242500603199005
dqn reward tensor(-245.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17894917726516724
dqn reward tensor(-131., device='cuda:0') e 0.05 loss_dqn tensor(1.5466e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.117875836789608
dqn reward tensor(-171.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1674e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030822457745671272
dqn reward tensor(-275.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5646e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03012276068329811
dqn reward tensor(-288.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2108074128627777
dqn reward tensor(-223., device='cuda:0') e 0.05 loss_dqn tensor(1.3472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045473188161849976
dqn reward tensor(-166.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2911e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026158906519412994
dqn reward tensor(-244.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07302290201187134
dqn reward tensor(-256.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3262e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0331154465675354
dqn reward tensor(-236.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6181e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2122618854045868
dqn reward tensor(-222., device='cuda:0') e 0.05 loss_dqn tensor(1.2293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.139756441116333
dqn reward tensor(-254.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14006082713603973
dqn reward tensor(-317.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04513530060648918
dqn reward tensor(-236.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0878e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10891624540090561
dqn reward tensor(-394.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08141480386257172
dqn reward tensor(-267.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1955e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33551889657974243
dqn reward tensor(-303.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2049e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04351689666509628
dqn reward tensor(-270.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17832738161087036
dqn reward tensor(-287.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14527806639671326
dqn reward tensor(-259.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06890204548835754
dqn reward tensor(-286.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4443e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2013019621372223
dqn reward tensor(-174.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4419e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32866308093070984
dqn reward tensor(-362., device='cuda:0') e 0.05 loss_dqn tensor(1.5368e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10019708424806595
dqn reward tensor(-211.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15717579424381256
dqn reward tensor(-146., device='cuda:0') e 0.05 loss_dqn tensor(1.6156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07636946439743042
dqn reward tensor(-248., device='cuda:0') e 0.05 loss_dqn tensor(1.2604e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11519331485033035
dqn reward tensor(-279.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4650e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12062472850084305
dqn reward tensor(-304.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05321617051959038
dqn reward tensor(-246.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34139519929885864
dqn reward tensor(-334.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.2050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06587636470794678
dqn reward tensor(-238.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8411e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14262434840202332
dqn reward tensor(-298.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20861011743545532
dqn reward tensor(-224.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1978e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02442547120153904
dqn reward tensor(-165.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2392e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32899296283721924
dqn reward tensor(-236.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2577e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14057011902332306
dqn reward tensor(-299.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2291196584701538
dqn reward tensor(-346.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5689e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21377015113830566
dqn reward tensor(-328.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4033e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09116751700639725
dqn reward tensor(-146.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1903e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26928913593292236
dqn reward tensor(-102.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1766899824142456
dqn reward tensor(-168.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2567e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17413455247879028
dqn reward tensor(-380., device='cuda:0') e 0.05 loss_dqn tensor(1.3980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08474519103765488
dqn reward tensor(-315.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1487959921360016
dqn reward tensor(-259.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1997e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09101945161819458
dqn reward tensor(-330.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18421167135238647
dqn reward tensor(-257.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2443585991859436
dqn reward tensor(-270.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1038e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18348810076713562
dqn reward tensor(-116.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1145e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10452836751937866
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058446191251277924
dqn reward tensor(-125.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1069e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26056554913520813
dqn reward tensor(-264.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.6137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03902082517743111
dqn reward tensor(-224.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14623430371284485
dqn reward tensor(-267.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20555442571640015
dqn reward tensor(-271.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11297933757305145
dqn reward tensor(-351.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0448e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14843153953552246
dqn reward tensor(-169.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1850e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07196767628192902
dqn reward tensor(-280.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7614e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08564281463623047
dqn reward tensor(-252.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9733e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10536283254623413
dqn reward tensor(-203.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0857943594455719
dqn reward tensor(-279.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1834542155265808
dqn reward tensor(-166.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13549187779426575
dqn reward tensor(-181.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6148e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21919701993465424
dqn reward tensor(-306.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0976e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14669570326805115
dqn reward tensor(-175., device='cuda:0') e 0.05 loss_dqn tensor(1.1393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05768432468175888
dqn reward tensor(-208.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0854e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07596845924854279
dqn reward tensor(-304.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0038e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791604295372963
dqn reward tensor(-169.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9517e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15077950060367584
dqn reward tensor(-164.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0882e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11852017045021057
dqn reward tensor(-235.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.1075e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0543551929295063
dqn reward tensor(-247.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1173008382320404
dqn reward tensor(-237.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.6661e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0843493714928627
dqn reward tensor(-214.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8539e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07447236776351929
dqn reward tensor(-76.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2795e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19165204465389252
dqn reward tensor(-248.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14345760643482208
dqn reward tensor(-231.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6309e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0958118885755539
dqn reward tensor(-187.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0233e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.227241650223732
dqn reward tensor(-200.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6282e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17994335293769836
dqn reward tensor(-272.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23946455121040344
dqn reward tensor(-145.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8092e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08679820597171783
dqn reward tensor(-92.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1453e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25398123264312744
dqn reward tensor(-85.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10749174654483795
dqn reward tensor(-266.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1729832887649536
dqn reward tensor(-290.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8816e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14766646921634674
dqn reward tensor(-144.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8922e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13158899545669556
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9282e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10887505114078522
dqn reward tensor(-272.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.5004e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06621260195970535
dqn reward tensor(-337.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08153466135263443
dqn reward tensor(-255.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3868e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09127791225910187
dqn reward tensor(-154.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0162e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17794710397720337
dqn reward tensor(-102.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.2497e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19543512165546417
dqn reward tensor(-151.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.8941e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0688827708363533
dqn reward tensor(-265.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.2631e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2043352723121643
dqn reward tensor(-151.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0835e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08812979608774185
dqn reward tensor(-294.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8985e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20939470827579498
dqn reward tensor(-214.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4255e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1788417398929596
dqn reward tensor(-67.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5056e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1976829171180725
dqn reward tensor(198.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.5021e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434551328420639
dqn reward tensor(85.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1311e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0625583603978157
dqn reward tensor(34.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6962e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08391636610031128
dqn reward tensor(341., device='cuda:0') e 0.05 loss_dqn tensor(4.8384e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11667602509260178
dqn reward tensor(312., device='cuda:0') e 0.05 loss_dqn tensor(4.5376e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1169285699725151
dqn reward tensor(484., device='cuda:0') e 0.05 loss_dqn tensor(3.5051e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3142715096473694
dqn reward tensor(405.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3806e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08147913962602615
dqn reward tensor(500.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5623e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21127912402153015
dqn reward tensor(358.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1750803142786026
dqn reward tensor(490.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0469e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16131728887557983
dqn reward tensor(332.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7852e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15499067306518555
dqn reward tensor(402.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2155e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1989079713821411
dqn reward tensor(593.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1816e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.410170316696167
dqn reward tensor(278., device='cuda:0') e 0.05 loss_dqn tensor(8.1921e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09289494156837463
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18073534965515137
dqn reward tensor(380.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7810e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03894231095910072
dqn reward tensor(379.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09674201160669327
dqn reward tensor(130., device='cuda:0') e 0.05 loss_dqn tensor(1.7523e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06953726708889008
dqn reward tensor(101.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1403e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021239349618554115
dqn reward tensor(175.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3802e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25730934739112854
dqn reward tensor(-26.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3967345356941223
dqn reward tensor(111.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7186e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24331533908843994
dqn reward tensor(170.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1864e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2452738732099533
dqn reward tensor(134.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1177e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04510020464658737
dqn reward tensor(84.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9139e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12909990549087524
dqn reward tensor(-5.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8010e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1218162328004837
dqn reward tensor(21.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0215e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1753568947315216
dqn reward tensor(58.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5717e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22813531756401062
dqn reward tensor(-177.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7253e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09316800534725189
dqn reward tensor(51.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6628e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09252048283815384
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1372e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2529415488243103
dqn reward tensor(-0.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3679e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19249075651168823
dqn reward tensor(32.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13493230938911438
dqn reward tensor(45.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8324e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603846549987793
dqn reward tensor(-161.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1347e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13196229934692383
dqn reward tensor(174.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2632e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15825489163398743
dqn reward tensor(147.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4589e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19602832198143005
dqn reward tensor(63.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5829e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09217511117458344
dqn reward tensor(-36.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.8231e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19561202824115753
dqn reward tensor(-240., device='cuda:0') e 0.05 loss_dqn tensor(9.8605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23731935024261475
dqn reward tensor(-148.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0249e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09483426809310913
dqn reward tensor(-97.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.0034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1375209242105484
dqn reward tensor(34.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0403e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13831964135169983
dqn reward tensor(223.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0536e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13563016057014465
dqn reward tensor(3.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2366e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12229762226343155
dqn reward tensor(58.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0603e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14224505424499512
dqn reward tensor(-46.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2776973247528076
dqn reward tensor(-84.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1062e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08439290523529053
dqn reward tensor(136.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1188e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07757796347141266
dqn reward tensor(155., device='cuda:0') e 0.05 loss_dqn tensor(1.1064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06594766676425934
dqn reward tensor(31.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2669e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13979998230934143
dqn reward tensor(-30.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13273410499095917
dqn reward tensor(-27.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08342377096414566
dqn reward tensor(12.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1426548808813095
dqn reward tensor(163.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1487e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2296997308731079
dqn reward tensor(38.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.6035e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1365305781364441
dqn reward tensor(-103.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25245359539985657
dqn reward tensor(-136.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12385931611061096
dqn reward tensor(-48.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1806e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1325453370809555
dqn reward tensor(-116.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2086e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19641652703285217
dqn reward tensor(9., device='cuda:0') e 0.05 loss_dqn tensor(1.2462e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19419077038764954
dqn reward tensor(119.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4158e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3051284849643707
dqn reward tensor(78.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2727e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09968593716621399
dqn reward tensor(-20., device='cuda:0') e 0.05 loss_dqn tensor(1.3017e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08583283424377441
dqn reward tensor(-294.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18158353865146637
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0902e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19373100996017456
dqn reward tensor(-62., device='cuda:0') e 0.05 loss_dqn tensor(2.7366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24428802728652954
dqn reward tensor(20.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3572e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07579246163368225
dqn reward tensor(-119.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4705e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08102523535490036
dqn reward tensor(-85.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1387184113264084
dqn reward tensor(23.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4058e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11807617545127869
dqn reward tensor(83.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5263e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13187532126903534
dqn reward tensor(141.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08127748966217041
dqn reward tensor(3.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31102609634399414
dqn reward tensor(-9.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13816571235656738
dqn reward tensor(-72.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3900e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1803145706653595
dqn reward tensor(10.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1661e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08619998395442963
dqn reward tensor(-55.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5125e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1784047782421112
dqn reward tensor(-154., device='cuda:0') e 0.05 loss_dqn tensor(7.1064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18147605657577515
dqn reward tensor(-99.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5309e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19544151425361633
dqn reward tensor(-30.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2509840130805969
dqn reward tensor(-187.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6862e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17862367630004883
dqn reward tensor(30.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18864494562149048
dqn reward tensor(-148.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3387e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09785783290863037
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1841e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09586834162473679
dqn reward tensor(48.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1245289295911789
dqn reward tensor(-54.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0124e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24641215801239014
dqn reward tensor(-80.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3573e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08275657147169113
dqn reward tensor(-139.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3663e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.083509661257267
dqn reward tensor(-26.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0151e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13231167197227478
dqn reward tensor(-23.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3691e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11860187351703644
dqn reward tensor(119., device='cuda:0') e 0.05 loss_dqn tensor(2.3702e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14400237798690796
dqn reward tensor(-115.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1533404290676117
dqn reward tensor(-195.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4423e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21539518237113953
dqn reward tensor(59., device='cuda:0') e 0.05 loss_dqn tensor(1.2156e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18929272890090942
dqn reward tensor(-22.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5860e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023206455633044243
dqn reward tensor(-114.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5768e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07814225554466248
dqn reward tensor(-58., device='cuda:0') e 0.05 loss_dqn tensor(8.3189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1323671042919159
dqn reward tensor(-254.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0779e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21231654286384583
dqn reward tensor(-217.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3519e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29996323585510254
dqn reward tensor(-30.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6278e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14290380477905273
dqn reward tensor(-179.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6438e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21575719118118286
dqn reward tensor(-215.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497950702905655
dqn reward tensor(139., device='cuda:0') e 0.05 loss_dqn tensor(3.1964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10210657119750977
dqn reward tensor(-214., device='cuda:0') e 0.05 loss_dqn tensor(1.8422e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18551753461360931
dqn reward tensor(-43.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22247597575187683
dqn reward tensor(-42., device='cuda:0') e 0.05 loss_dqn tensor(1.6625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14591532945632935
dqn reward tensor(-90.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6561e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0449678972363472
dqn reward tensor(-1.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03707749396562576
dqn reward tensor(-48.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3300e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07334335148334503
dqn reward tensor(129.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07964831590652466
dqn reward tensor(-175.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9067e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2494157999753952
dqn reward tensor(37., device='cuda:0') e 0.05 loss_dqn tensor(1.6104e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22821417450904846
dqn reward tensor(-56.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07757756114006042
dqn reward tensor(41., device='cuda:0') e 0.05 loss_dqn tensor(2.4622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26082658767700195
dqn reward tensor(-69.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08094381541013718
dqn reward tensor(-54.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8640e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2631976306438446
dqn reward tensor(-42.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6345e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31229493021965027
dqn reward tensor(9.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6764e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.218035027384758
dqn reward tensor(-105.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24563001096248627
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17186711728572845
dqn reward tensor(-251., device='cuda:0') e 0.05 loss_dqn tensor(1.4481e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17193296551704407
dqn reward tensor(-20.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6031e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24070538580417633
dqn reward tensor(-34.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12531350553035736
dqn reward tensor(-46.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5714e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18408317863941193
dqn reward tensor(-20.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7532e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14194971323013306
dqn reward tensor(95.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1338346302509308
dqn reward tensor(44.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7022e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25758808851242065
dqn reward tensor(-69., device='cuda:0') e 0.05 loss_dqn tensor(1.6884e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06039835140109062
dqn reward tensor(-7.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1498e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4672718942165375
dqn reward tensor(101.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13547822833061218
dqn reward tensor(38.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7039e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14336568117141724
dqn reward tensor(-390.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1104e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02611500769853592
dqn reward tensor(39.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21441905200481415
dqn reward tensor(-34.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2233e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24844792485237122
dqn reward tensor(139.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12751168012619019
dqn reward tensor(125., device='cuda:0') e 0.05 loss_dqn tensor(1.7490e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18473577499389648
dqn reward tensor(-59.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17364592850208282
dqn reward tensor(20.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7666e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05749879032373428
dqn reward tensor(-118.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7540e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08470726013183594
dqn reward tensor(43., device='cuda:0') e 0.05 loss_dqn tensor(1.7642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1369335651397705
dqn reward tensor(73.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7376e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12219725549221039
dqn reward tensor(37.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7545e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08331035077571869
dqn reward tensor(71.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2976e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24403145909309387
dqn reward tensor(-165.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3645e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21179386973381042
dqn reward tensor(-230.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7184e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19625011086463928
dqn reward tensor(-6.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0124e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03669114410877228
dqn reward tensor(-56.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0685e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13820038735866547
dqn reward tensor(-9.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07375568151473999
dqn reward tensor(-159.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08605503290891647
dqn reward tensor(-159.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08184237033128738
dqn reward tensor(-27.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22282245755195618
dqn reward tensor(-113.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0921e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14673586189746857
dqn reward tensor(-87.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2279e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26347100734710693
dqn reward tensor(-35.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2306e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1832035630941391
dqn reward tensor(-87.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8325e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036797069013118744
dqn reward tensor(-6.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0221e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14612556993961334
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08295053243637085
dqn reward tensor(-109.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.5719e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2805039882659912
dqn reward tensor(19.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8489e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1935167759656906
dqn reward tensor(7., device='cuda:0') e 0.05 loss_dqn tensor(1.0415e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1927177608013153
dqn reward tensor(-49.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3935e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11112327128648758
dqn reward tensor(-54.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2106e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25591593980789185
dqn reward tensor(-76.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9504e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11280272901058197
dqn reward tensor(30.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9685e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2736663520336151
dqn reward tensor(1.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4804e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13171114027500153
dqn reward tensor(-182., device='cuda:0') e 0.05 loss_dqn tensor(2.0164e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11169425398111343
dqn reward tensor(-56.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08165495842695236
dqn reward tensor(15.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0510e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15454456210136414
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0274e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15508484840393066
dqn reward tensor(-70.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5592e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31534454226493835
dqn reward tensor(-122.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0611e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06820350140333176
dqn reward tensor(-220.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3406e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2657143771648407
dqn reward tensor(-153.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2949e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15504056215286255
dqn reward tensor(6.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.3561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07385249435901642
dqn reward tensor(-229.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3919e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11274674534797668
dqn reward tensor(-80., device='cuda:0') e 0.05 loss_dqn tensor(2.4604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18881423771381378
dqn reward tensor(-71.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12164276093244553
dqn reward tensor(-18.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2927e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3194165527820587
dqn reward tensor(-129.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3134e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1579749882221222
dqn reward tensor(31.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14504575729370117
dqn reward tensor(33.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1075887531042099
dqn reward tensor(-126.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1050504744052887
dqn reward tensor(-218.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11639565229415894
dqn reward tensor(-76.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0341e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25849127769470215
dqn reward tensor(-165.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8880e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20834602415561676
dqn reward tensor(-96.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2851e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2361414134502411
dqn reward tensor(55.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14843621850013733
dqn reward tensor(-28.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15075558423995972
dqn reward tensor(-155.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5397e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19836142659187317
dqn reward tensor(-25., device='cuda:0') e 0.05 loss_dqn tensor(2.6212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09219047427177429
dqn reward tensor(-99.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17485201358795166
dqn reward tensor(20.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6800e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07469867169857025
dqn reward tensor(-137.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04335490241646767
dqn reward tensor(-152.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7218e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06968432664871216
dqn reward tensor(33.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09004656225442886
dqn reward tensor(-101., device='cuda:0') e 0.05 loss_dqn tensor(3.3078e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3415377140045166
dqn reward tensor(-108.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5308e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07821303606033325
dqn reward tensor(56.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3173392415046692
dqn reward tensor(-55.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8661e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1850639283657074
dqn reward tensor(-102.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.2316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08663485199213028
dqn reward tensor(24.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0683e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10139462351799011
dqn reward tensor(-62.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.8710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15536893904209137
dqn reward tensor(126.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8861e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435718834400177
dqn reward tensor(75.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22201450169086456
dqn reward tensor(-92.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0706e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16148197650909424
dqn reward tensor(24., device='cuda:0') e 0.05 loss_dqn tensor(2.9995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10119998455047607
dqn reward tensor(-62.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1561e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20248298346996307
dqn reward tensor(-99.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054142627865076065
dqn reward tensor(-80.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3599e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13764739036560059
dqn reward tensor(-23.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.5768e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18844200670719147
dqn reward tensor(-16.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1961e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16135123372077942
dqn reward tensor(54.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25132519006729126
dqn reward tensor(-223.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14070573449134827
dqn reward tensor(182.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1405e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11640056222677231
dqn reward tensor(1.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8854e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2509888708591461
dqn reward tensor(0.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9193e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23482799530029297
dqn reward tensor(-81.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4063e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2524145245552063
dqn reward tensor(161.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7513e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10881742835044861
dqn reward tensor(-161.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5003e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21597173810005188
dqn reward tensor(-159.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3025e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20843079686164856
dqn reward tensor(-244.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5723e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22854949533939362
dqn reward tensor(-29.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.5469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20235858857631683
dqn reward tensor(-259.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.8302e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2217320203781128
dqn reward tensor(-61.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9082e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2364042103290558
dqn reward tensor(-18., device='cuda:0') e 0.05 loss_dqn tensor(3.7228e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1889353096485138
dqn reward tensor(-21.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18988293409347534
dqn reward tensor(18.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7567e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11377329379320145
dqn reward tensor(-110.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6842e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2567129135131836
dqn reward tensor(54.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9280e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09229183197021484
dqn reward tensor(-154.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6290e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09396115690469742
dqn reward tensor(39., device='cuda:0') e 0.05 loss_dqn tensor(3.9570e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0891876369714737
dqn reward tensor(-141.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5922e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17370611429214478
dqn reward tensor(26.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18899431824684143
dqn reward tensor(26.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9111e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434757113456726
dqn reward tensor(-65.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9744e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1172264814376831
dqn reward tensor(-100.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3412e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09197255969047546
dqn reward tensor(22.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09162875264883041
dqn reward tensor(166.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1225254163146019
dqn reward tensor(-130.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1890e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18740074336528778
dqn reward tensor(-54.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1828e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07799617946147919
dqn reward tensor(-68.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1241e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07032172381877899
dqn reward tensor(24.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1098e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051345571875572205
dqn reward tensor(-100.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7020e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.070895716547966
dqn reward tensor(39.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.0443e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20158171653747559
dqn reward tensor(-66.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1158e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22689634561538696
dqn reward tensor(-177.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0838245302438736
dqn reward tensor(-47.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09097523987293243
dqn reward tensor(-84.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0311e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19833579659461975
dqn reward tensor(-95.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4289e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27577805519104004
dqn reward tensor(-55.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3484e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14191552996635437
dqn reward tensor(-82.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8565e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33070266246795654
dqn reward tensor(-142., device='cuda:0') e 0.05 loss_dqn tensor(5.4535e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18760672211647034
dqn reward tensor(-34.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19301114976406097
dqn reward tensor(-201.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.8158e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24022988975048065
dqn reward tensor(60.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2774e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23903214931488037
dqn reward tensor(9.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.1580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06419475376605988
dqn reward tensor(4.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0591e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04925237223505974
dqn reward tensor(-20.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9087e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2332906723022461
dqn reward tensor(250., device='cuda:0') e 0.05 loss_dqn tensor(4.9279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16974256932735443
dqn reward tensor(231.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08911678940057755
dqn reward tensor(26.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5474e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02522834949195385
dqn reward tensor(-133.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9142e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34429723024368286
dqn reward tensor(183., device='cuda:0') e 0.05 loss_dqn tensor(5.6968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13692614436149597
dqn reward tensor(302.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1836e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07729347795248032
dqn reward tensor(127.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.2049e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941528469324112
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(3.0449e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20574039220809937
dqn reward tensor(311., device='cuda:0') e 0.05 loss_dqn tensor(2.9731e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24423548579216003
dqn reward tensor(272.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9181e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07909459620714188
dqn reward tensor(177.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9502e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07599212229251862
dqn reward tensor(318.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7798e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05040676146745682
dqn reward tensor(350.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1424e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08281176537275314
dqn reward tensor(214., device='cuda:0') e 0.05 loss_dqn tensor(4.3215e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1765841394662857
dqn reward tensor(392.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5742e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08700589090585709
dqn reward tensor(568.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5698e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1339736431837082
dqn reward tensor(447.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08684983104467392
dqn reward tensor(233.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03389827534556389
dqn reward tensor(366., device='cuda:0') e 0.05 loss_dqn tensor(2.4215e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19862034916877747
dqn reward tensor(342.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3910e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09645077586174011
dqn reward tensor(117.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9592e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14354005455970764
dqn reward tensor(433., device='cuda:0') e 0.05 loss_dqn tensor(2.3241e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027237262576818466
dqn reward tensor(276.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2753e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0247371606528759
dqn reward tensor(441.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2335e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19596824049949646
dqn reward tensor(270.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020890265703201294
dqn reward tensor(235.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6234e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12639747560024261
dqn reward tensor(238.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1037e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35824233293533325
dqn reward tensor(380.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1476e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13321104645729065
dqn reward tensor(368.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5109e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20096322894096375
dqn reward tensor(202.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0518e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24361321330070496
dqn reward tensor(50.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1478036791086197
dqn reward tensor(434.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0174e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1688399612903595
dqn reward tensor(319.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4351e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09298388659954071
dqn reward tensor(322., device='cuda:0') e 0.05 loss_dqn tensor(3.9562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14695844054222107
dqn reward tensor(475.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.4396e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20442549884319305
dqn reward tensor(497.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4358e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17639954388141632
dqn reward tensor(362.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0060e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16083353757858276
dqn reward tensor(245.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.9916e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23081418871879578
dqn reward tensor(394.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9767e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20646867156028748
dqn reward tensor(16., device='cuda:0') e 0.05 loss_dqn tensor(1.9358e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06528756022453308
dqn reward tensor(243.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1412794291973114
dqn reward tensor(248.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1511957049369812
dqn reward tensor(340.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8762e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1462230384349823
dqn reward tensor(288.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8572e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08997869491577148
dqn reward tensor(299.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7717e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22230017185211182
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08143539726734161
dqn reward tensor(229.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4251e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16931942105293274
dqn reward tensor(133.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5930e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18301990628242493
dqn reward tensor(273., device='cuda:0') e 0.05 loss_dqn tensor(1.6849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1565709263086319
dqn reward tensor(360.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09196388721466064
dqn reward tensor(214.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.9714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25966423749923706
dqn reward tensor(240.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5919e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025478743016719818
dqn reward tensor(-10.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2522e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08175643533468246
dqn reward tensor(388.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5599e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08572575449943542
dqn reward tensor(440.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4822e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08005736768245697
dqn reward tensor(-45.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08343339711427689
dqn reward tensor(149.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12932202219963074
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5937e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1783917248249054
dqn reward tensor(431.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3788e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07880501449108124
dqn reward tensor(383.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3374e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07804373651742935
dqn reward tensor(340.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1510680764913559
dqn reward tensor(211.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20146214962005615
dqn reward tensor(473.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12414136528968811
dqn reward tensor(35.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2489e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22276002168655396
dqn reward tensor(26.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2145e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12739722430706024
dqn reward tensor(259., device='cuda:0') e 0.05 loss_dqn tensor(1.2268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13555552065372467
dqn reward tensor(276.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2533106207847595
dqn reward tensor(181.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8270e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22725117206573486
dqn reward tensor(183.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1453e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20699557662010193
dqn reward tensor(231., device='cuda:0') e 0.05 loss_dqn tensor(5.3031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06585237383842468
dqn reward tensor(350.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1008e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18596690893173218
dqn reward tensor(102.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0887e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15140676498413086
dqn reward tensor(243., device='cuda:0') e 0.05 loss_dqn tensor(1.2992e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056913405656814575
dqn reward tensor(252.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0130e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329115927219391
dqn reward tensor(235.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.9312e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17508932948112488
dqn reward tensor(387.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5098e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12265991419553757
dqn reward tensor(-25.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2848e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03155149146914482
dqn reward tensor(227.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.3831e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18194395303726196
dqn reward tensor(240.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3778e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12527163326740265
dqn reward tensor(177., device='cuda:0') e 0.05 loss_dqn tensor(8.0382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2774597108364105
dqn reward tensor(327.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8297e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23817229270935059
dqn reward tensor(281.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6077e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14237810671329498
dqn reward tensor(125.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.6913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0803467184305191
dqn reward tensor(378.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18728218972682953
dqn reward tensor(23.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6038e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17352020740509033
dqn reward tensor(500.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7249e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22680729627609253
dqn reward tensor(384.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7220e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10973045229911804
dqn reward tensor(111.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.7375e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03757298365235329
dqn reward tensor(132.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04050728678703308
dqn reward tensor(180.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09101380407810211
dqn reward tensor(243.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1688e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030775591731071472
dqn reward tensor(77.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9252e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1925210803747177
dqn reward tensor(126.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3565e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030764266848564148
dqn reward tensor(206.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10923261195421219
dqn reward tensor(261.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3379e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07447542250156403
dqn reward tensor(140.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021362468600273132
dqn reward tensor(200.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8788e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1911780834197998
dqn reward tensor(86.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19373494386672974
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5074e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31744396686553955
dqn reward tensor(256.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9907e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12357774376869202
dqn reward tensor(255.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.4234e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13345198333263397
dqn reward tensor(193.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5541e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34455928206443787
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1887e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026057478040456772
dqn reward tensor(300.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.1155e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12420698255300522
dqn reward tensor(101., device='cuda:0') e 0.05 loss_dqn tensor(4.1954e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13804858922958374
dqn reward tensor(154.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9723e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17034827172756195
dqn reward tensor(148.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13457712531089783
dqn reward tensor(311.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9776e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1777019202709198
dqn reward tensor(235.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6288e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11785769462585449
dqn reward tensor(338.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8220e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13695769011974335
dqn reward tensor(116.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6361e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22216951847076416
dqn reward tensor(242.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4087e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18983858823776245
dqn reward tensor(283.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2830e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1765996217727661
dqn reward tensor(373.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9223e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12949824333190918
dqn reward tensor(-19.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7788e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047562774270772934
Evaluating...
Train: {'rocauc': 0.706202381328293} 2.4206769466400146
=====Epoch 12=====
Training...
dqn reward tensor(256.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5431e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17292305827140808
dqn reward tensor(166., device='cuda:0') e 0.05 loss_dqn tensor(4.9254e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24047726392745972
dqn reward tensor(174.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3198040723800659
dqn reward tensor(192.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8570e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23438137769699097
dqn reward tensor(126.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4821e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1352698802947998
dqn reward tensor(28.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4445e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15250271558761597
dqn reward tensor(59.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4229e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09164945781230927
dqn reward tensor(19.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.9498e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.125706285238266
dqn reward tensor(-233., device='cuda:0') e 0.05 loss_dqn tensor(2.9798e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03821143880486488
dqn reward tensor(150., device='cuda:0') e 0.05 loss_dqn tensor(2.0874e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09453440457582474
dqn reward tensor(165.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3581e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16445964574813843
dqn reward tensor(196.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3649e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08359626680612564
dqn reward tensor(186.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.0643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29253700375556946
dqn reward tensor(126.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3833e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24597802758216858
dqn reward tensor(-9.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5283e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26312994956970215
dqn reward tensor(-52.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16026830673217773
dqn reward tensor(129.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5146e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1652454137802124
dqn reward tensor(253.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1319500356912613
dqn reward tensor(-14., device='cuda:0') e 0.05 loss_dqn tensor(3.7814e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10225435346364975
dqn reward tensor(166.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6509e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08875888586044312
dqn reward tensor(328.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6306e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17739146947860718
dqn reward tensor(-5., device='cuda:0') e 0.05 loss_dqn tensor(1.2415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04878302663564682
dqn reward tensor(-398.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.5956e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053105540573596954
dqn reward tensor(-18.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1665e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1546521931886673
dqn reward tensor(98.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3944e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.246991366147995
dqn reward tensor(-18.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0800e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11916816234588623
dqn reward tensor(83.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2741e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12495578080415726
dqn reward tensor(25.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2307e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06595998257398605
dqn reward tensor(80.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023678451776504517
dqn reward tensor(-3.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2347e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08911433070898056
dqn reward tensor(162.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6229e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13697171211242676
dqn reward tensor(143.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.1736e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07979373633861542
dqn reward tensor(150.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13252130150794983
dqn reward tensor(-78.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5615e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08245866000652313
dqn reward tensor(85.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7775e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1714082807302475
dqn reward tensor(59.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6151e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18702419102191925
dqn reward tensor(9.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1371e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019649822264909744
dqn reward tensor(158.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1536e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25574639439582825
dqn reward tensor(-34.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.9446e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3069775104522705
dqn reward tensor(234.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4505e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24806170165538788
dqn reward tensor(167.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03746136277914047
dqn reward tensor(-316.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34098416566848755
dqn reward tensor(88.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2902e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16036953032016754
dqn reward tensor(121.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5144e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1602782905101776
dqn reward tensor(10.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4024e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15934275090694427
dqn reward tensor(-7.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3449e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06883484125137329
dqn reward tensor(-11.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3161e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15590223670005798
dqn reward tensor(79.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0264e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10678992420434952
dqn reward tensor(-136.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3323000371456146
dqn reward tensor(63.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4336e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1768614798784256
dqn reward tensor(68.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6673e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1545388549566269
dqn reward tensor(-145.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7765e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23521488904953003
dqn reward tensor(241.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0787e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07357030361890793
dqn reward tensor(88.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09485866129398346
dqn reward tensor(57.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9612e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2056780755519867
dqn reward tensor(250.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1446e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17220519483089447
dqn reward tensor(46.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1084e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0952858254313469
dqn reward tensor(3.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08968040347099304
dqn reward tensor(37.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9246e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07461421191692352
dqn reward tensor(-17.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2868e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0776786357164383
dqn reward tensor(188.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6938e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12781383097171783
dqn reward tensor(87.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9240e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08088921010494232
dqn reward tensor(2.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6676e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0697191059589386
dqn reward tensor(-109.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1291e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3236903250217438
dqn reward tensor(-49.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7430e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08084533363580704
dqn reward tensor(111.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1301e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13617971539497375
dqn reward tensor(205., device='cuda:0') e 0.05 loss_dqn tensor(5.9986e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02612229995429516
dqn reward tensor(113., device='cuda:0') e 0.05 loss_dqn tensor(4.6697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26600658893585205
dqn reward tensor(-118.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8263e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20997276902198792
dqn reward tensor(163.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1311e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14747072756290436
dqn reward tensor(-170.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7498e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10010180622339249
dqn reward tensor(93.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.5351e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25893110036849976
dqn reward tensor(2.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.6346e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15133705735206604
dqn reward tensor(-47.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5179e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23981502652168274
dqn reward tensor(-175.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.4821e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10726355016231537
dqn reward tensor(-10.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5086e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14220616221427917
dqn reward tensor(-278.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7462e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1370081901550293
dqn reward tensor(17.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14203482866287231
dqn reward tensor(-226., device='cuda:0') e 0.05 loss_dqn tensor(1.0972e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12799695134162903
dqn reward tensor(34.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4039e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17532983422279358
dqn reward tensor(77.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06067294999957085
dqn reward tensor(127., device='cuda:0') e 0.05 loss_dqn tensor(6.6987e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10010705888271332
dqn reward tensor(-58., device='cuda:0') e 0.05 loss_dqn tensor(6.8347e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13038846850395203
dqn reward tensor(-180.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0793740302324295
dqn reward tensor(-260.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3297e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21085131168365479
dqn reward tensor(-209.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4187e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2945604920387268
dqn reward tensor(-115.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6918e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22612392902374268
dqn reward tensor(63.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7459e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23284348845481873
dqn reward tensor(4.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1559e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1636074185371399
dqn reward tensor(85.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8232e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21097306907176971
dqn reward tensor(-149.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.2998e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034635577350854874
dqn reward tensor(-102.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9922e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18287542462348938
dqn reward tensor(-82.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6031e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08247555047273636
dqn reward tensor(49.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.8643e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046247124671936035
dqn reward tensor(114.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20449668169021606
dqn reward tensor(-73.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0945587009191513
dqn reward tensor(-87.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0160e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048306021839380264
dqn reward tensor(-115., device='cuda:0') e 0.05 loss_dqn tensor(7.1544e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17184004187583923
dqn reward tensor(-183.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12082169950008392
dqn reward tensor(-70.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2524e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0974760502576828
dqn reward tensor(147.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6665e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07524174451828003
dqn reward tensor(-30.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0870003029704094
dqn reward tensor(70.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3509e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21289095282554626
dqn reward tensor(-130.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3093e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13324560225009918
dqn reward tensor(-9., device='cuda:0') e 0.05 loss_dqn tensor(9.0483e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08534803241491318
dqn reward tensor(-75.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1559e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35530757904052734
dqn reward tensor(231.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5023e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3032042682170868
dqn reward tensor(-49.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024203183129429817
dqn reward tensor(-36.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3064e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3445562720298767
dqn reward tensor(-88.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.4969e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11439632624387741
dqn reward tensor(-2.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.8166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037112560123205185
dqn reward tensor(-40.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2814350128173828
dqn reward tensor(-51., device='cuda:0') e 0.05 loss_dqn tensor(1.1342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14736035466194153
dqn reward tensor(-222.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0936e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22047682106494904
dqn reward tensor(51.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8058e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11654902249574661
dqn reward tensor(-124.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16146151721477509
dqn reward tensor(-45.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27525007724761963
dqn reward tensor(-12.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8080e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05378174036741257
dqn reward tensor(-271.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3994e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061920177191495895
dqn reward tensor(89.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0061e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19951501488685608
dqn reward tensor(-79.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.8429e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08161906898021698
dqn reward tensor(-180.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0261e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08607049286365509
dqn reward tensor(-125.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0290e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.236741304397583
dqn reward tensor(-39.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2474e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11421575397253036
dqn reward tensor(-85.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2836e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14697304368019104
dqn reward tensor(-23.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11339805275201797
dqn reward tensor(108.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21469172835350037
dqn reward tensor(-27.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1092e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19064031541347504
dqn reward tensor(-34.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13947594165802002
dqn reward tensor(-163.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12990699708461761
dqn reward tensor(21.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0399e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14120592176914215
dqn reward tensor(-96.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.0243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03844776749610901
dqn reward tensor(-234.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3753e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11832807213068008
dqn reward tensor(-111.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048751264810562134
dqn reward tensor(-81., device='cuda:0') e 0.05 loss_dqn tensor(6.0120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09226634353399277
dqn reward tensor(-105.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8257e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30424436926841736
dqn reward tensor(-85.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.4568e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03947876766324043
dqn reward tensor(-92.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.4096e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1574661135673523
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028968313708901405
dqn reward tensor(-209., device='cuda:0') e 0.05 loss_dqn tensor(3.5802e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061469271779060364
dqn reward tensor(-82.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5681e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12039364129304886
dqn reward tensor(-395.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1285e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2547416090965271
dqn reward tensor(-265.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5742e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2665908634662628
dqn reward tensor(-158.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5185e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19392967224121094
dqn reward tensor(-237.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1775e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1330324411392212
dqn reward tensor(-70.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1367e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09375841915607452
dqn reward tensor(-309.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7925e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13024185597896576
dqn reward tensor(-283.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4594e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14889222383499146
dqn reward tensor(-365.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0907324030995369
dqn reward tensor(-295.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15530088543891907
dqn reward tensor(-338.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17770883440971375
dqn reward tensor(-123.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15437418222427368
dqn reward tensor(-52.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5557e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.257185697555542
dqn reward tensor(-251.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0737e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17450013756752014
dqn reward tensor(-76.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17603224515914917
dqn reward tensor(-136.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8521e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1041131243109703
dqn reward tensor(-129.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2576e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15177100896835327
dqn reward tensor(-219.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2930e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09602906554937363
dqn reward tensor(-376.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16980090737342834
dqn reward tensor(-438., device='cuda:0') e 0.05 loss_dqn tensor(1.0644e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16208426654338837
dqn reward tensor(-210.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7499e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16697847843170166
dqn reward tensor(-166.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9809e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09341567009687424
dqn reward tensor(-319.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2817e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0787801742553711
dqn reward tensor(-227.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.3887e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1672973930835724
dqn reward tensor(-387.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1633e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19438007473945618
dqn reward tensor(-165.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0387e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1867387890815735
dqn reward tensor(-313.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20472809672355652
dqn reward tensor(-155., device='cuda:0') e 0.05 loss_dqn tensor(1.2524e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11846249550580978
dqn reward tensor(-116.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7896e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14733362197875977
dqn reward tensor(-282.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9178e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033859942108392715
dqn reward tensor(-155.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0776e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06787760555744171
dqn reward tensor(-128.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6701e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21336644887924194
dqn reward tensor(-201.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6036e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09106286615133286
dqn reward tensor(-331.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6986e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18888932466506958
dqn reward tensor(-322.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4837e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15354478359222412
dqn reward tensor(-119.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1538e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.176158145070076
dqn reward tensor(-222.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12912686169147491
dqn reward tensor(-264.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0938e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20929104089736938
dqn reward tensor(-220.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4197e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18638989329338074
dqn reward tensor(-150.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6150e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12823031842708588
dqn reward tensor(79., device='cuda:0') e 0.05 loss_dqn tensor(8.1715e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042922139167785645
dqn reward tensor(-166.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.0154e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1252167522907257
dqn reward tensor(-351.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8708e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09137151390314102
dqn reward tensor(-141.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2309657335281372
dqn reward tensor(-331.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9177e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06037452071905136
dqn reward tensor(-164.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5919e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07008852064609528
dqn reward tensor(-367.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0788e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25956469774246216
dqn reward tensor(-115.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3052e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22014082968235016
dqn reward tensor(-243., device='cuda:0') e 0.05 loss_dqn tensor(1.3773e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03888140618801117
dqn reward tensor(-146.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9284e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638132631778717
dqn reward tensor(-219.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9305e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18636123836040497
dqn reward tensor(-192., device='cuda:0') e 0.05 loss_dqn tensor(8.1766e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12664104998111725
dqn reward tensor(-315.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0908e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15787462890148163
dqn reward tensor(-183.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5040e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17419195175170898
dqn reward tensor(-160.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2164e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13512107729911804
dqn reward tensor(-363.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3562e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0877651646733284
dqn reward tensor(-446.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21425861120224
dqn reward tensor(-203.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3222e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15907160937786102
dqn reward tensor(-28.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1204e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10555572807788849
dqn reward tensor(-394.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0020e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09378813952207565
dqn reward tensor(-179.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2604e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10536661744117737
dqn reward tensor(-116.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9361e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13920710980892181
dqn reward tensor(-98.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2752e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1899501532316208
dqn reward tensor(-520.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9945e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16093814373016357
dqn reward tensor(-110.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7532e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2616140842437744
dqn reward tensor(-166.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19944849610328674
dqn reward tensor(-239.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11504437029361725
dqn reward tensor(-259., device='cuda:0') e 0.05 loss_dqn tensor(8.5942e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08723446726799011
dqn reward tensor(129.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9747e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14400219917297363
dqn reward tensor(-30.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1074155792593956
dqn reward tensor(-185., device='cuda:0') e 0.05 loss_dqn tensor(1.6583e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23512887954711914
dqn reward tensor(-216.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8204e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15567010641098022
dqn reward tensor(180.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0819e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10414747148752213
dqn reward tensor(77.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1280e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19724392890930176
dqn reward tensor(47.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0783531665802002
dqn reward tensor(67.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09899508953094482
dqn reward tensor(260., device='cuda:0') e 0.05 loss_dqn tensor(1.0945e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1259811967611313
dqn reward tensor(148.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.9303e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16301652789115906
dqn reward tensor(185.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1125e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13594689965248108
dqn reward tensor(239.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1204e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22015897929668427
dqn reward tensor(40.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7823e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03227469325065613
dqn reward tensor(147.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5928e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1819211095571518
dqn reward tensor(122.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9335e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20534838736057281
dqn reward tensor(22., device='cuda:0') e 0.05 loss_dqn tensor(1.7169e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15820163488388062
dqn reward tensor(321.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19709661602973938
dqn reward tensor(-43.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.6107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3097344934940338
dqn reward tensor(122.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11264317482709885
dqn reward tensor(146.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1591663956642151
dqn reward tensor(219.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26494014263153076
dqn reward tensor(87.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045171234756708145
dqn reward tensor(-77.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0805034339427948
dqn reward tensor(31.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32454347610473633
dqn reward tensor(18.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1657942831516266
dqn reward tensor(122.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6410e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23002156615257263
dqn reward tensor(-26.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0759e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18075327575206757
dqn reward tensor(266., device='cuda:0') e 0.05 loss_dqn tensor(1.9382e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06138400733470917
dqn reward tensor(-25.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6184e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22186189889907837
dqn reward tensor(-17.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5128e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12618222832679749
dqn reward tensor(-328., device='cuda:0') e 0.05 loss_dqn tensor(1.7350e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32217127084732056
dqn reward tensor(248.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052937135100364685
dqn reward tensor(128.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12940652668476105
dqn reward tensor(-22., device='cuda:0') e 0.05 loss_dqn tensor(1.6023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12887009978294373
dqn reward tensor(-55.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10575045645236969
dqn reward tensor(-4.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9533e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08529962599277496
dqn reward tensor(234.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6627e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08933284878730774
dqn reward tensor(59.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08287080377340317
dqn reward tensor(115.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6547e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02562822215259075
dqn reward tensor(327.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21136432886123657
dqn reward tensor(225.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09024731814861298
dqn reward tensor(217.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6916e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14810605347156525
dqn reward tensor(180.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.7889e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024998614564538002
dqn reward tensor(189.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8228e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24258849024772644
dqn reward tensor(30.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10153137892484665
dqn reward tensor(199.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8161e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2232663333415985
dqn reward tensor(112., device='cuda:0') e 0.05 loss_dqn tensor(2.2983e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06850522756576538
dqn reward tensor(115.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09405893087387085
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(2.2023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1390293836593628
dqn reward tensor(197.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24038656055927277
dqn reward tensor(-100.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.2703e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.203542098402977
dqn reward tensor(62.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18035364151000977
dqn reward tensor(89.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.0418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19825762510299683
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03820589557290077
dqn reward tensor(175.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14856953918933868
dqn reward tensor(108.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0785263255238533
dqn reward tensor(126., device='cuda:0') e 0.05 loss_dqn tensor(2.0325e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03729270398616791
dqn reward tensor(152.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0369485542178154
dqn reward tensor(135., device='cuda:0') e 0.05 loss_dqn tensor(2.0264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21122191846370697
dqn reward tensor(25.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05312323942780495
dqn reward tensor(146.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1056e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07592110335826874
dqn reward tensor(-168.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8304e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1262454241514206
dqn reward tensor(118.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4668e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11895090341567993
dqn reward tensor(118.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024025924503803253
dqn reward tensor(52.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2029e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20245979726314545
dqn reward tensor(-146.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5057e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28900817036628723
dqn reward tensor(-14., device='cuda:0') e 0.05 loss_dqn tensor(2.9857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15714162588119507
dqn reward tensor(-33.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6091e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04310433566570282
dqn reward tensor(242.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16397744417190552
dqn reward tensor(196.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2758e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11630088090896606
dqn reward tensor(125.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7352e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23242726922035217
dqn reward tensor(67.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1281808316707611
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04963625222444534
dqn reward tensor(28.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6614e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15731152892112732
dqn reward tensor(139.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.7844e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035875119268894196
dqn reward tensor(-70.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09405840933322906
dqn reward tensor(78.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24160699546337128
dqn reward tensor(-266.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15268780291080475
dqn reward tensor(198.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3929e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20714057981967926
dqn reward tensor(117.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3494e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04211524873971939
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1962767243385315
dqn reward tensor(-193.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5210e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07241307944059372
dqn reward tensor(161., device='cuda:0') e 0.05 loss_dqn tensor(3.3749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16908997297286987
dqn reward tensor(84.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8219e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057037968188524246
dqn reward tensor(222.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4689e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2784423828125
dqn reward tensor(121., device='cuda:0') e 0.05 loss_dqn tensor(2.9494e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18201619386672974
dqn reward tensor(33.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2187231481075287
dqn reward tensor(216.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4713e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12639740109443665
dqn reward tensor(-81.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6495e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04750201106071472
dqn reward tensor(171.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5595e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09468104690313339
dqn reward tensor(102.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3996e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09810715913772583
dqn reward tensor(-25.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8687e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04463953524827957
dqn reward tensor(-313.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3410e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12308479845523834
dqn reward tensor(188.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5577e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14511656761169434
dqn reward tensor(-81.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6294e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10015285760164261
dqn reward tensor(-181.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0177e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15581591427326202
dqn reward tensor(139.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019939344376325607
dqn reward tensor(135.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1732e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18584322929382324
dqn reward tensor(181.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11631286144256592
dqn reward tensor(225.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17862236499786377
dqn reward tensor(386.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5534e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12749218940734863
dqn reward tensor(-74.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4758e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10828039050102234
dqn reward tensor(-108.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1454867720603943
dqn reward tensor(128.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19378019869327545
dqn reward tensor(223.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6750e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21907161176204681
dqn reward tensor(197.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045541562139987946
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23257765173912048
dqn reward tensor(186.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1441207379102707
dqn reward tensor(74.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7047e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036974407732486725
dqn reward tensor(40., device='cuda:0') e 0.05 loss_dqn tensor(3.0830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19183333218097687
dqn reward tensor(211.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5521e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13299918174743652
dqn reward tensor(90.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4271e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09870322793722153
dqn reward tensor(309.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09566102921962738
dqn reward tensor(471.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.9822e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08539380133152008
dqn reward tensor(122., device='cuda:0') e 0.05 loss_dqn tensor(1.9604e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16574512422084808
dqn reward tensor(182.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8470e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22175908088684082
dqn reward tensor(136.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18698644638061523
dqn reward tensor(259.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14489421248435974
dqn reward tensor(176.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6540e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08273667097091675
dqn reward tensor(9., device='cuda:0') e 0.05 loss_dqn tensor(3.9731e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14206528663635254
dqn reward tensor(109.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8437e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16048650443553925
dqn reward tensor(136.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8735e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034708645194768906
dqn reward tensor(192.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26960572600364685
dqn reward tensor(128.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4771e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23508521914482117
dqn reward tensor(279., device='cuda:0') e 0.05 loss_dqn tensor(1.4929e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25277507305145264
dqn reward tensor(115.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4952e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09437991678714752
dqn reward tensor(205.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20646169781684875
dqn reward tensor(128.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09438656270503998
dqn reward tensor(52.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08348263055086136
dqn reward tensor(452.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4178e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22384442389011383
dqn reward tensor(255., device='cuda:0') e 0.05 loss_dqn tensor(1.4124e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10774824023246765
dqn reward tensor(263.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13597790896892548
dqn reward tensor(170.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3591e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21238063275814056
dqn reward tensor(239.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16276651620864868
dqn reward tensor(143.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06445948779582977
dqn reward tensor(136.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2038e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17089805006980896
dqn reward tensor(109.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16947650909423828
dqn reward tensor(141.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.7429e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12087775766849518
dqn reward tensor(110.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09842179715633392
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1228e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10569021105766296
dqn reward tensor(296.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3805e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11503072828054428
dqn reward tensor(92.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2023307979106903
dqn reward tensor(248.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1124e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14002662897109985
dqn reward tensor(119.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23699946701526642
dqn reward tensor(230.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23947773873806
dqn reward tensor(49., device='cuda:0') e 0.05 loss_dqn tensor(1.4576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03645007312297821
dqn reward tensor(44.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14860138297080994
dqn reward tensor(161.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1179e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04197164624929428
dqn reward tensor(87.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1910e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254689395427704
dqn reward tensor(122.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1490e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08375578373670578
dqn reward tensor(152.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09615646302700043
dqn reward tensor(196.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1540e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2583158612251282
dqn reward tensor(101.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4508e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1976509690284729
dqn reward tensor(-60.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0755e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2625102400779724
dqn reward tensor(138.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1784e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2513246536254883
dqn reward tensor(115.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13891643285751343
dqn reward tensor(182.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18848749995231628
dqn reward tensor(16., device='cuda:0') e 0.05 loss_dqn tensor(1.2669e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12551280856132507
dqn reward tensor(61.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13830693066120148
dqn reward tensor(-6.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12638190388679504
dqn reward tensor(65.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8372e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23791036009788513
dqn reward tensor(20.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6745e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11420714855194092
dqn reward tensor(-17., device='cuda:0') e 0.05 loss_dqn tensor(6.6136e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16049616038799286
dqn reward tensor(10., device='cuda:0') e 0.05 loss_dqn tensor(2.2974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.104793980717659
dqn reward tensor(158., device='cuda:0') e 0.05 loss_dqn tensor(3.8842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18502391874790192
dqn reward tensor(108.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17153915762901306
dqn reward tensor(92.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8230e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2175096869468689
dqn reward tensor(36.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6651e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1691029667854309
dqn reward tensor(119.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.8849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12365029007196426
dqn reward tensor(49.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0047e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1282498836517334
dqn reward tensor(33.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.7884e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0877561867237091
dqn reward tensor(199.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4214e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3219713866710663
dqn reward tensor(126.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9145e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0287499837577343
dqn reward tensor(208.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03713313117623329
dqn reward tensor(128.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0388e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2510156035423279
dqn reward tensor(364.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16950871050357819
dqn reward tensor(124.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2587e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.130793035030365
dqn reward tensor(249.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10222912579774857
dqn reward tensor(151.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1453707367181778
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23796561360359192
dqn reward tensor(125.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8599e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10789598524570465
dqn reward tensor(158.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2800e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13095910847187042
dqn reward tensor(200.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03393479436635971
dqn reward tensor(377., device='cuda:0') e 0.05 loss_dqn tensor(2.9549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06871010363101959
dqn reward tensor(157., device='cuda:0') e 0.05 loss_dqn tensor(9.7002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37111496925354004
dqn reward tensor(308.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0481315553188324
dqn reward tensor(163.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.2921e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1537860929965973
dqn reward tensor(341.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2508e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09869208186864853
dqn reward tensor(178.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.4697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08500687032938004
dqn reward tensor(142.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.4285e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10819108784198761
dqn reward tensor(202., device='cuda:0') e 0.05 loss_dqn tensor(2.5081e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19801819324493408
dqn reward tensor(185., device='cuda:0') e 0.05 loss_dqn tensor(2.5516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1819794774055481
dqn reward tensor(248.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19226238131523132
dqn reward tensor(71.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6519e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26861104369163513
dqn reward tensor(214.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.5707e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15175190567970276
dqn reward tensor(353.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0751853883266449
dqn reward tensor(276.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12203259766101837
dqn reward tensor(338.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15581780672073364
dqn reward tensor(212.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15538576245307922
dqn reward tensor(131.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7436e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16529807448387146
dqn reward tensor(-78.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1848e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2585144639015198
dqn reward tensor(154.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7646e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15374797582626343
dqn reward tensor(251.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.092911496758461
dqn reward tensor(118.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9836e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1984834223985672
dqn reward tensor(169.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9503e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24931488931179047
dqn reward tensor(193.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8694e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18551036715507507
dqn reward tensor(167.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.3039e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2255638837814331
dqn reward tensor(133.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7008e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1799200177192688
dqn reward tensor(85.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8848e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09574253857135773
dqn reward tensor(210.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9219e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0975414365530014
dqn reward tensor(303.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5000e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15067625045776367
dqn reward tensor(143.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23411056399345398
dqn reward tensor(303.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477882087230682
dqn reward tensor(221.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9694e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10839952528476715
dqn reward tensor(127.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0577e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11660239845514297
dqn reward tensor(43.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8687e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0963781327009201
dqn reward tensor(196.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1052e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10346769541501999
dqn reward tensor(78.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20265768468379974
dqn reward tensor(-167.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.1242e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11745063960552216
dqn reward tensor(-12.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3246e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06353352963924408
dqn reward tensor(186.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3361e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10615252703428268
dqn reward tensor(203., device='cuda:0') e 0.05 loss_dqn tensor(3.2739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13541297614574432
dqn reward tensor(269.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1867431104183197
dqn reward tensor(121.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2609e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13287363946437836
dqn reward tensor(304., device='cuda:0') e 0.05 loss_dqn tensor(3.4536e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06466584652662277
dqn reward tensor(273.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1072e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20279616117477417
dqn reward tensor(7.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5033e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07491350173950195
dqn reward tensor(-1.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7807e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2626919746398926
dqn reward tensor(292.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3470e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0382104367017746
dqn reward tensor(312.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4692e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030968844890594482
dqn reward tensor(140.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4447e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02764081209897995
dqn reward tensor(4., device='cuda:0') e 0.05 loss_dqn tensor(3.4808e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21712253987789154
dqn reward tensor(75.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.6606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09118004888296127
dqn reward tensor(332.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17379805445671082
dqn reward tensor(264.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046717915683984756
dqn reward tensor(-38.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09370249509811401
dqn reward tensor(-79., device='cuda:0') e 0.05 loss_dqn tensor(4.6872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02629026770591736
dqn reward tensor(376.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5302e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09562596678733826
dqn reward tensor(19.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025179116055369377
dqn reward tensor(37.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5321e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12245326489210129
dqn reward tensor(129.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3523e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016837604343891144
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5660e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017284784466028214
dqn reward tensor(182., device='cuda:0') e 0.05 loss_dqn tensor(3.6919e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12383238971233368
dqn reward tensor(88.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5559e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21659976243972778
dqn reward tensor(338., device='cuda:0') e 0.05 loss_dqn tensor(3.5303e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04698476195335388
dqn reward tensor(165.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1966862827539444
dqn reward tensor(167.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.5943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.39258474111557007
dqn reward tensor(265.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6191e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21532702445983887
dqn reward tensor(226.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6668e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22427108883857727
dqn reward tensor(455.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09287573397159576
dqn reward tensor(200.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20134799182415009
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1991482675075531
dqn reward tensor(139.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16311153769493103
dqn reward tensor(236.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7223e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10108853876590729
dqn reward tensor(138.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3375e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23435086011886597
dqn reward tensor(252.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14304417371749878
dqn reward tensor(85.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9844e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15005135536193848
dqn reward tensor(116.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0130e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16555622220039368
dqn reward tensor(89.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3193e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10031072795391083
dqn reward tensor(202.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.0120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1715339571237564
dqn reward tensor(259.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13887308537960052
dqn reward tensor(306., device='cuda:0') e 0.05 loss_dqn tensor(4.0015e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3050955533981323
dqn reward tensor(308.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9730e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09404771775007248
dqn reward tensor(334.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.7018e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030875734984874725
dqn reward tensor(269.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.2378e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1139870434999466
dqn reward tensor(209.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1281e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15491937100887299
dqn reward tensor(220.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1382e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04375225305557251
dqn reward tensor(213.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1695e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19933003187179565
dqn reward tensor(314.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8692e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18510937690734863
dqn reward tensor(-80.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15417923033237457
dqn reward tensor(127.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2422e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1453678011894226
dqn reward tensor(359.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3240918517112732
dqn reward tensor(329.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3421e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4515761137008667
dqn reward tensor(264.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2607e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11440685391426086
dqn reward tensor(106.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5677e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08335094153881073
dqn reward tensor(127.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13420362770557404
dqn reward tensor(198.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.1804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22675451636314392
dqn reward tensor(195.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04818892478942871
dqn reward tensor(70.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5778e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14941386878490448
dqn reward tensor(272.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.5833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14483512938022614
dqn reward tensor(70.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4640e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15284544229507446
dqn reward tensor(252.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.4934e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13504286110401154
dqn reward tensor(171.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2041e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12661758065223694
dqn reward tensor(160.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5577e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1890747994184494
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0324e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042868852615356445
dqn reward tensor(324.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07445742189884186
dqn reward tensor(311.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0016e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024528868496418
dqn reward tensor(255.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4881e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07237590849399567
dqn reward tensor(372.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13228537142276764
dqn reward tensor(191.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1313159316778183
dqn reward tensor(342.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6509e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24562671780586243
dqn reward tensor(162.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7337e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17106717824935913
dqn reward tensor(234.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6650e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27160516381263733
dqn reward tensor(266.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14865215122699738
dqn reward tensor(207.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0242e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09609240293502808
dqn reward tensor(272.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.8696e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3211439847946167
dqn reward tensor(243.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06474295258522034
dqn reward tensor(274.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7081e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14860515296459198
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5072e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1372024565935135
dqn reward tensor(396.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7966e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15439552068710327
dqn reward tensor(99.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8321e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.201570525765419
dqn reward tensor(382.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11596933007240295
dqn reward tensor(96.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8367e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13799890875816345
dqn reward tensor(138.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1046e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.194794699549675
dqn reward tensor(212.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12738291919231415
dqn reward tensor(230., device='cuda:0') e 0.05 loss_dqn tensor(4.7944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488812118768692
dqn reward tensor(-13.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.3970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03757525607943535
Evaluating...
Train: {'rocauc': 0.7371366663823403} 3.180701971054077
=====Epoch 13=====
Training...
dqn reward tensor(41.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4522e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08564213663339615
dqn reward tensor(193.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09107103943824768
dqn reward tensor(43.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0864957720041275
dqn reward tensor(267.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07426497340202332
dqn reward tensor(176.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6110e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04138585180044174
dqn reward tensor(174.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10430175065994263
dqn reward tensor(247.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8721e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08093224465847015
dqn reward tensor(111.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21664999425411224
dqn reward tensor(105.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1333e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28694528341293335
dqn reward tensor(144.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9420e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17821776866912842
dqn reward tensor(293.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.8412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05788470804691315
dqn reward tensor(107.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.2510e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13191469013690948
dqn reward tensor(233.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023764455690979958
dqn reward tensor(180.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.4400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21307472884655
dqn reward tensor(167.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5293e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08030430227518082
dqn reward tensor(144., device='cuda:0') e 0.05 loss_dqn tensor(5.6216e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08499699085950851
dqn reward tensor(160.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2071e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20868858695030212
dqn reward tensor(102.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.5702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20607754588127136
dqn reward tensor(273.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2991737425327301
dqn reward tensor(129., device='cuda:0') e 0.05 loss_dqn tensor(5.9274e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04161309078335762
dqn reward tensor(58.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1381e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0709092766046524
dqn reward tensor(224.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10806024074554443
dqn reward tensor(160.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8983e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10106513649225235
dqn reward tensor(268.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3227e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21006086468696594
dqn reward tensor(187., device='cuda:0') e 0.05 loss_dqn tensor(5.2321e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12241890281438828
dqn reward tensor(232.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0440e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09927287697792053
dqn reward tensor(56.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6230e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16326777637004852
dqn reward tensor(417.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9584e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043396636843681335
dqn reward tensor(239.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13440820574760437
dqn reward tensor(299., device='cuda:0') e 0.05 loss_dqn tensor(5.5936e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18834254145622253
dqn reward tensor(342.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2532e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27828529477119446
dqn reward tensor(69.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3203e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0271606482565403
dqn reward tensor(164.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13303980231285095
dqn reward tensor(242.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5895e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24892820417881012
dqn reward tensor(96.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18339256942272186
dqn reward tensor(211.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2386e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10645937919616699
dqn reward tensor(76.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2225e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14139722287654877
dqn reward tensor(75.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5035e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050847142934799194
dqn reward tensor(210.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16683173179626465
dqn reward tensor(69.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0822e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11527952551841736
dqn reward tensor(216.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.2088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3002667725086212
dqn reward tensor(91.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1975404918193817
dqn reward tensor(311.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13106538355350494
dqn reward tensor(233.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16644081473350525
dqn reward tensor(157.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05005635693669319
dqn reward tensor(55.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7990e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0562986359000206
dqn reward tensor(95.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9632e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10050532966852188
dqn reward tensor(196.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2757e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12699095904827118
dqn reward tensor(47.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2663e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1850529909133911
dqn reward tensor(293.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3360e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16309374570846558
dqn reward tensor(217.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2737465500831604
dqn reward tensor(274.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10877179354429245
dqn reward tensor(71.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08105891197919846
dqn reward tensor(266.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5018e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09872648864984512
dqn reward tensor(13., device='cuda:0') e 0.05 loss_dqn tensor(5.7586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07306098192930222
dqn reward tensor(148.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1463164985179901
dqn reward tensor(275.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10648749768733978
dqn reward tensor(232.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15370824933052063
dqn reward tensor(229.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4153e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0794936791062355
dqn reward tensor(-45.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499980241060257
dqn reward tensor(342.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09208555519580841
dqn reward tensor(247.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16447360813617706
dqn reward tensor(193.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5208e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09547621011734009
dqn reward tensor(68., device='cuda:0') e 0.05 loss_dqn tensor(5.6377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09537475556135178
dqn reward tensor(367.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2369702011346817
dqn reward tensor(168.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5816e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14102521538734436
dqn reward tensor(132.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5903e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24750876426696777
dqn reward tensor(98.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07157261669635773
dqn reward tensor(263.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.7052e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09651342779397964
dqn reward tensor(265.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08300721645355225
dqn reward tensor(405.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4251e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049909308552742004
dqn reward tensor(184.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0716e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14969974756240845
dqn reward tensor(377.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5111e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20624437928199768
dqn reward tensor(15.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6333e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037162818014621735
dqn reward tensor(-5.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.7451e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11946210265159607
dqn reward tensor(243.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17248663306236267
dqn reward tensor(65.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26784512400627136
dqn reward tensor(349.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2516734302043915
dqn reward tensor(160.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16360902786254883
dqn reward tensor(493., device='cuda:0') e 0.05 loss_dqn tensor(5.5440e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2127254605293274
dqn reward tensor(136.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5510e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07464471459388733
dqn reward tensor(200.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1496516466140747
dqn reward tensor(365., device='cuda:0') e 0.05 loss_dqn tensor(5.3387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1039268970489502
dqn reward tensor(311.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0500e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19758206605911255
dqn reward tensor(293.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1372363567352295
dqn reward tensor(320.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8126e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13000737130641937
dqn reward tensor(179.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0101e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03794403374195099
dqn reward tensor(427.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9067e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14578965306282043
dqn reward tensor(354.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6533e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09603068232536316
dqn reward tensor(243.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3510e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0865740031003952
dqn reward tensor(269.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7479e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1297537088394165
dqn reward tensor(245.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.9791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21353182196617126
dqn reward tensor(33.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.5429e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11428496241569519
dqn reward tensor(334.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04039379954338074
dqn reward tensor(315.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4500e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13124629855155945
dqn reward tensor(184.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.4941e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4183930456638336
dqn reward tensor(149.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5047e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08004877716302872
dqn reward tensor(326.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.4597e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08822640776634216
dqn reward tensor(37.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15007814764976501
dqn reward tensor(341.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.0558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2089385688304901
dqn reward tensor(301.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2115e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14963054656982422
dqn reward tensor(215.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2185e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2079821079969406
dqn reward tensor(115.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2520e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1593727171421051
dqn reward tensor(416.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.3169e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04653695970773697
dqn reward tensor(307.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.2581e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1380053460597992
dqn reward tensor(201.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1777162402868271
dqn reward tensor(470., device='cuda:0') e 0.05 loss_dqn tensor(4.3329e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10190083086490631
dqn reward tensor(235.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8449e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08922268450260162
dqn reward tensor(291.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06554097682237625
dqn reward tensor(470.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1270960569381714
dqn reward tensor(306.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4018e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1992112100124359
dqn reward tensor(215.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4298e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1768394559621811
dqn reward tensor(174., device='cuda:0') e 0.05 loss_dqn tensor(4.4193e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22551406919956207
dqn reward tensor(362.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2551e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15663811564445496
dqn reward tensor(393.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0653e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08152060210704803
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2278e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10870097577571869
dqn reward tensor(528.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11456145346164703
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2051757425069809
dqn reward tensor(305.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3885e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10344114154577255
dqn reward tensor(466.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2675e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14480352401733398
dqn reward tensor(230.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.9928e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09064335376024246
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682843714952469
dqn reward tensor(311.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07899543642997742
dqn reward tensor(368.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5380e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19753174483776093
dqn reward tensor(427.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1456e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24087698757648468
dqn reward tensor(465.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0645492672920227
dqn reward tensor(424.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3162e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27028799057006836
dqn reward tensor(387.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1914e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1961016058921814
dqn reward tensor(345.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.2686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08044487237930298
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27965807914733887
dqn reward tensor(351.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2696e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10350145399570465
dqn reward tensor(154.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06639401614665985
dqn reward tensor(282.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8283e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06282471120357513
dqn reward tensor(388.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2575e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04148714989423752
dqn reward tensor(191.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3425e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04975516349077225
dqn reward tensor(374.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12517069280147552
dqn reward tensor(445.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3164282739162445
dqn reward tensor(339.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6447e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15292470157146454
dqn reward tensor(454.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13385652005672455
dqn reward tensor(367.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07481379806995392
dqn reward tensor(325.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1773e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22711998224258423
dqn reward tensor(402.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30936408042907715
dqn reward tensor(444.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1489e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2053913027048111
dqn reward tensor(317.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1611e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16318564116954803
dqn reward tensor(404.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1922e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11290942132472992
dqn reward tensor(540.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1122e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10238707810640335
dqn reward tensor(370.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3044e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.115195631980896
dqn reward tensor(535.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0934569463133812
dqn reward tensor(399.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3076515793800354
dqn reward tensor(416.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25880399346351624
dqn reward tensor(214.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2696e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09228178858757019
dqn reward tensor(417.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1522e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26849573850631714
dqn reward tensor(543.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06541867554187775
dqn reward tensor(341.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22097937762737274
dqn reward tensor(423.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6522e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1455196589231491
dqn reward tensor(458., device='cuda:0') e 0.05 loss_dqn tensor(4.1776e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11171243339776993
dqn reward tensor(441.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.1960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26138442754745483
dqn reward tensor(361.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08190733194351196
dqn reward tensor(446.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11146525293588638
dqn reward tensor(468.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.5851e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10306212306022644
dqn reward tensor(355., device='cuda:0') e 0.05 loss_dqn tensor(4.4995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18604761362075806
dqn reward tensor(430.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3192e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1195213794708252
dqn reward tensor(332.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1171e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10700447857379913
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14633424580097198
dqn reward tensor(486.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0878e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14852935075759888
dqn reward tensor(65.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7278e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22581428289413452
dqn reward tensor(441.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0676e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05810283496975899
dqn reward tensor(498., device='cuda:0') e 0.05 loss_dqn tensor(3.9967e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13758023083209991
dqn reward tensor(585., device='cuda:0') e 0.05 loss_dqn tensor(4.0116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2191568911075592
dqn reward tensor(378.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.9150e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09380496293306351
dqn reward tensor(268.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06837710738182068
dqn reward tensor(361.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.4808e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27525076270103455
dqn reward tensor(451.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5005e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03176645189523697
dqn reward tensor(498., device='cuda:0') e 0.05 loss_dqn tensor(8.3622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11504028737545013
dqn reward tensor(547.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9948e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16985639929771423
dqn reward tensor(199.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6912e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12621021270751953
dqn reward tensor(346.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10716688632965088
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029263438656926155
dqn reward tensor(192.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0262e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17126154899597168
dqn reward tensor(488.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.0910e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027565181255340576
dqn reward tensor(349.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2523e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10728410631418228
dqn reward tensor(433.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4525e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15106046199798584
dqn reward tensor(449., device='cuda:0') e 0.05 loss_dqn tensor(3.9825e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14866480231285095
dqn reward tensor(439.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18268901109695435
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07857730984687805
dqn reward tensor(394.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3404e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3939524292945862
dqn reward tensor(499.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08896999061107635
dqn reward tensor(431.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0649e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08058415353298187
dqn reward tensor(479.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9466e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07418404519557953
dqn reward tensor(571.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9815e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12811671197414398
dqn reward tensor(416.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.4794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06230514496564865
dqn reward tensor(284.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7954e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2655395269393921
dqn reward tensor(402.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10970433056354523
dqn reward tensor(527., device='cuda:0') e 0.05 loss_dqn tensor(5.7615e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2625215947628021
dqn reward tensor(251., device='cuda:0') e 0.05 loss_dqn tensor(4.0091e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09885823726654053
dqn reward tensor(592.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9134e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10217402875423431
dqn reward tensor(469.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1070498526096344
dqn reward tensor(338.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0240e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08460395038127899
dqn reward tensor(522.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21927785873413086
dqn reward tensor(338.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6414e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07670670747756958
dqn reward tensor(483., device='cuda:0') e 0.05 loss_dqn tensor(8.6229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0829256922006607
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8051e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04816529527306557
dqn reward tensor(539.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7957e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10599729418754578
dqn reward tensor(419.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9158e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07056550681591034
dqn reward tensor(353.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8283e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1438799500465393
dqn reward tensor(320.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.0328e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13361194729804993
dqn reward tensor(337.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.8544e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025446783751249313
dqn reward tensor(385.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1196e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10696759819984436
dqn reward tensor(374.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2044e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1394396275281906
dqn reward tensor(496.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.8277e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26520010828971863
dqn reward tensor(431.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021981455385684967
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9353e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03563836216926575
dqn reward tensor(245.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8621e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.412024587392807
dqn reward tensor(495.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5433e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09031505137681961
dqn reward tensor(325.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0736e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08437493443489075
dqn reward tensor(188.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8798e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18822500109672546
dqn reward tensor(362., device='cuda:0') e 0.05 loss_dqn tensor(3.9509e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09305685013532639
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17942161858081818
dqn reward tensor(247.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0192e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12894928455352783
dqn reward tensor(368.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7672e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1832880973815918
dqn reward tensor(280.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06228506937623024
dqn reward tensor(313.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0834217518568039
dqn reward tensor(327.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20001234114170074
dqn reward tensor(380.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13260909914970398
dqn reward tensor(572.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7571e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14037908613681793
dqn reward tensor(570.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04967443645000458
dqn reward tensor(403.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1403566598892212
dqn reward tensor(348.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8369e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2816251218318939
dqn reward tensor(454., device='cuda:0') e 0.05 loss_dqn tensor(3.8349e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3248290419578552
dqn reward tensor(431.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0774e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1343916654586792
dqn reward tensor(467.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8637e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17951130867004395
dqn reward tensor(520.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6887e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06410355865955353
dqn reward tensor(184.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9278e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07438106834888458
dqn reward tensor(350.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.2701e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043385498225688934
dqn reward tensor(503.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.7617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621161699295044
dqn reward tensor(458.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3341e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13375040888786316
dqn reward tensor(364., device='cuda:0') e 0.05 loss_dqn tensor(4.7318e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13948814570903778
dqn reward tensor(384.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13737374544143677
dqn reward tensor(596.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9873e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02779237926006317
dqn reward tensor(465.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2431245744228363
dqn reward tensor(409.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0052e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17582082748413086
dqn reward tensor(370.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6475e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2263215184211731
dqn reward tensor(428.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7813e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13490527868270874
dqn reward tensor(436.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7197e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1479949802160263
dqn reward tensor(340.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0819e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07768238335847855
dqn reward tensor(420.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14501553773880005
dqn reward tensor(330.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16498103737831116
dqn reward tensor(433.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.6655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1528613418340683
dqn reward tensor(385.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7641e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12941353023052216
dqn reward tensor(324.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08615542948246002
dqn reward tensor(437.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14970579743385315
dqn reward tensor(535., device='cuda:0') e 0.05 loss_dqn tensor(3.5415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10457925498485565
dqn reward tensor(294.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6988e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09645149856805801
dqn reward tensor(409.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.7342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24073906242847443
dqn reward tensor(293.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10431650280952454
dqn reward tensor(379., device='cuda:0') e 0.05 loss_dqn tensor(7.0175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22819040715694427
dqn reward tensor(470.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5436e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07191330939531326
dqn reward tensor(376.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2914e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13807417452335358
dqn reward tensor(544.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4732e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12506817281246185
dqn reward tensor(407.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2455614060163498
dqn reward tensor(353.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05656157433986664
dqn reward tensor(526.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12416675686836243
dqn reward tensor(307.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6744e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20626704394817352
dqn reward tensor(517.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4475e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11754706501960754
dqn reward tensor(364.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08827102184295654
dqn reward tensor(377.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15379782021045685
dqn reward tensor(470.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12612447142601013
dqn reward tensor(318.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.6944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03142113611102104
dqn reward tensor(402.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.5480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28864604234695435
dqn reward tensor(446.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06508931517601013
dqn reward tensor(546.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028948578983545303
dqn reward tensor(511.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02793015167117119
dqn reward tensor(563.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2348e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07830747216939926
dqn reward tensor(628.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13130764663219452
dqn reward tensor(550.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10996176302433014
dqn reward tensor(740.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3496e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0824710875749588
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2334e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15686270594596863
dqn reward tensor(595.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18192383646965027
dqn reward tensor(699.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1822e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14830748736858368
dqn reward tensor(690.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.6289e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19531407952308655
dqn reward tensor(746.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1556e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1083640605211258
dqn reward tensor(742.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1020e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18295466899871826
dqn reward tensor(676., device='cuda:0') e 0.05 loss_dqn tensor(3.3840e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08316867053508759
dqn reward tensor(477.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4800e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05621505528688431
dqn reward tensor(809.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13646914064884186
dqn reward tensor(475.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09640642255544662
dqn reward tensor(692.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.0445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15305234491825104
dqn reward tensor(755.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04541349411010742
dqn reward tensor(672.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11738176643848419
dqn reward tensor(852.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13441750407218933
dqn reward tensor(576.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5671e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21400807797908783
dqn reward tensor(715.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7226e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08004949241876602
dqn reward tensor(614.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.2193e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18241941928863525
dqn reward tensor(662.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0600e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18420034646987915
dqn reward tensor(575.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26196637749671936
dqn reward tensor(778.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0899e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17086701095104218
dqn reward tensor(722., device='cuda:0') e 0.05 loss_dqn tensor(3.0527e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15185260772705078
dqn reward tensor(462.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1259155422449112
dqn reward tensor(796., device='cuda:0') e 0.05 loss_dqn tensor(3.2800e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1875462383031845
dqn reward tensor(818.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9439e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11145682632923126
dqn reward tensor(709.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3988e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07685677707195282
dqn reward tensor(815.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11843549460172653
dqn reward tensor(786.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21911567449569702
dqn reward tensor(835.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.7657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1265077292919159
dqn reward tensor(771.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07918207347393036
dqn reward tensor(784., device='cuda:0') e 0.05 loss_dqn tensor(3.2317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673746407032013
dqn reward tensor(603.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1959e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10798565298318863
dqn reward tensor(832., device='cuda:0') e 0.05 loss_dqn tensor(3.3144e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0989266186952591
dqn reward tensor(826.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9360e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017216665670275688
dqn reward tensor(647.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9966e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04741060733795166
dqn reward tensor(720.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19796517491340637
dqn reward tensor(700., device='cuda:0') e 0.05 loss_dqn tensor(5.0486e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1946120411157608
dqn reward tensor(735.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7789e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2616128623485565
dqn reward tensor(506.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23341354727745056
dqn reward tensor(810., device='cuda:0') e 0.05 loss_dqn tensor(3.0896e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17150166630744934
dqn reward tensor(733.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3096e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17857220768928528
dqn reward tensor(793.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14603975415229797
dqn reward tensor(633.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4486e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1730307638645172
dqn reward tensor(803.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5338e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039244890213012695
dqn reward tensor(691., device='cuda:0') e 0.05 loss_dqn tensor(2.2160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10549537092447281
dqn reward tensor(562.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2095e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27627184987068176
dqn reward tensor(643.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0303e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12819209694862366
dqn reward tensor(613.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1897e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3131451904773712
dqn reward tensor(669.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1642998456954956
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2069e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06672963500022888
dqn reward tensor(596.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0820e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24850881099700928
dqn reward tensor(606.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7966e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06734330952167511
dqn reward tensor(429.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08865292370319366
dqn reward tensor(446., device='cuda:0') e 0.05 loss_dqn tensor(1.7710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19370906054973602
dqn reward tensor(410.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5571e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07455874979496002
dqn reward tensor(481.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5534e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09013549983501434
dqn reward tensor(422.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.1467e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20555584132671356
dqn reward tensor(339.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0796e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17681778967380524
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19247984886169434
dqn reward tensor(360.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0321e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15471947193145752
dqn reward tensor(299.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4474e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08211115747690201
dqn reward tensor(341.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4737e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0259137824177742
dqn reward tensor(348., device='cuda:0') e 0.05 loss_dqn tensor(1.3906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05077756568789482
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13284827768802643
dqn reward tensor(332., device='cuda:0') e 0.05 loss_dqn tensor(3.7320e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19846995174884796
dqn reward tensor(297.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01687982864677906
dqn reward tensor(189.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07506103068590164
dqn reward tensor(436.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3479e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2193107157945633
dqn reward tensor(453.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016408434137701988
dqn reward tensor(252.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3038e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08157482743263245
dqn reward tensor(428.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2824e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08943420648574829
dqn reward tensor(413.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8350e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1748935878276825
dqn reward tensor(570.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14155139029026031
dqn reward tensor(577.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8788e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2958459258079529
dqn reward tensor(472., device='cuda:0') e 0.05 loss_dqn tensor(3.2355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18501481413841248
dqn reward tensor(611., device='cuda:0') e 0.05 loss_dqn tensor(4.5548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07380784302949905
dqn reward tensor(606.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1737847477197647
dqn reward tensor(594.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08727036416530609
dqn reward tensor(449.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3508778512477875
dqn reward tensor(529.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.5423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18411533534526825
dqn reward tensor(362.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1451e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16890284419059753
dqn reward tensor(554.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6395e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08909498155117035
dqn reward tensor(503.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5803e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23760591447353363
dqn reward tensor(457., device='cuda:0') e 0.05 loss_dqn tensor(1.1704e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11996480822563171
dqn reward tensor(384.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3441e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25194841623306274
dqn reward tensor(341.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0438e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15057490766048431
dqn reward tensor(630.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0798e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12423957139253616
dqn reward tensor(445.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6907e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08506107330322266
dqn reward tensor(459.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12298239767551422
dqn reward tensor(705.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18647384643554688
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23887160420417786
dqn reward tensor(540.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08764173090457916
dqn reward tensor(462., device='cuda:0') e 0.05 loss_dqn tensor(1.0594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021859725937247276
dqn reward tensor(308.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1651540994644165
dqn reward tensor(435.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0870e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15184268355369568
dqn reward tensor(254.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19730734825134277
dqn reward tensor(422.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20250724256038666
dqn reward tensor(440.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.6910e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029205381870269775
dqn reward tensor(232.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2036866545677185
dqn reward tensor(361.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21517199277877808
dqn reward tensor(236.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08621252328157425
dqn reward tensor(221.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21094195544719696
dqn reward tensor(397., device='cuda:0') e 0.05 loss_dqn tensor(1.0165e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08081671595573425
dqn reward tensor(440.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7143e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13015592098236084
dqn reward tensor(551.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8761e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09008340537548065
dqn reward tensor(300.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5504e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18166016042232513
dqn reward tensor(444.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7830e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12528447806835175
dqn reward tensor(292.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10223937034606934
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8735e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15900452435016632
dqn reward tensor(447., device='cuda:0') e 0.05 loss_dqn tensor(3.6551e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1811218410730362
dqn reward tensor(382.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8970e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4081610143184662
dqn reward tensor(488.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5176e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12188954651355743
dqn reward tensor(305.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0459e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07717947661876678
dqn reward tensor(500.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5505e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04834376275539398
dqn reward tensor(312.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13869258761405945
dqn reward tensor(480.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3422e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11753812432289124
dqn reward tensor(305.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.8405e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16858088970184326
dqn reward tensor(425.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8647e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061645541340112686
dqn reward tensor(349.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0965963676571846
dqn reward tensor(536.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7652e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14021125435829163
dqn reward tensor(397., device='cuda:0') e 0.05 loss_dqn tensor(3.0872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08694425970315933
dqn reward tensor(488.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3255e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0842309519648552
dqn reward tensor(436.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.7148e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06945965439081192
dqn reward tensor(377.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14876975119113922
dqn reward tensor(303.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7035e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08750036358833313
dqn reward tensor(423.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0246e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.167314350605011
dqn reward tensor(369.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2963245213031769
dqn reward tensor(398.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3426e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16491961479187012
dqn reward tensor(190.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5649e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16435545682907104
dqn reward tensor(592.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9365e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14141085743904114
dqn reward tensor(235., device='cuda:0') e 0.05 loss_dqn tensor(2.1974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07211771607398987
dqn reward tensor(364.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0864216536283493
dqn reward tensor(449., device='cuda:0') e 0.05 loss_dqn tensor(4.6029e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16225914657115936
dqn reward tensor(190.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6503e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17570021748542786
dqn reward tensor(298.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8247e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07727363705635071
dqn reward tensor(483., device='cuda:0') e 0.05 loss_dqn tensor(9.3839e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08333217352628708
dqn reward tensor(219.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2341e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23746711015701294
dqn reward tensor(388.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5127e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13088680803775787
dqn reward tensor(380.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2816e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1678602397441864
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2174e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15992018580436707
dqn reward tensor(291.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3524e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10594725608825684
dqn reward tensor(444.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9698e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25499552488327026
dqn reward tensor(462.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9051e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14808234572410583
dqn reward tensor(344.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1018e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0684494748711586
dqn reward tensor(260.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1394e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1796356737613678
dqn reward tensor(502.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2669e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17671702802181244
dqn reward tensor(378.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0253e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2752315402030945
dqn reward tensor(423.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.7734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11239415407180786
dqn reward tensor(427.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11786849796772003
dqn reward tensor(422.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4580e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20508801937103271
dqn reward tensor(331.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.7655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10763447731733322
dqn reward tensor(471., device='cuda:0') e 0.05 loss_dqn tensor(2.1205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2526775598526001
dqn reward tensor(448.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4056e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2576916515827179
dqn reward tensor(361.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3457e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0662645548582077
dqn reward tensor(475.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1552e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12484213709831238
dqn reward tensor(481.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15177373588085175
dqn reward tensor(433., device='cuda:0') e 0.05 loss_dqn tensor(8.3837e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07155061513185501
dqn reward tensor(487.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2189e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04302823916077614
dqn reward tensor(261.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5618e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07386264950037003
dqn reward tensor(377.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1114e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11999494582414627
dqn reward tensor(426.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4526e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1750667244195938
dqn reward tensor(671.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0758e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046761415898799896
dqn reward tensor(252.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20807260274887085
dqn reward tensor(264.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7953e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07201658934354782
dqn reward tensor(331.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6486e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.5276932716369629
dqn reward tensor(263., device='cuda:0') e 0.05 loss_dqn tensor(3.0478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12755753099918365
dqn reward tensor(480.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3166e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03887193650007248
dqn reward tensor(551.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7565e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04061971604824066
dqn reward tensor(371.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.4950e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13528266549110413
dqn reward tensor(327.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3326e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32575100660324097
dqn reward tensor(421.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8557e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04294542223215103
dqn reward tensor(221.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3726e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04559861123561859
dqn reward tensor(366.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3537e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07914387434720993
dqn reward tensor(496.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1233e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14016228914260864
dqn reward tensor(340.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5081e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062082283198833466
dqn reward tensor(449.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3552e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35134387016296387
dqn reward tensor(521.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2881e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07955336570739746
dqn reward tensor(294.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03869268670678139
dqn reward tensor(424.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045858174562454224
dqn reward tensor(390.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.2184e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03180671110749245
dqn reward tensor(318.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2147e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1598300188779831
dqn reward tensor(419.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9282e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023518528789281845
dqn reward tensor(298.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3551e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34396684169769287
dqn reward tensor(388.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.7893e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14219480752944946
dqn reward tensor(408.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07996325939893723
dqn reward tensor(314.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0353e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1994640976190567
dqn reward tensor(409.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3218e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14143787324428558
dqn reward tensor(469.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8655e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14014580845832825
dqn reward tensor(399.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21643073856830597
dqn reward tensor(303.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0219e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17239350080490112
dqn reward tensor(275.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.2161e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22107082605361938
dqn reward tensor(274.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14344920217990875
dqn reward tensor(343.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9187e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2750115990638733
dqn reward tensor(210.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.4874e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10056275129318237
dqn reward tensor(417.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0590e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16428326070308685
dqn reward tensor(437.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8275e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0947904884815216
dqn reward tensor(510., device='cuda:0') e 0.05 loss_dqn tensor(7.5932e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673876941204071
dqn reward tensor(494.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7617e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25480130314826965
dqn reward tensor(271., device='cuda:0') e 0.05 loss_dqn tensor(7.6780e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09841640293598175
dqn reward tensor(363., device='cuda:0') e 0.05 loss_dqn tensor(4.2516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08191840350627899
dqn reward tensor(197.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9770e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14905887842178345
dqn reward tensor(341.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0690e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06500392407178879
dqn reward tensor(396.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.1249e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1645687222480774
dqn reward tensor(310., device='cuda:0') e 0.05 loss_dqn tensor(3.7888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08476369827985764
dqn reward tensor(214.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3275e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24184957146644592
dqn reward tensor(414.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12673276662826538
dqn reward tensor(392.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.9440e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11712361872196198
dqn reward tensor(548.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3259e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12257813662290573
dqn reward tensor(295.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7274e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23852580785751343
dqn reward tensor(450.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5516e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18190737068653107
dqn reward tensor(388.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4259e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14440393447875977
dqn reward tensor(204.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8446e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02917657233774662
dqn reward tensor(279.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9899e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05108550935983658
dqn reward tensor(384.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12102466821670532
dqn reward tensor(327.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4770e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21354804933071136
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3399e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25010913610458374
dqn reward tensor(313.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.6443e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23728308081626892
dqn reward tensor(394.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5313e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1315632462501526
dqn reward tensor(345.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9493e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11371578276157379
dqn reward tensor(399., device='cuda:0') e 0.05 loss_dqn tensor(7.2278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07707935571670532
dqn reward tensor(445.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0420e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13798868656158447
dqn reward tensor(313.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.5509e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18589374423027039
dqn reward tensor(311., device='cuda:0') e 0.05 loss_dqn tensor(2.5089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04819577932357788
dqn reward tensor(592.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1239e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16667985916137695
dqn reward tensor(315.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0252e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08358445018529892
dqn reward tensor(365., device='cuda:0') e 0.05 loss_dqn tensor(7.0786e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07920363545417786
dqn reward tensor(421., device='cuda:0') e 0.05 loss_dqn tensor(9.9786e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21391363441944122
dqn reward tensor(361.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09161742031574249
dqn reward tensor(223.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12355928122997284
dqn reward tensor(280.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9000e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.096218541264534
dqn reward tensor(493.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02797689288854599
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7321e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15323799848556519
dqn reward tensor(406.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9768e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17082133889198303
dqn reward tensor(96.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1508813202381134
dqn reward tensor(557.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7868e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14940491318702698
dqn reward tensor(363.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3990e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10791604965925217
dqn reward tensor(507.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7338e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11834876239299774
dqn reward tensor(192.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0343e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15731605887413025
dqn reward tensor(448.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.8769e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20387524366378784
dqn reward tensor(26.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7597e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01390580739825964
Evaluating...
Train: {'rocauc': 0.7389303952859796} 6.318859100341797
=====Epoch 14=====
Training...
dqn reward tensor(346.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18412140011787415
dqn reward tensor(310., device='cuda:0') e 0.05 loss_dqn tensor(3.9461e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08980175107717514
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7069e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02912743017077446
dqn reward tensor(334.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07821786403656006
dqn reward tensor(242.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3744e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29630225896835327
dqn reward tensor(247., device='cuda:0') e 0.05 loss_dqn tensor(2.1707e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039369501173496246
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1937e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22151094675064087
dqn reward tensor(552.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07217255234718323
dqn reward tensor(306.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15555132925510406
dqn reward tensor(491.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9493e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19038492441177368
dqn reward tensor(202.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8581e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10684924572706223
dqn reward tensor(493.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.5106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23028002679347992
dqn reward tensor(462.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1077e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1787092238664627
dqn reward tensor(315.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8966e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1155652105808258
dqn reward tensor(516.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6271e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08344842493534088
dqn reward tensor(331.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09792117774486542
dqn reward tensor(297.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1203e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0590217262506485
dqn reward tensor(281.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.117939293384552
dqn reward tensor(567.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6759e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.220968097448349
dqn reward tensor(485.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.4953e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17323242127895355
dqn reward tensor(454.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0889e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18035778403282166
dqn reward tensor(653.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3652e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06085604056715965
dqn reward tensor(347.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08432184159755707
dqn reward tensor(238.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4042e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1047838032245636
dqn reward tensor(341.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6085e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11383216828107834
dqn reward tensor(471.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5614e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09350356459617615
dqn reward tensor(367.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0903e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1768094301223755
dqn reward tensor(326.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7513e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13389986753463745
dqn reward tensor(439.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9095e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07329824566841125
dqn reward tensor(310.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7934e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0886649414896965
dqn reward tensor(246.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09139704704284668
dqn reward tensor(425.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1078e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12677788734436035
dqn reward tensor(327.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9368e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020719442516565323
dqn reward tensor(360.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9813e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.133067324757576
dqn reward tensor(415.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8609e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22393518686294556
dqn reward tensor(524.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.2879e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1138727217912674
dqn reward tensor(317.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6405e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033825308084487915
dqn reward tensor(271.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7672e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09913209080696106
dqn reward tensor(317.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3785e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17934773862361908
dqn reward tensor(357.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6280e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20407429337501526
dqn reward tensor(455.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7211e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06964047253131866
dqn reward tensor(530.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5442e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19223624467849731
dqn reward tensor(464.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08657276630401611
dqn reward tensor(413.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3757e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10798617452383041
dqn reward tensor(570.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3657e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031663328409194946
dqn reward tensor(459.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5382e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05115887150168419
dqn reward tensor(424.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10113172978162766
dqn reward tensor(254.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3839e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29059797525405884
dqn reward tensor(346.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.5491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2328384816646576
dqn reward tensor(451.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2414e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2006809562444687
dqn reward tensor(312.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1328e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05389340594410896
dqn reward tensor(419.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7260e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3018440008163452
dqn reward tensor(442.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22741654515266418
dqn reward tensor(252.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.8212e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18747563660144806
dqn reward tensor(439.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3770e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19450679421424866
dqn reward tensor(438.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2178734540939331
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1027689278125763
dqn reward tensor(349., device='cuda:0') e 0.05 loss_dqn tensor(6.5209e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13715681433677673
dqn reward tensor(299.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4953e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0960938036441803
dqn reward tensor(464.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3015e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15783917903900146
dqn reward tensor(386.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1598e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13691827654838562
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1293e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0830601155757904
dqn reward tensor(326.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0234e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22586891055107117
dqn reward tensor(668.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6824e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23415730893611908
dqn reward tensor(392.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8987e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10329508781433105
dqn reward tensor(521.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1430e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24847456812858582
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9676e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1713685691356659
dqn reward tensor(476.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.1957e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12696342170238495
dqn reward tensor(569.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024921651929616928
dqn reward tensor(430.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4465e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22228217124938965
dqn reward tensor(533.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2795e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07574563473463058
dqn reward tensor(542.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8056e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09683861583471298
dqn reward tensor(297.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2544e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23443803191184998
dqn reward tensor(458.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7803e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17874550819396973
dqn reward tensor(468.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1305e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3470713794231415
dqn reward tensor(723.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0109e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07379337400197983
dqn reward tensor(362.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7543e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03444445878267288
dqn reward tensor(521.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1448497623205185
dqn reward tensor(528.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1511e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07704411447048187
dqn reward tensor(334., device='cuda:0') e 0.05 loss_dqn tensor(1.7924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2764218747615814
dqn reward tensor(323.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09646627306938171
dqn reward tensor(455.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8389e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1738838404417038
dqn reward tensor(551.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2353e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1331285536289215
dqn reward tensor(371.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2695e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10445377230644226
dqn reward tensor(364.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.8105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21186694502830505
dqn reward tensor(448., device='cuda:0') e 0.05 loss_dqn tensor(1.8143e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07707494497299194
dqn reward tensor(313.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23371025919914246
dqn reward tensor(522.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8694e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22441938519477844
dqn reward tensor(327.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2889e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26768577098846436
dqn reward tensor(442.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0992e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12067428976297379
dqn reward tensor(470.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.9478e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19478873908519745
dqn reward tensor(472.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1580e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1841558963060379
dqn reward tensor(509.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8065e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21608522534370422
dqn reward tensor(492.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2658e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14770004153251648
dqn reward tensor(344.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9975e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1456334888935089
dqn reward tensor(338.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3226e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041230328381061554
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2898e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07734624296426773
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7612e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13575732707977295
dqn reward tensor(615.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9586e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08334317803382874
dqn reward tensor(471.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3396e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09232649207115173
dqn reward tensor(457.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2163e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15148255228996277
dqn reward tensor(551.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2038e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019578447565436363
dqn reward tensor(527.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5891e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08264229446649551
dqn reward tensor(575.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5909e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20842930674552917
dqn reward tensor(271.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6370e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19914811849594116
dqn reward tensor(508.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1689e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10129067301750183
dqn reward tensor(535.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7777e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17664502561092377
dqn reward tensor(415.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8414e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14841178059577942
dqn reward tensor(334., device='cuda:0') e 0.05 loss_dqn tensor(7.9441e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1701701283454895
dqn reward tensor(504.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.4946e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13526153564453125
dqn reward tensor(430.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0363e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07829824090003967
dqn reward tensor(615., device='cuda:0') e 0.05 loss_dqn tensor(1.1796e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041158758103847504
dqn reward tensor(460.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8697e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07288986444473267
dqn reward tensor(435.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4633e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1600358784198761
dqn reward tensor(482.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0711e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18122266232967377
dqn reward tensor(354.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9612e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09739804267883301
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.165015310049057
dqn reward tensor(461.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413889169692993
dqn reward tensor(556., device='cuda:0') e 0.05 loss_dqn tensor(9.3101e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10933361947536469
dqn reward tensor(618.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7675e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13587835431098938
dqn reward tensor(477.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3366e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10947859287261963
dqn reward tensor(577.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2909e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19514794647693634
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3240e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06318211555480957
dqn reward tensor(512.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18382830917835236
dqn reward tensor(427.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20114508271217346
dqn reward tensor(530.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3990e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1270926296710968
dqn reward tensor(444.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.8132e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14552424848079681
dqn reward tensor(663.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7379e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22518321871757507
dqn reward tensor(506.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7288e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10905621945858002
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2655e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0995909571647644
dqn reward tensor(488.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1084e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0669323056936264
dqn reward tensor(468.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1689179688692093
dqn reward tensor(595.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1708e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11712420731782913
dqn reward tensor(401.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.3683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1416892409324646
dqn reward tensor(506.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2829e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10491134971380234
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18817032873630524
dqn reward tensor(531.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0553e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08671683073043823
dqn reward tensor(521.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8612e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35335099697113037
dqn reward tensor(703.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2863e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22076784074306488
dqn reward tensor(515.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0047e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037078507244586945
dqn reward tensor(640.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0143e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045299094170331955
dqn reward tensor(494.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4110e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1858437955379486
dqn reward tensor(520.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0044e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08006863296031952
dqn reward tensor(656.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7474e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0839083194732666
dqn reward tensor(378.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.6991e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12850543856620789
dqn reward tensor(512.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17448857426643372
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1825026273727417
dqn reward tensor(394.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0895e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07023301720619202
dqn reward tensor(636.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1690666526556015
dqn reward tensor(468.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2164e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21059615910053253
dqn reward tensor(601.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09187709540128708
dqn reward tensor(538.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0801e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905519813299179
dqn reward tensor(588.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3230e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1734771430492401
dqn reward tensor(447.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.6606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08250933885574341
dqn reward tensor(481.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9592e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24875351786613464
dqn reward tensor(600.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9324e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24655744433403015
dqn reward tensor(505.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4308e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13341741263866425
dqn reward tensor(419.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0301e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07346466928720474
dqn reward tensor(383.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21761217713356018
dqn reward tensor(606.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.5904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0842345803976059
dqn reward tensor(585.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3044e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051660023629665375
dqn reward tensor(498.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.2863e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32056161761283875
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06396674364805222
dqn reward tensor(446.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.1578e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621559113264084
dqn reward tensor(731.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6870e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2976006865501404
dqn reward tensor(532.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19798442721366882
dqn reward tensor(456.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0715e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1668800264596939
dqn reward tensor(607.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8809e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09833326935768127
dqn reward tensor(438.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8014e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0521734356880188
dqn reward tensor(563.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11947281658649445
dqn reward tensor(519.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4009e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20823030173778534
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13668298721313477
dqn reward tensor(635.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2419e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3025357723236084
dqn reward tensor(327.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14680717885494232
dqn reward tensor(562.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0688e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28104841709136963
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3395e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18131522834300995
dqn reward tensor(671.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0574e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13814039528369904
dqn reward tensor(303.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5896e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3101534843444824
dqn reward tensor(312.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2045629620552063
dqn reward tensor(589.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0882e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06627631187438965
dqn reward tensor(747.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0799e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23539024591445923
dqn reward tensor(503., device='cuda:0') e 0.05 loss_dqn tensor(8.4635e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1462935209274292
dqn reward tensor(568.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12309573590755463
dqn reward tensor(592.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1351e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0762421265244484
dqn reward tensor(603.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2228e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2610260248184204
dqn reward tensor(401.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.6023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08809320628643036
dqn reward tensor(506., device='cuda:0') e 0.05 loss_dqn tensor(1.1189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08236265182495117
dqn reward tensor(474., device='cuda:0') e 0.05 loss_dqn tensor(9.3043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25433605909347534
dqn reward tensor(594.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1752e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1709502935409546
dqn reward tensor(396.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11598111689090729
dqn reward tensor(538.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2654e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10731014609336853
dqn reward tensor(698.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1955e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030360328033566475
dqn reward tensor(574.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.8899e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16004537045955658
dqn reward tensor(456.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14199025928974152
dqn reward tensor(614.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2330e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18992310762405396
dqn reward tensor(680.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1807744801044464
dqn reward tensor(582.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8375e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16231262683868408
dqn reward tensor(609.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2000e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11223551630973816
dqn reward tensor(626.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11930853128433228
dqn reward tensor(472.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3522e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07832635939121246
dqn reward tensor(634.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.4663e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2037999927997589
dqn reward tensor(734.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1526564657688141
dqn reward tensor(587.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11742118000984192
dqn reward tensor(222.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3558e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14412620663642883
dqn reward tensor(538.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.0638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10657425224781036
dqn reward tensor(664.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24273018538951874
dqn reward tensor(413.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9286e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11183995753526688
dqn reward tensor(618.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2323e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09501102566719055
dqn reward tensor(567.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2665e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14094078540802002
dqn reward tensor(465.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3531e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0734376609325409
dqn reward tensor(727.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2032e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2473684549331665
dqn reward tensor(553.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6537e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09964805841445923
dqn reward tensor(430.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24414250254631042
dqn reward tensor(586.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04062196984887123
dqn reward tensor(554.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17148783802986145
dqn reward tensor(457.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.2993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1972687840461731
dqn reward tensor(593.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03280763328075409
dqn reward tensor(616.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20388735830783844
dqn reward tensor(473.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10613861680030823
dqn reward tensor(658.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029988333582878113
dqn reward tensor(606.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2370e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08968640118837357
dqn reward tensor(477.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5037e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029029734432697296
dqn reward tensor(575.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21332791447639465
dqn reward tensor(637.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0429e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17431771755218506
dqn reward tensor(562.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2862e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14980636537075043
dqn reward tensor(482.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33500954508781433
dqn reward tensor(491.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15920385718345642
dqn reward tensor(609.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09576293081045151
dqn reward tensor(522.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2632e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19955822825431824
dqn reward tensor(546.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1054e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.204979807138443
dqn reward tensor(289.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3433e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17058506608009338
dqn reward tensor(605.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2141e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20687036216259003
dqn reward tensor(572.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17868366837501526
dqn reward tensor(526.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15028679370880127
dqn reward tensor(550.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0402468740940094
dqn reward tensor(544.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3053e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14087776839733124
dqn reward tensor(613.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16840937733650208
dqn reward tensor(484.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.5334e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19411811232566833
dqn reward tensor(495.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1086e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10627374053001404
dqn reward tensor(620.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1701e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14453120529651642
dqn reward tensor(627.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.1113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905541867017746
dqn reward tensor(558., device='cuda:0') e 0.05 loss_dqn tensor(4.5913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12805470824241638
dqn reward tensor(697.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1878e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14728355407714844
dqn reward tensor(580.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5896e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1689603328704834
dqn reward tensor(538.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1358e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07011845707893372
dqn reward tensor(541.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7685e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03732236474752426
dqn reward tensor(611.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1897e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12379273772239685
dqn reward tensor(583.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10740381479263306
dqn reward tensor(698.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03892353177070618
dqn reward tensor(543.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08813329041004181
dqn reward tensor(643.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2241e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1764969378709793
dqn reward tensor(535.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1139e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1887519806623459
dqn reward tensor(560., device='cuda:0') e 0.05 loss_dqn tensor(2.3913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12339723855257034
dqn reward tensor(350.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5041e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19601841270923615
dqn reward tensor(644.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017821980640292168
dqn reward tensor(519., device='cuda:0') e 0.05 loss_dqn tensor(5.4511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06546886265277863
dqn reward tensor(674.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1153e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10387128591537476
dqn reward tensor(632., device='cuda:0') e 0.05 loss_dqn tensor(1.2407e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1487990915775299
dqn reward tensor(575.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3195e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24095235764980316
dqn reward tensor(442.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1819e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17648151516914368
dqn reward tensor(625.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2467e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14284472167491913
dqn reward tensor(618.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10859264433383942
dqn reward tensor(481.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07677917182445526
dqn reward tensor(547.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18836361169815063
dqn reward tensor(507.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4391e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1183943897485733
dqn reward tensor(740.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0783e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043860841542482376
dqn reward tensor(555.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10935617983341217
dqn reward tensor(562.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1452e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07276999205350876
dqn reward tensor(578.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2071e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029037868604063988
dqn reward tensor(558.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.166652113199234
dqn reward tensor(671.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12599147856235504
dqn reward tensor(527.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5291e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13832230865955353
dqn reward tensor(535.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4761e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11869868636131287
dqn reward tensor(550.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18303048610687256
dqn reward tensor(508., device='cuda:0') e 0.05 loss_dqn tensor(1.2344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03813521936535835
dqn reward tensor(554.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18245521187782288
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4216e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20520690083503723
dqn reward tensor(623., device='cuda:0') e 0.05 loss_dqn tensor(2.3890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22491534054279327
dqn reward tensor(617.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02402062900364399
dqn reward tensor(636.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2432e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1909075826406479
dqn reward tensor(657.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1538e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12253355979919434
dqn reward tensor(579.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11355607211589813
dqn reward tensor(709.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15240418910980225
dqn reward tensor(547.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3277e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04732147976756096
dqn reward tensor(696.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08305013924837112
dqn reward tensor(568., device='cuda:0') e 0.05 loss_dqn tensor(1.1128e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037923604249954224
dqn reward tensor(613.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2014e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16816860437393188
dqn reward tensor(391.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5517e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11461352556943893
dqn reward tensor(633.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1086e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16917544603347778
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6078e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0823928713798523
dqn reward tensor(609.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10467881709337234
dqn reward tensor(589.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1659676432609558
dqn reward tensor(726.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2253173291683197
dqn reward tensor(531., device='cuda:0') e 0.05 loss_dqn tensor(2.6073e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20038262009620667
dqn reward tensor(425.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12904027104377747
dqn reward tensor(405., device='cuda:0') e 0.05 loss_dqn tensor(1.4761e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052809201180934906
dqn reward tensor(604.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1472744345664978
dqn reward tensor(421.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1662e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.242184579372406
dqn reward tensor(437.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08001741766929626
dqn reward tensor(487.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0803e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18481415510177612
dqn reward tensor(102.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11504420638084412
dqn reward tensor(194.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0671e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10016734898090363
dqn reward tensor(217.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0040e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16232457756996155
dqn reward tensor(319.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17970317602157593
dqn reward tensor(-77.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1834e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22921553254127502
dqn reward tensor(30.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3071e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06155876815319061
dqn reward tensor(352., device='cuda:0') e 0.05 loss_dqn tensor(1.0437e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08360555768013
dqn reward tensor(144.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5515e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16009029746055603
dqn reward tensor(249.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20499634742736816
dqn reward tensor(217.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0772e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18513937294483185
dqn reward tensor(212.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0670e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1055811196565628
dqn reward tensor(249.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17182807624340057
dqn reward tensor(163.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14226919412612915
dqn reward tensor(318.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9000e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.39024001359939575
dqn reward tensor(170.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0010e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1323363482952118
dqn reward tensor(251.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9633e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08496221899986267
dqn reward tensor(429.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5719e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06203392893075943
dqn reward tensor(222.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8445e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10165854543447495
dqn reward tensor(140.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8411e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17501866817474365
dqn reward tensor(278.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4825e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1126399040222168
dqn reward tensor(-26.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4145e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16536931693553925
dqn reward tensor(86.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0401e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14605101943016052
dqn reward tensor(79.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0601e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16663441061973572
dqn reward tensor(290.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5370e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1818072646856308
dqn reward tensor(170., device='cuda:0') e 0.05 loss_dqn tensor(2.6760e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2060953676700592
dqn reward tensor(60.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4399e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10929238796234131
dqn reward tensor(70.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0524e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09691041707992554
dqn reward tensor(303.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04130140319466591
dqn reward tensor(93.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7662e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15183940529823303
dqn reward tensor(180.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0059e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02949642762541771
dqn reward tensor(-46.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1216e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16501066088676453
dqn reward tensor(-11., device='cuda:0') e 0.05 loss_dqn tensor(9.9950e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06788957118988037
dqn reward tensor(-154.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16584238409996033
dqn reward tensor(275.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8013e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16318778693675995
dqn reward tensor(430.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7857e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07139639556407928
dqn reward tensor(245.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15519866347312927
dqn reward tensor(-73.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0717e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2731495201587677
dqn reward tensor(130.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0287e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1498233675956726
dqn reward tensor(77.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24997004866600037
dqn reward tensor(197.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0013e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022066334262490273
dqn reward tensor(166.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6391e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1547565460205078
dqn reward tensor(46.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8226e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03317369148135185
dqn reward tensor(187.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5499e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0773615837097168
dqn reward tensor(58.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6499e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18501216173171997
dqn reward tensor(182.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3257e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1263563185930252
dqn reward tensor(182.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941080391407013
dqn reward tensor(146.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8288e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1723930537700653
dqn reward tensor(259.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04048123210668564
dqn reward tensor(234.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11405656486749649
dqn reward tensor(56., device='cuda:0') e 0.05 loss_dqn tensor(1.2465e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19007417559623718
dqn reward tensor(397.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0017e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14947509765625
dqn reward tensor(316., device='cuda:0') e 0.05 loss_dqn tensor(1.0139e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15899674594402313
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06370265036821365
dqn reward tensor(305.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.6624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2624427378177643
dqn reward tensor(118.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.3168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14349889755249023
dqn reward tensor(76.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0443e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15310868620872498
dqn reward tensor(266.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0596e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25206810235977173
dqn reward tensor(50.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1062e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11917991936206818
dqn reward tensor(84.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0600e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05394850671291351
dqn reward tensor(36.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3606841564178467
dqn reward tensor(120.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10387638956308365
dqn reward tensor(128.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0799e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054961271584033966
dqn reward tensor(40.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0538e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1055544838309288
dqn reward tensor(120.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0266e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13453267514705658
dqn reward tensor(131.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13314825296401978
dqn reward tensor(171., device='cuda:0') e 0.05 loss_dqn tensor(4.9194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27635666728019714
dqn reward tensor(237., device='cuda:0') e 0.05 loss_dqn tensor(1.0982e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05261673778295517
dqn reward tensor(146.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1242e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15025265514850616
dqn reward tensor(148.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1163e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03317982330918312
dqn reward tensor(133.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13807426393032074
dqn reward tensor(167.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04378741234540939
dqn reward tensor(215.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3454e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20367863774299622
dqn reward tensor(-57.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15395988523960114
dqn reward tensor(-0.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.7405e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1580812931060791
dqn reward tensor(193., device='cuda:0') e 0.05 loss_dqn tensor(1.1048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15465183556079865
dqn reward tensor(187.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1397569477558136
dqn reward tensor(140.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0484672486782074
dqn reward tensor(278., device='cuda:0') e 0.05 loss_dqn tensor(2.4165e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09242293238639832
dqn reward tensor(159.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14994345605373383
dqn reward tensor(199.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1654e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07566414773464203
dqn reward tensor(196.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09467726200819016
dqn reward tensor(-3., device='cuda:0') e 0.05 loss_dqn tensor(1.3044e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02871781773865223
dqn reward tensor(254.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1595475822687149
dqn reward tensor(165.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3869e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13642697036266327
dqn reward tensor(191.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7221e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12534210085868835
dqn reward tensor(77.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22647076845169067
dqn reward tensor(249.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0749363973736763
dqn reward tensor(402., device='cuda:0') e 0.05 loss_dqn tensor(8.0894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05309613049030304
dqn reward tensor(287.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2594793140888214
dqn reward tensor(376.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06875598430633545
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1841e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16255564987659454
dqn reward tensor(157.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08106210827827454
dqn reward tensor(82.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2200e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14708101749420166
dqn reward tensor(48., device='cuda:0') e 0.05 loss_dqn tensor(1.5417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11002625524997711
dqn reward tensor(237.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08999813348054886
dqn reward tensor(114.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07967434823513031
dqn reward tensor(112.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07919475436210632
dqn reward tensor(116.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15869146585464478
dqn reward tensor(212.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.2364e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07613002508878708
dqn reward tensor(416.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.6652e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791051983833313
dqn reward tensor(95.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20575088262557983
dqn reward tensor(277.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2531e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13700556755065918
dqn reward tensor(319.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2398e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12836897373199463
dqn reward tensor(251.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13203099370002747
dqn reward tensor(371.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1047181636095047
dqn reward tensor(-9.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.0308e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11175984144210815
dqn reward tensor(449.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8604e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12948492169380188
dqn reward tensor(161.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12336321920156479
dqn reward tensor(329.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1188e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2093982994556427
dqn reward tensor(346.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2197e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14246350526809692
dqn reward tensor(172.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.6283e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10894438624382019
dqn reward tensor(27.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4165e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10244470834732056
dqn reward tensor(349.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13327768445014954
dqn reward tensor(104.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6921e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09079864621162415
dqn reward tensor(152.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.1777e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0456099770963192
dqn reward tensor(138.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1401e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07212810218334198
dqn reward tensor(306., device='cuda:0') e 0.05 loss_dqn tensor(1.1902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08272652328014374
dqn reward tensor(286.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9668e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1277066320180893
dqn reward tensor(315., device='cuda:0') e 0.05 loss_dqn tensor(1.2470e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24807900190353394
dqn reward tensor(195.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22189714014530182
dqn reward tensor(150.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08925256133079529
dqn reward tensor(227.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15470196306705475
dqn reward tensor(74.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2569e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12076069414615631
dqn reward tensor(175., device='cuda:0') e 0.05 loss_dqn tensor(5.0550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2480594962835312
dqn reward tensor(365.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25184959173202515
dqn reward tensor(367.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3649e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09614576399326324
dqn reward tensor(213.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11129512637853622
dqn reward tensor(100.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2880e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1158236637711525
dqn reward tensor(138.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8338e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10764533281326294
dqn reward tensor(360.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18256144225597382
dqn reward tensor(387.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2489580512046814
dqn reward tensor(210.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2348e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12454962730407715
dqn reward tensor(194.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13929709792137146
dqn reward tensor(578.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2413e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05613616108894348
dqn reward tensor(263.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16458077728748322
dqn reward tensor(177.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3334e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18843555450439453
dqn reward tensor(404.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.4716e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05282624438405037
dqn reward tensor(496.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8597e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19802533090114594
dqn reward tensor(382., device='cuda:0') e 0.05 loss_dqn tensor(1.4347e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30006635189056396
dqn reward tensor(550.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4128e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09772483259439468
dqn reward tensor(419.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3302e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031133249402046204
dqn reward tensor(411.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15273216366767883
dqn reward tensor(348.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8701e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21648168563842773
dqn reward tensor(332.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3327e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06498617678880692
dqn reward tensor(571.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.147072896361351
dqn reward tensor(400.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5886e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02164529636502266
dqn reward tensor(614.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4263e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04347866773605347
dqn reward tensor(229.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10095500200986862
dqn reward tensor(459.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1271367073059082
dqn reward tensor(433.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5147e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07954095304012299
dqn reward tensor(267.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23308998346328735
dqn reward tensor(453.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18696299195289612
dqn reward tensor(289.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0214e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22925636172294617
dqn reward tensor(483.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13113722205162048
dqn reward tensor(396., device='cuda:0') e 0.05 loss_dqn tensor(1.5860e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09771645069122314
dqn reward tensor(549., device='cuda:0') e 0.05 loss_dqn tensor(3.7346e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04264744743704796
dqn reward tensor(345.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17987088859081268
dqn reward tensor(556.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11548475921154022
dqn reward tensor(467., device='cuda:0') e 0.05 loss_dqn tensor(3.7166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03778962045907974
dqn reward tensor(281., device='cuda:0') e 0.05 loss_dqn tensor(4.1444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13243603706359863
dqn reward tensor(590.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5950e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2351018488407135
dqn reward tensor(400.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5167e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10265029221773148
dqn reward tensor(468., device='cuda:0') e 0.05 loss_dqn tensor(2.8067e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15121325850486755
dqn reward tensor(288., device='cuda:0') e 0.05 loss_dqn tensor(4.0135e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09163901209831238
dqn reward tensor(550.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2778708338737488
dqn reward tensor(434.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5879e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10800620168447495
dqn reward tensor(481.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5662e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13119231164455414
dqn reward tensor(477.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0677594542503357
dqn reward tensor(522.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8619e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16622710227966309
dqn reward tensor(394.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0404e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19975179433822632
dqn reward tensor(345.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0889466404914856
dqn reward tensor(556., device='cuda:0') e 0.05 loss_dqn tensor(1.4605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08795328438282013
dqn reward tensor(443.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2935e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1355806142091751
dqn reward tensor(466.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12205443531274796
dqn reward tensor(550.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11722306162118912
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5610e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1189972534775734
dqn reward tensor(334.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026248445734381676
dqn reward tensor(589.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4413e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2514350712299347
dqn reward tensor(568.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02005953900516033
dqn reward tensor(576.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12228965759277344
dqn reward tensor(414.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1745910942554474
dqn reward tensor(449.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12794049084186554
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9658e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21504917740821838
dqn reward tensor(566.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07593391835689545
dqn reward tensor(447.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6111e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08454674482345581
dqn reward tensor(636.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7858e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2044738084077835
dqn reward tensor(551.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.8995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09091802686452866
dqn reward tensor(437.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6701e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07662659883499146
dqn reward tensor(535.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5045e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2353440672159195
dqn reward tensor(279.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4660e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03841965273022652
dqn reward tensor(439.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.2739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056230172514915466
dqn reward tensor(473.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5773e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07437731325626373
dqn reward tensor(515.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3270e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18293794989585876
dqn reward tensor(507.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10344748944044113
dqn reward tensor(431.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5673e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16026528179645538
dqn reward tensor(446.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1257e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19059810042381287
dqn reward tensor(518.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21967534720897675
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1326722502708435
dqn reward tensor(573.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13095039129257202
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791848674416542
dqn reward tensor(547.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07927979528903961
dqn reward tensor(555.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2678259611129761
dqn reward tensor(342.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14495328068733215
dqn reward tensor(259.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07415896654129028
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6289e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1052725613117218
dqn reward tensor(351.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.5634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09153512865304947
dqn reward tensor(362., device='cuda:0') e 0.05 loss_dqn tensor(1.7076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1337599903345108
dqn reward tensor(329.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5482e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14246346056461334
dqn reward tensor(476.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2272334098815918
dqn reward tensor(312.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3611e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14389237761497498
dqn reward tensor(125.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.9947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1599912941455841
dqn reward tensor(269.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6654e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13111698627471924
dqn reward tensor(621.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10989421606063843
dqn reward tensor(520.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08506038039922714
dqn reward tensor(56.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1343e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019207026809453964
Evaluating...
Train: {'rocauc': 0.7536069881522058} 6.995490550994873
=====Epoch 15=====
Training...
dqn reward tensor(647.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08393411338329315
dqn reward tensor(471.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.263398140668869
dqn reward tensor(445.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02477041631937027
dqn reward tensor(643., device='cuda:0') e 0.05 loss_dqn tensor(1.5650e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2920008599758148
dqn reward tensor(568.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20219723880290985
dqn reward tensor(437.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6361e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2436639666557312
dqn reward tensor(358., device='cuda:0') e 0.05 loss_dqn tensor(1.6539e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09117046743631363
dqn reward tensor(271., device='cuda:0') e 0.05 loss_dqn tensor(2.1755e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1923186033964157
dqn reward tensor(670.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5226e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1987839937210083
dqn reward tensor(474., device='cuda:0') e 0.05 loss_dqn tensor(4.2281e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14771656692028046
dqn reward tensor(350.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5920e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12312372028827667
dqn reward tensor(386.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20202021300792694
dqn reward tensor(336.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06915546953678131
dqn reward tensor(543.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3186e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049770139157772064
dqn reward tensor(536.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5011e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04139067977666855
dqn reward tensor(372.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6061e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11783185601234436
dqn reward tensor(388.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9403e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11961472034454346
dqn reward tensor(346.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3421e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17866159975528717
dqn reward tensor(593.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13778966665267944
dqn reward tensor(428.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20430122315883636
dqn reward tensor(495.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4805e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08377852290868759
dqn reward tensor(538.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10472649335861206
dqn reward tensor(557., device='cuda:0') e 0.05 loss_dqn tensor(1.5621e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11712859570980072
dqn reward tensor(483.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17808160185813904
dqn reward tensor(448.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1935e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13593070209026337
dqn reward tensor(556.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4572e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3152094781398773
dqn reward tensor(214.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8891e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031042154878377914
dqn reward tensor(408.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6818e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06751612573862076
dqn reward tensor(279.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.2501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1424013376235962
dqn reward tensor(419.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4796e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18364274501800537
dqn reward tensor(343.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.0032e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11972257494926453
dqn reward tensor(6.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08417294919490814
dqn reward tensor(288.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0564e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15734383463859558
dqn reward tensor(319.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8109e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08927223086357117
dqn reward tensor(342.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4014e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1897606998682022
dqn reward tensor(363.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5195e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16476808488368988
dqn reward tensor(476.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2747458219528198
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23380360007286072
dqn reward tensor(391.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643475443124771
dqn reward tensor(441.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08563075214624405
dqn reward tensor(381.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5672e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16672292351722717
dqn reward tensor(432.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8033e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13798432052135468
dqn reward tensor(605.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3582e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09700853377580643
dqn reward tensor(144.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5540e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20077353715896606
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5413e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06023643910884857
dqn reward tensor(365.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5717e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11338929831981659
dqn reward tensor(540.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5063e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14970344305038452
dqn reward tensor(195.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9759e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11655934154987335
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488652378320694
dqn reward tensor(412.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11336275935173035
dqn reward tensor(567.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34445497393608093
dqn reward tensor(510.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28763169050216675
dqn reward tensor(371.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8451e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16083696484565735
dqn reward tensor(463.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03830132633447647
dqn reward tensor(267.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.7191e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12303002923727036
dqn reward tensor(282.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10005778819322586
dqn reward tensor(593.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20598919689655304
dqn reward tensor(465.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14877885580062866
dqn reward tensor(512.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20723457634449005
dqn reward tensor(347.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1239e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13907121121883392
dqn reward tensor(304.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5906e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05983144789934158
dqn reward tensor(548.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7730e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25873953104019165
dqn reward tensor(412.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1939699351787567
dqn reward tensor(436.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4752e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12577936053276062
dqn reward tensor(436.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5140e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10146615654230118
dqn reward tensor(160.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12448678910732269
dqn reward tensor(459.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14514966309070587
dqn reward tensor(185.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2414e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13319391012191772
dqn reward tensor(348.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07672733068466187
dqn reward tensor(324.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7664e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06424249708652496
dqn reward tensor(496.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0215e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2762501835823059
dqn reward tensor(386.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0149e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03257588669657707
dqn reward tensor(375., device='cuda:0') e 0.05 loss_dqn tensor(1.6842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03975360095500946
dqn reward tensor(587.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9590e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08206423372030258
dqn reward tensor(394.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08897910267114639
dqn reward tensor(400.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19141027331352234
dqn reward tensor(195.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6753e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20579460263252258
dqn reward tensor(191.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9463e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017520669847726822
dqn reward tensor(460.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9190e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14268828928470612
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4364e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12409517168998718
dqn reward tensor(446.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6174e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18800780177116394
dqn reward tensor(401.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9764e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1259303092956543
dqn reward tensor(38.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2195e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0999005138874054
dqn reward tensor(419.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.9236e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01872844249010086
dqn reward tensor(360.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19040781259536743
dqn reward tensor(355., device='cuda:0') e 0.05 loss_dqn tensor(8.4703e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08688342571258545
dqn reward tensor(321.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8028e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24487242102622986
dqn reward tensor(109.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13679826259613037
dqn reward tensor(555.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4869e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11869131028652191
dqn reward tensor(399.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9711e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16985538601875305
dqn reward tensor(250.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6125e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19449734687805176
dqn reward tensor(572.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0488e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047919753938913345
dqn reward tensor(280.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9319e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12549516558647156
dqn reward tensor(280.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3345e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1840655505657196
dqn reward tensor(370.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3094e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22347867488861084
dqn reward tensor(345.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3288e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14297184348106384
dqn reward tensor(306.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16902746260166168
dqn reward tensor(448.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0282e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16568279266357422
dqn reward tensor(359.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5999e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1378118097782135
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621595323085785
dqn reward tensor(341.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7860e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15089145302772522
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10582040250301361
dqn reward tensor(-60.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1998e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21724314987659454
dqn reward tensor(350., device='cuda:0') e 0.05 loss_dqn tensor(2.1688e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09675455093383789
dqn reward tensor(296.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3388e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24729543924331665
dqn reward tensor(180.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057149939239025116
dqn reward tensor(450.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3641e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13872885704040527
dqn reward tensor(204.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21301302313804626
dqn reward tensor(439.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4823e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07368078827857971
dqn reward tensor(405.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2099e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24200338125228882
dqn reward tensor(430.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4026e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22448600828647614
dqn reward tensor(311.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6269e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041738711297512054
dqn reward tensor(-31.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6274e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20656879246234894
dqn reward tensor(510.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04995341598987579
dqn reward tensor(371.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1591e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1189129576086998
dqn reward tensor(422., device='cuda:0') e 0.05 loss_dqn tensor(1.3277e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08638818562030792
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1912e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25777676701545715
dqn reward tensor(481.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0918e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02846096083521843
dqn reward tensor(409.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1556e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12201221287250519
dqn reward tensor(166.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08867384493350983
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4591e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07412461936473846
dqn reward tensor(43.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4806e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08628237247467041
dqn reward tensor(371.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6037e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15807321667671204
dqn reward tensor(223.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.8276e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10842372477054596
dqn reward tensor(372.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6465e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08219735324382782
dqn reward tensor(279.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3607e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16771836578845978
dqn reward tensor(337., device='cuda:0') e 0.05 loss_dqn tensor(8.5082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2629602551460266
dqn reward tensor(289.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018566370010375977
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0756579115986824
dqn reward tensor(280.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3457e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06599710881710052
dqn reward tensor(345.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20756566524505615
dqn reward tensor(328.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3229e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.175619438290596
dqn reward tensor(513.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8098e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1913861334323883
dqn reward tensor(398.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1388e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08035534620285034
dqn reward tensor(365., device='cuda:0') e 0.05 loss_dqn tensor(8.2541e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0500713586807251
dqn reward tensor(126., device='cuda:0') e 0.05 loss_dqn tensor(2.8755e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30745312571525574
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2801e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030752109363675117
dqn reward tensor(500.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3730e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10278050601482391
dqn reward tensor(128.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4619e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043648965656757355
dqn reward tensor(210.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23719194531440735
dqn reward tensor(323.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15220077335834503
dqn reward tensor(329.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10540200769901276
dqn reward tensor(63.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21831391751766205
dqn reward tensor(-11.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5269e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23091527819633484
dqn reward tensor(73.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1226e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10222633183002472
dqn reward tensor(432.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3397e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03922642022371292
dqn reward tensor(30.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1318398118019104
dqn reward tensor(237.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8463e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16646982729434967
dqn reward tensor(396.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5531e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21600927412509918
dqn reward tensor(366.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5282e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24127927422523499
dqn reward tensor(160.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9247e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17131102085113525
dqn reward tensor(288.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2597e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14113345742225647
dqn reward tensor(303.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5578e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04643835872411728
dqn reward tensor(301.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2564e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11530562490224838
dqn reward tensor(256.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12498361617326736
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21676373481750488
dqn reward tensor(284.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3481e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10198262333869934
dqn reward tensor(375.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6720e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06494216620922089
dqn reward tensor(58.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1309e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1908121407032013
dqn reward tensor(301.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2238e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22921855747699738
dqn reward tensor(311.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0696e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09957154095172882
dqn reward tensor(492.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7795e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053369395434856415
dqn reward tensor(405.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.2276e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08357900381088257
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1588e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026479121297597885
dqn reward tensor(435., device='cuda:0') e 0.05 loss_dqn tensor(7.8981e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11220641434192657
dqn reward tensor(307.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18248355388641357
dqn reward tensor(196., device='cuda:0') e 0.05 loss_dqn tensor(1.7683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07901342213153839
dqn reward tensor(311.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1209e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22526443004608154
dqn reward tensor(405.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2432e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2559052109718323
dqn reward tensor(334.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3829e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0683116540312767
dqn reward tensor(311.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2538e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16068986058235168
dqn reward tensor(-104.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11920107901096344
dqn reward tensor(222.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23682594299316406
dqn reward tensor(226.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3708e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10365907102823257
dqn reward tensor(438.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1630e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14632004499435425
dqn reward tensor(282.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5863e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1937941461801529
dqn reward tensor(289., device='cuda:0') e 0.05 loss_dqn tensor(1.2379e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05647774040699005
dqn reward tensor(479.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0424e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20137915015220642
dqn reward tensor(422.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1731235235929489
dqn reward tensor(422.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040464162826538086
dqn reward tensor(217., device='cuda:0') e 0.05 loss_dqn tensor(8.8119e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23871728777885437
dqn reward tensor(370.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0334e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12735113501548767
dqn reward tensor(617.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5837e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13063016533851624
dqn reward tensor(479.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7149e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09043823182582855
dqn reward tensor(441.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5677e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06988660991191864
dqn reward tensor(103.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5770e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2059706747531891
dqn reward tensor(339.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.9315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12345446646213531
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0908e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07202421873807907
dqn reward tensor(368.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.6931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2606292963027954
dqn reward tensor(298.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06931106746196747
dqn reward tensor(225., device='cuda:0') e 0.05 loss_dqn tensor(9.3049e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06700290739536285
dqn reward tensor(555.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3081e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22449788451194763
dqn reward tensor(449., device='cuda:0') e 0.05 loss_dqn tensor(8.3930e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16271349787712097
dqn reward tensor(469.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0858e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23440369963645935
dqn reward tensor(442.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8204e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15767234563827515
dqn reward tensor(257.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04315250739455223
dqn reward tensor(464.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0337970145046711
dqn reward tensor(220.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14608396589756012
dqn reward tensor(363.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.5183e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10236294567584991
dqn reward tensor(428.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3286e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0358240008354187
dqn reward tensor(229.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6429e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0949302390217781
dqn reward tensor(301., device='cuda:0') e 0.05 loss_dqn tensor(9.0056e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13048309087753296
dqn reward tensor(447.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0417e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08434273302555084
dqn reward tensor(204.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0068e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435057371854782
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6744e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020249169319868088
dqn reward tensor(451.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5542e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07650422304868698
dqn reward tensor(324.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0698e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22001869976520538
dqn reward tensor(460., device='cuda:0') e 0.05 loss_dqn tensor(8.8337e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12160097807645798
dqn reward tensor(167.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7873e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07431544363498688
dqn reward tensor(346.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3806e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10150139033794403
dqn reward tensor(296.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7717e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3195088505744934
dqn reward tensor(238.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05599243938922882
dqn reward tensor(285.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5220e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12243734300136566
dqn reward tensor(316., device='cuda:0') e 0.05 loss_dqn tensor(8.5231e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2748257517814636
dqn reward tensor(302.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9711e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1746421754360199
dqn reward tensor(435.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7426e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15957306325435638
dqn reward tensor(363.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9547e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09115247428417206
dqn reward tensor(454.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8516e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16204297542572021
dqn reward tensor(190.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08814354240894318
dqn reward tensor(479.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8797e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17094573378562927
dqn reward tensor(351.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0565e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10228419303894043
dqn reward tensor(400.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1888e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08226698637008667
dqn reward tensor(524.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1978e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12393946945667267
dqn reward tensor(446.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.8209e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.189432293176651
dqn reward tensor(-47.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2493e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13312099874019623
dqn reward tensor(314., device='cuda:0') e 0.05 loss_dqn tensor(2.8784e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055252935737371445
dqn reward tensor(413.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3498e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0992627739906311
dqn reward tensor(356.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5742e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13928788900375366
dqn reward tensor(252.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.2705e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08604113012552261
dqn reward tensor(304.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019846875220537186
dqn reward tensor(415.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0749339759349823
dqn reward tensor(528.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7176e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07287520915269852
dqn reward tensor(369., device='cuda:0') e 0.05 loss_dqn tensor(1.8570e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1512114256620407
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017873333767056465
dqn reward tensor(194.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12064263224601746
dqn reward tensor(116.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2903941571712494
dqn reward tensor(455.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1435e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024344999343156815
dqn reward tensor(482., device='cuda:0') e 0.05 loss_dqn tensor(8.3353e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2296462059020996
dqn reward tensor(410.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6519e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30386385321617126
dqn reward tensor(398.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.5598e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05983387678861618
dqn reward tensor(171.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.1672e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11618037521839142
dqn reward tensor(479.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4518e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04228620231151581
dqn reward tensor(417.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8620e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673787236213684
dqn reward tensor(201.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0487e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06351668387651443
dqn reward tensor(288.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5938e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19030824303627014
dqn reward tensor(389.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2355e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17790737748146057
dqn reward tensor(132.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13868162035942078
dqn reward tensor(188.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6769e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08240430802106857
dqn reward tensor(207.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08138187229633331
dqn reward tensor(457.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6586e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10099690407514572
dqn reward tensor(320.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1664e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051495883613824844
dqn reward tensor(186.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2261967658996582
dqn reward tensor(330.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2723234295845032
dqn reward tensor(165.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0831e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11965509504079819
dqn reward tensor(391.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.7674e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15035763382911682
dqn reward tensor(400.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8192e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08949041366577148
dqn reward tensor(220., device='cuda:0') e 0.05 loss_dqn tensor(1.4469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25235316157341003
dqn reward tensor(190.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9727e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09765082597732544
dqn reward tensor(378.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.2667e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18446093797683716
dqn reward tensor(177.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2703969478607178
dqn reward tensor(378.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2440e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15482133626937866
dqn reward tensor(266.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3601e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05908779799938202
dqn reward tensor(245., device='cuda:0') e 0.05 loss_dqn tensor(2.5301e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35577502846717834
dqn reward tensor(308.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7753e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1617087721824646
dqn reward tensor(269., device='cuda:0') e 0.05 loss_dqn tensor(8.1079e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20320174098014832
dqn reward tensor(535.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9565e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09005676954984665
dqn reward tensor(297.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09010088443756104
dqn reward tensor(319.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0529e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1370043158531189
dqn reward tensor(294.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5346e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30655407905578613
dqn reward tensor(309.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09222396463155746
dqn reward tensor(163.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7767e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09660329669713974
dqn reward tensor(231.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.7800e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13918901979923248
dqn reward tensor(248.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08908971399068832
dqn reward tensor(386.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4778e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15515807271003723
dqn reward tensor(372.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8761e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28233975172042847
dqn reward tensor(88.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6562e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03255346417427063
dqn reward tensor(295.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.5695e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2379617691040039
dqn reward tensor(353.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09008330851793289
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2722e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19182133674621582
dqn reward tensor(341.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7171e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3084801137447357
dqn reward tensor(415.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4572e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1368965059518814
dqn reward tensor(304.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7362e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11793441325426102
dqn reward tensor(240.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07291733473539352
dqn reward tensor(387.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4254e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18355639278888702
dqn reward tensor(316.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.7773e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15893913805484772
dqn reward tensor(270.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0796e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12330029904842377
dqn reward tensor(357.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9078e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09848891943693161
dqn reward tensor(373.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6155e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10648408532142639
dqn reward tensor(203.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1802e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05574312061071396
dqn reward tensor(177.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6741e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11073718219995499
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08158646523952484
dqn reward tensor(480.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0355e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26223844289779663
dqn reward tensor(172.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3552e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1228095144033432
dqn reward tensor(436., device='cuda:0') e 0.05 loss_dqn tensor(8.1563e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16288655996322632
dqn reward tensor(-82.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09255488216876984
dqn reward tensor(263.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4601e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054104141891002655
dqn reward tensor(324.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1781918853521347
dqn reward tensor(391., device='cuda:0') e 0.05 loss_dqn tensor(8.4888e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30340683460235596
dqn reward tensor(317.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6857e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22029772400856018
dqn reward tensor(232.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3208e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1472722738981247
dqn reward tensor(-61.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0766e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14961835741996765
dqn reward tensor(241., device='cuda:0') e 0.05 loss_dqn tensor(1.3061e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25041741132736206
dqn reward tensor(428.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7869e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05997620150446892
dqn reward tensor(262.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2644e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15292586386203766
dqn reward tensor(380.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5900e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061332233250141144
dqn reward tensor(376.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11535713821649551
dqn reward tensor(362.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0310e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051564279943704605
dqn reward tensor(345.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20413529872894287
dqn reward tensor(506.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.4012e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0881127417087555
dqn reward tensor(259.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2161e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.42764368653297424
dqn reward tensor(-60.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7411e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1761147677898407
dqn reward tensor(67.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1305e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07847128808498383
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(1.4562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019839473068714142
dqn reward tensor(272.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9765e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07961848378181458
dqn reward tensor(372.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5031e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08328627049922943
dqn reward tensor(429.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1761e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2617807388305664
dqn reward tensor(-1.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4633e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08355896919965744
dqn reward tensor(202.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2687e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10517523437738419
dqn reward tensor(169.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2072238028049469
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7742e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06389382481575012
dqn reward tensor(416.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15585000813007355
dqn reward tensor(134.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2335e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07946062833070755
dqn reward tensor(180.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17749102413654327
dqn reward tensor(190.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0172e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17291724681854248
dqn reward tensor(321.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7959e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12419185042381287
dqn reward tensor(85.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1667e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0961049497127533
dqn reward tensor(160.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3122e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09845687448978424
dqn reward tensor(230.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8269e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1341562271118164
dqn reward tensor(175.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0918387621641159
dqn reward tensor(88.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14655476808547974
dqn reward tensor(234.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6864e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10797540843486786
dqn reward tensor(408.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4335e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07738351076841354
dqn reward tensor(381., device='cuda:0') e 0.05 loss_dqn tensor(2.6871e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13684040307998657
dqn reward tensor(244.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1380e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603774130344391
dqn reward tensor(347.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.179748997092247
dqn reward tensor(262.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6320e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28488683700561523
dqn reward tensor(491., device='cuda:0') e 0.05 loss_dqn tensor(7.7150e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20134544372558594
dqn reward tensor(273.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2887e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09177680313587189
dqn reward tensor(12.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3543e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04375222697854042
dqn reward tensor(286.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0150e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10106112062931061
dqn reward tensor(288.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2574e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23080696165561676
dqn reward tensor(211.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4066e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14738693833351135
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2725e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02674262598156929
dqn reward tensor(283.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14047624170780182
dqn reward tensor(-140.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07338488847017288
dqn reward tensor(266.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1409636288881302
dqn reward tensor(219., device='cuda:0') e 0.05 loss_dqn tensor(2.4253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06194008141756058
dqn reward tensor(408.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0953e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10492561757564545
dqn reward tensor(304.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.5879e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27384087443351746
dqn reward tensor(180.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5424e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10263212025165558
dqn reward tensor(487.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2956460118293762
dqn reward tensor(315.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1761e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05522913485765457
dqn reward tensor(225.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20610597729682922
dqn reward tensor(401.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3937e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27410173416137695
dqn reward tensor(244.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7784e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14881840348243713
dqn reward tensor(258.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9491e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12021719664335251
dqn reward tensor(379.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6057e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22462289035320282
dqn reward tensor(395.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3946e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11443373560905457
dqn reward tensor(464.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8576e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09352582693099976
dqn reward tensor(289., device='cuda:0') e 0.05 loss_dqn tensor(1.2746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16069594025611877
dqn reward tensor(255.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9745e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10669097304344177
dqn reward tensor(313.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5817e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1546204388141632
dqn reward tensor(151.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4394e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07517943531274796
dqn reward tensor(-117.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11753678321838379
dqn reward tensor(256.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1129e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14802342653274536
dqn reward tensor(281., device='cuda:0') e 0.05 loss_dqn tensor(8.4838e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02892134338617325
dqn reward tensor(170.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9400e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15758386254310608
dqn reward tensor(314.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6315e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26395702362060547
dqn reward tensor(230.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12699364125728607
dqn reward tensor(347.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2776e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017998885363340378
dqn reward tensor(163.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1877e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10052557289600372
dqn reward tensor(410., device='cuda:0') e 0.05 loss_dqn tensor(8.1982e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10832788795232773
dqn reward tensor(-52.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5909e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02247615158557892
dqn reward tensor(335.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9289e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27748173475265503
dqn reward tensor(72.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023097682744264603
dqn reward tensor(-151.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37354612350463867
dqn reward tensor(194.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0569e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12781597673892975
dqn reward tensor(283.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5798e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05716399848461151
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7371e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16540580987930298
dqn reward tensor(370.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1928e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07146135717630386
dqn reward tensor(318.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.4670e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12028097361326218
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0815e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2593482434749603
dqn reward tensor(222.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10258300602436066
dqn reward tensor(340.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3689e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12731784582138062
dqn reward tensor(172.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5124e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08227510005235672
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9461e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10122308135032654
dqn reward tensor(180.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06507229804992676
dqn reward tensor(417.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0777e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10796482115983963
dqn reward tensor(36.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9055e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15827977657318115
dqn reward tensor(326.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8421e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1776946485042572
dqn reward tensor(190.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6998e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0722029060125351
dqn reward tensor(388.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0065e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12761327624320984
dqn reward tensor(233.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643211841583252
dqn reward tensor(285., device='cuda:0') e 0.05 loss_dqn tensor(8.3392e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2024115025997162
dqn reward tensor(475.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8333e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14788293838500977
dqn reward tensor(306.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1547e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06886442750692368
dqn reward tensor(263.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3308e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10591252148151398
dqn reward tensor(631., device='cuda:0') e 0.05 loss_dqn tensor(1.5589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24689334630966187
dqn reward tensor(333.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027140775695443153
dqn reward tensor(523.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3693e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24170534312725067
dqn reward tensor(447.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3362e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3220709264278412
dqn reward tensor(247.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7525e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17049472033977509
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8080e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0993725061416626
dqn reward tensor(414.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8838e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11848167330026627
dqn reward tensor(369.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1522e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21238483488559723
dqn reward tensor(330.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.2439e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12925150990486145
dqn reward tensor(254.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1525014340877533
dqn reward tensor(212.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6595e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17067766189575195
dqn reward tensor(352.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.0231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1677912324666977
dqn reward tensor(442.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3982e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17257580161094666
dqn reward tensor(208.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2070e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1789878010749817
dqn reward tensor(349.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22889301180839539
dqn reward tensor(297.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4534e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14637310802936554
dqn reward tensor(295.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9142e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2212315946817398
dqn reward tensor(460.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2179e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06396468728780746
dqn reward tensor(205.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5571e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15887632966041565
dqn reward tensor(329.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2546e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1834704875946045
dqn reward tensor(339.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9636e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13827046751976013
dqn reward tensor(314.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.0981e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1756017506122589
dqn reward tensor(403.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.2129e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03962361812591553
dqn reward tensor(464.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1364e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09954996407032013
dqn reward tensor(387.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0945e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2665586769580841
dqn reward tensor(244.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4428e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08120353519916534
dqn reward tensor(204.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09184910356998444
dqn reward tensor(272.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1959e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05568087100982666
dqn reward tensor(31.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22033169865608215
dqn reward tensor(108.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0969e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025738578289747238
dqn reward tensor(398.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07475552707910538
dqn reward tensor(473.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3870e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019137736409902573
dqn reward tensor(447., device='cuda:0') e 0.05 loss_dqn tensor(3.2468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09270332008600235
dqn reward tensor(466.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6161e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13785387575626373
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19898194074630737
dqn reward tensor(202.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.8389e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2849257290363312
dqn reward tensor(227., device='cuda:0') e 0.05 loss_dqn tensor(9.2356e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19659391045570374
dqn reward tensor(185.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2363872230052948
dqn reward tensor(149.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1528e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11639620363712311
dqn reward tensor(282.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6025e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1649198979139328
dqn reward tensor(264.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5013e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04641526937484741
dqn reward tensor(232., device='cuda:0') e 0.05 loss_dqn tensor(9.3941e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1563839316368103
dqn reward tensor(333.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6249e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0358259491622448
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7718e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03842427209019661
dqn reward tensor(-26.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16009879112243652
dqn reward tensor(122.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27382394671440125
dqn reward tensor(343., device='cuda:0') e 0.05 loss_dqn tensor(9.3523e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08959650993347168
dqn reward tensor(195.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1598110944032669
dqn reward tensor(498.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1181e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22103039920330048
dqn reward tensor(316.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3006e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06880529969930649
dqn reward tensor(258., device='cuda:0') e 0.05 loss_dqn tensor(2.8501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13087643682956696
dqn reward tensor(219.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0255e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04481329023838043
dqn reward tensor(201.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6942e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13698866963386536
dqn reward tensor(354.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16669684648513794
dqn reward tensor(369.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3820e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24977794289588928
dqn reward tensor(196.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5736e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08982119709253311
dqn reward tensor(309.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3493e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06311813741922379
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.106116384267807
dqn reward tensor(391.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4812e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.245610773563385
dqn reward tensor(237.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9650e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26663824915885925
dqn reward tensor(180., device='cuda:0') e 0.05 loss_dqn tensor(1.9004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20106855034828186
dqn reward tensor(320.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7082e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2944340705871582
dqn reward tensor(201.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23410382866859436
dqn reward tensor(361.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8365e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13541805744171143
dqn reward tensor(274.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.9024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08467395603656769
dqn reward tensor(466.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4990e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066872835159302
dqn reward tensor(225., device='cuda:0') e 0.05 loss_dqn tensor(2.2456e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1067403107881546
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2537e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09753119945526123
dqn reward tensor(309.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11586520075798035
dqn reward tensor(293.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8885e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11918846517801285
dqn reward tensor(251.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0840e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11020202189683914
dqn reward tensor(29., device='cuda:0') e 0.05 loss_dqn tensor(2.4496e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08959103375673294
dqn reward tensor(325.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13046295940876007
dqn reward tensor(211.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0470e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07603010535240173
dqn reward tensor(242.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.9278e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14040200412273407
dqn reward tensor(218., device='cuda:0') e 0.05 loss_dqn tensor(2.7032e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21810351312160492
dqn reward tensor(242.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21931716799736023
dqn reward tensor(151.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6125e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012609997764229774
dqn reward tensor(414.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8249e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1339428722858429
dqn reward tensor(296.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9063e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046502284705638885
dqn reward tensor(183.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3461e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14540430903434753
dqn reward tensor(237., device='cuda:0') e 0.05 loss_dqn tensor(3.8011e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0407254621386528
dqn reward tensor(360.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16661295294761658
dqn reward tensor(366.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0923e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09062914550304413
dqn reward tensor(353.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0368e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18051913380622864
dqn reward tensor(301., device='cuda:0') e 0.05 loss_dqn tensor(3.3329e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20326930284500122
dqn reward tensor(278.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05616539716720581
dqn reward tensor(255.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8503e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09038189053535461
dqn reward tensor(374.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7612e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08277662098407745
dqn reward tensor(-92.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4057e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060651641339063644
dqn reward tensor(144.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3604e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25021135807037354
dqn reward tensor(-27.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3990e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13983958959579468
dqn reward tensor(407.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6595e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11463774740695953
dqn reward tensor(311., device='cuda:0') e 0.05 loss_dqn tensor(2.8890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3008066713809967
dqn reward tensor(500.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2117e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11829370260238647
dqn reward tensor(389.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6373e+09, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10735997557640076
dqn reward tensor(329., device='cuda:0') e 0.05 loss_dqn tensor(2.0878e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10958787798881531
dqn reward tensor(401.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5684e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12718266248703003
dqn reward tensor(223.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10849189758300781
dqn reward tensor(212.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09780186414718628
dqn reward tensor(290.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14005115628242493
dqn reward tensor(332.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22782406210899353
dqn reward tensor(348.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0158e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18637210130691528
dqn reward tensor(258.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4927e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03859647363424301
dqn reward tensor(305.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13315072655677795
dqn reward tensor(395., device='cuda:0') e 0.05 loss_dqn tensor(1.0831e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0602206215262413
dqn reward tensor(240.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17560908198356628
dqn reward tensor(343.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12778854370117188
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9502e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03242640942335129
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(1.0481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06792247295379639
dqn reward tensor(406., device='cuda:0') e 0.05 loss_dqn tensor(3.0611e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621674746274948
dqn reward tensor(403.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.8603e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2391367405653
dqn reward tensor(344.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15209126472473145
dqn reward tensor(358.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1078e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10820664465427399
dqn reward tensor(355.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1422336995601654
dqn reward tensor(238.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.5022e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13949932157993317
dqn reward tensor(454.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0815e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14884021878242493
dqn reward tensor(38.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9473e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021154407411813736
Evaluating...
Train: {'rocauc': 0.7493842815273077} 5.102833271026611
=====Epoch 16=====
Training...
dqn reward tensor(403.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0793e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13537102937698364
dqn reward tensor(400.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09548857808113098
dqn reward tensor(346.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047777000814676285
dqn reward tensor(105.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4425e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28736811876296997
dqn reward tensor(342., device='cuda:0') e 0.05 loss_dqn tensor(1.1160e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14531081914901733
dqn reward tensor(295.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.175761878490448
dqn reward tensor(491.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0375e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15249386429786682
dqn reward tensor(107.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8873e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2133094221353531
dqn reward tensor(450.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0870e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08129406720399857
dqn reward tensor(307.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.2095e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03151266649365425
dqn reward tensor(327.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2463017702102661
dqn reward tensor(408.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.2506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04412573575973511
dqn reward tensor(224.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12883058190345764
dqn reward tensor(319.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15617290139198303
dqn reward tensor(449.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12361512333154678
dqn reward tensor(308.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.9367e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04611217975616455
dqn reward tensor(299.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.2272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3083825707435608
dqn reward tensor(303.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0834580808877945
dqn reward tensor(316.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6764e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07248406112194061
dqn reward tensor(170.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18605728447437286
dqn reward tensor(258.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1079e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14252792298793793
dqn reward tensor(413.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0691e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10347241163253784
dqn reward tensor(250.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1130e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16405485570430756
dqn reward tensor(390., device='cuda:0') e 0.05 loss_dqn tensor(1.4891e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16641025245189667
dqn reward tensor(232.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6930e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04971791058778763
dqn reward tensor(356.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09679239988327026
dqn reward tensor(357., device='cuda:0') e 0.05 loss_dqn tensor(1.0883e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21667182445526123
dqn reward tensor(480.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.096751369535923
dqn reward tensor(542.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09134970605373383
dqn reward tensor(349.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1727e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16672810912132263
dqn reward tensor(328.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14265848696231842
dqn reward tensor(395.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1171e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13424213230609894
dqn reward tensor(123.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30184176564216614
dqn reward tensor(168.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8090e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09523764997720718
dqn reward tensor(341.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0840e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10095800459384918
dqn reward tensor(201.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0688573494553566
dqn reward tensor(387.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19298313558101654
dqn reward tensor(360.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1096e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12201516330242157
dqn reward tensor(542.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0613e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07568276673555374
dqn reward tensor(258.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0685e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14641612768173218
dqn reward tensor(597.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32555103302001953
dqn reward tensor(393.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0607e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08484448492527008
dqn reward tensor(420.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12254878878593445
dqn reward tensor(537.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0572e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11528722941875458
dqn reward tensor(262.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10432757437229156
dqn reward tensor(353.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041390664875507355
dqn reward tensor(182., device='cuda:0') e 0.05 loss_dqn tensor(1.5034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10387124121189117
dqn reward tensor(222., device='cuda:0') e 0.05 loss_dqn tensor(2.3313e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19511741399765015
dqn reward tensor(250.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6497e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27021634578704834
dqn reward tensor(407.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.8670e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038275450468063354
dqn reward tensor(341.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0645e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27810966968536377
dqn reward tensor(257.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8022e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11950763314962387
dqn reward tensor(494.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23934891819953918
dqn reward tensor(213.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8248e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04078124836087227
dqn reward tensor(288.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1181e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1955573558807373
dqn reward tensor(394.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6466e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04547209292650223
dqn reward tensor(441.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07649219036102295
dqn reward tensor(407.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1063e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22322826087474823
dqn reward tensor(373.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19730493426322937
dqn reward tensor(310.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5628e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15746384859085083
dqn reward tensor(526.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0133e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18739858269691467
dqn reward tensor(382.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10026557743549347
dqn reward tensor(289.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09098031371831894
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0507e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20355170965194702
dqn reward tensor(424.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1629500389099121
dqn reward tensor(378.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28244417905807495
dqn reward tensor(248.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08520033955574036
dqn reward tensor(161.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9056e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11369140446186066
dqn reward tensor(518.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22357438504695892
dqn reward tensor(263.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27149447798728943
dqn reward tensor(268.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3402e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13811980187892914
dqn reward tensor(145.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08124497532844543
dqn reward tensor(323.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16413754224777222
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0603e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941179782152176
dqn reward tensor(280.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07090961933135986
dqn reward tensor(136.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22117234766483307
dqn reward tensor(424.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0911e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0829390287399292
dqn reward tensor(306.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3700e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05019032955169678
dqn reward tensor(381.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3935e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15503373742103577
dqn reward tensor(283.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1055e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07328695058822632
dqn reward tensor(514.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0431e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10407520085573196
dqn reward tensor(318., device='cuda:0') e 0.05 loss_dqn tensor(1.8202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11201497912406921
dqn reward tensor(398.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7594e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05284426733851433
dqn reward tensor(407.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2221875786781311
dqn reward tensor(292., device='cuda:0') e 0.05 loss_dqn tensor(1.1198e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14569653570652008
dqn reward tensor(519.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6118e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19485129415988922
dqn reward tensor(491.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22703875601291656
dqn reward tensor(375.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24110954999923706
dqn reward tensor(265.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1493e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03407369926571846
dqn reward tensor(271.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8427e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3143033981323242
dqn reward tensor(227.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6944e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905043125152588
dqn reward tensor(351., device='cuda:0') e 0.05 loss_dqn tensor(2.0600e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12174008786678314
dqn reward tensor(495., device='cuda:0') e 0.05 loss_dqn tensor(1.0713e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0934910923242569
dqn reward tensor(519.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1587e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21864593029022217
dqn reward tensor(393.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.1625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1265472173690796
dqn reward tensor(509.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0886e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18169105052947998
dqn reward tensor(285.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941550374031067
dqn reward tensor(359.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8709e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1419534981250763
dqn reward tensor(279.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1286e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04750718176364899
dqn reward tensor(203.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0757e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14722375571727753
dqn reward tensor(222.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3871e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23352372646331787
dqn reward tensor(572.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16255033016204834
dqn reward tensor(274., device='cuda:0') e 0.05 loss_dqn tensor(2.2624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27564582228660583
dqn reward tensor(371.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06635948270559311
dqn reward tensor(501., device='cuda:0') e 0.05 loss_dqn tensor(2.9084e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035811230540275574
dqn reward tensor(404.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1091e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14671748876571655
dqn reward tensor(357.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4167e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17616641521453857
dqn reward tensor(459.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7745e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10731209069490433
dqn reward tensor(228., device='cuda:0') e 0.05 loss_dqn tensor(2.0315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15040886402130127
dqn reward tensor(246.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5430e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17187534272670746
dqn reward tensor(198., device='cuda:0') e 0.05 loss_dqn tensor(1.9136e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24124577641487122
dqn reward tensor(317.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1908e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03308207914233208
dqn reward tensor(242.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08466562628746033
dqn reward tensor(70.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08442308753728867
dqn reward tensor(336.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9041e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13462337851524353
dqn reward tensor(173.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1810e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2149122655391693
dqn reward tensor(252.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25942420959472656
dqn reward tensor(346.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1619e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10397844016551971
dqn reward tensor(408.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1140e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06482993811368942
dqn reward tensor(408.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4467e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22835461795330048
dqn reward tensor(358.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1120574027299881
dqn reward tensor(407.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1215e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05474023520946503
dqn reward tensor(154.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07880212366580963
dqn reward tensor(441., device='cuda:0') e 0.05 loss_dqn tensor(1.1699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12673744559288025
dqn reward tensor(450.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2192e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03895881399512291
dqn reward tensor(180., device='cuda:0') e 0.05 loss_dqn tensor(1.2213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07948017120361328
dqn reward tensor(178.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2965e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09451045095920563
dqn reward tensor(271.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15306219458580017
dqn reward tensor(299.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1608e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2150232046842575
dqn reward tensor(215., device='cuda:0') e 0.05 loss_dqn tensor(1.2201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09368661046028137
dqn reward tensor(440.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3076e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018915509805083275
dqn reward tensor(304.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2316170334815979
dqn reward tensor(304., device='cuda:0') e 0.05 loss_dqn tensor(1.2156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21793580055236816
dqn reward tensor(306., device='cuda:0') e 0.05 loss_dqn tensor(1.2203e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16705644130706787
dqn reward tensor(352.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7497e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2765341103076935
dqn reward tensor(370., device='cuda:0') e 0.05 loss_dqn tensor(1.1902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27925294637680054
dqn reward tensor(356.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1959e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1065007895231247
dqn reward tensor(494.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18975841999053955
dqn reward tensor(354.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2333526313304901
dqn reward tensor(319.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.2002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14152763783931732
dqn reward tensor(265.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2599e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1990516036748886
dqn reward tensor(316.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2704e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2068335860967636
dqn reward tensor(274.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3566e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15070706605911255
dqn reward tensor(242.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07484506815671921
dqn reward tensor(376.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0219e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11739495396614075
dqn reward tensor(449.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2217e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0784253403544426
dqn reward tensor(289.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5591e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061143532395362854
dqn reward tensor(270.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2694e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08420488238334656
dqn reward tensor(318.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2853e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12967127561569214
dqn reward tensor(400.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07695616036653519
dqn reward tensor(242.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2171e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09353762120008469
dqn reward tensor(317.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09406362473964691
dqn reward tensor(410.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2404e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08654019236564636
dqn reward tensor(268.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2959e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014208642765879631
dqn reward tensor(212.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25670909881591797
dqn reward tensor(223.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1224e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.011277945712208748
dqn reward tensor(466.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1507e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10221357643604279
dqn reward tensor(180.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2696e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21315722167491913
dqn reward tensor(329., device='cuda:0') e 0.05 loss_dqn tensor(2.5253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20163092017173767
dqn reward tensor(261.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1629e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28115445375442505
dqn reward tensor(550.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2016e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06751273572444916
dqn reward tensor(338.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23047128319740295
dqn reward tensor(-54.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0143e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18330484628677368
dqn reward tensor(427.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0725e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060000792145729065
dqn reward tensor(240.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2640e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17546454071998596
dqn reward tensor(213., device='cuda:0') e 0.05 loss_dqn tensor(1.2887e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1260625422000885
dqn reward tensor(476.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4147e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1295575648546219
dqn reward tensor(423.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2861e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13565340638160706
dqn reward tensor(317., device='cuda:0') e 0.05 loss_dqn tensor(2.3702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08056586235761642
dqn reward tensor(225.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2231224775314331
dqn reward tensor(405.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3257e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09223277121782303
dqn reward tensor(359.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.5738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1629142463207245
dqn reward tensor(189.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23316118121147156
dqn reward tensor(234.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.4240e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09820983558893204
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3456e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08735425770282745
dqn reward tensor(433.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10124213993549347
dqn reward tensor(72., device='cuda:0') e 0.05 loss_dqn tensor(1.7902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1716199517250061
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5096e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14259019494056702
dqn reward tensor(209.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3719e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08401162922382355
dqn reward tensor(473.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024238619953393936
dqn reward tensor(114.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.2484e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0917711928486824
dqn reward tensor(205.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3515e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10218653827905655
dqn reward tensor(337.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2727e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17666220664978027
dqn reward tensor(437.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2873e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12311994284391403
dqn reward tensor(411.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3187e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08557310700416565
dqn reward tensor(268.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3001115024089813
dqn reward tensor(356.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1114555075764656
dqn reward tensor(306.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.158786341547966
dqn reward tensor(399.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19151069223880768
dqn reward tensor(326.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12086653709411621
dqn reward tensor(240.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16010527312755585
dqn reward tensor(157.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3278e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14495594799518585
dqn reward tensor(162.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5293e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09544380754232407
dqn reward tensor(260.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9957e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10649824142456055
dqn reward tensor(403.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9707e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09673933684825897
dqn reward tensor(379.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9003e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13467508554458618
dqn reward tensor(409.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8389e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11785633862018585
dqn reward tensor(123.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0474545881152153
dqn reward tensor(296.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3982e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17500503361225128
dqn reward tensor(481.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04848979040980339
dqn reward tensor(385.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3328e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12214669585227966
dqn reward tensor(-14., device='cuda:0') e 0.05 loss_dqn tensor(4.6201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17597854137420654
dqn reward tensor(-219.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10957275331020355
dqn reward tensor(275.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3010e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22097733616828918
dqn reward tensor(320.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041330866515636444
dqn reward tensor(265., device='cuda:0') e 0.05 loss_dqn tensor(3.1059e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11262185871601105
dqn reward tensor(315.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8225e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19233843684196472
dqn reward tensor(526.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3277e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19107350707054138
dqn reward tensor(445., device='cuda:0') e 0.05 loss_dqn tensor(1.3479e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06676626205444336
dqn reward tensor(395.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3068e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12143616378307343
dqn reward tensor(613.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.5216e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1706334501504898
dqn reward tensor(199., device='cuda:0') e 0.05 loss_dqn tensor(2.5322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12963341176509857
dqn reward tensor(275.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054761797189712524
dqn reward tensor(412.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3335e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10605543106794357
dqn reward tensor(471.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3752e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09705838561058044
dqn reward tensor(406., device='cuda:0') e 0.05 loss_dqn tensor(1.3129e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20140108466148376
dqn reward tensor(455.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3854e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08325107395648956
dqn reward tensor(148., device='cuda:0') e 0.05 loss_dqn tensor(3.0379e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12480373680591583
dqn reward tensor(281.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4051e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17848150432109833
dqn reward tensor(493.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4523e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435943841934204
dqn reward tensor(204.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3677e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18551596999168396
dqn reward tensor(425.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3860e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17657403647899628
dqn reward tensor(313.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4493e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04498492181301117
dqn reward tensor(314.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5684e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18524685502052307
dqn reward tensor(182.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.9438e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2872292399406433
dqn reward tensor(399.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17470146715641022
dqn reward tensor(260.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4916e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3822198510169983
dqn reward tensor(205.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5269e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08294979482889175
dqn reward tensor(332.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16441908478736877
dqn reward tensor(279., device='cuda:0') e 0.05 loss_dqn tensor(1.4904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12091252207756042
dqn reward tensor(341.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4204e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11606341600418091
dqn reward tensor(244.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0983e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20237180590629578
dqn reward tensor(302.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6008e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1391770839691162
dqn reward tensor(427.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20508348941802979
dqn reward tensor(475., device='cuda:0') e 0.05 loss_dqn tensor(1.4138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11884436011314392
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19751065969467163
dqn reward tensor(242.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9094e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06078645959496498
dqn reward tensor(309.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11346129328012466
dqn reward tensor(494.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1287725269794464
dqn reward tensor(456.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8567e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06599566340446472
dqn reward tensor(298.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4067e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30146944522857666
dqn reward tensor(319.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15966452658176422
dqn reward tensor(172.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9696e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16049009561538696
dqn reward tensor(398.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4079e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051324233412742615
dqn reward tensor(258.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4600e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1018904373049736
dqn reward tensor(412.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4246e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08422887325286865
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10272547602653503
dqn reward tensor(419.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043863579630851746
dqn reward tensor(348.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3805e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0892450287938118
dqn reward tensor(349.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1000e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1362525075674057
dqn reward tensor(422.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0720e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022911138832569122
dqn reward tensor(103.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7459e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04922081530094147
dqn reward tensor(407.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4503e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.214328795671463
dqn reward tensor(314., device='cuda:0') e 0.05 loss_dqn tensor(2.7624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054613471031188965
dqn reward tensor(30.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9117e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09899963438510895
dqn reward tensor(362.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1295e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0754818394780159
dqn reward tensor(354.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18293246626853943
dqn reward tensor(237.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8337e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14725777506828308
dqn reward tensor(423.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5198e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05350380018353462
dqn reward tensor(331.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4402e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36034125089645386
dqn reward tensor(222., device='cuda:0') e 0.05 loss_dqn tensor(2.9032e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0689142644405365
dqn reward tensor(241.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14823667705059052
dqn reward tensor(325.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046882353723049164
dqn reward tensor(111.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04400450363755226
dqn reward tensor(553.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08930723369121552
dqn reward tensor(248.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14318260550498962
dqn reward tensor(262.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1583e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2598381042480469
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0216e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1815708726644516
dqn reward tensor(372.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6805e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15888848900794983
dqn reward tensor(359.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4413e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19405005872249603
dqn reward tensor(333.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08982082456350327
dqn reward tensor(276.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14817485213279724
dqn reward tensor(164.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0668e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07058775424957275
dqn reward tensor(404.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16665825247764587
dqn reward tensor(372.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.4525e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05278133600950241
dqn reward tensor(406., device='cuda:0') e 0.05 loss_dqn tensor(1.4898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159534752368927
dqn reward tensor(398.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4975e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1946672797203064
dqn reward tensor(240.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1915e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21094602346420288
dqn reward tensor(393.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4597e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029963504523038864
dqn reward tensor(292.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5880e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0605948343873024
dqn reward tensor(361.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6778e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15584327280521393
dqn reward tensor(214.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4908e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14773540198802948
dqn reward tensor(435.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5117e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13814769685268402
dqn reward tensor(163., device='cuda:0') e 0.05 loss_dqn tensor(3.5957e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14153967797756195
dqn reward tensor(191.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025182195007801056
dqn reward tensor(398.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5680e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22497479617595673
dqn reward tensor(347.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1992543339729309
dqn reward tensor(262.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31185686588287354
dqn reward tensor(465., device='cuda:0') e 0.05 loss_dqn tensor(2.2867e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026057643815875053
dqn reward tensor(572.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5380e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09981454908847809
dqn reward tensor(331.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.5659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060655731707811356
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6499e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12240153551101685
dqn reward tensor(288., device='cuda:0') e 0.05 loss_dqn tensor(1.6418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20939701795578003
dqn reward tensor(4.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20093606412410736
dqn reward tensor(316.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29663074016571045
dqn reward tensor(253., device='cuda:0') e 0.05 loss_dqn tensor(1.5845e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10108684748411179
dqn reward tensor(365., device='cuda:0') e 0.05 loss_dqn tensor(1.6276e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1616763174533844
dqn reward tensor(367.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.6296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1000620424747467
dqn reward tensor(294.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07773897051811218
dqn reward tensor(353.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3048e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09951946884393692
dqn reward tensor(243.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6286e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15974682569503784
dqn reward tensor(394.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24440190196037292
dqn reward tensor(307.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6220e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03570977598428726
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9393e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20518529415130615
dqn reward tensor(198.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5977e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09194903820753098
dqn reward tensor(285.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5812e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11754584312438965
dqn reward tensor(350.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0338e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10301626473665237
dqn reward tensor(224.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09040740877389908
dqn reward tensor(172.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.237600177526474
dqn reward tensor(393.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7782e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21072442829608917
dqn reward tensor(347.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1030627191066742
dqn reward tensor(257.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06795214861631393
dqn reward tensor(527.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1780e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05286847800016403
dqn reward tensor(226.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.2728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031196027994155884
dqn reward tensor(406.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2459e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23624148964881897
dqn reward tensor(176., device='cuda:0') e 0.05 loss_dqn tensor(3.1851e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638379991054535
dqn reward tensor(255.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2280178964138031
dqn reward tensor(281.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8301e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1864941418170929
dqn reward tensor(143.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19194522500038147
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9347e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2476574331521988
dqn reward tensor(195.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04779265820980072
dqn reward tensor(201.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9513e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04927467927336693
dqn reward tensor(434.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9636e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15696468949317932
dqn reward tensor(347.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6396e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.112690269947052
dqn reward tensor(230.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18509557843208313
dqn reward tensor(276.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9389e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06686828285455704
dqn reward tensor(301.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3354e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3279675245285034
dqn reward tensor(459.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9255e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.120642751455307
dqn reward tensor(349.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25352156162261963
dqn reward tensor(75.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052626121789216995
dqn reward tensor(318., device='cuda:0') e 0.05 loss_dqn tensor(1.9931e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03657010942697525
dqn reward tensor(263., device='cuda:0') e 0.05 loss_dqn tensor(2.0460e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1839427798986435
dqn reward tensor(346.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.0372e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2583532929420471
dqn reward tensor(405.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.7201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.155938059091568
dqn reward tensor(279.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1084e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06594731658697128
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3654e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03393689915537834
dqn reward tensor(184.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.4488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3043556213378906
dqn reward tensor(-46.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06337691843509674
dqn reward tensor(168.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.4696e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25896498560905457
dqn reward tensor(289.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1247e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08121680468320847
dqn reward tensor(461.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2814e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16766628623008728
dqn reward tensor(306.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0263e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06533802300691605
dqn reward tensor(197.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05196040868759155
dqn reward tensor(78.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08291330933570862
dqn reward tensor(442.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21485479176044464
dqn reward tensor(331.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1358e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0425356924533844
dqn reward tensor(410.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09549231827259064
dqn reward tensor(435.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1536e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08616292476654053
dqn reward tensor(160.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1510789543390274
dqn reward tensor(200.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2091e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054056450724601746
dqn reward tensor(372., device='cuda:0') e 0.05 loss_dqn tensor(4.3021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26082074642181396
dqn reward tensor(190.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7809e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06461590528488159
dqn reward tensor(452.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0645e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2796613574028015
dqn reward tensor(338.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1841e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.109223872423172
dqn reward tensor(197.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0461e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04303089156746864
dqn reward tensor(282.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2135e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03711927682161331
dqn reward tensor(324.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3227e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2967163622379303
dqn reward tensor(186.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.6193e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1439722776412964
dqn reward tensor(292.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1233e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02764388918876648
dqn reward tensor(305.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15403828024864197
dqn reward tensor(136.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6772e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15631252527236938
dqn reward tensor(249.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1756e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1473257839679718
dqn reward tensor(447.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2367854118347168
dqn reward tensor(245.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8180e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06834840774536133
dqn reward tensor(235.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14425545930862427
dqn reward tensor(492.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2669e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03139080852270126
dqn reward tensor(168.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3206e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21939897537231445
dqn reward tensor(282.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1781e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07607130706310272
dqn reward tensor(179.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3177e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12435037642717361
dqn reward tensor(158.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14566397666931152
dqn reward tensor(-80.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0934e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23743295669555664
dqn reward tensor(366.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2352e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10130937397480011
dqn reward tensor(227., device='cuda:0') e 0.05 loss_dqn tensor(3.3176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1140168309211731
dqn reward tensor(228.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.1593e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23808914422988892
dqn reward tensor(70.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2519e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08828195929527283
dqn reward tensor(342., device='cuda:0') e 0.05 loss_dqn tensor(2.2749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08335212618112564
dqn reward tensor(319.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9032e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25091660022735596
dqn reward tensor(170.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6483e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2750869691371918
dqn reward tensor(280.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1941e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12818115949630737
dqn reward tensor(466.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2266e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10434992611408234
dqn reward tensor(303.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14832279086112976
dqn reward tensor(322.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04552292078733444
dqn reward tensor(198.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9271e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17323662340641022
dqn reward tensor(452.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15391916036605835
dqn reward tensor(283.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28492045402526855
dqn reward tensor(362.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27766352891921997
dqn reward tensor(264.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4914e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05837450549006462
dqn reward tensor(401.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07187120616436005
dqn reward tensor(189.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.6376e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16529816389083862
dqn reward tensor(190.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26499733328819275
dqn reward tensor(249.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9741e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033539146184921265
dqn reward tensor(183.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.3486e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07667794078588486
dqn reward tensor(211.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5420e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17358751595020294
dqn reward tensor(432.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07283921539783478
dqn reward tensor(360.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14616316556930542
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0314517468214035
dqn reward tensor(259.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11514434218406677
dqn reward tensor(321.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10521581768989563
dqn reward tensor(243.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3449e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10246295481920242
dqn reward tensor(84.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499403864145279
dqn reward tensor(184.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2528e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20296582579612732
dqn reward tensor(364.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020329587161540985
dqn reward tensor(447.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20891249179840088
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10256931930780411
dqn reward tensor(340.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9489e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13508057594299316
dqn reward tensor(132.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2218851000070572
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6134e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03203669562935829
dqn reward tensor(390.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3130e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07843086123466492
dqn reward tensor(277.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4693e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09482583403587341
dqn reward tensor(344.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6932e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3679520785808563
dqn reward tensor(248.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8676e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1379086971282959
dqn reward tensor(221.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5623e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10741043090820312
dqn reward tensor(211.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.5178e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0510222464799881
dqn reward tensor(390.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1819082647562027
dqn reward tensor(164., device='cuda:0') e 0.05 loss_dqn tensor(1.0119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08759141713380814
dqn reward tensor(205., device='cuda:0') e 0.05 loss_dqn tensor(6.8250e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2733774185180664
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11403248459100723
dqn reward tensor(334.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9003e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11399528384208679
dqn reward tensor(134.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9672e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15277723968029022
dqn reward tensor(60.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13100801408290863
dqn reward tensor(195.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16582858562469482
dqn reward tensor(44.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9059e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2529833912849426
dqn reward tensor(367.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7294e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23651298880577087
dqn reward tensor(263., device='cuda:0') e 0.05 loss_dqn tensor(5.6056e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053805332630872726
dqn reward tensor(228.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.3268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05686114355921745
dqn reward tensor(193.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1347e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07854649424552917
dqn reward tensor(316.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04367547854781151
dqn reward tensor(304.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3476e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17547167837619781
dqn reward tensor(267.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254573166370392
dqn reward tensor(274.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2570e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14382989704608917
dqn reward tensor(262.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0780363455414772
dqn reward tensor(389.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4862e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12869642674922943
dqn reward tensor(195.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.3296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12748710811138153
dqn reward tensor(489.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16379427909851074
dqn reward tensor(97., device='cuda:0') e 0.05 loss_dqn tensor(4.5859e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2768073081970215
dqn reward tensor(306.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2616e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020997850224375725
dqn reward tensor(314.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2305e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1280028223991394
dqn reward tensor(357.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.6211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09102243185043335
dqn reward tensor(292.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2976e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0628913938999176
dqn reward tensor(219.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12120779603719711
dqn reward tensor(118.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8582e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19248837232589722
dqn reward tensor(341.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19439071416854858
dqn reward tensor(202.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0728e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1813405454158783
dqn reward tensor(328.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21288537979125977
dqn reward tensor(318.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7262e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3466350734233856
dqn reward tensor(166., device='cuda:0') e 0.05 loss_dqn tensor(4.3310e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09006650745868683
dqn reward tensor(162.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.3088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12840694189071655
dqn reward tensor(189.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3178e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18275649845600128
dqn reward tensor(211.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6133e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08395759761333466
dqn reward tensor(203.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7800e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11727733165025711
dqn reward tensor(255.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9467e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07322026789188385
dqn reward tensor(264.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08276297152042389
dqn reward tensor(275., device='cuda:0') e 0.05 loss_dqn tensor(2.9014e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.078450046479702
dqn reward tensor(134.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7058e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06868322193622589
dqn reward tensor(155.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9647e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020606085658073425
dqn reward tensor(374., device='cuda:0') e 0.05 loss_dqn tensor(2.2268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02475561387836933
dqn reward tensor(368.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6380e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.215297132730484
dqn reward tensor(187.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3129e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08216182887554169
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3513e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10808290541172028
dqn reward tensor(332.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1027e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05947578698396683
dqn reward tensor(200.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012582660652697086
dqn reward tensor(521.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1991e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1359374225139618
dqn reward tensor(141., device='cuda:0') e 0.05 loss_dqn tensor(2.3168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10663678497076035
dqn reward tensor(207.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0198e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07098022103309631
dqn reward tensor(235.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2542e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18317237496376038
dqn reward tensor(204.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.3254e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.009475809521973133
dqn reward tensor(201., device='cuda:0') e 0.05 loss_dqn tensor(7.5506e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08880117535591125
dqn reward tensor(399.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12943214178085327
dqn reward tensor(264.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20449121296405792
dqn reward tensor(304.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1196e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2514554262161255
dqn reward tensor(204.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015828147530555725
dqn reward tensor(386.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09739436209201813
dqn reward tensor(234.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13343007862567902
dqn reward tensor(248., device='cuda:0') e 0.05 loss_dqn tensor(4.5998e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11809278279542923
dqn reward tensor(62.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08249616622924805
dqn reward tensor(201.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034207746386528015
dqn reward tensor(28.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3825e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06828805059194565
dqn reward tensor(189.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4775e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23409715294837952
dqn reward tensor(248., device='cuda:0') e 0.05 loss_dqn tensor(2.4774e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08009955286979675
dqn reward tensor(265.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.7639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14314424991607666
dqn reward tensor(211.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1094e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1967782825231552
dqn reward tensor(445.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8013e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3204651176929474
dqn reward tensor(113., device='cuda:0') e 0.05 loss_dqn tensor(4.8614e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0448809377849102
dqn reward tensor(200., device='cuda:0') e 0.05 loss_dqn tensor(2.3827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18818369507789612
dqn reward tensor(229.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23005446791648865
dqn reward tensor(-79.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7376e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16910195350646973
dqn reward tensor(233.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5574e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10711007565259933
dqn reward tensor(309.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5332e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499350368976593
dqn reward tensor(241.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5613e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16864390671253204
dqn reward tensor(134.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2385e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11234088242053986
dqn reward tensor(153.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5248e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2765806317329407
dqn reward tensor(293.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5952e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21849748492240906
dqn reward tensor(375.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043197281658649445
dqn reward tensor(342.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08505909144878387
dqn reward tensor(266.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15995842218399048
dqn reward tensor(96.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1855047047138214
dqn reward tensor(251., device='cuda:0') e 0.05 loss_dqn tensor(4.5838e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22631123661994934
dqn reward tensor(171.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.3100e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11391519010066986
dqn reward tensor(253.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3405e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11413674801588058
dqn reward tensor(135.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042344458401203156
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3658e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11558955162763596
dqn reward tensor(93.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0392e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03167823329567909
dqn reward tensor(329., device='cuda:0') e 0.05 loss_dqn tensor(5.0686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03015855699777603
dqn reward tensor(25.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1456e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18288984894752502
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(4.1640e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02221611514687538
dqn reward tensor(163.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1351e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021141521632671356
dqn reward tensor(187.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16450171172618866
dqn reward tensor(105.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7031e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01778431050479412
dqn reward tensor(105., device='cuda:0') e 0.05 loss_dqn tensor(7.4187e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2347920536994934
dqn reward tensor(268.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.2441e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18480978906154633
dqn reward tensor(228.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7232e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2276190221309662
dqn reward tensor(54.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03448810800909996
dqn reward tensor(196.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3513104319572449
dqn reward tensor(227.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0730171650648117
dqn reward tensor(29.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8612e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4812355637550354
Evaluating...
Train: {'rocauc': 0.7647877774282933} 3.3279054164886475
=====Epoch 17=====
Training...
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2936e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0358712375164032
dqn reward tensor(166.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12013231217861176
dqn reward tensor(192.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666077822446823
dqn reward tensor(125.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9320e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29980069398880005
dqn reward tensor(137., device='cuda:0') e 0.05 loss_dqn tensor(2.3445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17788998782634735
dqn reward tensor(106.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14233985543251038
dqn reward tensor(219.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1441737711429596
dqn reward tensor(258.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1951e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10755611956119537
dqn reward tensor(167.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1825e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07948832958936691
dqn reward tensor(208.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33709263801574707
dqn reward tensor(258., device='cuda:0') e 0.05 loss_dqn tensor(2.2111e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15385855734348297
dqn reward tensor(161.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1707e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06119871512055397
dqn reward tensor(28.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3387e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08374865353107452
dqn reward tensor(222.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1066e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08334578573703766
dqn reward tensor(181.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4847e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18628361821174622
dqn reward tensor(299., device='cuda:0') e 0.05 loss_dqn tensor(2.1984e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14694978296756744
dqn reward tensor(201.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4281e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15058809518814087
dqn reward tensor(-0.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1512439250946045
dqn reward tensor(-100.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.3525e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202167928218842
dqn reward tensor(174.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0789e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05834628641605377
dqn reward tensor(192.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4199896454811096
dqn reward tensor(57.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1755426973104477
dqn reward tensor(22.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2379e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02602008730173111
dqn reward tensor(31.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8484e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035682037472724915
dqn reward tensor(163., device='cuda:0') e 0.05 loss_dqn tensor(6.6315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17903906106948853
dqn reward tensor(131.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2548e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22634750604629517
dqn reward tensor(122.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4635e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791657418012619
dqn reward tensor(213.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9647e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14669206738471985
dqn reward tensor(105.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12386195361614227
dqn reward tensor(210., device='cuda:0') e 0.05 loss_dqn tensor(3.0560e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058063119649887085
dqn reward tensor(111.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3511e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20849964022636414
dqn reward tensor(-18.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1759e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08697871118783951
dqn reward tensor(146.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0571e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21575285494327545
dqn reward tensor(265.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9484e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11305520683526993
dqn reward tensor(224., device='cuda:0') e 0.05 loss_dqn tensor(2.2461e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10964909940958023
dqn reward tensor(109.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9803e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12743441760540009
dqn reward tensor(140.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.1729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18917186558246613
dqn reward tensor(-38., device='cuda:0') e 0.05 loss_dqn tensor(4.8699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04703686758875847
dqn reward tensor(62.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0956e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17722675204277039
dqn reward tensor(289.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09693403542041779
dqn reward tensor(17.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9403e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11637842655181885
dqn reward tensor(77.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3510e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1464129537343979
dqn reward tensor(161.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0496e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11000218987464905
dqn reward tensor(-36.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0716e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1380762755870819
dqn reward tensor(175.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2275e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06244231015443802
dqn reward tensor(-174.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9590e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.170880526304245
dqn reward tensor(152., device='cuda:0') e 0.05 loss_dqn tensor(4.3003e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254269778728485
dqn reward tensor(154.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2806e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11384372413158417
dqn reward tensor(202.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8093e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30416005849838257
dqn reward tensor(79.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7001e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22633402049541473
dqn reward tensor(138., device='cuda:0') e 0.05 loss_dqn tensor(2.2099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023168645799160004
dqn reward tensor(-127.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3421e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11113956570625305
dqn reward tensor(76.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2687e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0770275741815567
dqn reward tensor(93.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3001e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06768306344747543
dqn reward tensor(232.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18378400802612305
dqn reward tensor(241.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5246e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02841123566031456
dqn reward tensor(118., device='cuda:0') e 0.05 loss_dqn tensor(4.0741e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1613338589668274
dqn reward tensor(346.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06578709930181503
dqn reward tensor(241., device='cuda:0') e 0.05 loss_dqn tensor(2.3003e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13933496177196503
dqn reward tensor(235.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10948394238948822
dqn reward tensor(70.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3635e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0932520255446434
dqn reward tensor(111.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7688e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08265776187181473
dqn reward tensor(186.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0999317467212677
dqn reward tensor(94.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33642715215682983
dqn reward tensor(-126., device='cuda:0') e 0.05 loss_dqn tensor(2.9409e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09027718007564545
dqn reward tensor(220.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3272e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08703921735286713
dqn reward tensor(89.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23814541101455688
dqn reward tensor(80.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2512e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027004841715097427
dqn reward tensor(48.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19176596403121948
dqn reward tensor(173.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.7030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12700387835502625
dqn reward tensor(69.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4382e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1767224669456482
dqn reward tensor(-18.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031379904597997665
dqn reward tensor(254.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15181921422481537
dqn reward tensor(215.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2607e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10502703487873077
dqn reward tensor(329.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4514e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028183966875076294
dqn reward tensor(244.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.4874e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1918577253818512
dqn reward tensor(-128., device='cuda:0') e 0.05 loss_dqn tensor(7.6765e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28017884492874146
dqn reward tensor(216., device='cuda:0') e 0.05 loss_dqn tensor(2.4066e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13879652321338654
dqn reward tensor(124.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1955e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17444653809070587
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04033762589097023
dqn reward tensor(278.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0791e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1671939492225647
dqn reward tensor(63.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2163e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052438922226428986
dqn reward tensor(48.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5096e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04872807860374451
dqn reward tensor(83.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.3710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09874190390110016
dqn reward tensor(225.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1801e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15129201114177704
dqn reward tensor(190.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8987e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13472217321395874
dqn reward tensor(104.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11189503967761993
dqn reward tensor(212.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5894e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0896199494600296
dqn reward tensor(131.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4620e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1909390538930893
dqn reward tensor(311.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.4291e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12634937465190887
dqn reward tensor(161.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1877324879169464
dqn reward tensor(107.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7298e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254757046699524
dqn reward tensor(246.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06323866546154022
dqn reward tensor(50.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8868e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0411955788731575
dqn reward tensor(360.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.3746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2905633747577667
dqn reward tensor(38.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6402e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04951557144522667
dqn reward tensor(203.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8732e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17366205155849457
dqn reward tensor(231.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1402e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19677245616912842
dqn reward tensor(237., device='cuda:0') e 0.05 loss_dqn tensor(2.3979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13460281491279602
dqn reward tensor(112.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3991e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25067687034606934
dqn reward tensor(58., device='cuda:0') e 0.05 loss_dqn tensor(2.4808e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13844522833824158
dqn reward tensor(-210.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6916e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11748538166284561
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(7.9898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08754445612430573
dqn reward tensor(57.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1858919858932495
dqn reward tensor(191.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3430e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1567486822605133
dqn reward tensor(-123.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20033174753189087
dqn reward tensor(102.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20521491765975952
dqn reward tensor(-36.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1961018443107605
dqn reward tensor(81.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5297e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053627464920282364
dqn reward tensor(281.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2026e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07727943360805511
dqn reward tensor(157.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0510e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22702226042747498
dqn reward tensor(93.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4564e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07061590254306793
dqn reward tensor(188.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.4268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12511524558067322
dqn reward tensor(76.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7798e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043489597737789154
dqn reward tensor(165.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4172e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09791664034128189
dqn reward tensor(-339.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03264644742012024
dqn reward tensor(4.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2275e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2377811223268509
dqn reward tensor(230.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18184785544872284
dqn reward tensor(186.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15877538919448853
dqn reward tensor(36., device='cuda:0') e 0.05 loss_dqn tensor(2.5012e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026184502989053726
dqn reward tensor(207.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4976e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16160666942596436
dqn reward tensor(123.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6454e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039958417415618896
dqn reward tensor(130., device='cuda:0') e 0.05 loss_dqn tensor(4.6117e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1961176097393036
dqn reward tensor(-36., device='cuda:0') e 0.05 loss_dqn tensor(2.6179e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13343794643878937
dqn reward tensor(184.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5637e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03578580170869827
dqn reward tensor(83., device='cuda:0') e 0.05 loss_dqn tensor(5.0868e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09638670831918716
dqn reward tensor(143., device='cuda:0') e 0.05 loss_dqn tensor(2.4122e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31525927782058716
dqn reward tensor(44.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8113e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2765381634235382
dqn reward tensor(78.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1956e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1541772484779358
dqn reward tensor(178.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4853e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052088867872953415
dqn reward tensor(64., device='cuda:0') e 0.05 loss_dqn tensor(2.4041e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11867405474185944
dqn reward tensor(-117.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4239e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0634281188249588
dqn reward tensor(173.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4566e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11707506328821182
dqn reward tensor(103.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21115146577358246
dqn reward tensor(104.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17028340697288513
dqn reward tensor(2.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9010e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11003345996141434
dqn reward tensor(31.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4403e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3715763986110687
dqn reward tensor(18.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.7782e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06375224888324738
dqn reward tensor(101.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.8940e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14128918945789337
dqn reward tensor(-78.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23025259375572205
dqn reward tensor(-18.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0682e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12489618360996246
dqn reward tensor(-34.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5458e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14335763454437256
dqn reward tensor(61.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12565842270851135
dqn reward tensor(-276.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4742e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19198769330978394
dqn reward tensor(129.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.9742e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1078629344701767
dqn reward tensor(3.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0060e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1348431259393692
dqn reward tensor(281.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15791021287441254
dqn reward tensor(132.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16696161031723022
dqn reward tensor(7.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8243e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08831934630870819
dqn reward tensor(294.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3322656750679016
dqn reward tensor(0.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4818e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02892919071018696
dqn reward tensor(-135.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1139201819896698
dqn reward tensor(215.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09306815266609192
dqn reward tensor(141.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1058e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09998789429664612
dqn reward tensor(83.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5956e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06937247514724731
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24781790375709534
dqn reward tensor(98.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5259e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09789194166660309
dqn reward tensor(-45.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.2686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16488365828990936
dqn reward tensor(73., device='cuda:0') e 0.05 loss_dqn tensor(2.6994e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17255230247974396
dqn reward tensor(198.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9486e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254415214061737
dqn reward tensor(-136., device='cuda:0') e 0.05 loss_dqn tensor(8.4962e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17085205018520355
dqn reward tensor(29.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4352e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17034493386745453
dqn reward tensor(-133.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09088671207427979
dqn reward tensor(-11.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5058e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3378680944442749
dqn reward tensor(185.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0790e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14378730952739716
dqn reward tensor(27.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2789e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1715898960828781
dqn reward tensor(125.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5621e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046282246708869934
dqn reward tensor(220., device='cuda:0') e 0.05 loss_dqn tensor(2.4986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15391221642494202
dqn reward tensor(-10.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12921614944934845
dqn reward tensor(-65., device='cuda:0') e 0.05 loss_dqn tensor(3.1854e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15851953625679016
dqn reward tensor(53.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4808e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09180797636508942
dqn reward tensor(-155.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5217e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03399915620684624
dqn reward tensor(139.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10988602042198181
dqn reward tensor(120.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0256e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030609451234340668
dqn reward tensor(32.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12259873747825623
dqn reward tensor(25.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4197e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11910508573055267
dqn reward tensor(166.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03954242914915085
dqn reward tensor(239.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3121e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16933195292949677
dqn reward tensor(2.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1798e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1238793134689331
dqn reward tensor(9.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.6481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434510350227356
dqn reward tensor(-39.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5273e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14929410815238953
dqn reward tensor(102.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5577e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22143258154392242
dqn reward tensor(287.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.6364e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026045696809887886
dqn reward tensor(168.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.6208e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0295222420245409
dqn reward tensor(304.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6012e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11544865369796753
dqn reward tensor(141.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2560409605503082
dqn reward tensor(264.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.2218e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26265668869018555
dqn reward tensor(55.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09779193997383118
dqn reward tensor(269.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1208e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3209494352340698
dqn reward tensor(255.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.9541e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09068553894758224
dqn reward tensor(180.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24268776178359985
dqn reward tensor(376.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5219e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10268036276102066
dqn reward tensor(123.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16941912472248077
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09088202565908432
dqn reward tensor(157.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9065e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23548458516597748
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.210979163646698
dqn reward tensor(305.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1276e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09791135787963867
dqn reward tensor(279.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2701055705547333
dqn reward tensor(203.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.160730242729187
dqn reward tensor(218.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24874117970466614
dqn reward tensor(311.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.2725e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19348379969596863
dqn reward tensor(166.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3206e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11883702129125595
dqn reward tensor(278.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8967e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04208773374557495
dqn reward tensor(253.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1674775630235672
dqn reward tensor(24.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6607e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23890891671180725
dqn reward tensor(333.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3367e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03592585027217865
dqn reward tensor(514.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12032866477966309
dqn reward tensor(83.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20968195796012878
dqn reward tensor(74.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.9009e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2992823123931885
dqn reward tensor(57.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1173e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055742960423231125
dqn reward tensor(248.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038436420261859894
dqn reward tensor(273.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08565595746040344
dqn reward tensor(188.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10546502470970154
dqn reward tensor(294.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8290e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17964211106300354
dqn reward tensor(65.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2348e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08332525193691254
dqn reward tensor(182.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20295166969299316
dqn reward tensor(90.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14375293254852295
dqn reward tensor(244.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17143912613391876
dqn reward tensor(411.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8184e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2344481647014618
dqn reward tensor(121.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9364e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23851542174816132
dqn reward tensor(360.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8543e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19395479559898376
dqn reward tensor(292.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8046e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0898052304983139
dqn reward tensor(346.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8656e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10875342786312103
dqn reward tensor(277.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10963602364063263
dqn reward tensor(140.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8060e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10845190286636353
dqn reward tensor(67.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15570762753486633
dqn reward tensor(318.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9535e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0944981575012207
dqn reward tensor(174.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2630e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10617847740650177
dqn reward tensor(45.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7939e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08558706939220428
dqn reward tensor(392.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8658e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11949485540390015
dqn reward tensor(168.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0389e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1584201455116272
dqn reward tensor(149.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8617e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16546213626861572
dqn reward tensor(129.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30170387029647827
dqn reward tensor(193.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07243965566158295
dqn reward tensor(181.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.39159029722213745
dqn reward tensor(334.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8406e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1350586712360382
dqn reward tensor(4.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1326e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08672293275594711
dqn reward tensor(340.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12516853213310242
dqn reward tensor(289.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20236890017986298
dqn reward tensor(118.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5789e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10267017036676407
dqn reward tensor(170., device='cuda:0') e 0.05 loss_dqn tensor(1.9751e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028999069705605507
dqn reward tensor(199.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1233975738286972
dqn reward tensor(99.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21776923537254333
dqn reward tensor(309.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1784e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09035858511924744
dqn reward tensor(139.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9676e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21127022802829742
dqn reward tensor(3.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1109204888343811
dqn reward tensor(301.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9005e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1472405195236206
dqn reward tensor(302.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9451e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15530294179916382
dqn reward tensor(260., device='cuda:0') e 0.05 loss_dqn tensor(1.9747e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07200050354003906
dqn reward tensor(136.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0362e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026986252516508102
dqn reward tensor(169.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8447e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13198597729206085
dqn reward tensor(184.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.069245845079422
dqn reward tensor(236.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.6205e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09260530024766922
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9956e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13043147325515747
dqn reward tensor(134.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.266508549451828
dqn reward tensor(174.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7934e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10714566707611084
dqn reward tensor(214.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9891e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2692866325378418
dqn reward tensor(193.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.3943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06662316620349884
dqn reward tensor(351., device='cuda:0') e 0.05 loss_dqn tensor(1.9713e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13574931025505066
dqn reward tensor(326.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0532e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027306606993079185
dqn reward tensor(299.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2519773244857788
dqn reward tensor(14.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1260e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20146654546260834
dqn reward tensor(40.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7753e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1701974868774414
dqn reward tensor(291.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643550544977188
dqn reward tensor(172.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.2465e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06288573145866394
dqn reward tensor(260.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0343e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11849288642406464
dqn reward tensor(240.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6215e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27027562260627747
dqn reward tensor(154.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1207e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22185029089450836
dqn reward tensor(244.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.0532e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14439204335212708
dqn reward tensor(160.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1704e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15820541977882385
dqn reward tensor(-281.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1549e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2131076157093048
dqn reward tensor(126.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08272460848093033
dqn reward tensor(56.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07195720821619034
dqn reward tensor(254.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0810e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11292913556098938
dqn reward tensor(362.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0844e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0788930281996727
dqn reward tensor(280.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1533854901790619
dqn reward tensor(285.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03631101921200752
dqn reward tensor(172., device='cuda:0') e 0.05 loss_dqn tensor(2.1170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04449126869440079
dqn reward tensor(335.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1183e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3236728310585022
dqn reward tensor(157.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3051e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02197236567735672
dqn reward tensor(296.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1657e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20701941847801208
dqn reward tensor(163.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18264101445674896
dqn reward tensor(52.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5819e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06011131405830383
dqn reward tensor(-210.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0458e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08391954004764557
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1717e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17921024560928345
dqn reward tensor(266.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1613989621400833
dqn reward tensor(87.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7836e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022182248532772064
dqn reward tensor(442.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1499e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21296831965446472
dqn reward tensor(240.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07815799862146378
dqn reward tensor(109.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.6884e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18230128288269043
dqn reward tensor(-257.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11782374233007431
dqn reward tensor(208.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1877e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0902450904250145
dqn reward tensor(71.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040189020335674286
dqn reward tensor(198.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3274e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425386369228363
dqn reward tensor(17.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2522e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09697426855564117
dqn reward tensor(166.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3959e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13740450143814087
dqn reward tensor(214.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4883e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09017035365104675
dqn reward tensor(128., device='cuda:0') e 0.05 loss_dqn tensor(3.6307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12035989761352539
dqn reward tensor(49.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3174e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17694330215454102
dqn reward tensor(293.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2471e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3259657621383667
dqn reward tensor(286., device='cuda:0') e 0.05 loss_dqn tensor(2.1992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14368188381195068
dqn reward tensor(-125.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8350e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14723490178585052
dqn reward tensor(97.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9230e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16857287287712097
dqn reward tensor(228.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8430e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1704740822315216
dqn reward tensor(207.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2485e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09084399044513702
dqn reward tensor(326.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5669e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15296348929405212
dqn reward tensor(153.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06798757612705231
dqn reward tensor(124., device='cuda:0') e 0.05 loss_dqn tensor(2.3078e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16817232966423035
dqn reward tensor(250.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9725e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07950904965400696
dqn reward tensor(219.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.5388e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13373804092407227
dqn reward tensor(131.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7199e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056537359952926636
dqn reward tensor(62., device='cuda:0') e 0.05 loss_dqn tensor(5.0596e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16487252712249756
dqn reward tensor(133.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.2912e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16804157197475433
dqn reward tensor(155.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2351e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.083966463804245
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3602e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13357511162757874
dqn reward tensor(155.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2738e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08274337649345398
dqn reward tensor(259.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07656548172235489
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2717e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2480127215385437
dqn reward tensor(-95.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1924e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03312692046165466
dqn reward tensor(297.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2726e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05523846670985222
dqn reward tensor(112.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7526e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13810279965400696
dqn reward tensor(373.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3191e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2004561871290207
dqn reward tensor(469.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3253e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14414092898368835
dqn reward tensor(30., device='cuda:0') e 0.05 loss_dqn tensor(4.3817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01987289823591709
dqn reward tensor(244., device='cuda:0') e 0.05 loss_dqn tensor(2.3985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21933037042617798
dqn reward tensor(169.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15766459703445435
dqn reward tensor(9.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0095e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18729443848133087
dqn reward tensor(-32.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.4804e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14637701213359833
dqn reward tensor(-4.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4677e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08662831038236618
dqn reward tensor(103.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0034e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09926038980484009
dqn reward tensor(49.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05853068828582764
dqn reward tensor(137.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09703671932220459
dqn reward tensor(-11.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09123554825782776
dqn reward tensor(164.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3334e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682301163673401
dqn reward tensor(97.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8836e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18606910109519958
dqn reward tensor(322.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05086793005466461
dqn reward tensor(199.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6686e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13345253467559814
dqn reward tensor(-13.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0555e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13886266946792603
dqn reward tensor(345.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.0639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04080568999052048
dqn reward tensor(282., device='cuda:0') e 0.05 loss_dqn tensor(2.4863e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11707515269517899
dqn reward tensor(179.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6848e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1662481427192688
dqn reward tensor(62.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1966649740934372
dqn reward tensor(182.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14518171548843384
dqn reward tensor(237.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03605052828788757
dqn reward tensor(221., device='cuda:0') e 0.05 loss_dqn tensor(2.7151e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10006138682365417
dqn reward tensor(168.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07942871749401093
dqn reward tensor(-9.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.8342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13629284501075745
dqn reward tensor(77., device='cuda:0') e 0.05 loss_dqn tensor(6.5292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17136883735656738
dqn reward tensor(24.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2632e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12812559306621552
dqn reward tensor(7., device='cuda:0') e 0.05 loss_dqn tensor(3.7683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1611465960741043
dqn reward tensor(-60.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6247e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026552196592092514
dqn reward tensor(-15.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8871e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06581641733646393
dqn reward tensor(41., device='cuda:0') e 0.05 loss_dqn tensor(3.5748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03140886500477791
dqn reward tensor(53.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7429e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22331874072551727
dqn reward tensor(26.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22359278798103333
dqn reward tensor(-100.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14892885088920593
dqn reward tensor(90.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9885e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045200228691101074
dqn reward tensor(-187.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.2537e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17042627930641174
dqn reward tensor(-3.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04606238380074501
dqn reward tensor(-1.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6231e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1467079520225525
dqn reward tensor(-26.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.4006e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08172433078289032
dqn reward tensor(-114., device='cuda:0') e 0.05 loss_dqn tensor(3.9450e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22443214058876038
dqn reward tensor(46.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3197e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3158908486366272
dqn reward tensor(-156.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0675e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14240412414073944
dqn reward tensor(68.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08870524168014526
dqn reward tensor(49.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18361809849739075
dqn reward tensor(-450.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0810e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1758388876914978
dqn reward tensor(-0.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4419e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17613759636878967
dqn reward tensor(-6., device='cuda:0') e 0.05 loss_dqn tensor(5.3501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12643177807331085
dqn reward tensor(-108.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05621989816427231
dqn reward tensor(22.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2365e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0732053890824318
dqn reward tensor(57.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1457e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05578780919313431
dqn reward tensor(170., device='cuda:0') e 0.05 loss_dqn tensor(3.2817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16456735134124756
dqn reward tensor(-24.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17946547269821167
dqn reward tensor(-72.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6991e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19469594955444336
dqn reward tensor(28.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4238e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13937747478485107
dqn reward tensor(-42.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1990085244178772
dqn reward tensor(-166.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3786e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10846716165542603
dqn reward tensor(-220.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21055954694747925
dqn reward tensor(-210.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6443e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15215888619422913
dqn reward tensor(-52.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12932372093200684
dqn reward tensor(-332.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.8665e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19873443245887756
dqn reward tensor(-113.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.0274e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046890608966350555
dqn reward tensor(3.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3528e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12396836280822754
dqn reward tensor(142.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1285e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15848976373672485
dqn reward tensor(-17.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.9771e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638052761554718
dqn reward tensor(-116.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7161e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1118544191122055
dqn reward tensor(31.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.2721e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12144698947668076
dqn reward tensor(229.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8820e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09062495827674866
dqn reward tensor(93.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1531e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03723271191120148
dqn reward tensor(-232.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5672e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20140109956264496
dqn reward tensor(-6.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2178e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3272947371006012
dqn reward tensor(109.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2864e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20887121558189392
dqn reward tensor(-137., device='cuda:0') e 0.05 loss_dqn tensor(4.1933e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11029569804668427
dqn reward tensor(211.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3737e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037926215678453445
dqn reward tensor(104.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2768e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09852408617734909
dqn reward tensor(88.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0771e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26605284214019775
dqn reward tensor(98.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14733746647834778
dqn reward tensor(87.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2029e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05215470492839813
dqn reward tensor(253.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5028e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1698126643896103
dqn reward tensor(115.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9588e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0398111417889595
dqn reward tensor(184.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159611776471138
dqn reward tensor(-14.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2739e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14392714202404022
dqn reward tensor(-272.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8712e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24163682758808136
dqn reward tensor(332.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3531e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18329936265945435
dqn reward tensor(147.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.0690e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11701817810535431
dqn reward tensor(28.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20481519401073456
dqn reward tensor(79.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0556e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04158031940460205
dqn reward tensor(217.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9158e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08881626278162003
dqn reward tensor(101.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6629e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1181894913315773
dqn reward tensor(37.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0779377818107605
dqn reward tensor(57.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7235e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21086013317108154
dqn reward tensor(85.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0785e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07843419909477234
dqn reward tensor(168.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21168753504753113
dqn reward tensor(95.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5899e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09173767268657684
dqn reward tensor(-144.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5855e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04393920302391052
dqn reward tensor(202.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9856e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12307040393352509
dqn reward tensor(243., device='cuda:0') e 0.05 loss_dqn tensor(2.8964e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04548562318086624
dqn reward tensor(269.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8383e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09032581746578217
dqn reward tensor(51.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5017e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017958760261535645
dqn reward tensor(92.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1443323940038681
dqn reward tensor(150.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017690442502498627
dqn reward tensor(297.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1386277973651886
dqn reward tensor(182.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3132e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0712481290102005
dqn reward tensor(-30.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0394e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037725649774074554
dqn reward tensor(70.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5368e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36379474401474
dqn reward tensor(281.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8194e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12286565452814102
dqn reward tensor(-47., device='cuda:0') e 0.05 loss_dqn tensor(3.0428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047356076538562775
dqn reward tensor(193.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0056e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09734653681516647
dqn reward tensor(190.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2142e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1694076806306839
dqn reward tensor(-41.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.4563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09861981868743896
dqn reward tensor(124.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0488e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10101266205310822
dqn reward tensor(123.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8624e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14713403582572937
dqn reward tensor(269.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9816e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.183071106672287
dqn reward tensor(32.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0347e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16272974014282227
dqn reward tensor(-3.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.9520e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12296780198812485
dqn reward tensor(249.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0210e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09349963068962097
dqn reward tensor(22.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1403e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0417708083987236
dqn reward tensor(172., device='cuda:0') e 0.05 loss_dqn tensor(2.9949e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13457521796226501
dqn reward tensor(151.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9059e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3070286512374878
dqn reward tensor(52.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0816e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08699851483106613
dqn reward tensor(185.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17174048721790314
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9496e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06674747914075851
dqn reward tensor(83.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26766425371170044
dqn reward tensor(35.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9845e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666727364063263
dqn reward tensor(129., device='cuda:0') e 0.05 loss_dqn tensor(7.0064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10709597170352936
dqn reward tensor(73.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24935005605220795
dqn reward tensor(126.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1627e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10041036456823349
dqn reward tensor(247.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08605815470218658
dqn reward tensor(76.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050951480865478516
dqn reward tensor(40.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08447444438934326
dqn reward tensor(51.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6241e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04929902404546738
dqn reward tensor(107.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08962449431419373
dqn reward tensor(181., device='cuda:0') e 0.05 loss_dqn tensor(3.2037e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06081020459532738
dqn reward tensor(320.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.2722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22988015413284302
dqn reward tensor(96.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2309e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23002353310585022
dqn reward tensor(85.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0106e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20246969163417816
dqn reward tensor(230.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0567e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13002392649650574
dqn reward tensor(192.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3282e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25322192907333374
dqn reward tensor(-103.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12058159708976746
dqn reward tensor(233., device='cuda:0') e 0.05 loss_dqn tensor(3.1523e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2510923445224762
dqn reward tensor(175., device='cuda:0') e 0.05 loss_dqn tensor(4.3939e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25442230701446533
dqn reward tensor(153.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12196958065032959
dqn reward tensor(153.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0765848457813263
dqn reward tensor(174., device='cuda:0') e 0.05 loss_dqn tensor(4.4356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13067695498466492
dqn reward tensor(120.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20718978345394135
dqn reward tensor(146., device='cuda:0') e 0.05 loss_dqn tensor(5.1985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.136211559176445
dqn reward tensor(8.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4477e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24936965107917786
dqn reward tensor(132.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1678e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1203872412443161
dqn reward tensor(46.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7116e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05678848177194595
dqn reward tensor(107.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05637994408607483
dqn reward tensor(246.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0406e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19586284458637238
dqn reward tensor(289.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1214e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12470942735671997
dqn reward tensor(-17.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2099e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12001654505729675
dqn reward tensor(14.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09069064259529114
dqn reward tensor(126.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2760e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03497222438454628
dqn reward tensor(139., device='cuda:0') e 0.05 loss_dqn tensor(6.3480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18546196818351746
dqn reward tensor(-5.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3237e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10540248453617096
dqn reward tensor(-118.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2730e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10529903322458267
dqn reward tensor(306.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0320e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015744391828775406
dqn reward tensor(220.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1174e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18718737363815308
dqn reward tensor(120.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37959814071655273
dqn reward tensor(-9.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8396e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06703682988882065
dqn reward tensor(-37.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9459e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10728928446769714
dqn reward tensor(112.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3537e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21764270961284637
dqn reward tensor(133.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17327269911766052
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1345e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057062312960624695
dqn reward tensor(11.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3179e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11772114038467407
dqn reward tensor(235.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030322372913360596
dqn reward tensor(72.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.0516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1651964783668518
dqn reward tensor(193.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3418e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040719419717788696
dqn reward tensor(56.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9360e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12470556795597076
dqn reward tensor(324.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3270e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2549927830696106
dqn reward tensor(170.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8768e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15517517924308777
dqn reward tensor(8.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15567854046821594
dqn reward tensor(-66.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2027815282344818
dqn reward tensor(-44.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2672768235206604
dqn reward tensor(44.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0671246349811554
dqn reward tensor(207.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2752e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15056388080120087
dqn reward tensor(235.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3845e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2637260854244232
dqn reward tensor(173.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2384e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2964468002319336
dqn reward tensor(82.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3746e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08693032711744308
dqn reward tensor(258.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08376584947109222
dqn reward tensor(102.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19087602198123932
dqn reward tensor(122.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2619e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20328062772750854
dqn reward tensor(-46.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21845263242721558
dqn reward tensor(86.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3564e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2345992475748062
dqn reward tensor(-2.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2875e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05855141952633858
dqn reward tensor(363.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2019e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26381272077560425
dqn reward tensor(221.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.2568e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07117688655853271
dqn reward tensor(279.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5434e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1046399474143982
dqn reward tensor(163.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054450102150440216
dqn reward tensor(256.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2065e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12990188598632812
dqn reward tensor(191.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3408e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20242881774902344
dqn reward tensor(8.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4930e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028493816033005714
Evaluating...
Train: {'rocauc': 0.7620747254576867} 2.0424249172210693
=====Epoch 18=====
Training...
dqn reward tensor(-18., device='cuda:0') e 0.05 loss_dqn tensor(9.1653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11833442747592926
dqn reward tensor(73., device='cuda:0') e 0.05 loss_dqn tensor(3.2841e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08668147772550583
dqn reward tensor(133.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3770e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2119094729423523
dqn reward tensor(171.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3518e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2120789885520935
dqn reward tensor(60.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08705469965934753
dqn reward tensor(162., device='cuda:0') e 0.05 loss_dqn tensor(3.0495e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3170555830001831
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2159e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09568776190280914
dqn reward tensor(-231.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7014e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10640205442905426
dqn reward tensor(86.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0973e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21064308285713196
dqn reward tensor(30.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905335783958435
dqn reward tensor(183.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03310300409793854
dqn reward tensor(189.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20124289393424988
dqn reward tensor(360., device='cuda:0') e 0.05 loss_dqn tensor(3.2946e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25302591919898987
dqn reward tensor(296.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3625e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057034946978092194
dqn reward tensor(-8.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14907698333263397
dqn reward tensor(93.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11912596970796585
dqn reward tensor(165.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.0037e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13387100398540497
dqn reward tensor(95.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05601301044225693
dqn reward tensor(-133.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09127472341060638
dqn reward tensor(82.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1734e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09702208638191223
dqn reward tensor(89., device='cuda:0') e 0.05 loss_dqn tensor(5.3626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3653082847595215
dqn reward tensor(171.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5319e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1452101171016693
dqn reward tensor(425.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7767e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17048926651477814
dqn reward tensor(214., device='cuda:0') e 0.05 loss_dqn tensor(3.2880e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1540364921092987
dqn reward tensor(-135., device='cuda:0') e 0.05 loss_dqn tensor(6.8176e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11603900790214539
dqn reward tensor(2., device='cuda:0') e 0.05 loss_dqn tensor(6.7416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15301676094532013
dqn reward tensor(109.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5515e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025602202862501144
dqn reward tensor(-93.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1519e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045469675213098526
dqn reward tensor(-168.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1316336989402771
dqn reward tensor(135.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407504826784134
dqn reward tensor(321.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2202e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08962307870388031
dqn reward tensor(18.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0445e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12962256371974945
dqn reward tensor(-57.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1695e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23643213510513306
dqn reward tensor(147.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2479e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14807134866714478
dqn reward tensor(31.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07570278644561768
dqn reward tensor(53.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4369e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10088726133108139
dqn reward tensor(255.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1716e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1655556857585907
dqn reward tensor(93.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1762e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13971534371376038
dqn reward tensor(93.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0295e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07746314257383347
dqn reward tensor(79.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1053e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060940440744161606
dqn reward tensor(-69.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1507e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16039083898067474
dqn reward tensor(-157.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7543e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08216029405593872
dqn reward tensor(8.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1349073201417923
dqn reward tensor(66.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4673e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.133527010679245
dqn reward tensor(64.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14521172642707825
dqn reward tensor(92., device='cuda:0') e 0.05 loss_dqn tensor(3.2638e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20694662630558014
dqn reward tensor(122.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.9974e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0736759752035141
dqn reward tensor(125.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07422677427530289
dqn reward tensor(260., device='cuda:0') e 0.05 loss_dqn tensor(3.1212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19888541102409363
dqn reward tensor(52.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4311e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2521175742149353
dqn reward tensor(157.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21025292575359344
dqn reward tensor(-279.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2388e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21705874800682068
dqn reward tensor(79.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12291933596134186
dqn reward tensor(-29.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1872e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17527467012405396
dqn reward tensor(-218., device='cuda:0') e 0.05 loss_dqn tensor(3.4363e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11014222353696823
dqn reward tensor(78., device='cuda:0') e 0.05 loss_dqn tensor(4.3170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09942594915628433
dqn reward tensor(150.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.7209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16367101669311523
dqn reward tensor(31.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7024e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1725335717201233
dqn reward tensor(-77.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6647e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09275296330451965
dqn reward tensor(-18.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.8680e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15601694583892822
dqn reward tensor(122.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05731503665447235
dqn reward tensor(-57.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14476169645786285
dqn reward tensor(-101.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12576152384281158
dqn reward tensor(65.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8105e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15807798504829407
dqn reward tensor(126.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.9352e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1664801836013794
dqn reward tensor(92.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4901e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11285722255706787
dqn reward tensor(-27.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5170e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25439271330833435
dqn reward tensor(-27.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.9097e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02598438411951065
dqn reward tensor(108.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5678e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1910628229379654
dqn reward tensor(280.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10395008325576782
dqn reward tensor(122.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7757e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09736493974924088
dqn reward tensor(111.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8904e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03017943724989891
dqn reward tensor(33.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7609e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056563470512628555
dqn reward tensor(76.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4845e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.262939453125
dqn reward tensor(-198.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8154e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06435573101043701
dqn reward tensor(147.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26461073756217957
dqn reward tensor(30.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9346e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1674286127090454
dqn reward tensor(24.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5107e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054989855736494064
dqn reward tensor(-188.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15121494233608246
dqn reward tensor(132., device='cuda:0') e 0.05 loss_dqn tensor(2.8296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13515549898147583
dqn reward tensor(-0.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6744e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03995587304234505
dqn reward tensor(2.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425352543592453
dqn reward tensor(-111.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04111900180578232
dqn reward tensor(-308.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8018e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09648971259593964
dqn reward tensor(-66.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8336e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18894556164741516
dqn reward tensor(-98.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8808e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11057102680206299
dqn reward tensor(-4., device='cuda:0') e 0.05 loss_dqn tensor(3.0291e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17613399028778076
dqn reward tensor(23., device='cuda:0') e 0.05 loss_dqn tensor(2.9390e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06800228357315063
dqn reward tensor(-188.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0680e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.310855895280838
dqn reward tensor(-169.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6162e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2592623233795166
dqn reward tensor(-91.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1492e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04041080176830292
dqn reward tensor(-171.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037109844386577606
dqn reward tensor(-515.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11347995698451996
dqn reward tensor(-8., device='cuda:0') e 0.05 loss_dqn tensor(3.0555e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29589441418647766
dqn reward tensor(38.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6011e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07066818326711655
dqn reward tensor(-55.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0787506103515625
dqn reward tensor(-26.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6068e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13854312896728516
dqn reward tensor(-205.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3614e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11745058000087738
dqn reward tensor(-133.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9406e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17938967049121857
dqn reward tensor(-203.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5004e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043435417115688324
dqn reward tensor(-194.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2074e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14109033346176147
dqn reward tensor(-61.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9846e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2577672004699707
dqn reward tensor(-172.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9775e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1568073332309723
dqn reward tensor(-89.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11799914389848709
dqn reward tensor(59.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2771e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18735554814338684
dqn reward tensor(-35.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1307e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08600059151649475
dqn reward tensor(-91.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8407e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04249729961156845
dqn reward tensor(-140.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9045e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17710483074188232
dqn reward tensor(10., device='cuda:0') e 0.05 loss_dqn tensor(3.1323e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11797952651977539
dqn reward tensor(-146.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13906559348106384
dqn reward tensor(-351.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2972e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12903183698654175
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6834e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17676793038845062
dqn reward tensor(-341.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4668e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06288166344165802
dqn reward tensor(-143.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2723e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24946141242980957
dqn reward tensor(-101.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12451615929603577
dqn reward tensor(-61.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1612e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09367594122886658
dqn reward tensor(-95.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3117e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17062623798847198
dqn reward tensor(-47.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1943e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17847777903079987
dqn reward tensor(-307.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5368e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03395089507102966
dqn reward tensor(-251., device='cuda:0') e 0.05 loss_dqn tensor(3.4172e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15400516986846924
dqn reward tensor(-153.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0201e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10036442428827286
dqn reward tensor(-283.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8885e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28064650297164917
dqn reward tensor(152.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1876e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1771026998758316
dqn reward tensor(2.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2525e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19778940081596375
dqn reward tensor(-110.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6157e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06823863089084625
dqn reward tensor(86.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0654e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31001007556915283
dqn reward tensor(-57.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19548222422599792
dqn reward tensor(-164.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9328e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1807524561882019
dqn reward tensor(-212.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2642e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18012017011642456
dqn reward tensor(-294., device='cuda:0') e 0.05 loss_dqn tensor(6.1075e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051310133188962936
dqn reward tensor(-17.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7570e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15851031243801117
dqn reward tensor(-99.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4436e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17691102623939514
dqn reward tensor(-205.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05356893688440323
dqn reward tensor(-183.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7550e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04351736977696419
dqn reward tensor(-107.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5322e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14149370789527893
dqn reward tensor(-89.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1180e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045179273933172226
dqn reward tensor(-114.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6101e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13091760873794556
dqn reward tensor(-107., device='cuda:0') e 0.05 loss_dqn tensor(5.3013e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.267281174659729
dqn reward tensor(-174.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6714e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07448824495077133
dqn reward tensor(29.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5091e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18818888068199158
dqn reward tensor(-134.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.4360e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1383330523967743
dqn reward tensor(-263.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3986e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.41243457794189453
dqn reward tensor(-242.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05249156802892685
dqn reward tensor(-98.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2982e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13304293155670166
dqn reward tensor(-184.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.3428e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15091678500175476
dqn reward tensor(-136.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5882e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14175783097743988
dqn reward tensor(-94.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8355e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04901069030165672
dqn reward tensor(-408.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2889e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13921397924423218
dqn reward tensor(1.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1114e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13823747634887695
dqn reward tensor(-261.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7938e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31071537733078003
dqn reward tensor(73.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2505e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25704553723335266
dqn reward tensor(-140.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08809159696102142
dqn reward tensor(-90.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9476e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19170567393302917
dqn reward tensor(-130.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9652e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21741431951522827
dqn reward tensor(18.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4705e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23333331942558289
dqn reward tensor(-171.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13755330443382263
dqn reward tensor(-178., device='cuda:0') e 0.05 loss_dqn tensor(3.6811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13173052668571472
dqn reward tensor(-83.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5515e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07712925225496292
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4731e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1006251871585846
dqn reward tensor(-53.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4783e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09824144840240479
dqn reward tensor(-95.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5271e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07827994972467422
dqn reward tensor(-249.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06885110586881638
dqn reward tensor(-19., device='cuda:0') e 0.05 loss_dqn tensor(3.6137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05525905638933182
dqn reward tensor(-30.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0789e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08390618115663528
dqn reward tensor(174.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37318354845046997
dqn reward tensor(12.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09537703543901443
dqn reward tensor(-99.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8552e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14814594388008118
dqn reward tensor(46.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6540e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12467484176158905
dqn reward tensor(-37.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.1545e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12750595808029175
dqn reward tensor(-138.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2589e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1146463006734848
dqn reward tensor(67.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26949143409729004
dqn reward tensor(-42.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7547e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19186526536941528
dqn reward tensor(48.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9865e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10544063150882721
dqn reward tensor(-231.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8346e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14762841165065765
dqn reward tensor(-170., device='cuda:0') e 0.05 loss_dqn tensor(5.4628e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2201806902885437
dqn reward tensor(46.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1336773782968521
dqn reward tensor(46.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0478e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12741407752037048
dqn reward tensor(-78.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7300e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06582126021385193
dqn reward tensor(-147.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6372e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20890316367149353
dqn reward tensor(-192.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1757e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19083645939826965
dqn reward tensor(-81.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0464e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05298792943358421
dqn reward tensor(-81., device='cuda:0') e 0.05 loss_dqn tensor(3.7803e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11386752128601074
dqn reward tensor(100., device='cuda:0') e 0.05 loss_dqn tensor(3.5785e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13064339756965637
dqn reward tensor(63.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1773921251296997
dqn reward tensor(56.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4376e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11155936121940613
dqn reward tensor(-141.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6341e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04743180423974991
dqn reward tensor(-86.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6015e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1032048836350441
dqn reward tensor(46., device='cuda:0') e 0.05 loss_dqn tensor(4.7018e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621430218219757
dqn reward tensor(-469.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4889e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16785138845443726
dqn reward tensor(-69.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8977e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08748595416545868
dqn reward tensor(285.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.3312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09176255017518997
dqn reward tensor(35.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2848e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07491078972816467
dqn reward tensor(-321.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019799642264842987
dqn reward tensor(142.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17841209471225739
dqn reward tensor(102.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12008316814899445
dqn reward tensor(-171., device='cuda:0') e 0.05 loss_dqn tensor(4.6349e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18782858550548553
dqn reward tensor(-26.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2507e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10287198424339294
dqn reward tensor(-77.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3608e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014195701107382774
dqn reward tensor(49.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23642224073410034
dqn reward tensor(87.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4153e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14306727051734924
dqn reward tensor(11.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5996e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0895104929804802
dqn reward tensor(124.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.4632e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07361534982919693
dqn reward tensor(-315.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2605e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14914289116859436
dqn reward tensor(-73.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6886e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09948800504207611
dqn reward tensor(380.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10593870282173157
dqn reward tensor(35.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2721e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050410788506269455
dqn reward tensor(-109.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2883e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17460857331752777
dqn reward tensor(-35.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.5961e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10242237150669098
dqn reward tensor(231.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1748e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032983072102069855
dqn reward tensor(-102.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3622e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1880902349948883
dqn reward tensor(152.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2977e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058302588760852814
dqn reward tensor(62.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7297e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09168664366006851
dqn reward tensor(31.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1708623170852661
dqn reward tensor(155.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3487e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1698058545589447
dqn reward tensor(37.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3514e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26885682344436646
dqn reward tensor(-43.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8330e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09605827182531357
dqn reward tensor(49.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5342e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19068500399589539
dqn reward tensor(141.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.5152e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09801110625267029
dqn reward tensor(93.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056336306035518646
dqn reward tensor(235.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3182e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25243645906448364
dqn reward tensor(-2.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1929e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09632311761379242
dqn reward tensor(256.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2818e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11768466979265213
dqn reward tensor(303.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6388e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04234538972377777
dqn reward tensor(-31.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03201183304190636
dqn reward tensor(256., device='cuda:0') e 0.05 loss_dqn tensor(5.2349e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1740587055683136
dqn reward tensor(42.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1493e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17660854756832123
dqn reward tensor(80.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4832e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09414396435022354
dqn reward tensor(182.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16648393869400024
dqn reward tensor(56.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.0573e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11271989345550537
dqn reward tensor(165.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2680e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10020089894533157
dqn reward tensor(68.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3469e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037630487233400345
dqn reward tensor(-203., device='cuda:0') e 0.05 loss_dqn tensor(4.5072e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0548689067363739
dqn reward tensor(151.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5159e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.091117724776268
dqn reward tensor(96.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3287e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19365397095680237
dqn reward tensor(-59.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1137e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18231511116027832
dqn reward tensor(28.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8286e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08853224664926529
dqn reward tensor(130.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3651e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06673578172922134
dqn reward tensor(42.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3923e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01900249719619751
dqn reward tensor(84.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07832113653421402
dqn reward tensor(67.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7767e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.135625958442688
dqn reward tensor(275.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1741e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22441959381103516
dqn reward tensor(-32., device='cuda:0') e 0.05 loss_dqn tensor(4.5779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20441846549510956
dqn reward tensor(125.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4852e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0479092076420784
dqn reward tensor(59.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3221e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15311571955680847
dqn reward tensor(162.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1486319750547409
dqn reward tensor(82.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6389e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1052081286907196
dqn reward tensor(-117.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6620e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38594406843185425
dqn reward tensor(168.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2581e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16513022780418396
dqn reward tensor(207.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2576e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10853396356105804
dqn reward tensor(145., device='cuda:0') e 0.05 loss_dqn tensor(4.3135e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10255037993192673
dqn reward tensor(197.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5435e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1730053424835205
dqn reward tensor(77.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1109e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524060219526291
dqn reward tensor(37.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7637e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11848984658718109
dqn reward tensor(62.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2941e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13666316866874695
dqn reward tensor(-175.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4002e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840795814990997
dqn reward tensor(-46.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3036e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13476914167404175
dqn reward tensor(174.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1890e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13001304864883423
dqn reward tensor(160., device='cuda:0') e 0.05 loss_dqn tensor(3.5257e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09355255961418152
dqn reward tensor(38.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0826835110783577
dqn reward tensor(142.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1258e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052911631762981415
dqn reward tensor(194.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1928e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17706583440303802
dqn reward tensor(-537.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2724e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1060384064912796
dqn reward tensor(21.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0588344931602478
dqn reward tensor(-25.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1289144605398178
dqn reward tensor(126.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13884028792381287
dqn reward tensor(36.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2583e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07940015941858292
dqn reward tensor(-17., device='cuda:0') e 0.05 loss_dqn tensor(3.1960e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10237035155296326
dqn reward tensor(76.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9191e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08791930228471756
dqn reward tensor(9.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07485517114400864
dqn reward tensor(-153., device='cuda:0') e 0.05 loss_dqn tensor(3.1826e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2063310146331787
dqn reward tensor(-195., device='cuda:0') e 0.05 loss_dqn tensor(3.1339e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2616861164569855
dqn reward tensor(158.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3900e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06396828591823578
dqn reward tensor(212.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.1209e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08726875483989716
dqn reward tensor(147.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4304e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2554827034473419
dqn reward tensor(-40.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1821e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06133487448096275
dqn reward tensor(74.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1968e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10594748705625534
dqn reward tensor(138., device='cuda:0') e 0.05 loss_dqn tensor(3.1771e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1847117841243744
dqn reward tensor(49.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5415e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.252241313457489
dqn reward tensor(24.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.3749e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14133398234844208
dqn reward tensor(77.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1517e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16446906328201294
dqn reward tensor(62.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5221e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11359547823667526
dqn reward tensor(92.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5468e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09032048285007477
dqn reward tensor(82.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10377022624015808
dqn reward tensor(371.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2412e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27772417664527893
dqn reward tensor(249., device='cuda:0') e 0.05 loss_dqn tensor(3.2457e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08067358285188675
dqn reward tensor(-45.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8431e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1404448002576828
dqn reward tensor(213.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.2416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11729969084262848
dqn reward tensor(-52.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2967e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19181561470031738
dqn reward tensor(186.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2627e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26092442870140076
dqn reward tensor(197.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1793e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0891619399189949
dqn reward tensor(135.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1264e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524927318096161
dqn reward tensor(161.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4312e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05420240759849548
dqn reward tensor(83.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.9448e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11731195449829102
dqn reward tensor(-19.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07350023835897446
dqn reward tensor(155., device='cuda:0') e 0.05 loss_dqn tensor(3.1566e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10837139189243317
dqn reward tensor(-42., device='cuda:0') e 0.05 loss_dqn tensor(3.4840e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0600837804377079
dqn reward tensor(183.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1741e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09426748007535934
dqn reward tensor(-30.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13483291864395142
dqn reward tensor(-99.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0842e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07766137272119522
dqn reward tensor(-216.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.5392e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04535834118723869
dqn reward tensor(237.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.5248e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12948347628116608
dqn reward tensor(203.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1050e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06593768298625946
dqn reward tensor(-108.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0431e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09930103272199631
dqn reward tensor(40.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1471e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01730547472834587
dqn reward tensor(35.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1315e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15734167397022247
dqn reward tensor(-73.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2952e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06983156502246857
dqn reward tensor(178.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.4373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1560511738061905
dqn reward tensor(159.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6480e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1298528015613556
dqn reward tensor(-76.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9017e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07895416766405106
dqn reward tensor(102.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9827e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16335546970367432
dqn reward tensor(-2., device='cuda:0') e 0.05 loss_dqn tensor(5.6316e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04270648583769798
dqn reward tensor(221.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01849323883652687
dqn reward tensor(92., device='cuda:0') e 0.05 loss_dqn tensor(9.0323e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19487164914608002
dqn reward tensor(44.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5631e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14013072848320007
dqn reward tensor(307.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10200868546962738
dqn reward tensor(271.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12967054545879364
dqn reward tensor(210.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3385e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06438079476356506
dqn reward tensor(189.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5444e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13819542527198792
dqn reward tensor(138.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4285e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413488328456879
dqn reward tensor(214.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.2578e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11456046253442764
dqn reward tensor(231.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3597e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15912199020385742
dqn reward tensor(327.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.1702e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.342090368270874
dqn reward tensor(345.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3163e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2739413380622864
dqn reward tensor(365.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.3159e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2299642264842987
dqn reward tensor(343.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6575e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18933582305908203
dqn reward tensor(-69.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6545e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09770223498344421
dqn reward tensor(189.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8057e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08686251938343048
dqn reward tensor(388.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.1854e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10628992319107056
dqn reward tensor(93.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.6228e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1849481463432312
dqn reward tensor(426.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1788e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08207917958498001
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2509e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18290752172470093
dqn reward tensor(331.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3361e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061164937913417816
dqn reward tensor(420.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0843e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06180174648761749
dqn reward tensor(339., device='cuda:0') e 0.05 loss_dqn tensor(6.5718e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1424461007118225
dqn reward tensor(282.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.2705e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10512363165616989
dqn reward tensor(404.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8502e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023165777325630188
dqn reward tensor(421.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4043e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11025020480155945
dqn reward tensor(195.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2516e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1943008303642273
dqn reward tensor(138.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6999e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21186189353466034
dqn reward tensor(235.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9192e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1355559527873993
dqn reward tensor(233.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0172e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13973471522331238
dqn reward tensor(173.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5722e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.270087331533432
dqn reward tensor(-97.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1382228583097458
dqn reward tensor(363.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23062044382095337
dqn reward tensor(239.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2130e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12855534255504608
dqn reward tensor(217., device='cuda:0') e 0.05 loss_dqn tensor(8.0805e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21523146331310272
dqn reward tensor(392.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.0720e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18103235960006714
dqn reward tensor(153.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9481e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058710891753435135
dqn reward tensor(225.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5989e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17673206329345703
dqn reward tensor(413.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1203e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24196884036064148
dqn reward tensor(246.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9408e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06903482973575592
dqn reward tensor(11., device='cuda:0') e 0.05 loss_dqn tensor(3.8526e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1405780017375946
dqn reward tensor(235.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.9396e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1029440239071846
dqn reward tensor(-84., device='cuda:0') e 0.05 loss_dqn tensor(3.9725e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14365236461162567
dqn reward tensor(374.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.125385582447052
dqn reward tensor(292.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8013e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10450220853090286
dqn reward tensor(397.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0761e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0999310091137886
dqn reward tensor(466.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8162e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09147853404283524
dqn reward tensor(509.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13657672703266144
dqn reward tensor(324.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1966322958469391
dqn reward tensor(543., device='cuda:0') e 0.05 loss_dqn tensor(2.7545e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0884820967912674
dqn reward tensor(252.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9913e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08709774166345596
dqn reward tensor(248., device='cuda:0') e 0.05 loss_dqn tensor(7.3898e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13799208402633667
dqn reward tensor(274.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7279e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15865035355091095
dqn reward tensor(568.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6586e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08301244676113129
dqn reward tensor(-31.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7794e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2752044200897217
dqn reward tensor(410.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6331e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2264098823070526
dqn reward tensor(414.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6539e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19302165508270264
dqn reward tensor(404.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5763e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01597389578819275
dqn reward tensor(401.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08804354071617126
dqn reward tensor(603.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6892e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2816717028617859
dqn reward tensor(239.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9659e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3453022837638855
dqn reward tensor(374.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8754e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13822802901268005
dqn reward tensor(508.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8501e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17709873616695404
dqn reward tensor(475.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7688e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05328172445297241
dqn reward tensor(308.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9987e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09998132288455963
dqn reward tensor(528.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5779e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15754073858261108
dqn reward tensor(326.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29605919122695923
dqn reward tensor(398.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4699e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12034938484430313
dqn reward tensor(469.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5089e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09777764976024628
dqn reward tensor(400.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5168e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16765251755714417
dqn reward tensor(421.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20064324140548706
dqn reward tensor(285.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3765e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17521196603775024
dqn reward tensor(255.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5366e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.271463006734848
dqn reward tensor(471., device='cuda:0') e 0.05 loss_dqn tensor(2.6064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07431843876838684
dqn reward tensor(537., device='cuda:0') e 0.05 loss_dqn tensor(2.3963e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19596049189567566
dqn reward tensor(350.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4400e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17361128330230713
dqn reward tensor(315.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8122e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20152030885219574
dqn reward tensor(370.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1182062178850174
dqn reward tensor(413.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.0923e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1165623739361763
dqn reward tensor(436.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5284e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04927678406238556
dqn reward tensor(296.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5104e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2548190653324127
dqn reward tensor(392.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4377e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13871873915195465
dqn reward tensor(399.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4268e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13943231105804443
dqn reward tensor(654.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8544e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11517716199159622
dqn reward tensor(280.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3987e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27437883615493774
dqn reward tensor(368.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4226e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20686763525009155
dqn reward tensor(580.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7694e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13017384707927704
dqn reward tensor(557.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10480020940303802
dqn reward tensor(570.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1361e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26704853773117065
dqn reward tensor(356.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1189e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13843630254268646
dqn reward tensor(461.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9030e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05821280553936958
dqn reward tensor(530.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06418928503990173
dqn reward tensor(558., device='cuda:0') e 0.05 loss_dqn tensor(4.9888e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07195106148719788
dqn reward tensor(435.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8305e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19287940859794617
dqn reward tensor(466.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2534e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038039568811655045
dqn reward tensor(660.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.8108e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09983067214488983
dqn reward tensor(559.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.4529e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1609984040260315
dqn reward tensor(438., device='cuda:0') e 0.05 loss_dqn tensor(5.5359e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02784404531121254
dqn reward tensor(524.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9996e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09812529385089874
dqn reward tensor(669.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14314283430576324
dqn reward tensor(636., device='cuda:0') e 0.05 loss_dqn tensor(5.8689e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15197524428367615
dqn reward tensor(497.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.1726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365195035934448
dqn reward tensor(622.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4136e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15447765588760376
dqn reward tensor(561.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3037e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20942622423171997
dqn reward tensor(645.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3985e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07845599204301834
dqn reward tensor(671., device='cuda:0') e 0.05 loss_dqn tensor(6.4580e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023366408422589302
dqn reward tensor(501.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7500e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0750720202922821
dqn reward tensor(570.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07495864480733871
dqn reward tensor(572.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06699080020189285
dqn reward tensor(633.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1183e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30569157004356384
dqn reward tensor(697.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3384e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2014763355255127
dqn reward tensor(526.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2186e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08753381669521332
dqn reward tensor(643.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.6780e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13931839168071747
dqn reward tensor(587.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.3801e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030953947454690933
dqn reward tensor(633.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3780e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0269046388566494
dqn reward tensor(600.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1866e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20469391345977783
dqn reward tensor(629.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9211e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0260922834277153
dqn reward tensor(508.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0415e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26507771015167236
dqn reward tensor(510.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0012e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2136378288269043
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0328e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08014187216758728
dqn reward tensor(650.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9512e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10727504640817642
dqn reward tensor(535.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.7632e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18170534074306488
dqn reward tensor(620.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.6640e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10227328538894653
dqn reward tensor(700.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8147e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10082874447107315
dqn reward tensor(554.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3547e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10077191144227982
dqn reward tensor(693.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0780e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2382926344871521
dqn reward tensor(519.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1167299896478653
dqn reward tensor(577.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4175e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14582480490207672
dqn reward tensor(715.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1567e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0339415967464447
dqn reward tensor(808.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3743e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16993916034698486
dqn reward tensor(698.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2123e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19452935457229614
dqn reward tensor(575.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11679325252771378
dqn reward tensor(525., device='cuda:0') e 0.05 loss_dqn tensor(8.0306e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1542263776063919
dqn reward tensor(640.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.3593e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08075214922428131
dqn reward tensor(787.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0839e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.166208878159523
dqn reward tensor(572.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.8386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028951331973075867
dqn reward tensor(662.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2816891074180603
dqn reward tensor(528.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20239895582199097
dqn reward tensor(788.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3296e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08953528851270676
dqn reward tensor(452., device='cuda:0') e 0.05 loss_dqn tensor(2.5159e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15197494626045227
dqn reward tensor(644.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.6733e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13077902793884277
dqn reward tensor(669.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.1627e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08795146644115448
dqn reward tensor(549.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0163e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03805190697312355
dqn reward tensor(596.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7273e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09766896069049835
dqn reward tensor(582.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.2373e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.188887357711792
dqn reward tensor(563.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8109e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16178591549396515
dqn reward tensor(553.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0156e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1387123465538025
dqn reward tensor(623.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8902e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15943564474582672
dqn reward tensor(604.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0846e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09704650193452835
dqn reward tensor(733.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8140e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943201333284378
dqn reward tensor(667.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11399810016155243
dqn reward tensor(615.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03607344254851341
dqn reward tensor(665.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09802315384149551
dqn reward tensor(774.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15122541785240173
dqn reward tensor(634.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8584e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035505760461091995
dqn reward tensor(757.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6306e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18591046333312988
dqn reward tensor(710.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7344e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18982848525047302
dqn reward tensor(592.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8790e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17759329080581665
dqn reward tensor(745., device='cuda:0') e 0.05 loss_dqn tensor(8.5844e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.127486914396286
dqn reward tensor(713.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8765e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057421132922172546
dqn reward tensor(659.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.2610e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06059909984469414
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6710e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3405003249645233
dqn reward tensor(494.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.4631e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11347395926713943
dqn reward tensor(621.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06620481610298157
dqn reward tensor(710., device='cuda:0') e 0.05 loss_dqn tensor(9.1627e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26797741651535034
dqn reward tensor(650.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1680e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10298778861761093
dqn reward tensor(657.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.9080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18075737357139587
dqn reward tensor(634.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0973e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.110942542552948
dqn reward tensor(691.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3084e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12112422287464142
dqn reward tensor(646.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2127e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18967261910438538
dqn reward tensor(695.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0007e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1418745219707489
dqn reward tensor(599.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8871e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11306088417768478
dqn reward tensor(669.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08430097997188568
dqn reward tensor(801.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8483e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19324743747711182
dqn reward tensor(653.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1204e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17024551331996918
dqn reward tensor(605.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.2279e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11864358186721802
dqn reward tensor(642., device='cuda:0') e 0.05 loss_dqn tensor(7.2532e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17039792239665985
dqn reward tensor(692.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8696e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24200326204299927
dqn reward tensor(681.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6167e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2322964370250702
dqn reward tensor(647.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2550e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08462678641080856
dqn reward tensor(504.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6849e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18793177604675293
dqn reward tensor(682.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2402e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23626863956451416
dqn reward tensor(495.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6993e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2847141921520233
dqn reward tensor(598.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1167646050453186
dqn reward tensor(467.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11853054165840149
dqn reward tensor(673.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1691e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1859295815229416
dqn reward tensor(428.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1261e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17871485650539398
dqn reward tensor(596.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3697e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09212255477905273
dqn reward tensor(635.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4223e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1494857668876648
dqn reward tensor(569.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21255730092525482
dqn reward tensor(357.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0831e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14929769933223724
dqn reward tensor(247.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1608e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1688961386680603
dqn reward tensor(611.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14276307821273804
dqn reward tensor(409.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29928264021873474
dqn reward tensor(289.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.5852e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19985803961753845
dqn reward tensor(502.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2028e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26390084624290466
dqn reward tensor(596.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9395e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11281193792819977
dqn reward tensor(464.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3360e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1152077466249466
dqn reward tensor(605.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6072e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1411270946264267
dqn reward tensor(542., device='cuda:0') e 0.05 loss_dqn tensor(4.3628e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04533093050122261
dqn reward tensor(547.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9562e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0406084805727005
dqn reward tensor(605.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042342882603406906
dqn reward tensor(357.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4701e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19277647137641907
dqn reward tensor(41.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3140e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025110436603426933
Evaluating...
Train: {'rocauc': 0.7632174556789322} 8.300407409667969
=====Epoch 19=====
Training...
dqn reward tensor(692.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9416e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33234941959381104
dqn reward tensor(624.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11088720709085464
dqn reward tensor(673.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12053870409727097
dqn reward tensor(610.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7782e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2975466251373291
dqn reward tensor(298.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4395e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09608224034309387
dqn reward tensor(607.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3835e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2500259280204773
dqn reward tensor(485.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.3802e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16112186014652252
dqn reward tensor(523.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10935359448194504
dqn reward tensor(460.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2018e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11025604605674744
dqn reward tensor(543.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8788e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23304153978824615
dqn reward tensor(447.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.7305e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11706351488828659
dqn reward tensor(479., device='cuda:0') e 0.05 loss_dqn tensor(4.7045e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08218219876289368
dqn reward tensor(666.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3755e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06483299285173416
dqn reward tensor(541.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06001570075750351
dqn reward tensor(472.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4430e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.174238920211792
dqn reward tensor(244.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7870e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23858246207237244
dqn reward tensor(540.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17001843452453613
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9753e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16322378814220428
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0220e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12926313281059265
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0955e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11208697408437729
dqn reward tensor(636.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0152e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02882682904601097
dqn reward tensor(290.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06955639272928238
dqn reward tensor(571.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9086e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11791194975376129
dqn reward tensor(610.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.9859e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16907964646816254
dqn reward tensor(413.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9951e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09178367257118225
dqn reward tensor(681., device='cuda:0') e 0.05 loss_dqn tensor(4.3931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09205755591392517
dqn reward tensor(607.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5861e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14822514355182648
dqn reward tensor(318.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5240e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.226684108376503
dqn reward tensor(585.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0069e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2686842083930969
dqn reward tensor(526.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0370e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07495348155498505
dqn reward tensor(705.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0084e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23612651228904724
dqn reward tensor(780.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1045e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07075603306293488
dqn reward tensor(503.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0183e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11565904319286346
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9010e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06986363232135773
dqn reward tensor(575.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0375e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13975147902965546
dqn reward tensor(386.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4610e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18486224114894867
dqn reward tensor(426.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6923e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1706218272447586
dqn reward tensor(580.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16109608113765717
dqn reward tensor(563.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052693307399749756
dqn reward tensor(497.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6176e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040686506778001785
dqn reward tensor(737.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0422e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09838766604661942
dqn reward tensor(545.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6869e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08466754853725433
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4172e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10890328884124756
dqn reward tensor(299.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11141387373209
dqn reward tensor(524.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13764461874961853
dqn reward tensor(523.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0323e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10981817543506622
dqn reward tensor(478.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5136e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10925737768411636
dqn reward tensor(265.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.2036e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044166672974824905
dqn reward tensor(405.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1137e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.078958660364151
dqn reward tensor(505.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1498e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13208605349063873
dqn reward tensor(444.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22907976806163788
dqn reward tensor(625., device='cuda:0') e 0.05 loss_dqn tensor(1.0570e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019359663128852844
dqn reward tensor(528.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09051348268985748
dqn reward tensor(521.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08855342864990234
dqn reward tensor(536.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0540e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17285171151161194
dqn reward tensor(335., device='cuda:0') e 0.05 loss_dqn tensor(1.1105e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11285363137722015
dqn reward tensor(490.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0174e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18693336844444275
dqn reward tensor(662.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1207e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18824055790901184
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3071e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16836349666118622
dqn reward tensor(418.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08586001396179199
dqn reward tensor(506.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0688e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062402911484241486
dqn reward tensor(611., device='cuda:0') e 0.05 loss_dqn tensor(1.1002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07143684476613998
dqn reward tensor(505.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2971e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08730306476354599
dqn reward tensor(282.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0819e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10278131812810898
dqn reward tensor(528.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1765e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904851198196411
dqn reward tensor(542.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1368e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17954877018928528
dqn reward tensor(370.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08250760287046432
dqn reward tensor(434.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1439e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2351992279291153
dqn reward tensor(472.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.5738e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029390625655651093
dqn reward tensor(471.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12858782708644867
dqn reward tensor(553.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1991e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21027688682079315
dqn reward tensor(371.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9735e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04889178276062012
dqn reward tensor(477.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1053810715675354
dqn reward tensor(406.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24735094606876373
dqn reward tensor(530.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5406e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06180812418460846
dqn reward tensor(512.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4276e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10988683253526688
dqn reward tensor(502.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25032854080200195
dqn reward tensor(390.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16240160167217255
dqn reward tensor(427.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16388541460037231
dqn reward tensor(565.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8098e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09824477881193161
dqn reward tensor(518.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1481e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0864311158657074
dqn reward tensor(420.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8248e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20097187161445618
dqn reward tensor(386.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8073e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2150508612394333
dqn reward tensor(471.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0989e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12762968242168427
dqn reward tensor(579.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0539e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14682099223136902
dqn reward tensor(603.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15356311202049255
dqn reward tensor(660.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0040e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1834908425807953
dqn reward tensor(525.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2158e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11938049644231796
dqn reward tensor(623.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0271e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1590043306350708
dqn reward tensor(401.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9969e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2141885757446289
dqn reward tensor(613.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6177e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09225751459598541
dqn reward tensor(510.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19506031274795532
dqn reward tensor(544.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0841e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15453556180000305
dqn reward tensor(323.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8638e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.38046908378601074
dqn reward tensor(516.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17950403690338135
dqn reward tensor(467.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.9554e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0921429693698883
dqn reward tensor(563.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1026e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09408696740865707
dqn reward tensor(470.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.8035e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15468916296958923
dqn reward tensor(303.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9261e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20836450159549713
dqn reward tensor(620.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0169e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07501193135976791
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17948877811431885
dqn reward tensor(675., device='cuda:0') e 0.05 loss_dqn tensor(1.1447e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12383082509040833
dqn reward tensor(501.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5843e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21475206315517426
dqn reward tensor(359.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1358e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09341630339622498
dqn reward tensor(577.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8740e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0502166710793972
dqn reward tensor(489., device='cuda:0') e 0.05 loss_dqn tensor(4.0036e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12206215411424637
dqn reward tensor(779.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0955e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11461666226387024
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7842e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12220297753810883
dqn reward tensor(484.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9262e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13638590276241302
dqn reward tensor(373.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0128e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12509997189044952
dqn reward tensor(410.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3452e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31049826741218567
dqn reward tensor(517.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5061e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06331048160791397
dqn reward tensor(718.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20445796847343445
dqn reward tensor(486.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1274e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1243976354598999
dqn reward tensor(601.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21016594767570496
dqn reward tensor(290.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7120e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04074317216873169
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3415e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16002272069454193
dqn reward tensor(583.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5754e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16328071057796478
dqn reward tensor(388.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6962e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09804867208003998
dqn reward tensor(313.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.5990e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10752144455909729
dqn reward tensor(486., device='cuda:0') e 0.05 loss_dqn tensor(1.1541e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16785675287246704
dqn reward tensor(603.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15469856560230255
dqn reward tensor(481.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06323026120662689
dqn reward tensor(230.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2036137580871582
dqn reward tensor(719., device='cuda:0') e 0.05 loss_dqn tensor(1.1757e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.203840434551239
dqn reward tensor(575.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6644e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047723252326250076
dqn reward tensor(373.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11571794748306274
dqn reward tensor(559.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1671e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03337818384170532
dqn reward tensor(574.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1024e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07867097854614258
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11512256413698196
dqn reward tensor(644.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11113633215427399
dqn reward tensor(417.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1220e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07114453613758087
dqn reward tensor(357.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2244e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10236161947250366
dqn reward tensor(646.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.6894e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018012885004281998
dqn reward tensor(405.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09225861728191376
dqn reward tensor(436.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1364e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15551114082336426
dqn reward tensor(308.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2848e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13204877078533173
dqn reward tensor(531.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15523919463157654
dqn reward tensor(541., device='cuda:0') e 0.05 loss_dqn tensor(1.1636e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14699676632881165
dqn reward tensor(593.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2318e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06317970901727676
dqn reward tensor(552.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1194e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16448399424552917
dqn reward tensor(527.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9691e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26307153701782227
dqn reward tensor(393.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1617e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07086163759231567
dqn reward tensor(505.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1936e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03676478937268257
dqn reward tensor(559.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2403840571641922
dqn reward tensor(484.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0820e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25649338960647583
dqn reward tensor(610.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4133e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03767672926187515
dqn reward tensor(542.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2687e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19472715258598328
dqn reward tensor(546.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3459e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10232257843017578
dqn reward tensor(468.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1074e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053167156875133514
dqn reward tensor(530.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3599e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2076442539691925
dqn reward tensor(562.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2631e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07894477248191833
dqn reward tensor(411.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10258425772190094
dqn reward tensor(516.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2777e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14940841495990753
dqn reward tensor(411.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8749e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2428002655506134
dqn reward tensor(510.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4133e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050527263432741165
dqn reward tensor(460.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6465e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040800563991069794
dqn reward tensor(365.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6299e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07197852432727814
dqn reward tensor(355.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2020e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32203590869903564
dqn reward tensor(407.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4673e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06883533298969269
dqn reward tensor(572.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3800e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21310408413410187
dqn reward tensor(631.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4653e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23537352681159973
dqn reward tensor(479., device='cuda:0') e 0.05 loss_dqn tensor(1.2175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10424616187810898
dqn reward tensor(522.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4086e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09582599997520447
dqn reward tensor(560., device='cuda:0') e 0.05 loss_dqn tensor(9.9676e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10105431079864502
dqn reward tensor(442.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1672e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13774842023849487
dqn reward tensor(327.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8744e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10261116176843643
dqn reward tensor(406.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0775e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22183388471603394
dqn reward tensor(568.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3680e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1537352204322815
dqn reward tensor(428.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3941e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0388764813542366
dqn reward tensor(471., device='cuda:0') e 0.05 loss_dqn tensor(6.6644e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1028585135936737
dqn reward tensor(741., device='cuda:0') e 0.05 loss_dqn tensor(4.9980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03382084518671036
dqn reward tensor(511.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.9467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12737715244293213
dqn reward tensor(428.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1167e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11085881292819977
dqn reward tensor(516.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7291e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09911511838436127
dqn reward tensor(577.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4072e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14578038454055786
dqn reward tensor(570.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6692e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20393148064613342
dqn reward tensor(487.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3094e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30606332421302795
dqn reward tensor(359.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1214035153388977
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12054517865180969
dqn reward tensor(443., device='cuda:0') e 0.05 loss_dqn tensor(2.4873e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06781613826751709
dqn reward tensor(514., device='cuda:0') e 0.05 loss_dqn tensor(6.6482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15792378783226013
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4675e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04445397108793259
dqn reward tensor(467.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032374896109104156
dqn reward tensor(499.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1873e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1274765431880951
dqn reward tensor(357.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.5887e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1881456822156906
dqn reward tensor(598.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2283605933189392
dqn reward tensor(571., device='cuda:0') e 0.05 loss_dqn tensor(1.0834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09276482462882996
dqn reward tensor(370.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.9338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05302247777581215
dqn reward tensor(399.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08707012236118317
dqn reward tensor(454.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8019e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13909359276294708
dqn reward tensor(566.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.2263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10703504085540771
dqn reward tensor(589.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0030e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039258237928152084
dqn reward tensor(526., device='cuda:0') e 0.05 loss_dqn tensor(1.4478e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2757781744003296
dqn reward tensor(380.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4659e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14536315202713013
dqn reward tensor(629.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4469e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29595932364463806
dqn reward tensor(497.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19953295588493347
dqn reward tensor(384.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4750e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08915814012289047
dqn reward tensor(615.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9015e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08897524327039719
dqn reward tensor(498.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6350e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08180741965770721
dqn reward tensor(341.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12271438539028168
dqn reward tensor(550.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03961954265832901
dqn reward tensor(613.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3713e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29243001341819763
dqn reward tensor(478., device='cuda:0') e 0.05 loss_dqn tensor(1.4142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14838334918022156
dqn reward tensor(430.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6137e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043067339807748795
dqn reward tensor(353.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.7871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2714203894138336
dqn reward tensor(657.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5005e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09834014624357224
dqn reward tensor(485.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9137e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14058685302734375
dqn reward tensor(394.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0165e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15102362632751465
dqn reward tensor(328.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8357e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07753646373748779
dqn reward tensor(504., device='cuda:0') e 0.05 loss_dqn tensor(4.2989e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16376066207885742
dqn reward tensor(587.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4896e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10542890429496765
dqn reward tensor(597.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0605410672724247
dqn reward tensor(419., device='cuda:0') e 0.05 loss_dqn tensor(1.6430e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20552434027194977
dqn reward tensor(661.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3711e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1822959929704666
dqn reward tensor(252.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2229e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05880839377641678
dqn reward tensor(528.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4722e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17935755848884583
dqn reward tensor(475.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12902984023094177
dqn reward tensor(328.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2475e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3553881347179413
dqn reward tensor(351.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.5867e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11309948563575745
dqn reward tensor(535.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3875e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16215360164642334
dqn reward tensor(476.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4091e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1415683627128601
dqn reward tensor(589.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1693e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1987532079219818
dqn reward tensor(462.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045167263597249985
dqn reward tensor(486.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4512e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10449424386024475
dqn reward tensor(612.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9563e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11906129121780396
dqn reward tensor(440.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07950805127620697
dqn reward tensor(587.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7072e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19777923822402954
dqn reward tensor(424.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0778491348028183
dqn reward tensor(417., device='cuda:0') e 0.05 loss_dqn tensor(6.2565e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029165176674723625
dqn reward tensor(538.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4546e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25354430079460144
dqn reward tensor(490., device='cuda:0') e 0.05 loss_dqn tensor(1.4588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09483945369720459
dqn reward tensor(559.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5049e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053436070680618286
dqn reward tensor(476.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0929e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13369658589363098
dqn reward tensor(509.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1342555284500122
dqn reward tensor(555.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.0477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18544906377792358
dqn reward tensor(543.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5649e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12287870049476624
dqn reward tensor(553.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3961e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19878748059272766
dqn reward tensor(417.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.7827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26842984557151794
dqn reward tensor(435., device='cuda:0') e 0.05 loss_dqn tensor(6.8939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0752495601773262
dqn reward tensor(506.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9459e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09037142992019653
dqn reward tensor(433.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4328e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08387041091918945
dqn reward tensor(321.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8862e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20287775993347168
dqn reward tensor(570.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3649e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13632959127426147
dqn reward tensor(451.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12279105931520462
dqn reward tensor(655.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1383846402168274
dqn reward tensor(457.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22984318435192108
dqn reward tensor(564.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7316e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14309263229370117
dqn reward tensor(609.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3071e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06751719862222672
dqn reward tensor(463.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6805e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20032458007335663
dqn reward tensor(406.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6508e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047877173870801926
dqn reward tensor(391.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.6182e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049773890525102615
dqn reward tensor(474.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10385074466466904
dqn reward tensor(575.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3547e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09013892710208893
dqn reward tensor(477.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5625e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11157837510108948
dqn reward tensor(400.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1167297512292862
dqn reward tensor(571.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2198355495929718
dqn reward tensor(621., device='cuda:0') e 0.05 loss_dqn tensor(1.4174e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020569495856761932
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4307e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021585237234830856
dqn reward tensor(514.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6134e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19263620674610138
dqn reward tensor(441., device='cuda:0') e 0.05 loss_dqn tensor(2.9895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028658494353294373
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3659e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15922892093658447
dqn reward tensor(691.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2691243290901184
dqn reward tensor(504.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6791e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023906873539090157
dqn reward tensor(609.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15088707208633423
dqn reward tensor(437.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13903005421161652
dqn reward tensor(540.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1085e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07454749196767807
dqn reward tensor(705.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3920e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1396966576576233
dqn reward tensor(553.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6734e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10214289277791977
dqn reward tensor(597.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8520e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16114075481891632
dqn reward tensor(679.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5035e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.260820597410202
dqn reward tensor(436.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4815e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10145119577646255
dqn reward tensor(454., device='cuda:0') e 0.05 loss_dqn tensor(1.4828e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10379370301961899
dqn reward tensor(471.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1852516531944275
dqn reward tensor(551.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4067e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14853960275650024
dqn reward tensor(459., device='cuda:0') e 0.05 loss_dqn tensor(1.4537e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22610849142074585
dqn reward tensor(669.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3919e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12060049176216125
dqn reward tensor(503.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10698315501213074
dqn reward tensor(400.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6973e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08650514483451843
dqn reward tensor(448.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4677e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1940997838973999
dqn reward tensor(599.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.176215261220932
dqn reward tensor(569.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8757e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3106381297111511
dqn reward tensor(342.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7072e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17412853240966797
dqn reward tensor(467.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4977e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16816005110740662
dqn reward tensor(483.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.9128e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1461600810289383
dqn reward tensor(483., device='cuda:0') e 0.05 loss_dqn tensor(1.3980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2260594218969345
dqn reward tensor(557.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4900e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14144238829612732
dqn reward tensor(506.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4868e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0921449288725853
dqn reward tensor(533.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5097e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11367955803871155
dqn reward tensor(581.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09070132672786713
dqn reward tensor(407., device='cuda:0') e 0.05 loss_dqn tensor(1.5471e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22070226073265076
dqn reward tensor(557.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15903079509735107
dqn reward tensor(562.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4597e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1081581637263298
dqn reward tensor(503.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05110101401805878
dqn reward tensor(473.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17787912487983704
dqn reward tensor(477.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9833e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032636724412441254
dqn reward tensor(479.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12651140987873077
dqn reward tensor(601.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8826e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24260812997817993
dqn reward tensor(405.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.256828248500824
dqn reward tensor(679., device='cuda:0') e 0.05 loss_dqn tensor(1.3701e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3155369758605957
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5213e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15641041100025177
dqn reward tensor(464., device='cuda:0') e 0.05 loss_dqn tensor(1.7391e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07097844779491425
dqn reward tensor(552.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05376411974430084
dqn reward tensor(553.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025202453136444092
dqn reward tensor(389.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09357231855392456
dqn reward tensor(679.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3630e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2047719657421112
dqn reward tensor(493.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5057e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09702802449464798
dqn reward tensor(557.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9935e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2362971305847168
dqn reward tensor(441.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5125e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18340545892715454
dqn reward tensor(382.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1693490743637085
dqn reward tensor(502.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.2084e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1345154494047165
dqn reward tensor(639.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4745e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09103438258171082
dqn reward tensor(574.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3882e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2302401065826416
dqn reward tensor(444.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5095e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23920798301696777
dqn reward tensor(292.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1416860818862915
dqn reward tensor(578.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5434e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12621557712554932
dqn reward tensor(666.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5634e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1460028439760208
dqn reward tensor(578.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5487e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051222674548625946
dqn reward tensor(531.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10704883188009262
dqn reward tensor(609.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4797e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1661815643310547
dqn reward tensor(543.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4410e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09544231742620468
dqn reward tensor(563.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5442e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12493916600942612
dqn reward tensor(509.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5392e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0655348151922226
dqn reward tensor(492.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2154e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.165202334523201
dqn reward tensor(504., device='cuda:0') e 0.05 loss_dqn tensor(3.6745e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14692151546478271
dqn reward tensor(491.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2348e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03919064998626709
dqn reward tensor(562., device='cuda:0') e 0.05 loss_dqn tensor(4.5403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05704713985323906
dqn reward tensor(462.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5415e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12716439366340637
dqn reward tensor(579., device='cuda:0') e 0.05 loss_dqn tensor(1.0978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16478577256202698
dqn reward tensor(501.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4298e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2715194821357727
dqn reward tensor(691.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021916449069976807
dqn reward tensor(549.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5473e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11245352029800415
dqn reward tensor(241.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1133330687880516
dqn reward tensor(596.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02008180320262909
dqn reward tensor(661.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10686081647872925
dqn reward tensor(484.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4983e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07315455377101898
dqn reward tensor(623.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5536e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1184607520699501
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3846e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0908934473991394
dqn reward tensor(649.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1838400661945343
dqn reward tensor(480.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08906081318855286
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7612e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2724694609642029
dqn reward tensor(484.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04165272414684296
dqn reward tensor(667.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3122921884059906
dqn reward tensor(559.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09617714583873749
dqn reward tensor(634.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4867e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11561281979084015
dqn reward tensor(392.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2501e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21555811166763306
dqn reward tensor(300.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0798e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09818176180124283
dqn reward tensor(403.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.5586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23136481642723083
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6915e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0814426839351654
dqn reward tensor(452.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5545e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09547366946935654
dqn reward tensor(576.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5825e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2179935872554779
dqn reward tensor(388.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0993e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055699724704027176
dqn reward tensor(450.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1889030933380127
dqn reward tensor(510.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30006447434425354
dqn reward tensor(593.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08521506190299988
dqn reward tensor(403., device='cuda:0') e 0.05 loss_dqn tensor(2.0372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10564502328634262
dqn reward tensor(541.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5178e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08015263080596924
dqn reward tensor(680.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4054e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04741841182112694
dqn reward tensor(329.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5169e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19397105276584625
dqn reward tensor(509.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1247e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048457786440849304
dqn reward tensor(446.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8116e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03333393856883049
dqn reward tensor(407.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6296e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10627055913209915
dqn reward tensor(516.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14996877312660217
dqn reward tensor(387.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1144e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08389885723590851
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7484e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24943231046199799
dqn reward tensor(418., device='cuda:0') e 0.05 loss_dqn tensor(6.5372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04695580154657364
dqn reward tensor(425.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8499e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19503797590732574
dqn reward tensor(450.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6140e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10723531991243362
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08719269931316376
dqn reward tensor(662.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25905540585517883
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4489e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05912761762738228
dqn reward tensor(541.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23350106179714203
dqn reward tensor(486.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6671e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2765636146068573
dqn reward tensor(324.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.3133e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1872999370098114
dqn reward tensor(556.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1855e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23001866042613983
dqn reward tensor(467.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7716e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19219081103801727
dqn reward tensor(464.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08282823860645294
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5339e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15062406659126282
dqn reward tensor(514.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5697e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15865784883499146
dqn reward tensor(495.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4802e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1891942173242569
dqn reward tensor(427.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5642e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06705410033464432
dqn reward tensor(477.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9758e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14111576974391937
dqn reward tensor(388.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6476e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15469171106815338
dqn reward tensor(490.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5322e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24229680001735687
dqn reward tensor(494.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1784094274044037
dqn reward tensor(343.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5541e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15737195312976837
dqn reward tensor(455., device='cuda:0') e 0.05 loss_dqn tensor(1.5822e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07856155931949615
dqn reward tensor(318.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8032e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20967578887939453
dqn reward tensor(268.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6215e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09221208840608597
dqn reward tensor(438.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5634e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13642436265945435
dqn reward tensor(483.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5409e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07273455709218979
dqn reward tensor(508.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5966e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12569105625152588
dqn reward tensor(440., device='cuda:0') e 0.05 loss_dqn tensor(6.4403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025667648762464523
dqn reward tensor(446.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9981e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035057127475738525
dqn reward tensor(431.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0367e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1639358252286911
dqn reward tensor(465.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2674502730369568
dqn reward tensor(650.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22360500693321228
dqn reward tensor(373., device='cuda:0') e 0.05 loss_dqn tensor(1.2314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09011515229940414
dqn reward tensor(452.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21659250557422638
dqn reward tensor(512.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18389645218849182
dqn reward tensor(651.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1877e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1829647421836853
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5727e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15153959393501282
dqn reward tensor(571., device='cuda:0') e 0.05 loss_dqn tensor(1.4700e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11321328580379486
dqn reward tensor(457.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4837e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12487605959177017
dqn reward tensor(441.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04287666454911232
dqn reward tensor(445., device='cuda:0') e 0.05 loss_dqn tensor(1.5656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10694955289363861
dqn reward tensor(438.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08298926800489426
dqn reward tensor(619.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.3447e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12425630539655685
dqn reward tensor(516.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5562e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0859675258398056
dqn reward tensor(454.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19866687059402466
dqn reward tensor(476., device='cuda:0') e 0.05 loss_dqn tensor(9.5731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11803070455789566
dqn reward tensor(435.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7196e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09797164052724838
dqn reward tensor(416.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0213e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035678572952747345
dqn reward tensor(505., device='cuda:0') e 0.05 loss_dqn tensor(1.5204e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14270274341106415
dqn reward tensor(446.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21431007981300354
dqn reward tensor(252.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8387e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05098370835185051
dqn reward tensor(376.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3650e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1741291880607605
dqn reward tensor(492., device='cuda:0') e 0.05 loss_dqn tensor(1.5643e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04648084565997124
dqn reward tensor(387., device='cuda:0') e 0.05 loss_dqn tensor(7.7571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12386706471443176
dqn reward tensor(524.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12770825624465942
dqn reward tensor(391.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2589e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11524699628353119
dqn reward tensor(570., device='cuda:0') e 0.05 loss_dqn tensor(1.0773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03173355013132095
dqn reward tensor(467.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6700e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09070108830928802
dqn reward tensor(454.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1684429943561554
dqn reward tensor(377.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01932394504547119
dqn reward tensor(396.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8666e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15172937512397766
dqn reward tensor(469., device='cuda:0') e 0.05 loss_dqn tensor(1.5553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07268116623163223
dqn reward tensor(506.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5636e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1360921859741211
dqn reward tensor(493.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.4908e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12869609892368317
dqn reward tensor(493., device='cuda:0') e 0.05 loss_dqn tensor(4.4182e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07448965311050415
dqn reward tensor(392.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05176893621683121
dqn reward tensor(366.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1115994080901146
dqn reward tensor(600.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4327e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14048625528812408
dqn reward tensor(483.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5332e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1209234893321991
dqn reward tensor(516.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5245e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14516472816467285
dqn reward tensor(468.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5732e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03663089871406555
dqn reward tensor(387., device='cuda:0') e 0.05 loss_dqn tensor(1.6502e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1575007140636444
dqn reward tensor(505., device='cuda:0') e 0.05 loss_dqn tensor(4.2099e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047679152339696884
dqn reward tensor(623.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08385763317346573
dqn reward tensor(447.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09574031084775925
dqn reward tensor(465.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6153e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16419529914855957
dqn reward tensor(416.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9114e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22645509243011475
dqn reward tensor(637.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.3455e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06831613928079605
dqn reward tensor(557., device='cuda:0') e 0.05 loss_dqn tensor(2.6503e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1211218535900116
dqn reward tensor(414.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1906e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08680209517478943
dqn reward tensor(463.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1663e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1351696401834488
dqn reward tensor(257.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4843e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057964473962783813
dqn reward tensor(384.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0765e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3330991268157959
dqn reward tensor(494.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.9827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09197889268398285
dqn reward tensor(320., device='cuda:0') e 0.05 loss_dqn tensor(1.5535e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1752370446920395
dqn reward tensor(519.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4938e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12584608793258667
dqn reward tensor(402.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07500939071178436
dqn reward tensor(550., device='cuda:0') e 0.05 loss_dqn tensor(1.6173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04629606381058693
dqn reward tensor(527.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0167e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0963490754365921
dqn reward tensor(624.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07186825573444366
dqn reward tensor(462.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4602e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20997139811515808
dqn reward tensor(381.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1471993625164032
dqn reward tensor(645.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6418e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19804473221302032
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5810e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17833387851715088
dqn reward tensor(571.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6971e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07770109921693802
dqn reward tensor(526.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5562e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0814787819981575
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2105068564414978
dqn reward tensor(354.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061625197529792786
dqn reward tensor(549.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2165060192346573
dqn reward tensor(518.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052816301584243774
dqn reward tensor(440.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6594e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19806569814682007
dqn reward tensor(562.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7261e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055344246327877045
dqn reward tensor(400.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6765e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10068542510271072
dqn reward tensor(565.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0200e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0914062112569809
dqn reward tensor(504.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7450e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08333054184913635
dqn reward tensor(441.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2795892655849457
dqn reward tensor(572.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1112283319234848
dqn reward tensor(540.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6873e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36446869373321533
dqn reward tensor(572., device='cuda:0') e 0.05 loss_dqn tensor(1.1480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028109481558203697
dqn reward tensor(594.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6732e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023362278938293457
dqn reward tensor(392., device='cuda:0') e 0.05 loss_dqn tensor(1.7547e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1881604790687561
dqn reward tensor(474.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17820128798484802
dqn reward tensor(606.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5451e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4104462265968323
dqn reward tensor(523.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5260e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12384290993213654
dqn reward tensor(410.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0526e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05781693011522293
dqn reward tensor(512.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08872877806425095
dqn reward tensor(456.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1384e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13123783469200134
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6317e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18820956349372864
dqn reward tensor(481.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4435e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027816060930490494
dqn reward tensor(415.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4347e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045591145753860474
dqn reward tensor(409.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1260e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14953234791755676
dqn reward tensor(449.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19074705243110657
dqn reward tensor(524.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0525e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18083573877811432
dqn reward tensor(330.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4961e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10531565546989441
dqn reward tensor(562.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4791e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07865867018699646
dqn reward tensor(408.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9786e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12106546014547348
dqn reward tensor(528.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4957e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14316116273403168
dqn reward tensor(689.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37464553117752075
dqn reward tensor(597.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4966e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13021928071975708
dqn reward tensor(433.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.5352e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13149413466453552
dqn reward tensor(423.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4422e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16912692785263062
dqn reward tensor(415., device='cuda:0') e 0.05 loss_dqn tensor(1.5431e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09755176305770874
dqn reward tensor(463.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4155e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042097121477127075
dqn reward tensor(284.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5007e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14689363539218903
dqn reward tensor(424.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4265e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31085681915283203
dqn reward tensor(408.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5001e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1415521800518036
dqn reward tensor(529.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4074e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1444224715232849
dqn reward tensor(649.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4117e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.094673752784729
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7465e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12501713633537292
dqn reward tensor(304.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2418e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08229542523622513
dqn reward tensor(472.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2521681785583496
dqn reward tensor(564.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3131e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17222078144550323
dqn reward tensor(428.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.5564e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14866885542869568
dqn reward tensor(411.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06874115020036697
dqn reward tensor(349.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4670e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056893058121204376
dqn reward tensor(475.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.0799e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10110083222389221
dqn reward tensor(256.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4741e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09367231279611588
dqn reward tensor(521., device='cuda:0') e 0.05 loss_dqn tensor(1.4500e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2074105143547058
dqn reward tensor(167.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7140e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19811254739761353
dqn reward tensor(66.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1799e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01742105558514595
Evaluating...
Train: {'rocauc': 0.764816893533067} 6.431962966918945
=====Epoch 20=====
Training...
dqn reward tensor(147.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4023e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07661084830760956
dqn reward tensor(216.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19640983641147614
dqn reward tensor(317.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4376e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22370660305023193
dqn reward tensor(545.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2072e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03179004043340683
dqn reward tensor(420.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8465e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2523729205131531
dqn reward tensor(324.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4492e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21619556844234467
dqn reward tensor(356.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2213e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21461385488510132
dqn reward tensor(360., device='cuda:0') e 0.05 loss_dqn tensor(1.5068e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1612357497215271
dqn reward tensor(408.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5104e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09126259386539459
dqn reward tensor(285.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2681e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16507406532764435
dqn reward tensor(531.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21411444246768951
dqn reward tensor(292.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5765e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09817595779895782
dqn reward tensor(551.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4411e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23072941601276398
dqn reward tensor(486.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4520e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21818649768829346
dqn reward tensor(415.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5499e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1268920600414276
dqn reward tensor(433.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4589e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10481356829404831
dqn reward tensor(420.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8421e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09649284183979034
dqn reward tensor(426.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4914e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15125207602977753
dqn reward tensor(339.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.5636e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15788955986499786
dqn reward tensor(574.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4773e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06738631427288055
dqn reward tensor(161.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7344e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16894125938415527
dqn reward tensor(326.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1157e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024360021576285362
dqn reward tensor(341.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2866628170013428
dqn reward tensor(520.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5258e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10343998670578003
dqn reward tensor(406.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20884177088737488
dqn reward tensor(426.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5479e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1014890968799591
dqn reward tensor(416.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3358e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1494043469429016
dqn reward tensor(621.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4634e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029748931527137756
dqn reward tensor(379.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07389569282531738
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05848124250769615
dqn reward tensor(209.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.5018e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14479084312915802
dqn reward tensor(432.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8368e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10511608421802521
dqn reward tensor(561.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022731009870767593
dqn reward tensor(361.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14137853682041168
dqn reward tensor(459.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5040e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06828293949365616
dqn reward tensor(327.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6377e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23442047834396362
dqn reward tensor(453.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7889e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0775221586227417
dqn reward tensor(378.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5064e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13222111761569977
dqn reward tensor(447., device='cuda:0') e 0.05 loss_dqn tensor(2.0858e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23292133212089539
dqn reward tensor(441.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5333e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20146600902080536
dqn reward tensor(230., device='cuda:0') e 0.05 loss_dqn tensor(6.6664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2416580468416214
dqn reward tensor(557.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2224e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07052262127399445
dqn reward tensor(503.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4881e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12987752258777618
dqn reward tensor(280.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18119752407073975
dqn reward tensor(507.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4839e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12458579242229462
dqn reward tensor(442.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13213734328746796
dqn reward tensor(387.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.3620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13802210986614227
dqn reward tensor(409.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11084351688623428
dqn reward tensor(497.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4938e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670023798942566
dqn reward tensor(282.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10129799693822861
dqn reward tensor(323.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8061e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13829053938388824
dqn reward tensor(590.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3583e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025295015424489975
dqn reward tensor(438.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5498e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12326569855213165
dqn reward tensor(355.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8838e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16739994287490845
dqn reward tensor(280.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4555e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020420359447598457
dqn reward tensor(333.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05147302523255348
dqn reward tensor(469.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.2709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17425739765167236
dqn reward tensor(404.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19731685519218445
dqn reward tensor(518., device='cuda:0') e 0.05 loss_dqn tensor(9.4918e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23839996755123138
dqn reward tensor(498.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4708e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0295802503824234
dqn reward tensor(210.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4602e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16177040338516235
dqn reward tensor(520.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05173606798052788
dqn reward tensor(364.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027739815413951874
dqn reward tensor(553.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5254e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10994717478752136
dqn reward tensor(236.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15227535367012024
dqn reward tensor(347.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2307e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23960724472999573
dqn reward tensor(616.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.4817e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1387166529893875
dqn reward tensor(437.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2941e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1871037483215332
dqn reward tensor(428.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5096e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035911209881305695
dqn reward tensor(462.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4914e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15631112456321716
dqn reward tensor(431.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5056e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13807767629623413
dqn reward tensor(374.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17199787497520447
dqn reward tensor(538., device='cuda:0') e 0.05 loss_dqn tensor(1.5074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1473819613456726
dqn reward tensor(448.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3068e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12867537140846252
dqn reward tensor(413.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5082e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11799626052379608
dqn reward tensor(508.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.243211030960083
dqn reward tensor(438.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9053e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2193019688129425
dqn reward tensor(572., device='cuda:0') e 0.05 loss_dqn tensor(1.3858e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06327590346336365
dqn reward tensor(411., device='cuda:0') e 0.05 loss_dqn tensor(9.9531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09010621905326843
dqn reward tensor(266.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06841026246547699
dqn reward tensor(456.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4854e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03756624087691307
dqn reward tensor(338.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07776182144880295
dqn reward tensor(385.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0418e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09391365945339203
dqn reward tensor(399.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0529e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02381325140595436
dqn reward tensor(251.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2450e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.101915642619133
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5212e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0863342136144638
dqn reward tensor(383.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4762e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16351957619190216
dqn reward tensor(507.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4630e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20054340362548828
dqn reward tensor(317.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5397e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22833436727523804
dqn reward tensor(373.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.1700e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22681397199630737
dqn reward tensor(439.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5765e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05448217689990997
dqn reward tensor(399.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13370677828788757
dqn reward tensor(249.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4067e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18113216757774353
dqn reward tensor(615.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07688780128955841
dqn reward tensor(445.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11187246441841125
dqn reward tensor(398.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0152e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1098402738571167
dqn reward tensor(582., device='cuda:0') e 0.05 loss_dqn tensor(1.5359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07823258638381958
dqn reward tensor(242., device='cuda:0') e 0.05 loss_dqn tensor(1.0663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20887251198291779
dqn reward tensor(304.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.5447e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10659068077802658
dqn reward tensor(419.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5200e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11902767419815063
dqn reward tensor(486.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.6131e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10182715207338333
dqn reward tensor(530.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5047e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10245519131422043
dqn reward tensor(402., device='cuda:0') e 0.05 loss_dqn tensor(1.8915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18358245491981506
dqn reward tensor(259.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9509e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24126960337162018
dqn reward tensor(516.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4662e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1275234818458557
dqn reward tensor(483.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22014877200126648
dqn reward tensor(386.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0667e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1443888396024704
dqn reward tensor(311.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10850521922111511
dqn reward tensor(446.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9199e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04649028182029724
dqn reward tensor(466.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15284070372581482
dqn reward tensor(434.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5792e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03812037780880928
dqn reward tensor(336.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4889e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09172263741493225
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5039e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1090463399887085
dqn reward tensor(578.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5396e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09981293231248856
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0213e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06479626893997192
dqn reward tensor(412.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5361e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22939635813236237
dqn reward tensor(523.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1211e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01646384410560131
dqn reward tensor(444.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4918e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08410637825727463
dqn reward tensor(368.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524198353290558
dqn reward tensor(115.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8982e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2202168107032776
dqn reward tensor(569.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21844105422496796
dqn reward tensor(437.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7224e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14852210879325867
dqn reward tensor(355.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5454e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027490802109241486
dqn reward tensor(443., device='cuda:0') e 0.05 loss_dqn tensor(1.5860e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09915092587471008
dqn reward tensor(534.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22092029452323914
dqn reward tensor(490.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6229e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24206629395484924
dqn reward tensor(615.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4729e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0917358547449112
dqn reward tensor(516.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5190e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06776183843612671
dqn reward tensor(214.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5742e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09229572862386703
dqn reward tensor(282.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3974e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14903922379016876
dqn reward tensor(463.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5903e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11179311573505402
dqn reward tensor(504.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5418e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2051769196987152
dqn reward tensor(498.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2786e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08562873303890228
dqn reward tensor(376., device='cuda:0') e 0.05 loss_dqn tensor(1.6330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20904816687107086
dqn reward tensor(465.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5838e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0670398622751236
dqn reward tensor(344.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.6814e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10926438122987747
dqn reward tensor(478.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5958e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08001814782619476
dqn reward tensor(436.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1515519618988037
dqn reward tensor(499.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2390e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09942913055419922
dqn reward tensor(366.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6892e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15301953256130219
dqn reward tensor(355.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9297e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477132558822632
dqn reward tensor(414.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4329e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07633669674396515
dqn reward tensor(398., device='cuda:0') e 0.05 loss_dqn tensor(1.4915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29586848616600037
dqn reward tensor(352.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3710e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.212264284491539
dqn reward tensor(297., device='cuda:0') e 0.05 loss_dqn tensor(2.5694e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08758614212274551
dqn reward tensor(463., device='cuda:0') e 0.05 loss_dqn tensor(1.6257e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10954906046390533
dqn reward tensor(404., device='cuda:0') e 0.05 loss_dqn tensor(2.7568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11058749258518219
dqn reward tensor(484.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039995353668928146
dqn reward tensor(391.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26893794536590576
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6650e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16274042427539825
dqn reward tensor(359.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18435370922088623
dqn reward tensor(366.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1733027696609497
dqn reward tensor(463.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5110e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14073546230793
dqn reward tensor(435.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6959e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13691574335098267
dqn reward tensor(246.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13794243335723877
dqn reward tensor(433.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2878e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2061789482831955
dqn reward tensor(488., device='cuda:0') e 0.05 loss_dqn tensor(1.5887e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2153788059949875
dqn reward tensor(466.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6141e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0571436882019043
dqn reward tensor(168.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11617913842201233
dqn reward tensor(436.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8966e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12859217822551727
dqn reward tensor(456.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1661798655986786
dqn reward tensor(489.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05388747900724411
dqn reward tensor(499.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03230268508195877
dqn reward tensor(382.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5411e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09249445050954819
dqn reward tensor(410.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1615016758441925
dqn reward tensor(528.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4993e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20644038915634155
dqn reward tensor(531.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7825e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1884113997220993
dqn reward tensor(538.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1003e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19581636786460876
dqn reward tensor(367., device='cuda:0') e 0.05 loss_dqn tensor(1.6373e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08577007055282593
dqn reward tensor(427.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0808e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05663308501243591
dqn reward tensor(364.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6028e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.227107971906662
dqn reward tensor(374.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2324e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21854399144649506
dqn reward tensor(371.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4081e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10162901878356934
dqn reward tensor(563.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5739e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13567385077476501
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(1.3288e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08864344656467438
dqn reward tensor(357.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7990e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10869929194450378
dqn reward tensor(264.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04963657259941101
dqn reward tensor(392.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3209e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10449180006980896
dqn reward tensor(419.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10531553626060486
dqn reward tensor(440.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14727838337421417
dqn reward tensor(506., device='cuda:0') e 0.05 loss_dqn tensor(1.5594e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06887413561344147
dqn reward tensor(514.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07893621921539307
dqn reward tensor(529.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5813e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2817131280899048
dqn reward tensor(191.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13991445302963257
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5896e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28092852234840393
dqn reward tensor(472.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6054e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1469731479883194
dqn reward tensor(310.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035330601036548615
dqn reward tensor(418.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026388753205537796
dqn reward tensor(650.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2769835591316223
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19406233727931976
dqn reward tensor(338.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2363193780183792
dqn reward tensor(457.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1245039850473404
dqn reward tensor(440.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0987829715013504
dqn reward tensor(329., device='cuda:0') e 0.05 loss_dqn tensor(1.3672e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3205825686454773
dqn reward tensor(559.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9934e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1372411698102951
dqn reward tensor(332.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7962e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2013535350561142
dqn reward tensor(380.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11365757137537003
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06936754286289215
dqn reward tensor(446., device='cuda:0') e 0.05 loss_dqn tensor(1.6178e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.298942893743515
dqn reward tensor(669.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16608354449272156
dqn reward tensor(478.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.6508e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03502238541841507
dqn reward tensor(389.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11822245270013809
dqn reward tensor(380.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5800e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17983226478099823
dqn reward tensor(554.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5237e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0910763144493103
dqn reward tensor(447.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14814209938049316
dqn reward tensor(389.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03843680024147034
dqn reward tensor(359.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2132168859243393
dqn reward tensor(580.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07409948110580444
dqn reward tensor(263.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9017e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11746929585933685
dqn reward tensor(379.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6579e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029570408165454865
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15200833976268768
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19193077087402344
dqn reward tensor(408.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06108354032039642
dqn reward tensor(540., device='cuda:0') e 0.05 loss_dqn tensor(1.6969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11170397698879242
dqn reward tensor(517.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023686377331614494
dqn reward tensor(251.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1575e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16298702359199524
dqn reward tensor(470.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02337498776614666
dqn reward tensor(440.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062215931713581085
dqn reward tensor(694.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7573e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15010389685630798
dqn reward tensor(502.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2607622742652893
dqn reward tensor(430.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2552330791950226
dqn reward tensor(431.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4555e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0412183441221714
dqn reward tensor(524., device='cuda:0') e 0.05 loss_dqn tensor(1.5516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21543768048286438
dqn reward tensor(388.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2935e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09573036432266235
dqn reward tensor(578.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9323e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26833921670913696
dqn reward tensor(177.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7813e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0783555805683136
dqn reward tensor(521.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22123710811138153
dqn reward tensor(410., device='cuda:0') e 0.05 loss_dqn tensor(1.6036e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1964396834373474
dqn reward tensor(431.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6306e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22221620380878448
dqn reward tensor(460.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09843558818101883
dqn reward tensor(396.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11392179131507874
dqn reward tensor(466.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6897e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061369746923446655
dqn reward tensor(412., device='cuda:0') e 0.05 loss_dqn tensor(1.2446e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15255296230316162
dqn reward tensor(450.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2229e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17816922068595886
dqn reward tensor(339.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11881628632545471
dqn reward tensor(548.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047039616852998734
dqn reward tensor(388.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3146e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15923821926116943
dqn reward tensor(395.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1714e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12645187973976135
dqn reward tensor(311.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5449e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4095057249069214
dqn reward tensor(292.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3711e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09661391377449036
dqn reward tensor(619.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8692e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08217370510101318
dqn reward tensor(420.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01893923059105873
dqn reward tensor(528.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021304287016391754
dqn reward tensor(615.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7025e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016319096088409424
dqn reward tensor(535.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6417e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13495615124702454
dqn reward tensor(315.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020000088959932327
dqn reward tensor(533.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5490e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11507491022348404
dqn reward tensor(305.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10959437489509583
dqn reward tensor(602.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17255118489265442
dqn reward tensor(458.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15730434656143188
dqn reward tensor(614.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19360807538032532
dqn reward tensor(567.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13159732520580292
dqn reward tensor(477.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015358026139438152
dqn reward tensor(439.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7362e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09406013786792755
dqn reward tensor(276.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7255e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11990778893232346
dqn reward tensor(350.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15803614258766174
dqn reward tensor(461.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4757e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019654521718621254
dqn reward tensor(379.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.6770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21470396220684052
dqn reward tensor(336.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5484e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023546691983938217
dqn reward tensor(505.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5464e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10299091041088104
dqn reward tensor(563.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.079864501953125
dqn reward tensor(256.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5105e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020256636664271355
dqn reward tensor(406.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6027e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0900726318359375
dqn reward tensor(549.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16911418735980988
dqn reward tensor(402.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15795710682868958
dqn reward tensor(158.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07248403877019882
dqn reward tensor(338.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15439088642597198
dqn reward tensor(527.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8075e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1327880173921585
dqn reward tensor(463.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6211e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07958345860242844
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6523e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17131257057189941
dqn reward tensor(556., device='cuda:0') e 0.05 loss_dqn tensor(1.5359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19032198190689087
dqn reward tensor(461.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08807029575109482
dqn reward tensor(350.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0444e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04892341420054436
dqn reward tensor(318.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21970295906066895
dqn reward tensor(357.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5951e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2511869966983795
dqn reward tensor(415.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09920939803123474
dqn reward tensor(273.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06813327223062515
dqn reward tensor(425.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07187509536743164
dqn reward tensor(599., device='cuda:0') e 0.05 loss_dqn tensor(1.5230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12801265716552734
dqn reward tensor(459.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9881e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14592361450195312
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6677e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2382616251707077
dqn reward tensor(399.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6652e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835803896188736
dqn reward tensor(422.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3557e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0990472361445427
dqn reward tensor(532.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3143e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08264397829771042
dqn reward tensor(529.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6599e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09852523356676102
dqn reward tensor(623.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2390269637107849
dqn reward tensor(458.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4151e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2083624303340912
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25732260942459106
dqn reward tensor(218.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2025103121995926
dqn reward tensor(336.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09155747294425964
dqn reward tensor(335.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2171078622341156
dqn reward tensor(574.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08682290464639664
dqn reward tensor(414.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6907e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1330839842557907
dqn reward tensor(323.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0989e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11763487011194229
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5719e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10998941957950592
dqn reward tensor(254.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4999e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18321853876113892
dqn reward tensor(536.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6304e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08054997771978378
dqn reward tensor(426.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6663e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1817675083875656
dqn reward tensor(459., device='cuda:0') e 0.05 loss_dqn tensor(1.2892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043217141181230545
dqn reward tensor(423.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1722373515367508
dqn reward tensor(562.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4339e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03278612345457077
dqn reward tensor(493.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4326959252357483
dqn reward tensor(443., device='cuda:0') e 0.05 loss_dqn tensor(2.7594e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19424031674861908
dqn reward tensor(379.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13697412610054016
dqn reward tensor(430.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6498e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11036455631256104
dqn reward tensor(435.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6328e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12291191518306732
dqn reward tensor(436.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07000607997179031
dqn reward tensor(363.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1842e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18231463432312012
dqn reward tensor(472.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.8338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13212819397449493
dqn reward tensor(482.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18449082970619202
dqn reward tensor(512.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2604725956916809
dqn reward tensor(449.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04070766642689705
dqn reward tensor(345.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24842068552970886
dqn reward tensor(303.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8455e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19443562626838684
dqn reward tensor(432.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3368e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14983123540878296
dqn reward tensor(358.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6746e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21486163139343262
dqn reward tensor(526.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11205528676509857
dqn reward tensor(401.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6634e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15783658623695374
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12138360738754272
dqn reward tensor(445.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6799e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0801706537604332
dqn reward tensor(507.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6754e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15072757005691528
dqn reward tensor(478.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10440702736377716
dqn reward tensor(475.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5655e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13003027439117432
dqn reward tensor(412.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3262e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17104214429855347
dqn reward tensor(407.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1528012603521347
dqn reward tensor(349.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3179e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14868366718292236
dqn reward tensor(293.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09908519685268402
dqn reward tensor(358.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2325931191444397
dqn reward tensor(446.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.1520e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07039105892181396
dqn reward tensor(300.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.4728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2793858051300049
dqn reward tensor(448.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6918e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.160052090883255
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0539e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12445490062236786
dqn reward tensor(293.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6657e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2014528512954712
dqn reward tensor(358.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13983164727687836
dqn reward tensor(393.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10091136395931244
dqn reward tensor(414., device='cuda:0') e 0.05 loss_dqn tensor(1.7497e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09217751771211624
dqn reward tensor(537.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6194e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24715907871723175
dqn reward tensor(357.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.0830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1050821989774704
dqn reward tensor(615.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3513e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12534256279468536
dqn reward tensor(455.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1305299997329712
dqn reward tensor(405.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3275e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09310568869113922
dqn reward tensor(409.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1385537087917328
dqn reward tensor(414.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7764e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29048436880111694
dqn reward tensor(419.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7414e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20325857400894165
dqn reward tensor(564.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08986890316009521
dqn reward tensor(505.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5892e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17649482190608978
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3396e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05904868245124817
dqn reward tensor(538.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5256e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05430427938699722
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6906e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0535789430141449
dqn reward tensor(580., device='cuda:0') e 0.05 loss_dqn tensor(1.6745e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08056852221488953
dqn reward tensor(412.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7370e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553204476833344
dqn reward tensor(537.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2080981433391571
dqn reward tensor(385.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07815201580524445
dqn reward tensor(552.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5865e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03755725175142288
dqn reward tensor(598.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6328e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29205483198165894
dqn reward tensor(384.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13720890879631042
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03495195508003235
dqn reward tensor(233.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1976706087589264
dqn reward tensor(280., device='cuda:0') e 0.05 loss_dqn tensor(9.9094e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022323831915855408
dqn reward tensor(494.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1885e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08041335642337799
dqn reward tensor(463.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6391e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03436911851167679
dqn reward tensor(459.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21285727620124817
dqn reward tensor(326.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4548e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15374702215194702
dqn reward tensor(563.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3795e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025009674951434135
dqn reward tensor(539.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3844e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10814577341079712
dqn reward tensor(410.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1211e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21208181977272034
dqn reward tensor(374.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20019128918647766
dqn reward tensor(602., device='cuda:0') e 0.05 loss_dqn tensor(1.4577e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024700326845049858
dqn reward tensor(441.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3239e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12757828831672668
dqn reward tensor(215.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.9620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11587375402450562
dqn reward tensor(600.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2919e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16699862480163574
dqn reward tensor(450.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09622961282730103
dqn reward tensor(632.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8946e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23044812679290771
dqn reward tensor(403.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4402e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022241290658712387
dqn reward tensor(457.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2949e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15640047192573547
dqn reward tensor(428.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5945e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09687186777591705
dqn reward tensor(413., device='cuda:0') e 0.05 loss_dqn tensor(9.9716e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11111709475517273
dqn reward tensor(352.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10776015371084213
dqn reward tensor(493.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039943404495716095
dqn reward tensor(400.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18885208666324615
dqn reward tensor(285.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2954244017601013
dqn reward tensor(386.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6050e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2160002887248993
dqn reward tensor(335.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17187635600566864
dqn reward tensor(533., device='cuda:0') e 0.05 loss_dqn tensor(1.3788e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07816311717033386
dqn reward tensor(396., device='cuda:0') e 0.05 loss_dqn tensor(8.5493e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12015967071056366
dqn reward tensor(397.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051180146634578705
dqn reward tensor(204.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1601046621799469
dqn reward tensor(426.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5372e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12787792086601257
dqn reward tensor(324.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08194275200366974
dqn reward tensor(381.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6420e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11164560914039612
dqn reward tensor(529.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23319807648658752
dqn reward tensor(397.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02112652361392975
dqn reward tensor(434.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6207e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08612892776727676
dqn reward tensor(473.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2400e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14764507114887238
dqn reward tensor(640.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11706463247537613
dqn reward tensor(402.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6279e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2380516231060028
dqn reward tensor(642.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1752672642469406
dqn reward tensor(141.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3423e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024690736085176468
dqn reward tensor(370.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18006600439548492
dqn reward tensor(343.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4018e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23760932683944702
dqn reward tensor(366.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22125545144081116
dqn reward tensor(214.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08063849806785583
dqn reward tensor(563.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0745e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058739110827445984
dqn reward tensor(321.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1728e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35080409049987793
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3226e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18427912890911102
dqn reward tensor(350.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8860e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1219296008348465
dqn reward tensor(326.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12278129160404205
dqn reward tensor(417.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8327e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08914715051651001
dqn reward tensor(585.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13576814532279968
dqn reward tensor(380., device='cuda:0') e 0.05 loss_dqn tensor(3.1487e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.137589231133461
dqn reward tensor(141.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.4973e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22188781201839447
dqn reward tensor(320.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08128165453672409
dqn reward tensor(435.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0123e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12635868787765503
dqn reward tensor(140.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14641043543815613
dqn reward tensor(202.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8950e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17012116312980652
dqn reward tensor(388.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1081e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1613655388355255
dqn reward tensor(156.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22695021331310272
dqn reward tensor(200.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10017390549182892
dqn reward tensor(405.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07753325998783112
dqn reward tensor(317.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2992e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027661394327878952
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4191e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02990567684173584
dqn reward tensor(458., device='cuda:0') e 0.05 loss_dqn tensor(1.3045e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15792089700698853
dqn reward tensor(207.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18778355419635773
dqn reward tensor(192.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17936530709266663
dqn reward tensor(470.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08149021118879318
dqn reward tensor(476.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3114e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11759249866008759
dqn reward tensor(185.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16868829727172852
dqn reward tensor(451.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2987e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1060958206653595
dqn reward tensor(273.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3625e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08288286626338959
dqn reward tensor(408.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.8100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17228379845619202
dqn reward tensor(425.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3291e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07616043835878372
dqn reward tensor(482.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042040493339300156
dqn reward tensor(396.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2876e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12463756650686264
dqn reward tensor(338.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5799e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027254564687609673
dqn reward tensor(524.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2490e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14174239337444305
dqn reward tensor(451.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.1810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07870107889175415
dqn reward tensor(446.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054658904671669006
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3138e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.330644816160202
dqn reward tensor(323.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.2489e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13902054727077484
dqn reward tensor(381.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.9211e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03181522339582443
dqn reward tensor(319.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3262e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10597044229507446
dqn reward tensor(553.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2760e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1114000678062439
dqn reward tensor(320.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3956e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12518443167209625
dqn reward tensor(558.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02773040160536766
dqn reward tensor(444., device='cuda:0') e 0.05 loss_dqn tensor(1.1060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04995431378483772
dqn reward tensor(329.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1538303941488266
dqn reward tensor(401.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1595e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18081487715244293
dqn reward tensor(281.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3385e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12617084383964539
dqn reward tensor(438.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026094108819961548
dqn reward tensor(444.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5506e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17300570011138916
dqn reward tensor(482.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3618e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09718714654445648
dqn reward tensor(303.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1962e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10183731466531754
dqn reward tensor(490.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3020e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19234701991081238
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2229e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2419356107711792
dqn reward tensor(370.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16358709335327148
dqn reward tensor(519.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18310435116291046
dqn reward tensor(404.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2410e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06332551687955856
dqn reward tensor(406.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.089644655585289
dqn reward tensor(437.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0036e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2691304087638855
dqn reward tensor(339.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2660515606403351
dqn reward tensor(215.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.1164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1304011344909668
dqn reward tensor(477.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.9254e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08341463655233383
dqn reward tensor(423., device='cuda:0') e 0.05 loss_dqn tensor(1.0602e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05017969012260437
dqn reward tensor(550.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3122e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10950499773025513
dqn reward tensor(485.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16541123390197754
dqn reward tensor(421., device='cuda:0') e 0.05 loss_dqn tensor(8.0794e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19578556716442108
dqn reward tensor(315.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07427391409873962
dqn reward tensor(267.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5594e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2619444727897644
dqn reward tensor(521.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1282123327255249
dqn reward tensor(551.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3466e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27150410413742065
dqn reward tensor(267.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17575693130493164
dqn reward tensor(334.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3227e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07083361595869064
dqn reward tensor(167., device='cuda:0') e 0.05 loss_dqn tensor(3.3909e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12928301095962524
dqn reward tensor(326.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3563e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3260420858860016
dqn reward tensor(514.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1923e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07574981451034546
dqn reward tensor(505., device='cuda:0') e 0.05 loss_dqn tensor(9.3972e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20303502678871155
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2632e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15035104751586914
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0145e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14212295413017273
dqn reward tensor(520.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3434e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08260515332221985
dqn reward tensor(368.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08107659220695496
dqn reward tensor(242.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4889e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07772725820541382
dqn reward tensor(276.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20490525662899017
dqn reward tensor(465.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2418e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07132864743471146
dqn reward tensor(388.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3303e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07794518023729324
dqn reward tensor(235., device='cuda:0') e 0.05 loss_dqn tensor(1.1448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08142797648906708
dqn reward tensor(388.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5284e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13168750703334808
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2581542730331421
dqn reward tensor(523.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3262e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12107174843549728
dqn reward tensor(480.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14988145232200623
dqn reward tensor(426.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8872e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11581285297870636
dqn reward tensor(397.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17360559105873108
dqn reward tensor(328.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3015e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09779931604862213
dqn reward tensor(436.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0753e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040666013956069946
dqn reward tensor(496.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2017e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027103766798973083
dqn reward tensor(434.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3299e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09921776503324509
dqn reward tensor(499.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5163e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1097680926322937
dqn reward tensor(298.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.3702e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2266930341720581
dqn reward tensor(252., device='cuda:0') e 0.05 loss_dqn tensor(1.4420e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09928856790065765
dqn reward tensor(144.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025467319414019585
dqn reward tensor(520.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2426e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27107352018356323
dqn reward tensor(398.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0591e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17016905546188354
dqn reward tensor(444.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2999e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11474873870611191
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1324e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13720276951789856
dqn reward tensor(493.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3156e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2004876285791397
dqn reward tensor(385.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1263e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08769024163484573
dqn reward tensor(512.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2805e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16114741563796997
dqn reward tensor(409.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06202641502022743
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1972e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16341358423233032
dqn reward tensor(614.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16456693410873413
dqn reward tensor(363.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20154280960559845
dqn reward tensor(426.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03749570995569229
dqn reward tensor(622.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16510096192359924
dqn reward tensor(477.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12732581794261932
dqn reward tensor(430.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2594e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1690826117992401
dqn reward tensor(39.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5633e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.633845329284668
Evaluating...
Train: {'rocauc': 0.7683359823179126} 6.668549060821533
{'Train': [{'rocauc': 0.7683359823179126}, 6.668549060821533], 'Validation': [{'rocauc': 0.7421094209288654}, 5.6691765785217285], 'Test': [{'rocauc': 0.7363255373800189}, 5.907546520233154], 'LR': 9.963665628205869e-05, 'Mem': 0.945, 'Cached': 1.178}
=====Epoch 21=====
Training...
dqn reward tensor(366.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1208e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0920940637588501
dqn reward tensor(323.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0811e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13151958584785461
dqn reward tensor(440.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1662202775478363
dqn reward tensor(276., device='cuda:0') e 0.05 loss_dqn tensor(1.5519e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09104914963245392
dqn reward tensor(543.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09168398380279541
dqn reward tensor(110.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4343e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060699108988046646
dqn reward tensor(500.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1739984154701233
dqn reward tensor(664., device='cuda:0') e 0.05 loss_dqn tensor(1.1949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23612241446971893
dqn reward tensor(506., device='cuda:0') e 0.05 loss_dqn tensor(1.2472e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09624479711055756
dqn reward tensor(564., device='cuda:0') e 0.05 loss_dqn tensor(1.3171e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17995616793632507
dqn reward tensor(600., device='cuda:0') e 0.05 loss_dqn tensor(1.0530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08395945280790329
dqn reward tensor(600.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0423e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22892986238002777
dqn reward tensor(477.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2536e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03427378833293915
dqn reward tensor(383.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2449e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1620556116104126
dqn reward tensor(530.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3827e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0805341899394989
dqn reward tensor(505.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3287e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13594362139701843
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08345873653888702
dqn reward tensor(496.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3428e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1521957367658615
dqn reward tensor(462.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3609e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16865496337413788
dqn reward tensor(409.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3921e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2269059121608734
dqn reward tensor(259.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0372e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07989850640296936
dqn reward tensor(471.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04374808073043823
dqn reward tensor(390.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2589e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2751705050468445
dqn reward tensor(380.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12976594269275665
dqn reward tensor(495.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2561e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17535753548145294
dqn reward tensor(434.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9784e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12992411851882935
dqn reward tensor(373.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14533376693725586
dqn reward tensor(393., device='cuda:0') e 0.05 loss_dqn tensor(1.0743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08214998990297318
dqn reward tensor(356.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11214673519134521
dqn reward tensor(432.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18567290902137756
dqn reward tensor(671.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1273621916770935
dqn reward tensor(309., device='cuda:0') e 0.05 loss_dqn tensor(1.6457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16004310548305511
dqn reward tensor(385.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0306e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09725648164749146
dqn reward tensor(522.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08496484160423279
dqn reward tensor(490.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7143e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11036776006221771
dqn reward tensor(460.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20656050741672516
dqn reward tensor(591.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2646e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1230926513671875
dqn reward tensor(268.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17728552222251892
dqn reward tensor(279.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1665e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07433533668518066
dqn reward tensor(496.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3975e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2788437008857727
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(1.1968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19680194556713104
dqn reward tensor(263.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1731e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10111933201551437
dqn reward tensor(485.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4608e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1714954376220703
dqn reward tensor(438.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2042902559041977
dqn reward tensor(466.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12102103978395462
dqn reward tensor(405.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8512e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08066920936107635
dqn reward tensor(495.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027059197425842285
dqn reward tensor(564.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3258e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.141157865524292
dqn reward tensor(412.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3487e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08084927499294281
dqn reward tensor(445., device='cuda:0') e 0.05 loss_dqn tensor(1.1043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24062541127204895
dqn reward tensor(418.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19881285727024078
dqn reward tensor(484.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2606230080127716
dqn reward tensor(259.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3215e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0292467400431633
dqn reward tensor(251.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6130e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1059286966919899
dqn reward tensor(428.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1182917058467865
dqn reward tensor(164.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7054e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21198809146881104
dqn reward tensor(482.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.8193e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1348312497138977
dqn reward tensor(260.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3961e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0694156214594841
dqn reward tensor(458.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2177819460630417
dqn reward tensor(403.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09673934429883957
dqn reward tensor(434.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0105e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18518805503845215
dqn reward tensor(448., device='cuda:0') e 0.05 loss_dqn tensor(1.3424e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047563157975673676
dqn reward tensor(502.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2526e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17429831624031067
dqn reward tensor(305.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1652e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12853725254535675
dqn reward tensor(577.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8778e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03489674627780914
dqn reward tensor(442., device='cuda:0') e 0.05 loss_dqn tensor(1.2896e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03238185495138168
dqn reward tensor(399.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3655e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10238079726696014
dqn reward tensor(558.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3318e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14404602348804474
dqn reward tensor(405.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3578e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16217637062072754
dqn reward tensor(609., device='cuda:0') e 0.05 loss_dqn tensor(1.3170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10652459412813187
dqn reward tensor(272.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11110712587833405
dqn reward tensor(405.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4485e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07947784662246704
dqn reward tensor(676., device='cuda:0') e 0.05 loss_dqn tensor(1.3789e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14705444872379303
dqn reward tensor(371.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2999e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12202123552560806
dqn reward tensor(285.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6438e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2184438407421112
dqn reward tensor(425.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03702658787369728
dqn reward tensor(300.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8125e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039333462715148926
dqn reward tensor(504.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9367e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03438246622681618
dqn reward tensor(340.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.9639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07403026521205902
dqn reward tensor(426.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3231e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03647879511117935
dqn reward tensor(531.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2773e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09643913805484772
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3266e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02780872955918312
dqn reward tensor(560., device='cuda:0') e 0.05 loss_dqn tensor(1.2748e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13484881818294525
dqn reward tensor(398.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3340e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14030683040618896
dqn reward tensor(425.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1389e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058924272656440735
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3041e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07086476683616638
dqn reward tensor(279.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1361e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20493575930595398
dqn reward tensor(398., device='cuda:0') e 0.05 loss_dqn tensor(1.3090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15811097621917725
dqn reward tensor(447.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21680331230163574
dqn reward tensor(428.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0918e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29108116030693054
dqn reward tensor(358.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10905643552541733
dqn reward tensor(462.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.3453e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07972448319196701
dqn reward tensor(473.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.3748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25098422169685364
dqn reward tensor(491.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8723e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2041553258895874
dqn reward tensor(435.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3952e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0808975100517273
dqn reward tensor(402.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2782e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13181644678115845
dqn reward tensor(343.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0516e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17400573194026947
dqn reward tensor(440., device='cuda:0') e 0.05 loss_dqn tensor(9.1063e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1618748903274536
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(1.0872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1199248805642128
dqn reward tensor(397.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2689e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24240441620349884
dqn reward tensor(334.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2933e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16244548559188843
dqn reward tensor(439.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4317e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2095014601945877
dqn reward tensor(316., device='cuda:0') e 0.05 loss_dqn tensor(1.3253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08994501829147339
dqn reward tensor(352.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15167389810085297
dqn reward tensor(429.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2108e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08260320872068405
dqn reward tensor(449.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1046871542930603
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(1.1113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09432131797075272
dqn reward tensor(310.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4018e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07035649567842484
dqn reward tensor(598.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.189438596367836
dqn reward tensor(515.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3220e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10610008239746094
dqn reward tensor(331.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5292e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20064595341682434
dqn reward tensor(555., device='cuda:0') e 0.05 loss_dqn tensor(9.2502e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13244128227233887
dqn reward tensor(324.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1225e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014567992649972439
dqn reward tensor(595.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3827e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09838517010211945
dqn reward tensor(473., device='cuda:0') e 0.05 loss_dqn tensor(1.8442e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14103108644485474
dqn reward tensor(432.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07629904896020889
dqn reward tensor(423., device='cuda:0') e 0.05 loss_dqn tensor(1.3391e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3215498924255371
dqn reward tensor(472.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09485098719596863
dqn reward tensor(345., device='cuda:0') e 0.05 loss_dqn tensor(2.5358e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13303907215595245
dqn reward tensor(550., device='cuda:0') e 0.05 loss_dqn tensor(1.0128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12370964884757996
dqn reward tensor(367.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4209e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035659000277519226
dqn reward tensor(279.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4401e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21793565154075623
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5323e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1595573127269745
dqn reward tensor(245.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1563873589038849
dqn reward tensor(552., device='cuda:0') e 0.05 loss_dqn tensor(1.3957e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0414545051753521
dqn reward tensor(278.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7901e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2674909234046936
dqn reward tensor(671.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3007e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10231874883174896
dqn reward tensor(381.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3783e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03742539882659912
dqn reward tensor(320.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11301334202289581
dqn reward tensor(498.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18827936053276062
dqn reward tensor(494.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8337e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05571568012237549
dqn reward tensor(423.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10051462799310684
dqn reward tensor(443.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4011e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03836757689714432
dqn reward tensor(370.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4524e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10061156749725342
dqn reward tensor(238., device='cuda:0') e 0.05 loss_dqn tensor(2.0730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0772862583398819
dqn reward tensor(131.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8399e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11038412153720856
dqn reward tensor(403.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2242550104856491
dqn reward tensor(456.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13306649029254913
dqn reward tensor(579.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.4903e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014510486274957657
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9074e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08206624537706375
dqn reward tensor(502.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3515e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06506556272506714
dqn reward tensor(274.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.5061e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23802489042282104
dqn reward tensor(400., device='cuda:0') e 0.05 loss_dqn tensor(1.4407e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24654854834079742
dqn reward tensor(408.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6191e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22984108328819275
dqn reward tensor(435.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3589e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05551404505968094
dqn reward tensor(472.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07149435579776764
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8987e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18279169499874115
dqn reward tensor(483.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3901e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03032795898616314
dqn reward tensor(339.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3823e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0371946319937706
dqn reward tensor(325.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.9847e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08858910948038101
dqn reward tensor(403.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08371289819478989
dqn reward tensor(465.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3427e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20665019750595093
dqn reward tensor(399.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1659e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26005321741104126
dqn reward tensor(500.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08813120424747467
dqn reward tensor(327.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0555e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1901441514492035
dqn reward tensor(411.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032110244035720825
dqn reward tensor(307.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6072e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1990392506122589
dqn reward tensor(406.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07498345524072647
dqn reward tensor(503., device='cuda:0') e 0.05 loss_dqn tensor(1.4290e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033495448529720306
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6855e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25136834383010864
dqn reward tensor(345., device='cuda:0') e 0.05 loss_dqn tensor(1.1867e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27596235275268555
dqn reward tensor(495.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3930e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11818504333496094
dqn reward tensor(248.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08832605183124542
dqn reward tensor(352.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7773e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12782591581344604
dqn reward tensor(490.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3169e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08676503598690033
dqn reward tensor(348.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059695206582546234
dqn reward tensor(435.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0647e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206113636493683
dqn reward tensor(422.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050935789942741394
dqn reward tensor(558.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3908e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31178563833236694
dqn reward tensor(439.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3987e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1756696254014969
dqn reward tensor(378.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4131e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1307935118675232
dqn reward tensor(385.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.9641e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15499451756477356
dqn reward tensor(188., device='cuda:0') e 0.05 loss_dqn tensor(1.0555e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07967368513345718
dqn reward tensor(229.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5184e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2689879238605499
dqn reward tensor(245.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16361835598945618
dqn reward tensor(414.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10850812494754791
dqn reward tensor(510.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0986742153763771
dqn reward tensor(311.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5046e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11440329253673553
dqn reward tensor(494.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07247275114059448
dqn reward tensor(407., device='cuda:0') e 0.05 loss_dqn tensor(1.4330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.146291583776474
dqn reward tensor(384., device='cuda:0') e 0.05 loss_dqn tensor(1.0340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0916733518242836
dqn reward tensor(365.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14685338735580444
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19515591859817505
dqn reward tensor(334.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8064e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03026178479194641
dqn reward tensor(498.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4807e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2077571153640747
dqn reward tensor(319.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25331634283065796
dqn reward tensor(630.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3606e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2837323844432831
dqn reward tensor(494.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4447e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2009616196155548
dqn reward tensor(594., device='cuda:0') e 0.05 loss_dqn tensor(1.2970e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10352720320224762
dqn reward tensor(313.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3363e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374666392803192
dqn reward tensor(262.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2845e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1946633756160736
dqn reward tensor(229.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1769828200340271
dqn reward tensor(289.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10996267199516296
dqn reward tensor(533., device='cuda:0') e 0.05 loss_dqn tensor(1.4770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052581317722797394
dqn reward tensor(308.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4052e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1433626115322113
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8927e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2556028962135315
dqn reward tensor(396.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05796048417687416
dqn reward tensor(294.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.7480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23855909705162048
dqn reward tensor(304.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0710e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10888519883155823
dqn reward tensor(311.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08242480456829071
dqn reward tensor(421.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1480102241039276
dqn reward tensor(530.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5560e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0965464636683464
dqn reward tensor(259.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1365690529346466
dqn reward tensor(645.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4264e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09694855660200119
dqn reward tensor(399.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4414e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13442747294902802
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4744e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24196302890777588
dqn reward tensor(207.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033901505172252655
dqn reward tensor(392.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1568346917629242
dqn reward tensor(280.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5407e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06407468020915985
dqn reward tensor(419.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0744e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1935494840145111
dqn reward tensor(409.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5899e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1608419567346573
dqn reward tensor(376.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5236e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23121069371700287
dqn reward tensor(187., device='cuda:0') e 0.05 loss_dqn tensor(1.1583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27132055163383484
dqn reward tensor(453.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0728258267045021
dqn reward tensor(292.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15624946355819702
dqn reward tensor(305.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3916e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1406853199005127
dqn reward tensor(365.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04249674826860428
dqn reward tensor(316.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5724e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09837144613265991
dqn reward tensor(339.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1712e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2669224739074707
dqn reward tensor(460.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12616954743862152
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5063e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07392766326665878
dqn reward tensor(306.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2030501514673233
dqn reward tensor(491.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6635e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2739729881286621
dqn reward tensor(421.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4710e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06407131254673004
dqn reward tensor(316.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3806e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2577219009399414
dqn reward tensor(253.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1312e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12268379330635071
dqn reward tensor(379.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09120208024978638
dqn reward tensor(538.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4587e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2853488326072693
dqn reward tensor(308.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05303139239549637
dqn reward tensor(336.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18627434968948364
dqn reward tensor(445.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14825642108917236
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13737520575523376
dqn reward tensor(390.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3438e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08730507642030716
dqn reward tensor(344.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6120e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10754215717315674
dqn reward tensor(463.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1594022512435913
dqn reward tensor(302.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4814e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19783565402030945
dqn reward tensor(225.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.3281e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17334672808647156
dqn reward tensor(537.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2797856628894806
dqn reward tensor(272.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3345e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07201279699802399
dqn reward tensor(276., device='cuda:0') e 0.05 loss_dqn tensor(2.2071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10423614829778671
dqn reward tensor(442., device='cuda:0') e 0.05 loss_dqn tensor(1.0208e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04012034088373184
dqn reward tensor(271.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9529e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11664903163909912
dqn reward tensor(531.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9985e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041779108345508575
dqn reward tensor(323.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6423e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10668028891086578
dqn reward tensor(361.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1113702803850174
dqn reward tensor(319.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06320913881063461
dqn reward tensor(268.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5584e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14642129838466644
dqn reward tensor(502.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8258e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1948363333940506
dqn reward tensor(515.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12533427774906158
dqn reward tensor(451.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21661703288555145
dqn reward tensor(389.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.0828e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16533786058425903
dqn reward tensor(333.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6464e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047972679138183594
dqn reward tensor(272.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2262e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14283543825149536
dqn reward tensor(339.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7945e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904706358909607
dqn reward tensor(316.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5483e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23856714367866516
dqn reward tensor(382.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11240659654140472
dqn reward tensor(410., device='cuda:0') e 0.05 loss_dqn tensor(2.2717e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19324851036071777
dqn reward tensor(400., device='cuda:0') e 0.05 loss_dqn tensor(7.2295e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14989478886127472
dqn reward tensor(306.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0284e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11068370938301086
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4975e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22537976503372192
dqn reward tensor(459.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4544e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17655916512012482
dqn reward tensor(279.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2713e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03934210166335106
dqn reward tensor(482.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7797e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15287193655967712
dqn reward tensor(512.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4946e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22013720870018005
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18085721135139465
dqn reward tensor(153.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5231e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16397544741630554
dqn reward tensor(523.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4646e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09238766878843307
dqn reward tensor(415., device='cuda:0') e 0.05 loss_dqn tensor(1.5944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2448837161064148
dqn reward tensor(467., device='cuda:0') e 0.05 loss_dqn tensor(1.2042e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17438490688800812
dqn reward tensor(331.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0930342972278595
dqn reward tensor(516., device='cuda:0') e 0.05 loss_dqn tensor(2.0810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25890079140663147
dqn reward tensor(452.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5690e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16993120312690735
dqn reward tensor(484.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2817193865776062
dqn reward tensor(172.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3868e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035441603511571884
dqn reward tensor(474.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5653e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029807763174176216
dqn reward tensor(534.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1034030020236969
dqn reward tensor(540.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15182343125343323
dqn reward tensor(426.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21358004212379456
dqn reward tensor(422.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11834478378295898
dqn reward tensor(336.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1744469702243805
dqn reward tensor(398.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22115829586982727
dqn reward tensor(308., device='cuda:0') e 0.05 loss_dqn tensor(1.5871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21745604276657104
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4214e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07779932022094727
dqn reward tensor(352.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19262638688087463
dqn reward tensor(511.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22214975953102112
dqn reward tensor(630.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30998826026916504
dqn reward tensor(521.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4518e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10432155430316925
dqn reward tensor(362., device='cuda:0') e 0.05 loss_dqn tensor(1.5071e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09843842685222626
dqn reward tensor(442.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09307458996772766
dqn reward tensor(309.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2635e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11237935721874237
dqn reward tensor(280.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18250012397766113
dqn reward tensor(360.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7483e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11022356152534485
dqn reward tensor(241.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0496e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11522030085325241
dqn reward tensor(329.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08702760189771652
dqn reward tensor(409.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3959e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034106552600860596
dqn reward tensor(357.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5275e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2952492833137512
dqn reward tensor(408.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20625662803649902
dqn reward tensor(417.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0730813518166542
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4157e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20625020563602448
dqn reward tensor(291.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.0937e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21792885661125183
dqn reward tensor(480.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017369132488965988
dqn reward tensor(517.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413667887449265
dqn reward tensor(238.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3753e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16992419958114624
dqn reward tensor(182.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06701727211475372
dqn reward tensor(329.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8687e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12978458404541016
dqn reward tensor(335.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8093e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02038753591477871
dqn reward tensor(397.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04648026078939438
dqn reward tensor(357.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1100117489695549
dqn reward tensor(412.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.6631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026584193110466003
dqn reward tensor(367.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0724e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08607278764247894
dqn reward tensor(510.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4297e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09081064909696579
dqn reward tensor(555.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2739943861961365
dqn reward tensor(388.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6336e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09030233323574066
dqn reward tensor(544.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2766e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26472485065460205
dqn reward tensor(153.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1265285164117813
dqn reward tensor(537.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4590e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06805142760276794
dqn reward tensor(595.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5449e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026188187301158905
dqn reward tensor(543.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5343e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.075616255402565
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9204e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0683189406991005
dqn reward tensor(319.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6051e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0671229213476181
dqn reward tensor(525., device='cuda:0') e 0.05 loss_dqn tensor(1.5225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3105529546737671
dqn reward tensor(433.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08875445276498795
dqn reward tensor(338.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5273e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2390359491109848
dqn reward tensor(348.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6368e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20427678525447845
dqn reward tensor(243.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4342e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05463504418730736
dqn reward tensor(511.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3499056398868561
dqn reward tensor(593.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19146090745925903
dqn reward tensor(299.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1397806704044342
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08149218559265137
dqn reward tensor(475.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5168e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14300861954689026
dqn reward tensor(513.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6026e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15117137134075165
dqn reward tensor(456., device='cuda:0') e 0.05 loss_dqn tensor(1.5956e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18012948334217072
dqn reward tensor(549., device='cuda:0') e 0.05 loss_dqn tensor(1.1215e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09736279398202896
dqn reward tensor(536.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5845e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1389542818069458
dqn reward tensor(540., device='cuda:0') e 0.05 loss_dqn tensor(1.5957e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18910452723503113
dqn reward tensor(360.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20391124486923218
dqn reward tensor(484.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2306087613105774
dqn reward tensor(449.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6332e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3664519786834717
dqn reward tensor(391., device='cuda:0') e 0.05 loss_dqn tensor(1.6377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1378796398639679
dqn reward tensor(655.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08319417387247086
dqn reward tensor(401.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.3613e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052863623946905136
dqn reward tensor(384.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10315050184726715
dqn reward tensor(503.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6881e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13732784986495972
dqn reward tensor(302.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5919e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034434810280799866
dqn reward tensor(500.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5727e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11394201964139938
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5395e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04413200169801712
dqn reward tensor(305., device='cuda:0') e 0.05 loss_dqn tensor(3.7933e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035119108855724335
dqn reward tensor(184.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0640e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2821992337703705
dqn reward tensor(461.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.089513398706913
dqn reward tensor(420.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5417e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0664399191737175
dqn reward tensor(441.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6567e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13840007781982422
dqn reward tensor(461.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5782e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2065037041902542
dqn reward tensor(314.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6968e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09258497506380081
dqn reward tensor(364.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14684027433395386
dqn reward tensor(176.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6850e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09282612800598145
dqn reward tensor(436.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13359367847442627
dqn reward tensor(418.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21309852600097656
dqn reward tensor(647.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14749523997306824
dqn reward tensor(393.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6401e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06335717439651489
dqn reward tensor(349.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7727e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02626127377152443
dqn reward tensor(408.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.2732e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06410655379295349
dqn reward tensor(404.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7130e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12438660115003586
dqn reward tensor(367.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13422822952270508
dqn reward tensor(559., device='cuda:0') e 0.05 loss_dqn tensor(9.5953e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23553848266601562
dqn reward tensor(549.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4839e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030099771916866302
dqn reward tensor(354.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.5991e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0649927407503128
dqn reward tensor(314., device='cuda:0') e 0.05 loss_dqn tensor(1.6942e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07898438721895218
dqn reward tensor(359.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09001345932483673
dqn reward tensor(229.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06565062701702118
dqn reward tensor(373.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10544535517692566
dqn reward tensor(377.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7177e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20161448419094086
dqn reward tensor(379.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7121e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21873685717582703
dqn reward tensor(357.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02676287852227688
dqn reward tensor(424., device='cuda:0') e 0.05 loss_dqn tensor(1.6851e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2069314867258072
dqn reward tensor(324.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7257e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10229730606079102
dqn reward tensor(471.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1901e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1680869460105896
dqn reward tensor(628.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19042755663394928
dqn reward tensor(431.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09307363629341125
dqn reward tensor(577.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6218e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290888786315918
dqn reward tensor(325.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1305180937051773
dqn reward tensor(573.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6617e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432437002658844
dqn reward tensor(510.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12450025975704193
dqn reward tensor(484.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2421e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1915779411792755
dqn reward tensor(399.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6046e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08656274527311325
dqn reward tensor(444.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6160e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11483671516180038
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1214519590139389
dqn reward tensor(409.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17698071897029877
dqn reward tensor(579.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5918e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1958087533712387
dqn reward tensor(450.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6955e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07231330126523972
dqn reward tensor(210.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.0488e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10586470365524292
dqn reward tensor(416.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5549e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19603225588798523
dqn reward tensor(519., device='cuda:0') e 0.05 loss_dqn tensor(1.5344e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11691738665103912
dqn reward tensor(620.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17643746733665466
dqn reward tensor(337.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06490478664636612
dqn reward tensor(418.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5982e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09937078505754471
dqn reward tensor(280., device='cuda:0') e 0.05 loss_dqn tensor(2.0286e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14228877425193787
dqn reward tensor(417.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2451e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06820523738861084
dqn reward tensor(386.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.6919e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02830107882618904
dqn reward tensor(363.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6880e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053618475794792175
dqn reward tensor(546.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2475794404745102
dqn reward tensor(440., device='cuda:0') e 0.05 loss_dqn tensor(1.6461e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15943773090839386
dqn reward tensor(11.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4956e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19044211506843567
dqn reward tensor(567.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2229546308517456
dqn reward tensor(370.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6704e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11604219675064087
dqn reward tensor(476., device='cuda:0') e 0.05 loss_dqn tensor(1.4970e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07599765807390213
dqn reward tensor(368.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10413261502981186
dqn reward tensor(505.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2126e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682184934616089
dqn reward tensor(280.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15823589265346527
dqn reward tensor(361.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6995e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202153027057648
dqn reward tensor(515.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03693825379014015
dqn reward tensor(270.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09165441989898682
dqn reward tensor(445.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7517e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21111656725406647
dqn reward tensor(414., device='cuda:0') e 0.05 loss_dqn tensor(3.7153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25855889916419983
dqn reward tensor(420.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6782e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20645546913146973
dqn reward tensor(303.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23048874735832214
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2025e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19659970700740814
dqn reward tensor(462.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6430e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10765263438224792
dqn reward tensor(335.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10273929685354233
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05119875818490982
dqn reward tensor(237.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0468e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044979602098464966
dqn reward tensor(197.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15382571518421173
dqn reward tensor(514.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6544e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12970659136772156
dqn reward tensor(497.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3226e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14394521713256836
dqn reward tensor(383., device='cuda:0') e 0.05 loss_dqn tensor(1.7092e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12058648467063904
dqn reward tensor(408.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04634314030408859
dqn reward tensor(338.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8646e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16057038307189941
dqn reward tensor(534.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5809e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038923412561416626
dqn reward tensor(488.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06591739505529404
dqn reward tensor(139.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1711081564426422
dqn reward tensor(332.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7039e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09142094850540161
dqn reward tensor(460.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7490e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13143959641456604
dqn reward tensor(249., device='cuda:0') e 0.05 loss_dqn tensor(1.8136e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09742925316095352
dqn reward tensor(314.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08524259179830551
dqn reward tensor(371.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6438e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11839868128299713
dqn reward tensor(644., device='cuda:0') e 0.05 loss_dqn tensor(1.5482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15915800631046295
dqn reward tensor(259.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.6179e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013571427203714848
dqn reward tensor(518.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6644e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1417192965745926
dqn reward tensor(537.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0178e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06504130363464355
dqn reward tensor(445.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1800709068775177
dqn reward tensor(153.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0981719046831131
dqn reward tensor(381., device='cuda:0') e 0.05 loss_dqn tensor(2.6937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1755882203578949
dqn reward tensor(140.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3174e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020780373364686966
dqn reward tensor(362.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5288e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33545202016830444
dqn reward tensor(449., device='cuda:0') e 0.05 loss_dqn tensor(8.1866e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2524697184562683
dqn reward tensor(514.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7246e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1263400763273239
dqn reward tensor(287.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2966e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13789299130439758
dqn reward tensor(342.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6730e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19604849815368652
dqn reward tensor(480.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7233e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07403473556041718
dqn reward tensor(462.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24592021107673645
dqn reward tensor(365., device='cuda:0') e 0.05 loss_dqn tensor(1.0698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1784181296825409
dqn reward tensor(326.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.5396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14609971642494202
dqn reward tensor(458.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16272646188735962
dqn reward tensor(261.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7960e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04661886766552925
dqn reward tensor(425.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7890e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1604902148246765
dqn reward tensor(609.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12042418122291565
dqn reward tensor(266.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06314176321029663
dqn reward tensor(346.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1439e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11164961010217667
dqn reward tensor(177.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0989e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027382690459489822
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5764e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029556196182966232
dqn reward tensor(180.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06688268482685089
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3245e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04086931049823761
dqn reward tensor(328.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0533e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2845994830131531
dqn reward tensor(498.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5734e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10136445611715317
dqn reward tensor(346.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8527e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2476562261581421
dqn reward tensor(349.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2848e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.356100469827652
dqn reward tensor(225., device='cuda:0') e 0.05 loss_dqn tensor(1.6807e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1512385606765747
dqn reward tensor(315.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5775e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07064250111579895
dqn reward tensor(179.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13592608273029327
dqn reward tensor(377.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06720932573080063
dqn reward tensor(105.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6208e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05131999030709267
dqn reward tensor(413.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5154e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20107249915599823
dqn reward tensor(268.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0041e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1903526932001114
dqn reward tensor(293.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2350e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1430240422487259
dqn reward tensor(229.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18397139012813568
dqn reward tensor(311.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2538459897041321
dqn reward tensor(323.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5509e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06592094898223877
dqn reward tensor(386.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5392e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09363676607608795
dqn reward tensor(435.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4807e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07283560931682587
dqn reward tensor(529.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1004e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10457468032836914
dqn reward tensor(423., device='cuda:0') e 0.05 loss_dqn tensor(1.1648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12165682017803192
dqn reward tensor(331.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045487672090530396
dqn reward tensor(17.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0224e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1626262217760086
dqn reward tensor(91.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9494e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1873636245727539
dqn reward tensor(374.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5547e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17985735833644867
dqn reward tensor(412.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5713e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10903096199035645
dqn reward tensor(352.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5978e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21325339376926422
dqn reward tensor(137.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1227e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23582060635089874
dqn reward tensor(262.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5708e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08033488690853119
dqn reward tensor(359.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.194996178150177
dqn reward tensor(370.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2511e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08299428224563599
dqn reward tensor(295.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12294434756040573
dqn reward tensor(252.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13422760367393494
dqn reward tensor(280.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2530e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07277652621269226
dqn reward tensor(155.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22936007380485535
dqn reward tensor(265.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3209e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042317211627960205
dqn reward tensor(291.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07683917135000229
dqn reward tensor(172.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.6423e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09022294729948044
dqn reward tensor(23.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0151e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19545777142047882
dqn reward tensor(136.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0564e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27418291568756104
dqn reward tensor(254.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056414298713207245
dqn reward tensor(411.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1504923552274704
dqn reward tensor(254.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5888e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497361660003662
dqn reward tensor(240.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4941e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03724151849746704
dqn reward tensor(403.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13149091601371765
dqn reward tensor(323.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07456274330615997
dqn reward tensor(177., device='cuda:0') e 0.05 loss_dqn tensor(4.4721e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055471789091825485
dqn reward tensor(294.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.6307e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028534583747386932
dqn reward tensor(315.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2199e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028469722718000412
dqn reward tensor(473., device='cuda:0') e 0.05 loss_dqn tensor(1.5162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1522800326347351
dqn reward tensor(516.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5063e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13877564668655396
dqn reward tensor(345.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6280e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2763933539390564
dqn reward tensor(257., device='cuda:0') e 0.05 loss_dqn tensor(1.4981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06205010414123535
dqn reward tensor(179.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015551380813121796
dqn reward tensor(356.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25253742933273315
dqn reward tensor(30.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.008143036626279354
Evaluating...
Train: {'rocauc': 0.7804815501291156} 4.732923984527588
=====Epoch 22=====
Training...
dqn reward tensor(157.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2237e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07632945477962494
dqn reward tensor(321.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16427569091320038
dqn reward tensor(227.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6025e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1402452439069748
dqn reward tensor(362.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20933887362480164
dqn reward tensor(391.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20432838797569275
dqn reward tensor(334.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6023e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14563600718975067
dqn reward tensor(462.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1527e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12047591805458069
dqn reward tensor(467.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0124e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0882878303527832
dqn reward tensor(263.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6544e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06471452116966248
dqn reward tensor(299.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09524324536323547
dqn reward tensor(320.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5102e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030666951090097427
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2755e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15122252702713013
dqn reward tensor(165.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09234754741191864
dqn reward tensor(181.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08563953638076782
dqn reward tensor(477.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1277199685573578
dqn reward tensor(241.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04447794333100319
dqn reward tensor(387.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5790e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11387643218040466
dqn reward tensor(230.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5564e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08105829358100891
dqn reward tensor(210.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10642670840024948
dqn reward tensor(378.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7264e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1744093894958496
dqn reward tensor(94.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4664e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2697724401950836
dqn reward tensor(333.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6979e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11241838335990906
dqn reward tensor(506.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2963e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22869957983493805
dqn reward tensor(380.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19666802883148193
dqn reward tensor(156.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13555657863616943
dqn reward tensor(100.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3999e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029786065220832825
dqn reward tensor(343., device='cuda:0') e 0.05 loss_dqn tensor(1.7152e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16706839203834534
dqn reward tensor(307.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7007e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1578969806432724
dqn reward tensor(353.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07254545390605927
dqn reward tensor(107.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08642042428255081
dqn reward tensor(369.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.7938e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19877967238426208
dqn reward tensor(323.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.6126e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05970430374145508
dqn reward tensor(539.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6821e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14662399888038635
dqn reward tensor(265., device='cuda:0') e 0.05 loss_dqn tensor(1.1650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07735602557659149
dqn reward tensor(482.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18164387345314026
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7655e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12754738330841064
dqn reward tensor(283.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03372348099946976
dqn reward tensor(374.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6814e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0891532301902771
dqn reward tensor(252., device='cuda:0') e 0.05 loss_dqn tensor(1.7857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10599575936794281
dqn reward tensor(347.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7035e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1157648041844368
dqn reward tensor(427.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1419607251882553
dqn reward tensor(259.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21236683428287506
dqn reward tensor(358.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7013e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05605195090174675
dqn reward tensor(409.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06352617591619492
dqn reward tensor(308.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26295143365859985
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7382e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07736486941576004
dqn reward tensor(462.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6285e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029174992814660072
dqn reward tensor(409., device='cuda:0') e 0.05 loss_dqn tensor(1.5779e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12908247113227844
dqn reward tensor(459., device='cuda:0') e 0.05 loss_dqn tensor(1.6880e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20436501502990723
dqn reward tensor(295.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11570610105991364
dqn reward tensor(292.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23486094176769257
dqn reward tensor(255.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1554683893918991
dqn reward tensor(291.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21442675590515137
dqn reward tensor(474.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6775e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13015598058700562
dqn reward tensor(346.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7290e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16925333440303802
dqn reward tensor(526.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7609e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09177698940038681
dqn reward tensor(262.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2038e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14447474479675293
dqn reward tensor(286., device='cuda:0') e 0.05 loss_dqn tensor(1.2094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1809701919555664
dqn reward tensor(480.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07835975289344788
dqn reward tensor(-233., device='cuda:0') e 0.05 loss_dqn tensor(4.0702e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25476017594337463
dqn reward tensor(317.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6501e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638878583908081
dqn reward tensor(327.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11017002165317535
dqn reward tensor(370.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6597e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09719353914260864
dqn reward tensor(449.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6549e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09147994220256805
dqn reward tensor(377.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1838620901107788
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4301e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21597108244895935
dqn reward tensor(337.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12425462901592255
dqn reward tensor(198.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08721210062503815
dqn reward tensor(301., device='cuda:0') e 0.05 loss_dqn tensor(1.6931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06278324127197266
dqn reward tensor(121.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0940709188580513
dqn reward tensor(424.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2228e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22732040286064148
dqn reward tensor(432.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4612e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07729429006576538
dqn reward tensor(393.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1891e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03410199657082558
dqn reward tensor(252.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12069234251976013
dqn reward tensor(353.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0692949891090393
dqn reward tensor(458.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5016e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1216387152671814
dqn reward tensor(449.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2995e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05977252498269081
dqn reward tensor(138.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4325e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16881829500198364
dqn reward tensor(399.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4936e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11168152093887329
dqn reward tensor(343.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4906e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015257121995091438
dqn reward tensor(321., device='cuda:0') e 0.05 loss_dqn tensor(1.1237e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31008175015449524
dqn reward tensor(115.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013819892890751362
dqn reward tensor(339.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4577e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017400886863470078
dqn reward tensor(304.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4548e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2509169578552246
dqn reward tensor(436., device='cuda:0') e 0.05 loss_dqn tensor(6.2053e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14855800569057465
dqn reward tensor(365.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23855188488960266
dqn reward tensor(-164.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0982205867767334
dqn reward tensor(384.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5165e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13988305628299713
dqn reward tensor(377.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1082e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2947208881378174
dqn reward tensor(60.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3927e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06822092086076736
dqn reward tensor(168.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23891150951385498
dqn reward tensor(293.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17816996574401855
dqn reward tensor(316.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4888e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06482500582933426
dqn reward tensor(362.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4780e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21954765915870667
dqn reward tensor(252.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09781663119792938
dqn reward tensor(209.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3600e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05855756998062134
dqn reward tensor(424.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4369e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09689311683177948
dqn reward tensor(301.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1568315625190735
dqn reward tensor(251., device='cuda:0') e 0.05 loss_dqn tensor(1.1691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057802535593509674
dqn reward tensor(396.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8514e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16455398499965668
dqn reward tensor(364.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4191e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07745852321386337
dqn reward tensor(511.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7970e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0700080469250679
dqn reward tensor(354.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3780e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13638322055339813
dqn reward tensor(336.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4654e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11907756328582764
dqn reward tensor(-8.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9435e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2225329726934433
dqn reward tensor(308.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5157e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017619339749217033
dqn reward tensor(428., device='cuda:0') e 0.05 loss_dqn tensor(9.2183e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28043651580810547
dqn reward tensor(308.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09430288523435593
dqn reward tensor(294.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20935627818107605
dqn reward tensor(357.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4086e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16768524050712585
dqn reward tensor(250.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27191266417503357
dqn reward tensor(248.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05339069664478302
dqn reward tensor(362.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0409e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07928445935249329
dqn reward tensor(443.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6758e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12649953365325928
dqn reward tensor(263.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3807e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13677963614463806
dqn reward tensor(136.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11048135161399841
dqn reward tensor(289.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1545746922492981
dqn reward tensor(424.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4023e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10519063472747803
dqn reward tensor(358.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0749e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15218470990657806
dqn reward tensor(370., device='cuda:0') e 0.05 loss_dqn tensor(2.6893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23708021640777588
dqn reward tensor(396.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1253199428319931
dqn reward tensor(388., device='cuda:0') e 0.05 loss_dqn tensor(1.3923e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060617610812187195
dqn reward tensor(488.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3214e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2315339595079422
dqn reward tensor(307.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04487980902194977
dqn reward tensor(283.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1238604411482811
dqn reward tensor(329.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4069e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11826898157596588
dqn reward tensor(280.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07509226351976395
dqn reward tensor(32.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08373444527387619
dqn reward tensor(362.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9031e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3168984353542328
dqn reward tensor(97.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8133e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062256474047899246
dqn reward tensor(115.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1108e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2469881772994995
dqn reward tensor(403.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1856469064950943
dqn reward tensor(422., device='cuda:0') e 0.05 loss_dqn tensor(1.1864e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04361143708229065
dqn reward tensor(249.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.2491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06603172421455383
dqn reward tensor(477.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2047e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09825529903173447
dqn reward tensor(219.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12908396124839783
dqn reward tensor(373.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1578e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17267295718193054
dqn reward tensor(241., device='cuda:0') e 0.05 loss_dqn tensor(5.4070e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10749613493680954
dqn reward tensor(199.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2094e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15216495096683502
dqn reward tensor(186.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1490e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05649418383836746
dqn reward tensor(274.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3219e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16174903512001038
dqn reward tensor(322.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7395e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11577087640762329
dqn reward tensor(326.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0810e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18841925263404846
dqn reward tensor(238.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1945e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08637477457523346
dqn reward tensor(201.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8974e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0874205082654953
dqn reward tensor(325.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24136444926261902
dqn reward tensor(91.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9366e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1948431134223938
dqn reward tensor(208.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1436e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14309486746788025
dqn reward tensor(183., device='cuda:0') e 0.05 loss_dqn tensor(1.6218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0908174067735672
dqn reward tensor(-48., device='cuda:0') e 0.05 loss_dqn tensor(7.6934e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23985594511032104
dqn reward tensor(350.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13849768042564392
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12047422677278519
dqn reward tensor(307., device='cuda:0') e 0.05 loss_dqn tensor(1.2055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049021948128938675
dqn reward tensor(246.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2065e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053168267011642456
dqn reward tensor(321.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07241667062044144
dqn reward tensor(301.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1602e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15397615730762482
dqn reward tensor(294.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1847e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10392994433641434
dqn reward tensor(204.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06445880979299545
dqn reward tensor(319.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13577665388584137
dqn reward tensor(222.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1992e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0332273431122303
dqn reward tensor(248., device='cuda:0') e 0.05 loss_dqn tensor(1.1837e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0248846597969532
dqn reward tensor(155.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33964017033576965
dqn reward tensor(261.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2020e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3445754945278168
dqn reward tensor(235.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02758086659014225
dqn reward tensor(273.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03804890066385269
dqn reward tensor(192.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.5721e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09974682331085205
dqn reward tensor(-77., device='cuda:0') e 0.05 loss_dqn tensor(1.5528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14240092039108276
dqn reward tensor(282.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1387e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08294717967510223
dqn reward tensor(193.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.2545e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066400468349457
dqn reward tensor(291.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07530808448791504
dqn reward tensor(202.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0822e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19888637959957123
dqn reward tensor(180.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05021926388144493
dqn reward tensor(118.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.1300e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23424313962459564
dqn reward tensor(226.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413404881954193
dqn reward tensor(221.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0682e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1404886394739151
dqn reward tensor(157.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0430e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08103858679533005
dqn reward tensor(191.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05455698445439339
dqn reward tensor(320.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9868e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09126204252243042
dqn reward tensor(199.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0830e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1565311998128891
dqn reward tensor(-155.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.9272e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036564916372299194
dqn reward tensor(116.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12381023913621902
dqn reward tensor(87.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6282e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15158122777938843
dqn reward tensor(122.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0332e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19368664920330048
dqn reward tensor(338.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8639e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29585862159729004
dqn reward tensor(233.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10146308690309525
dqn reward tensor(145.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0183e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22344300150871277
dqn reward tensor(187.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0314e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057667478919029236
dqn reward tensor(215.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0646e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12227802723646164
dqn reward tensor(-25.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0723e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2301209270954132
dqn reward tensor(-126.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.6725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1195431724190712
dqn reward tensor(309.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0718e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0551900640130043
dqn reward tensor(203.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7599e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21927395462989807
dqn reward tensor(227.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1771e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.095530666410923
dqn reward tensor(112.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5697e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17583821713924408
dqn reward tensor(198.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0479e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13814139366149902
dqn reward tensor(124., device='cuda:0') e 0.05 loss_dqn tensor(1.0609e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15694119036197662
dqn reward tensor(85.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09227226674556732
dqn reward tensor(39.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0754e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11271364986896515
dqn reward tensor(260., device='cuda:0') e 0.05 loss_dqn tensor(9.8947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04906708002090454
dqn reward tensor(144.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08575277030467987
dqn reward tensor(178.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08455318957567215
dqn reward tensor(232.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8342e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10090523958206177
dqn reward tensor(83.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7337e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26945409178733826
dqn reward tensor(41., device='cuda:0') e 0.05 loss_dqn tensor(7.0985e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10702414065599442
dqn reward tensor(-23.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10281072556972504
dqn reward tensor(178.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12233756482601166
dqn reward tensor(149.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7989e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329677700996399
dqn reward tensor(28.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0455e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07516882568597794
dqn reward tensor(270.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1209e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2622721195220947
dqn reward tensor(-37.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06892801076173782
dqn reward tensor(286.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2146403193473816
dqn reward tensor(-227.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18602585792541504
dqn reward tensor(92.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.4077e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09665724635124207
dqn reward tensor(84.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0021e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08822321891784668
dqn reward tensor(124.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0706e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05807815492153168
dqn reward tensor(146., device='cuda:0') e 0.05 loss_dqn tensor(6.1735e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18057920038700104
dqn reward tensor(-95.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0154e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13249829411506653
dqn reward tensor(127.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.5732e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14188477396965027
dqn reward tensor(95.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7634e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17145557701587677
dqn reward tensor(119.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0019e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18015184998512268
dqn reward tensor(410., device='cuda:0') e 0.05 loss_dqn tensor(6.2070e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11255265027284622
dqn reward tensor(207.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9369e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18354429304599762
dqn reward tensor(84.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5358e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1668510138988495
dqn reward tensor(189., device='cuda:0') e 0.05 loss_dqn tensor(9.1563e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0744200125336647
dqn reward tensor(80.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04846673458814621
dqn reward tensor(61.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22088542580604553
dqn reward tensor(236., device='cuda:0') e 0.05 loss_dqn tensor(5.4722e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14897124469280243
dqn reward tensor(146.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6857e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20491346716880798
dqn reward tensor(343.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7265e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16102254390716553
dqn reward tensor(185.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0193e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16063269972801208
dqn reward tensor(-1., device='cuda:0') e 0.05 loss_dqn tensor(5.3734e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2086157500743866
dqn reward tensor(-116.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0147e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12193220853805542
dqn reward tensor(142.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9156e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052117325365543365
dqn reward tensor(-28.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6527e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08958487212657928
dqn reward tensor(225.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1404e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034487977623939514
dqn reward tensor(-8., device='cuda:0') e 0.05 loss_dqn tensor(3.3473e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17473435401916504
dqn reward tensor(41.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2123e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1307305544614792
dqn reward tensor(21.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0240e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19569146633148193
dqn reward tensor(102.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0745e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03388731926679611
dqn reward tensor(-1.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6560e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15752777457237244
dqn reward tensor(116.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5715e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14812888205051422
dqn reward tensor(30.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0795e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07548600435256958
dqn reward tensor(254.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1802e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16670532524585724
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0493e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16000878810882568
dqn reward tensor(124.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4595e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.223174050450325
dqn reward tensor(-122.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0295e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17880865931510925
dqn reward tensor(-23.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1660570651292801
dqn reward tensor(56.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6348e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23741865158081055
dqn reward tensor(-102., device='cuda:0') e 0.05 loss_dqn tensor(1.0236e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15339621901512146
dqn reward tensor(70.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7677e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10200884938240051
dqn reward tensor(-40.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21310026943683624
dqn reward tensor(136.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9073e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14633865654468536
dqn reward tensor(-146.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05198804289102554
dqn reward tensor(-119., device='cuda:0') e 0.05 loss_dqn tensor(5.8454e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24992062151432037
dqn reward tensor(-123.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4180e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13314571976661682
dqn reward tensor(-125.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9629e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20807594060897827
dqn reward tensor(90.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8973e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06545273214578629
dqn reward tensor(-11., device='cuda:0') e 0.05 loss_dqn tensor(6.2689e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16951996088027954
dqn reward tensor(75.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.4970e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07262702286243439
dqn reward tensor(96.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.9320e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17557837069034576
dqn reward tensor(99.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0084e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12492860108613968
dqn reward tensor(66.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0659e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1390133500099182
dqn reward tensor(115.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8184e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19961875677108765
dqn reward tensor(200.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0052e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15348337590694427
dqn reward tensor(106.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08856761455535889
dqn reward tensor(96.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0424e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040839821100234985
dqn reward tensor(68.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4438e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15235401690006256
dqn reward tensor(97.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2748e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11607928574085236
dqn reward tensor(-92.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16486060619354248
dqn reward tensor(-20.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12584878504276276
dqn reward tensor(-93.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1168e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1431565284729004
dqn reward tensor(-0.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0308e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11989343166351318
dqn reward tensor(49.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.4125e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17028900980949402
dqn reward tensor(104.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1934727430343628
dqn reward tensor(104.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0346e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12957312166690826
dqn reward tensor(-52.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9660e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07394225150346756
dqn reward tensor(163.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3778e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11470670998096466
dqn reward tensor(40.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19661614298820496
dqn reward tensor(6.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9819e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14311853051185608
dqn reward tensor(-44.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0314e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03560728579759598
dqn reward tensor(57.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0462e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1727498471736908
dqn reward tensor(162.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1214e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06766961514949799
dqn reward tensor(-64.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0137e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10646635293960571
dqn reward tensor(15.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0123e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18138208985328674
dqn reward tensor(-119.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0077e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14805066585540771
dqn reward tensor(195.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4190e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09710235893726349
dqn reward tensor(213.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8185e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1669379472732544
dqn reward tensor(92.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7316e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0930967777967453
dqn reward tensor(22.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028630048036575317
dqn reward tensor(117.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0578e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12219201028347015
dqn reward tensor(281.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0439e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10808105021715164
dqn reward tensor(18.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0941028967499733
dqn reward tensor(157.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6784e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.070253886282444
dqn reward tensor(30.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04914737865328789
dqn reward tensor(73.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.6082e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2891772389411926
dqn reward tensor(-31., device='cuda:0') e 0.05 loss_dqn tensor(1.0759e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1457291841506958
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08306765556335449
dqn reward tensor(93.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4237e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17874960601329803
dqn reward tensor(54.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5902e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044437237083911896
dqn reward tensor(133.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.4320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1607791930437088
dqn reward tensor(-149., device='cuda:0') e 0.05 loss_dqn tensor(2.8504e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25132742524147034
dqn reward tensor(93.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18433701992034912
dqn reward tensor(272.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15297311544418335
dqn reward tensor(27.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7178e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18133245408535004
dqn reward tensor(-10.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3360e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14520958065986633
dqn reward tensor(-116.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0746e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10225648432970047
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19491459429264069
dqn reward tensor(-24., device='cuda:0') e 0.05 loss_dqn tensor(5.4908e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11557332426309586
dqn reward tensor(17.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06185176968574524
dqn reward tensor(233.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8601e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11954532563686371
dqn reward tensor(62.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0689e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0964585542678833
dqn reward tensor(138.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0504e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16339337825775146
dqn reward tensor(-382.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2999e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2699919641017914
dqn reward tensor(-44.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5447e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0952521562576294
dqn reward tensor(154.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0146e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19687622785568237
dqn reward tensor(80., device='cuda:0') e 0.05 loss_dqn tensor(5.9062e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08029410243034363
dqn reward tensor(109., device='cuda:0') e 0.05 loss_dqn tensor(1.0452e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13673940300941467
dqn reward tensor(229.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7227e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1429116129875183
dqn reward tensor(66., device='cuda:0') e 0.05 loss_dqn tensor(1.0688e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11941178143024445
dqn reward tensor(52.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0647e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05421140044927597
dqn reward tensor(95.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4747e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11900150775909424
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09344740211963654
dqn reward tensor(75.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1226e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10754171758890152
dqn reward tensor(58., device='cuda:0') e 0.05 loss_dqn tensor(5.5543e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08113040775060654
dqn reward tensor(-66.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8172e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14002405107021332
dqn reward tensor(108.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0351e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19735391438007355
dqn reward tensor(-181.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13494203984737396
dqn reward tensor(40.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4919e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0658196359872818
dqn reward tensor(59.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0633e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22296422719955444
dqn reward tensor(37.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0971e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17540627717971802
dqn reward tensor(-128., device='cuda:0') e 0.05 loss_dqn tensor(9.9988e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16007134318351746
dqn reward tensor(4.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3723e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16163088381290436
dqn reward tensor(6.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0883e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08810865879058838
dqn reward tensor(-15.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11751475185155869
dqn reward tensor(183.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13862940669059753
dqn reward tensor(154.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0614e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08928245306015015
dqn reward tensor(32.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1160e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035380952060222626
dqn reward tensor(-24., device='cuda:0') e 0.05 loss_dqn tensor(6.0580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10791781544685364
dqn reward tensor(58.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0901e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11902481317520142
dqn reward tensor(59.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3563e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.162352055311203
dqn reward tensor(192.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0292e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08888789266347885
dqn reward tensor(100.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018305867910385132
dqn reward tensor(-56.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2197e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1502494513988495
dqn reward tensor(197.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0155e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14200535416603088
dqn reward tensor(315.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08926798403263092
dqn reward tensor(-0.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33653151988983154
dqn reward tensor(-13.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8156e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10661684721708298
dqn reward tensor(145.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12805891036987305
dqn reward tensor(178.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4274e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01742052659392357
dqn reward tensor(2.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022197095677256584
dqn reward tensor(175.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9398e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15384408831596375
dqn reward tensor(-6.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8277e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10507462918758392
dqn reward tensor(77.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9308e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2694101333618164
dqn reward tensor(-141.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0005e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18332414329051971
dqn reward tensor(216.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0519e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10644897073507309
dqn reward tensor(-3.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0746e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20241202414035797
dqn reward tensor(202.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0147e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20656122267246246
dqn reward tensor(-10., device='cuda:0') e 0.05 loss_dqn tensor(5.0205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09396924078464508
dqn reward tensor(97.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0788e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2883194386959076
dqn reward tensor(56.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7661e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14819061756134033
dqn reward tensor(-38.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5532e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14056220650672913
dqn reward tensor(93., device='cuda:0') e 0.05 loss_dqn tensor(4.5644e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16373313963413239
dqn reward tensor(-185., device='cuda:0') e 0.05 loss_dqn tensor(1.3980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.086903415620327
dqn reward tensor(-31.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7986e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1122649759054184
dqn reward tensor(124.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6265e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05882560461759567
dqn reward tensor(82.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6716e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05385720729827881
dqn reward tensor(-81.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4502e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038945697247982025
dqn reward tensor(253.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.5987e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022744201123714447
dqn reward tensor(80.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0743e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03013167716562748
dqn reward tensor(240.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0711e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14282235503196716
dqn reward tensor(-11., device='cuda:0') e 0.05 loss_dqn tensor(1.0585e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21053043007850647
dqn reward tensor(45.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1469e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09768210351467133
dqn reward tensor(262.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4317e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17439870536327362
dqn reward tensor(42.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2835e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07890728116035461
dqn reward tensor(-2., device='cuda:0') e 0.05 loss_dqn tensor(1.1150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0491894856095314
dqn reward tensor(-135.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5276e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059850819408893585
dqn reward tensor(198.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2374878078699112
dqn reward tensor(194.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3500e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10717184841632843
dqn reward tensor(125.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0674e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08594083040952682
dqn reward tensor(193.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0519e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06904546171426773
dqn reward tensor(107.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2739146053791046
dqn reward tensor(42.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.9393e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16302356123924255
dqn reward tensor(168.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1097e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09529685229063034
dqn reward tensor(41.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2558e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05486948415637016
dqn reward tensor(143.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1126e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15636470913887024
dqn reward tensor(48.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0853e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10562601685523987
dqn reward tensor(-197.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4546e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0932178869843483
dqn reward tensor(247.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19336572289466858
dqn reward tensor(59.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9899e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03937623277306557
dqn reward tensor(1.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1549e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16651971638202667
dqn reward tensor(63.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4611e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18873649835586548
dqn reward tensor(99.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1427e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12672005593776703
dqn reward tensor(-7.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6792e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24324224889278412
dqn reward tensor(101., device='cuda:0') e 0.05 loss_dqn tensor(1.0857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12118417024612427
dqn reward tensor(-208., device='cuda:0') e 0.05 loss_dqn tensor(1.0393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21584253013134003
dqn reward tensor(187.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8760e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05679289996623993
dqn reward tensor(193.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.8485e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21307548880577087
dqn reward tensor(-40.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19347605109214783
dqn reward tensor(130.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0726429745554924
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19115102291107178
dqn reward tensor(93.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2866e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.288051575422287
dqn reward tensor(-18.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0983e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15068477392196655
dqn reward tensor(186.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2144e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2771332561969757
dqn reward tensor(-25.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4460e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24556568264961243
dqn reward tensor(178.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4406e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16102471947669983
dqn reward tensor(-38.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8308e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1836138665676117
dqn reward tensor(14.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12338051199913025
dqn reward tensor(-2.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1573e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041891686618328094
dqn reward tensor(39.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0132e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16713351011276245
dqn reward tensor(30.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4412e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25654277205467224
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0168e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09946267306804657
dqn reward tensor(6.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046977605670690536
dqn reward tensor(-62.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2550e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15543411672115326
dqn reward tensor(-117.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16166114807128906
dqn reward tensor(36., device='cuda:0') e 0.05 loss_dqn tensor(1.0317e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1125577837228775
dqn reward tensor(-76.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0693e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19587291777133942
dqn reward tensor(70., device='cuda:0') e 0.05 loss_dqn tensor(9.3406e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13460847735404968
dqn reward tensor(-35.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0793e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1787092089653015
dqn reward tensor(0.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27644312381744385
dqn reward tensor(96.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0545e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1447519063949585
dqn reward tensor(130.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06319138407707214
dqn reward tensor(-92.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0281e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06718400120735168
dqn reward tensor(78.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5121e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06097738444805145
dqn reward tensor(129.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20689716935157776
dqn reward tensor(-88.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17702257633209229
dqn reward tensor(-73., device='cuda:0') e 0.05 loss_dqn tensor(5.3728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08687004446983337
dqn reward tensor(-45., device='cuda:0') e 0.05 loss_dqn tensor(1.1016e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18497209250926971
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1532e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07385175675153732
dqn reward tensor(3.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.0617e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943344458937645
dqn reward tensor(-50.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9110e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13727764785289764
dqn reward tensor(51.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1113e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11517392843961716
dqn reward tensor(-58.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.5348e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04105282947421074
dqn reward tensor(-41.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1794e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18598273396492004
dqn reward tensor(86.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09684798866510391
dqn reward tensor(11.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0590e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21845480799674988
dqn reward tensor(129.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7772e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030271273106336594
dqn reward tensor(121.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20637884736061096
dqn reward tensor(24., device='cuda:0') e 0.05 loss_dqn tensor(3.8585e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05322045832872391
dqn reward tensor(73.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0393e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2356409877538681
dqn reward tensor(-21.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0517e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17190632224082947
dqn reward tensor(-47.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3344e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09439355880022049
dqn reward tensor(-50.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09096134454011917
dqn reward tensor(83.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0167e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1432127058506012
dqn reward tensor(-43.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8319e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16754040122032166
dqn reward tensor(67.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0637e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1369408369064331
dqn reward tensor(21.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0670e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07399134337902069
dqn reward tensor(-145.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06827672570943832
dqn reward tensor(28., device='cuda:0') e 0.05 loss_dqn tensor(5.1328e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18476715683937073
dqn reward tensor(96.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9670e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21458423137664795
dqn reward tensor(62.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8235e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13292935490608215
dqn reward tensor(88.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0039e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08124493062496185
dqn reward tensor(-127.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0647e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09061694145202637
dqn reward tensor(-169.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1749e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1366487592458725
dqn reward tensor(-279.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7833e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09610990434885025
dqn reward tensor(10., device='cuda:0') e 0.05 loss_dqn tensor(1.0463e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22709906101226807
dqn reward tensor(-210.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0323e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1122792437672615
dqn reward tensor(-36.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0724e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12464740872383118
dqn reward tensor(-127.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.3555e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15520483255386353
dqn reward tensor(-45.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.3254e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08011436462402344
dqn reward tensor(-42.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8033e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21312594413757324
dqn reward tensor(-1.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6943e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2569717466831207
dqn reward tensor(96.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3220e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25755900144577026
dqn reward tensor(-100.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1929e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09091424942016602
dqn reward tensor(95.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5104e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0906214565038681
dqn reward tensor(-155.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0471e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19526194036006927
dqn reward tensor(15.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1799e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06514669954776764
dqn reward tensor(26.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8506e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2350214719772339
dqn reward tensor(-59.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0565e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0746849998831749
dqn reward tensor(-46.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0495e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13208523392677307
dqn reward tensor(-42., device='cuda:0') e 0.05 loss_dqn tensor(4.5501e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059465646743774414
dqn reward tensor(-25.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1435e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2734668254852295
dqn reward tensor(-64.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0692e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12442149966955185
dqn reward tensor(42.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0355e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499009132385254
dqn reward tensor(12., device='cuda:0') e 0.05 loss_dqn tensor(2.3316e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1350841522216797
dqn reward tensor(104., device='cuda:0') e 0.05 loss_dqn tensor(1.1364e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1165519580245018
dqn reward tensor(69.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0879e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04764508455991745
dqn reward tensor(142.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3841e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20821619033813477
dqn reward tensor(72.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2705625295639038
dqn reward tensor(41.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0279e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08171217888593674
dqn reward tensor(-169.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5736e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11188904196023941
dqn reward tensor(165.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5196e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09171167016029358
dqn reward tensor(-40., device='cuda:0') e 0.05 loss_dqn tensor(5.9884e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09761877357959747
dqn reward tensor(-52.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06121055409312248
dqn reward tensor(34.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9200e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18815875053405762
dqn reward tensor(-25.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1069236695766449
dqn reward tensor(108.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2930e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03291254863142967
dqn reward tensor(2.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4400e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08910488337278366
dqn reward tensor(-101.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0877e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08131156861782074
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0103e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11801932007074356
dqn reward tensor(64.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0591e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08288558572530746
dqn reward tensor(16.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1913e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14227408170700073
dqn reward tensor(-239.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5711e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013722484931349754
dqn reward tensor(-91.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5629e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2160208523273468
dqn reward tensor(29.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12135373055934906
dqn reward tensor(-19.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3114e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11729784309864044
dqn reward tensor(18.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0718e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14464367926120758
dqn reward tensor(67.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0567e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018406204879283905
dqn reward tensor(163.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.9255e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22309063374996185
dqn reward tensor(-242.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5585e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22229671478271484
dqn reward tensor(6.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0960e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24140232801437378
dqn reward tensor(18., device='cuda:0') e 0.05 loss_dqn tensor(1.0531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2033453732728958
dqn reward tensor(6.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28402179479599
dqn reward tensor(-135.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1376e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17653875052928925
dqn reward tensor(62.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1303e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09319157898426056
dqn reward tensor(71., device='cuda:0') e 0.05 loss_dqn tensor(3.2086e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2333218902349472
dqn reward tensor(58.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1024e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21296197175979614
dqn reward tensor(-146.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5122e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15399596095085144
dqn reward tensor(60.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1905e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07363347709178925
dqn reward tensor(77.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1881605088710785
dqn reward tensor(-96.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1609901487827301
dqn reward tensor(37.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1018e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15733346343040466
dqn reward tensor(-137.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9443e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051381323486566544
dqn reward tensor(-323.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1397e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290397197008133
dqn reward tensor(-91.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5865e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12988051772117615
dqn reward tensor(-7.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 1.6422160863876343
Evaluating...
Train: {'rocauc': 0.7779848441447719} -0.5646291375160217
=====Epoch 23=====
Training...
dqn reward tensor(-3.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.5102e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16857951879501343
dqn reward tensor(-40.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1093e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08644300699234009
dqn reward tensor(-156.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5645e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159939706325531
dqn reward tensor(79.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9200e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19628790020942688
dqn reward tensor(-217.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4801e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10755652189254761
dqn reward tensor(34.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.7050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12011592090129852
dqn reward tensor(-133.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3735e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1716281771659851
dqn reward tensor(1.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2830e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18657325208187103
dqn reward tensor(-22.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1131e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19670617580413818
dqn reward tensor(-245., device='cuda:0') e 0.05 loss_dqn tensor(1.1553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10532616078853607
dqn reward tensor(17., device='cuda:0') e 0.05 loss_dqn tensor(3.2491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2803654670715332
dqn reward tensor(-96.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05568212643265724
dqn reward tensor(25.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1410127580165863
dqn reward tensor(-69.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0837816670536995
dqn reward tensor(56.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9138e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07339493930339813
dqn reward tensor(-92.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9918e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10976332426071167
dqn reward tensor(-189.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.5248e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12251898646354675
dqn reward tensor(-79.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4640e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21028240025043488
dqn reward tensor(67.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0423e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23516160249710083
dqn reward tensor(-322.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6631e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18716689944267273
dqn reward tensor(-87.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.3149e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04194359481334686
dqn reward tensor(-175.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2120e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12858973443508148
dqn reward tensor(-107.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7023e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08260727673768997
dqn reward tensor(-242.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0409e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05356024205684662
dqn reward tensor(12.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2715793251991272
dqn reward tensor(-112.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6267e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08843746781349182
dqn reward tensor(48., device='cuda:0') e 0.05 loss_dqn tensor(1.6420e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14755785465240479
dqn reward tensor(166.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0364e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06987518817186356
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05345921218395233
dqn reward tensor(-5.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13975390791893005
dqn reward tensor(-150.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5655e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2586555480957031
dqn reward tensor(-194.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4841e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03886300325393677
dqn reward tensor(38.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1083e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17919771373271942
dqn reward tensor(-4.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2474e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23787479102611542
dqn reward tensor(-177.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4008e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13054338097572327
dqn reward tensor(58.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0460e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1381371170282364
dqn reward tensor(-91.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3973e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11815042793750763
dqn reward tensor(-97.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3376e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20532776415348053
dqn reward tensor(-132.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5283e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07397456467151642
dqn reward tensor(-81.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0086e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29883137345314026
dqn reward tensor(122.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11650823056697845
dqn reward tensor(-45.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8385e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1611911654472351
dqn reward tensor(-114.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15414272248744965
dqn reward tensor(120.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8504e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12928399443626404
dqn reward tensor(-189.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5733e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621740162372589
dqn reward tensor(-22.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7314e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08965315669775009
dqn reward tensor(-41.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7313e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12671132385730743
dqn reward tensor(-121.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3852e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06191346421837807
dqn reward tensor(-91.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5525e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11459439992904663
dqn reward tensor(-109.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1069e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12952253222465515
dqn reward tensor(-111.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3992e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13359645009040833
dqn reward tensor(-12.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6729e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13285145163536072
dqn reward tensor(44., device='cuda:0') e 0.05 loss_dqn tensor(1.5572e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.068520188331604
dqn reward tensor(-120.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6007e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033426228910684586
dqn reward tensor(66.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5021e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01636326313018799
dqn reward tensor(207.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2905e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14819671213626862
dqn reward tensor(-113.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9817e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08575347810983658
dqn reward tensor(-171.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7166e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24818691611289978
dqn reward tensor(57.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1442e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20895236730575562
dqn reward tensor(-282.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9356e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23883119225502014
dqn reward tensor(237.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0971e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08857756108045578
dqn reward tensor(-99.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11634036153554916
dqn reward tensor(-113.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9433e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15304794907569885
dqn reward tensor(-50., device='cuda:0') e 0.05 loss_dqn tensor(2.1905e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08971190452575684
dqn reward tensor(-18.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7462e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1878966987133026
dqn reward tensor(-63.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2727e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14178135991096497
dqn reward tensor(-250.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1365298330783844
dqn reward tensor(18.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04331863671541214
dqn reward tensor(-59.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4750e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11391963809728622
dqn reward tensor(-5.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4567e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17344599962234497
dqn reward tensor(-176.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8957e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18689337372779846
dqn reward tensor(-104.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08596854656934738
dqn reward tensor(9.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9652e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15869547426700592
dqn reward tensor(20.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1472e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0951204001903534
dqn reward tensor(-18.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2978e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1934337019920349
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20327576994895935
dqn reward tensor(-3.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4576e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046647295355796814
dqn reward tensor(-51.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2673e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1254405975341797
dqn reward tensor(93.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0753e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15361353754997253
dqn reward tensor(-66.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7554e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15299952030181885
dqn reward tensor(-105., device='cuda:0') e 0.05 loss_dqn tensor(7.3792e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06166347116231918
dqn reward tensor(-42.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0947e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08212573826313019
dqn reward tensor(-321., device='cuda:0') e 0.05 loss_dqn tensor(8.4747e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03149429336190224
dqn reward tensor(96.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3342e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2580633759498596
dqn reward tensor(58.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6213e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19296333193778992
dqn reward tensor(-55.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8114e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0482625737786293
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2845e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24759714305400848
dqn reward tensor(-21.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3348e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.088579922914505
dqn reward tensor(40.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2093e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20311054587364197
dqn reward tensor(83.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2567e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23042692244052887
dqn reward tensor(-125., device='cuda:0') e 0.05 loss_dqn tensor(8.2557e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28584837913513184
dqn reward tensor(-132.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13893577456474304
dqn reward tensor(83.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3743e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18019303679466248
dqn reward tensor(-100.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9465e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1782829761505127
dqn reward tensor(-109., device='cuda:0') e 0.05 loss_dqn tensor(9.9378e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2029411792755127
dqn reward tensor(4.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.1494e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15151578187942505
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2683e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18040797114372253
dqn reward tensor(-23., device='cuda:0') e 0.05 loss_dqn tensor(8.5626e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09701710939407349
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(8.0453e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14672911167144775
dqn reward tensor(-25.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5147e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08021870255470276
dqn reward tensor(-251.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08691540360450745
dqn reward tensor(7.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7848e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1883918046951294
dqn reward tensor(7.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3126e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17321234941482544
dqn reward tensor(-223.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9846e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30161428451538086
dqn reward tensor(-8.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3980e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.168423131108284
dqn reward tensor(37.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0629e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0740540623664856
dqn reward tensor(89., device='cuda:0') e 0.05 loss_dqn tensor(8.6212e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17708814144134521
dqn reward tensor(-142.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1449e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12117283046245575
dqn reward tensor(-44.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2391e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0374540239572525
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(2.1311e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09836887568235397
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0133e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13398399949073792
dqn reward tensor(-57.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1357e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09337564557790756
dqn reward tensor(13.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09864035993814468
dqn reward tensor(-17.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3825e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11170069128274918
dqn reward tensor(-135.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1782e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037842757999897
dqn reward tensor(-38.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6164e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05460505932569504
dqn reward tensor(40.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8833e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058969467878341675
dqn reward tensor(97.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8495e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21829405426979065
dqn reward tensor(-60.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3760e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08891013264656067
dqn reward tensor(-51.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2963e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10781505703926086
dqn reward tensor(21.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9955e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06979502737522125
dqn reward tensor(-64.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8477e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15525390207767487
dqn reward tensor(66.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8911e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23503059148788452
dqn reward tensor(-164.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16661691665649414
dqn reward tensor(-215.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0877888947725296
dqn reward tensor(-90., device='cuda:0') e 0.05 loss_dqn tensor(8.5119e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13024672865867615
dqn reward tensor(-99.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6693e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08738020062446594
dqn reward tensor(-202.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1828e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1260363757610321
dqn reward tensor(-141., device='cuda:0') e 0.05 loss_dqn tensor(3.4810e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03204613924026489
dqn reward tensor(-220.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1979e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13171876966953278
dqn reward tensor(-157.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2356e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040106870234012604
dqn reward tensor(-72.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7060e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07034392654895782
dqn reward tensor(-250.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.0417e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23167568445205688
dqn reward tensor(108.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9646e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03528442233800888
dqn reward tensor(-88.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2298e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16759958863258362
dqn reward tensor(-360.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4830e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10281628370285034
dqn reward tensor(8.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2169e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04762928932905197
dqn reward tensor(-104.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5475e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06582802534103394
dqn reward tensor(-7.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15900327265262604
dqn reward tensor(-45.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6321e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15057799220085144
dqn reward tensor(-24.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8660e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09173468500375748
dqn reward tensor(-46.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2912e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15583163499832153
dqn reward tensor(60., device='cuda:0') e 0.05 loss_dqn tensor(1.4669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22453606128692627
dqn reward tensor(-69.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9122e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09319350123405457
dqn reward tensor(58.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9849e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21971194446086884
dqn reward tensor(204.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.3592e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09498366713523865
dqn reward tensor(11.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02450060285627842
dqn reward tensor(-1., device='cuda:0') e 0.05 loss_dqn tensor(1.1881e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3086188733577728
dqn reward tensor(92.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6755e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10317534953355789
dqn reward tensor(51.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21549513936042786
dqn reward tensor(-165.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0373e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029651913791894913
dqn reward tensor(-81.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4218e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09523764252662659
dqn reward tensor(102., device='cuda:0') e 0.05 loss_dqn tensor(8.0025e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04427960142493248
dqn reward tensor(8.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4811e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05342858284711838
dqn reward tensor(17.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19364723563194275
dqn reward tensor(50.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1103e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16992004215717316
dqn reward tensor(-62.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9402e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036130644381046295
dqn reward tensor(66.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7250e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17625299096107483
dqn reward tensor(-88.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2987e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13654713332653046
dqn reward tensor(-90.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8088e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14542216062545776
dqn reward tensor(21.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2990e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11889801919460297
dqn reward tensor(-0.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1929e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2679552733898163
dqn reward tensor(81.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13113072514533997
dqn reward tensor(29.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1710e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07222169637680054
dqn reward tensor(-103.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3635e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04507552459836006
dqn reward tensor(-61.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7615e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11348867416381836
dqn reward tensor(214.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08957535028457642
dqn reward tensor(-14.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4830e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08554432541131973
dqn reward tensor(-60., device='cuda:0') e 0.05 loss_dqn tensor(1.2936e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13323715329170227
dqn reward tensor(37.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6668e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14529621601104736
dqn reward tensor(-153.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2648e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16378512978553772
dqn reward tensor(19., device='cuda:0') e 0.05 loss_dqn tensor(2.2729e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12734505534172058
dqn reward tensor(108.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6683e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2375229448080063
dqn reward tensor(172.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7456e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05898022651672363
dqn reward tensor(111., device='cuda:0') e 0.05 loss_dqn tensor(1.6857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11969849467277527
dqn reward tensor(4.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21346282958984375
dqn reward tensor(-284.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2021e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2299346625804901
dqn reward tensor(40.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3684e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05289196968078613
dqn reward tensor(-49.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.8563e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13616490364074707
dqn reward tensor(-2.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7352e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16669321060180664
dqn reward tensor(-47.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1199e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15213939547538757
dqn reward tensor(75.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1983e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20322181284427643
dqn reward tensor(-167.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5414e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0500967875123024
dqn reward tensor(174.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6127e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11459743976593018
dqn reward tensor(-2.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8439e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20972682535648346
dqn reward tensor(-8.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4775e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1035541445016861
dqn reward tensor(91.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3131e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32763466238975525
dqn reward tensor(-80.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344287395477295
dqn reward tensor(-33.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7051e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14724287390708923
dqn reward tensor(119.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6489e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07953324168920517
dqn reward tensor(51., device='cuda:0') e 0.05 loss_dqn tensor(2.7856e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18015915155410767
dqn reward tensor(-14.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4995e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1040385365486145
dqn reward tensor(29.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5105e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08287803828716278
dqn reward tensor(105., device='cuda:0') e 0.05 loss_dqn tensor(9.7190e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0429033562541008
dqn reward tensor(71.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049649953842163086
dqn reward tensor(89.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0892e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06432415544986725
dqn reward tensor(20.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9081e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08886030316352844
dqn reward tensor(118.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8292e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0912168100476265
dqn reward tensor(121.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8569e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16121050715446472
dqn reward tensor(127.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5460e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17451761662960052
dqn reward tensor(-6., device='cuda:0') e 0.05 loss_dqn tensor(2.0020e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24862384796142578
dqn reward tensor(-107.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4681e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04856275022029877
dqn reward tensor(-42.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.8606e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20961345732212067
dqn reward tensor(-77.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7187e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0730995163321495
dqn reward tensor(-148.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3000e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07890836149454117
dqn reward tensor(41., device='cuda:0') e 0.05 loss_dqn tensor(8.9971e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03245459869503975
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0011e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07942759245634079
dqn reward tensor(-127.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.0064e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22685208916664124
dqn reward tensor(229.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07243417948484421
dqn reward tensor(-132.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0602e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15353131294250488
dqn reward tensor(-118.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15295252203941345
dqn reward tensor(119., device='cuda:0') e 0.05 loss_dqn tensor(9.4643e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1803821623325348
dqn reward tensor(-70.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2790926396846771
dqn reward tensor(222.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7455e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1366843432188034
dqn reward tensor(314., device='cuda:0') e 0.05 loss_dqn tensor(1.5515e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12327396869659424
dqn reward tensor(34.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4898e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17612582445144653
dqn reward tensor(-66., device='cuda:0') e 0.05 loss_dqn tensor(3.4168e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14548727869987488
dqn reward tensor(-60.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8222e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15059857070446014
dqn reward tensor(66.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09622461348772049
dqn reward tensor(95.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2102e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16652759909629822
dqn reward tensor(-52.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0068e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15711206197738647
dqn reward tensor(105.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23214533925056458
dqn reward tensor(165.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13641799986362457
dqn reward tensor(37.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0460e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13367289304733276
dqn reward tensor(49.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1315995752811432
dqn reward tensor(6.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6653e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18518580496311188
dqn reward tensor(135.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5114e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1458825021982193
dqn reward tensor(110.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0067e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19924159348011017
dqn reward tensor(41.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2170122265815735
dqn reward tensor(326.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5757e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25743797421455383
dqn reward tensor(135.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0035e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524496078491211
dqn reward tensor(152.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10411547124385834
dqn reward tensor(-159., device='cuda:0') e 0.05 loss_dqn tensor(3.0508e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13617467880249023
dqn reward tensor(-12.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6039e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09831905364990234
dqn reward tensor(19.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.2931e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2152041792869568
dqn reward tensor(49.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09317514300346375
dqn reward tensor(90.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0289e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11687447875738144
dqn reward tensor(159.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11212792992591858
dqn reward tensor(184.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23227961361408234
dqn reward tensor(76.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2267063558101654
dqn reward tensor(-39.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10918557643890381
dqn reward tensor(110.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.7464e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09713909029960632
dqn reward tensor(-15.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0525e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055635519325733185
dqn reward tensor(149.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6939e+10, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17886275053024292
dqn reward tensor(41.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.9057e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0985393077135086
dqn reward tensor(-43.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1420622318983078
dqn reward tensor(184.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0188e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08118954300880432
dqn reward tensor(-11.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2362220585346222
dqn reward tensor(81., device='cuda:0') e 0.05 loss_dqn tensor(1.0408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10520154982805252
dqn reward tensor(16.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1126e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27092835307121277
dqn reward tensor(47.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11612091958522797
dqn reward tensor(-11.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5475e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19132250547409058
dqn reward tensor(222.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0744e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19301560521125793
dqn reward tensor(111.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17940036952495575
dqn reward tensor(160., device='cuda:0') e 0.05 loss_dqn tensor(1.4739e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0952659398317337
dqn reward tensor(-8.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16535669565200806
dqn reward tensor(42.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1031e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15155430138111115
dqn reward tensor(206.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15404024720191956
dqn reward tensor(-61., device='cuda:0') e 0.05 loss_dqn tensor(1.1158e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04354535788297653
dqn reward tensor(125., device='cuda:0') e 0.05 loss_dqn tensor(1.0834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15653535723686218
dqn reward tensor(221.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1016e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1594540774822235
dqn reward tensor(153., device='cuda:0') e 0.05 loss_dqn tensor(2.3631e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0329463854432106
dqn reward tensor(92.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5267e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0744628831744194
dqn reward tensor(-100.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3949e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17078423500061035
dqn reward tensor(62.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1027e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20104171335697174
dqn reward tensor(194.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1303e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030081219971179962
dqn reward tensor(247.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08725693076848984
dqn reward tensor(229.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.3128e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06279932707548141
dqn reward tensor(187.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0814e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14317111670970917
dqn reward tensor(-37.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1558e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3020612895488739
dqn reward tensor(72.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023129161447286606
dqn reward tensor(119.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1376e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11789409816265106
dqn reward tensor(159.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0353e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033252425491809845
dqn reward tensor(108.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09030033648014069
dqn reward tensor(200.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0397e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24440200626850128
dqn reward tensor(84.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3655e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03260635584592819
dqn reward tensor(212.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1513e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10320744663476944
dqn reward tensor(113.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7907e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1790047287940979
dqn reward tensor(-85.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.1839e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09685619920492172
dqn reward tensor(242.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1377e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1366630643606186
dqn reward tensor(187.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0858e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10117305815219879
dqn reward tensor(154.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0836e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08944346010684967
dqn reward tensor(200.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1439e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12678074836730957
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1352e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07477240264415741
dqn reward tensor(170.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0717e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15983854234218597
dqn reward tensor(257.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5065e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18719488382339478
dqn reward tensor(276.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1996317207813263
dqn reward tensor(-26.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.2886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04339299350976944
dqn reward tensor(227.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0898e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2753199338912964
dqn reward tensor(121.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1166e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1776944398880005
dqn reward tensor(79.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3232e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24588778614997864
dqn reward tensor(26.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2798e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17407816648483276
dqn reward tensor(83.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1602e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0566808357834816
dqn reward tensor(76.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1286e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27496159076690674
dqn reward tensor(-31.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1598e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10163289308547974
dqn reward tensor(25.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0795e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1788029819726944
dqn reward tensor(79.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08810058981180191
dqn reward tensor(100.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0180e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08051573485136032
dqn reward tensor(-190.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5502e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04110964760184288
dqn reward tensor(-35.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6346e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414945125579834
dqn reward tensor(129.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11644620448350906
dqn reward tensor(298.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3040e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1535876989364624
dqn reward tensor(53.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1569e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09131213277578354
dqn reward tensor(118.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.7627e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12661780416965485
dqn reward tensor(253.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13722459971904755
dqn reward tensor(219., device='cuda:0') e 0.05 loss_dqn tensor(2.5588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10938332974910736
dqn reward tensor(226.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1279e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07454575598239899
dqn reward tensor(106.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2907e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13524813950061798
dqn reward tensor(217., device='cuda:0') e 0.05 loss_dqn tensor(1.1636e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03897849842905998
dqn reward tensor(245.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1077e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025413043797016144
dqn reward tensor(-4., device='cuda:0') e 0.05 loss_dqn tensor(2.4111e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.278417706489563
dqn reward tensor(134.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1235e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10410667955875397
dqn reward tensor(-103.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1454e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08653181046247482
dqn reward tensor(54.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05823766440153122
dqn reward tensor(-61.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6074e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1596413254737854
dqn reward tensor(227., device='cuda:0') e 0.05 loss_dqn tensor(1.1062e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2699491083621979
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(4.3430e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030178198590874672
dqn reward tensor(218., device='cuda:0') e 0.05 loss_dqn tensor(2.2758e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21576380729675293
dqn reward tensor(86.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7584e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16927401721477509
dqn reward tensor(102.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03860865533351898
dqn reward tensor(132.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1950e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1481313407421112
dqn reward tensor(252.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1564483940601349
dqn reward tensor(191.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.0586e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0878196656703949
dqn reward tensor(274.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4373e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2199554443359375
dqn reward tensor(344.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14055150747299194
dqn reward tensor(174.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.4030e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1032131165266037
dqn reward tensor(197.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1302e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04292906075716019
dqn reward tensor(290.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11427563428878784
dqn reward tensor(209.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1909e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2481565624475479
dqn reward tensor(186.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.7553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09758751839399338
dqn reward tensor(153.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6352e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10712699592113495
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9957e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13388241827487946
dqn reward tensor(79.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06324006617069244
dqn reward tensor(29.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.1047e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14236025512218475
dqn reward tensor(250.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1120e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1577269732952118
dqn reward tensor(134.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1103e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12205132842063904
dqn reward tensor(244.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1003e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02759048342704773
dqn reward tensor(188.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2143e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03435784950852394
dqn reward tensor(111.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2027e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07542684674263
dqn reward tensor(94.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5972e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2609236240386963
dqn reward tensor(226.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21236518025398254
dqn reward tensor(154.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1732e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2335001528263092
dqn reward tensor(336.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09042946994304657
dqn reward tensor(187.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1621e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26064690947532654
dqn reward tensor(124.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08030709624290466
dqn reward tensor(300.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1386e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07945335656404495
dqn reward tensor(95.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.2730e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07836195081472397
dqn reward tensor(2.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2310e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028386006131768227
dqn reward tensor(115.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3707e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03400722146034241
dqn reward tensor(50.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5849e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07382752746343613
dqn reward tensor(232.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.7770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.47156256437301636
dqn reward tensor(225.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2584e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14695918560028076
dqn reward tensor(90.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3013e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20370765030384064
dqn reward tensor(151.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.9729e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10157078504562378
dqn reward tensor(149.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09759609401226044
dqn reward tensor(133.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03381397947669029
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1668e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14199568331241608
dqn reward tensor(233.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2095e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14341136813163757
dqn reward tensor(309.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042775098234415054
dqn reward tensor(287.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1577e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03922905772924423
dqn reward tensor(320.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7819e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030490683391690254
dqn reward tensor(67.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1412e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09469011425971985
dqn reward tensor(277.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09517852216959
dqn reward tensor(36.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10744378715753555
dqn reward tensor(319.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5108e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.196125328540802
dqn reward tensor(127.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4417e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16714373230934143
dqn reward tensor(100.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0092e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1565740406513214
dqn reward tensor(219.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6968e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27031880617141724
dqn reward tensor(88.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8235e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1997431516647339
dqn reward tensor(134., device='cuda:0') e 0.05 loss_dqn tensor(1.1911e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1396741271018982
dqn reward tensor(169.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2878e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1558772176504135
dqn reward tensor(18.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.5953e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22888880968093872
dqn reward tensor(405., device='cuda:0') e 0.05 loss_dqn tensor(1.2095e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.160458043217659
dqn reward tensor(83., device='cuda:0') e 0.05 loss_dqn tensor(1.2132e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20896263420581818
dqn reward tensor(181.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1781e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06701788306236267
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1061e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06965780258178711
dqn reward tensor(75.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7894e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2790738046169281
dqn reward tensor(170.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6194e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1544474959373474
dqn reward tensor(-81.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6006e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14661093056201935
dqn reward tensor(133.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1405872404575348
dqn reward tensor(150.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2239e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18137246370315552
dqn reward tensor(256.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0181e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2591947615146637
dqn reward tensor(233.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2258e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10413414239883423
dqn reward tensor(384.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048550885170698166
dqn reward tensor(114.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03265699744224548
dqn reward tensor(83.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0733e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08732157945632935
dqn reward tensor(177.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06562039256095886
dqn reward tensor(64.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08211199939250946
dqn reward tensor(205.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2677599787712097
dqn reward tensor(225.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.5017e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01666521467268467
dqn reward tensor(-19.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1758451908826828
dqn reward tensor(281.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0527e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2656412124633789
dqn reward tensor(195.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2347e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30606263875961304
dqn reward tensor(153.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2101e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2297225296497345
dqn reward tensor(182.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2891e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12548618018627167
dqn reward tensor(253., device='cuda:0') e 0.05 loss_dqn tensor(1.1796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11231198161840439
dqn reward tensor(217.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1700e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14047177135944366
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1864e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17496848106384277
dqn reward tensor(26.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6787e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15423166751861572
dqn reward tensor(152.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2540e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09351486712694168
dqn reward tensor(230.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1981e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07255075871944427
dqn reward tensor(345.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5504e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07901210337877274
dqn reward tensor(169.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0120e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06763507425785065
dqn reward tensor(245.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24606114625930786
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2442e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05454040691256523
dqn reward tensor(136.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2596e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14010974764823914
dqn reward tensor(141.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4562e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1165633499622345
dqn reward tensor(272.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2076e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1865527182817459
dqn reward tensor(179.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23883672058582306
dqn reward tensor(95.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1347157508134842
dqn reward tensor(234.6875, device='cuda:0') e 0.05 loss_dqn tensor(7.1315e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12880223989486694
dqn reward tensor(101.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7093e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15700118243694305
dqn reward tensor(214.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5000e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20045489072799683
dqn reward tensor(-131.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.2904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11823645234107971
dqn reward tensor(180.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22731871902942657
dqn reward tensor(309.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2241e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10078024864196777
dqn reward tensor(205.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.5093e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1243872195482254
dqn reward tensor(286.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08777892589569092
dqn reward tensor(88.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2688e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24849581718444824
dqn reward tensor(153., device='cuda:0') e 0.05 loss_dqn tensor(2.6253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1474199891090393
dqn reward tensor(195.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4172e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040998950600624084
dqn reward tensor(294.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22895464301109314
dqn reward tensor(47.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8314e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07621567696332932
dqn reward tensor(78.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1663496494293213
dqn reward tensor(337.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16020318865776062
dqn reward tensor(200.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2713e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09077826142311096
dqn reward tensor(315.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2085e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1327783167362213
dqn reward tensor(193.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3655e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06966868042945862
dqn reward tensor(87., device='cuda:0') e 0.05 loss_dqn tensor(1.2651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21433889865875244
dqn reward tensor(292.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2339e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07879070192575455
dqn reward tensor(175.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06877679377794266
dqn reward tensor(59.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6744e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15497124195098877
dqn reward tensor(-17., device='cuda:0') e 0.05 loss_dqn tensor(1.2375e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07129523158073425
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(4.2529e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12138873338699341
dqn reward tensor(245.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1947e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1632668524980545
dqn reward tensor(246.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043040502816438675
dqn reward tensor(-80.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5988e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2367100715637207
dqn reward tensor(99.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9051e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05878785625100136
dqn reward tensor(264.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2185e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07188577204942703
dqn reward tensor(118.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2282e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2787454426288605
dqn reward tensor(78.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497495472431183
dqn reward tensor(188.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6331e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23273280262947083
dqn reward tensor(203.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2903728783130646
dqn reward tensor(237.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.8446e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08406609296798706
dqn reward tensor(122.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5398e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1642061173915863
dqn reward tensor(166.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6746e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17961782217025757
dqn reward tensor(271.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.8480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13272053003311157
dqn reward tensor(171.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16056346893310547
dqn reward tensor(372.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1908e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14661630988121033
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7323e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23905324935913086
dqn reward tensor(-75.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12619051337242126
dqn reward tensor(320., device='cuda:0') e 0.05 loss_dqn tensor(1.2834e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04106280952692032
dqn reward tensor(219.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5586e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23134370148181915
dqn reward tensor(51.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19074152410030365
dqn reward tensor(131.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3152e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037346407771110535
dqn reward tensor(275.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2726e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11596562713384628
dqn reward tensor(329., device='cuda:0') e 0.05 loss_dqn tensor(1.2857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23679867386817932
dqn reward tensor(189.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3130e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19276443123817444
dqn reward tensor(126.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7492e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12555380165576935
dqn reward tensor(11.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2735e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16786566376686096
dqn reward tensor(108., device='cuda:0') e 0.05 loss_dqn tensor(1.3037e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10390730202198029
dqn reward tensor(228.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3956e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20103126764297485
dqn reward tensor(103.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4515e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1855989545583725
dqn reward tensor(97.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1750e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24747997522354126
dqn reward tensor(101.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08462083339691162
dqn reward tensor(102.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5164e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21107175946235657
dqn reward tensor(81.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11815137416124344
dqn reward tensor(11.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3539e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09233890473842621
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1684337556362152
dqn reward tensor(49.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.5305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15652970969676971
dqn reward tensor(283.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9552e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09886591136455536
dqn reward tensor(25., device='cuda:0') e 0.05 loss_dqn tensor(1.2902e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1144590675830841
dqn reward tensor(246.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1934213936328888
dqn reward tensor(381., device='cuda:0') e 0.05 loss_dqn tensor(1.1959e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05512034520506859
dqn reward tensor(93.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3064e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08371205627918243
dqn reward tensor(132.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2904e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038197558373212814
dqn reward tensor(33.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365610778331757
dqn reward tensor(195.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.0479e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2555043697357178
dqn reward tensor(195.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4727e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10908061265945435
dqn reward tensor(189.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2838e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12408122420310974
dqn reward tensor(125.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2680e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09058471769094467
dqn reward tensor(183.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3099e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13520129024982452
dqn reward tensor(117.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2458e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14183616638183594
dqn reward tensor(207.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08621339499950409
dqn reward tensor(245.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08024585247039795
dqn reward tensor(76.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6566e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07689967751502991
dqn reward tensor(218.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2068e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019596058875322342
dqn reward tensor(-32., device='cuda:0') e 0.05 loss_dqn tensor(3.2923e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11171871423721313
dqn reward tensor(313.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2313e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19130666553974152
dqn reward tensor(168.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029272329062223434
dqn reward tensor(119.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1316e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09576618671417236
dqn reward tensor(314.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1598e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08991411328315735
dqn reward tensor(148.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5996e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057541754096746445
dqn reward tensor(201.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1738e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05453837662935257
dqn reward tensor(139.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1535e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06659595668315887
dqn reward tensor(143.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4271e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015398407354950905
dqn reward tensor(-8.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2208629846572876
dqn reward tensor(411.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1013e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06849723309278488
dqn reward tensor(338.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041132472455501556
dqn reward tensor(284.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7647e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07013411819934845
dqn reward tensor(74.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16902756690979004
dqn reward tensor(241.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7966e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05513356626033783
dqn reward tensor(26., device='cuda:0') e 0.05 loss_dqn tensor(4.4581e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013242410495877266
dqn reward tensor(167.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2224e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11245397478342056
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8551e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11998824030160904
dqn reward tensor(112.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2518e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019239012151956558
dqn reward tensor(208.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3044e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08492744714021683
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(2.5466e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14103667438030243
dqn reward tensor(189.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14302995800971985
dqn reward tensor(207.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017797863110899925
dqn reward tensor(230.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0690e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20995523035526276
dqn reward tensor(112.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4654e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08586517721414566
dqn reward tensor(295.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9659e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10412903130054474
dqn reward tensor(158., device='cuda:0') e 0.05 loss_dqn tensor(5.1408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07887981832027435
dqn reward tensor(3.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9960e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03399629145860672
Evaluating...
Train: {'rocauc': 0.7790466080148024} 3.1650171279907227
=====Epoch 24=====
Training...
dqn reward tensor(93.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3083e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020259328186511993
dqn reward tensor(178.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9955e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14225347340106964
dqn reward tensor(305.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3264e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12176604568958282
dqn reward tensor(127.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2599e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23694469034671783
dqn reward tensor(135.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3079e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08329322934150696
dqn reward tensor(153.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7331e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30720412731170654
dqn reward tensor(422.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3167e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20567049086093903
dqn reward tensor(226.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1020e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16072791814804077
dqn reward tensor(131.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4031e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1317964792251587
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4693e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05397912114858627
dqn reward tensor(142.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3343e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15984193980693817
dqn reward tensor(163.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2025291472673416
dqn reward tensor(193.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06980512291193008
dqn reward tensor(99.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4868e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06943489611148834
dqn reward tensor(83.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13475342094898224
dqn reward tensor(337.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4241e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10941916704177856
dqn reward tensor(379.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055289048701524734
dqn reward tensor(150.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6087e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09881706535816193
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2068e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042869932949543
dqn reward tensor(139.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3771e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061848364770412445
dqn reward tensor(117.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.1858e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21490642428398132
dqn reward tensor(414.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2644e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09489893168210983
dqn reward tensor(323.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1236e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3993499279022217
dqn reward tensor(189.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043475858867168427
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026189317926764488
dqn reward tensor(229.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4128e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07807616889476776
dqn reward tensor(228.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0921e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27561745047569275
dqn reward tensor(266.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3144e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02756974659860134
dqn reward tensor(253.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.0910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13379661738872528
dqn reward tensor(182.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3171e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14788930118083954
dqn reward tensor(279.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2699e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021613841876387596
dqn reward tensor(245.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.8977e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05139715597033501
dqn reward tensor(349.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.9264e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10466010123491287
dqn reward tensor(3.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3504e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10928425192832947
dqn reward tensor(325., device='cuda:0') e 0.05 loss_dqn tensor(1.3215e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08465655893087387
dqn reward tensor(305.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3546e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051486268639564514
dqn reward tensor(173., device='cuda:0') e 0.05 loss_dqn tensor(1.3242e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07210205495357513
dqn reward tensor(254.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2945e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14319127798080444
dqn reward tensor(316.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2756e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18767517805099487
dqn reward tensor(196.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0833e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08213880658149719
dqn reward tensor(-15.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4177e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09532001614570618
dqn reward tensor(244.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1998e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17245915532112122
dqn reward tensor(319.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10837715119123459
dqn reward tensor(195.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.6257e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06382830440998077
dqn reward tensor(389.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4079e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0988648384809494
dqn reward tensor(294.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3451e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048231031745672226
dqn reward tensor(354., device='cuda:0') e 0.05 loss_dqn tensor(1.3796e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052681416273117065
dqn reward tensor(247.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3998e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04085662215948105
dqn reward tensor(216.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4037e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1045641154050827
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(1.4050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08557982742786407
dqn reward tensor(183.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3259e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1062135249376297
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9332e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2271116077899933
dqn reward tensor(220.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4599e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14548134803771973
dqn reward tensor(172.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6984e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11778266727924347
dqn reward tensor(239.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3422e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12984594702720642
dqn reward tensor(87.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8011e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03181091696023941
dqn reward tensor(149.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4336e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15527808666229248
dqn reward tensor(297.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10889395326375961
dqn reward tensor(-62., device='cuda:0') e 0.05 loss_dqn tensor(1.8359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23282724618911743
dqn reward tensor(133.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21723973751068115
dqn reward tensor(306.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3663e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13395091891288757
dqn reward tensor(122.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3620e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15771514177322388
dqn reward tensor(253.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8587e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08925288170576096
dqn reward tensor(168.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3010e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21745562553405762
dqn reward tensor(-86.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09300785511732101
dqn reward tensor(233.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1875e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11287754774093628
dqn reward tensor(330.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4687e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14720460772514343
dqn reward tensor(218.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3738e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18333834409713745
dqn reward tensor(144.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8798e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15878348052501678
dqn reward tensor(155.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3274e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12599894404411316
dqn reward tensor(147.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17851120233535767
dqn reward tensor(207.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0465906597673893
dqn reward tensor(211.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0239e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04634872078895569
dqn reward tensor(217.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1861e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09511449187994003
dqn reward tensor(337.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0226e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1848571002483368
dqn reward tensor(244.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07363316416740417
dqn reward tensor(237.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3095e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057909633964300156
dqn reward tensor(229.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6595e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03691743314266205
dqn reward tensor(305.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03283561021089554
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4059e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11024743318557739
dqn reward tensor(219.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3643e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1523321568965912
dqn reward tensor(187.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3357e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22393086552619934
dqn reward tensor(264.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2615e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13741613924503326
dqn reward tensor(44., device='cuda:0') e 0.05 loss_dqn tensor(1.8220e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04545054957270622
dqn reward tensor(288.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2894e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06621687859296799
dqn reward tensor(295.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10357893258333206
dqn reward tensor(237.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7034e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31420281529426575
dqn reward tensor(323.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4691e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03315102308988571
dqn reward tensor(176.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4652e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20384261012077332
dqn reward tensor(147.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3925e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029710453003644943
dqn reward tensor(276.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19291383028030396
dqn reward tensor(201.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4186e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13029871881008148
dqn reward tensor(192., device='cuda:0') e 0.05 loss_dqn tensor(3.1882e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08878294378519058
dqn reward tensor(86.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11482885479927063
dqn reward tensor(232.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08431660383939743
dqn reward tensor(367.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3870e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018979405984282494
dqn reward tensor(91.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6519e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12773291766643524
dqn reward tensor(285.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3957e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11960266530513763
dqn reward tensor(-109.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5805e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13435474038124084
dqn reward tensor(298.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2762e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1619034707546234
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8615e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1951754093170166
dqn reward tensor(277.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1611153930425644
dqn reward tensor(71.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4833e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23685842752456665
dqn reward tensor(218.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4800e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.286115825176239
dqn reward tensor(304.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10965447127819061
dqn reward tensor(246.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4228e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12329255789518356
dqn reward tensor(249.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0930076465010643
dqn reward tensor(55.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9444e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06271936744451523
dqn reward tensor(360.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1639e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08051849901676178
dqn reward tensor(300.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4846e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18874965608119965
dqn reward tensor(232.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0427e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20049601793289185
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05717332661151886
dqn reward tensor(176.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4545e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11343353986740112
dqn reward tensor(190.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4465e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2893449664115906
dqn reward tensor(152.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5806e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09457181394100189
dqn reward tensor(137., device='cuda:0') e 0.05 loss_dqn tensor(3.7491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15949493646621704
dqn reward tensor(340.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4092e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30747318267822266
dqn reward tensor(105.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6395e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14892631769180298
dqn reward tensor(93.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03774641826748848
dqn reward tensor(338., device='cuda:0') e 0.05 loss_dqn tensor(2.7197e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10493305325508118
dqn reward tensor(278., device='cuda:0') e 0.05 loss_dqn tensor(3.2033e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13990648090839386
dqn reward tensor(368.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4371e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15470896661281586
dqn reward tensor(298.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4466e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0426735058426857
dqn reward tensor(265., device='cuda:0') e 0.05 loss_dqn tensor(1.4205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055089253932237625
dqn reward tensor(178.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8151e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05783229321241379
dqn reward tensor(321.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02475101500749588
dqn reward tensor(516.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12602415680885315
dqn reward tensor(390.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4336e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11804395914077759
dqn reward tensor(179.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23459948599338531
dqn reward tensor(457.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020663250237703323
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5532e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19519895315170288
dqn reward tensor(397., device='cuda:0') e 0.05 loss_dqn tensor(1.3773e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18090322613716125
dqn reward tensor(199.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2226e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21769997477531433
dqn reward tensor(270.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2266e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11173012852668762
dqn reward tensor(396.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4015e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36185532808303833
dqn reward tensor(209.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4795e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06745537370443344
dqn reward tensor(336.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3953e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17736411094665527
dqn reward tensor(299.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4844e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06497713178396225
dqn reward tensor(272.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10110799968242645
dqn reward tensor(318.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06920028477907181
dqn reward tensor(137.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4809e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11906072497367859
dqn reward tensor(334.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14873476326465607
dqn reward tensor(96.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4299e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0788709819316864
dqn reward tensor(270.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4498e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1938188374042511
dqn reward tensor(251.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4375e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13509541749954224
dqn reward tensor(140.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2033601552248001
dqn reward tensor(140.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04073246568441391
dqn reward tensor(218.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6322e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06665869057178497
dqn reward tensor(360.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08487121760845184
dqn reward tensor(371.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.3150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16490057110786438
dqn reward tensor(381.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6243e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16015656292438507
dqn reward tensor(360., device='cuda:0') e 0.05 loss_dqn tensor(3.5779e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12178391218185425
dqn reward tensor(-54.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4389e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08911602199077606
dqn reward tensor(248.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07422386854887009
dqn reward tensor(381.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.4234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20175355672836304
dqn reward tensor(255.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3936e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08891478180885315
dqn reward tensor(248.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2537368834018707
dqn reward tensor(392., device='cuda:0') e 0.05 loss_dqn tensor(1.4892e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13101857900619507
dqn reward tensor(178.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3781e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08904755860567093
dqn reward tensor(331.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14371494948863983
dqn reward tensor(355.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4180e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09052924066781998
dqn reward tensor(378.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1510152667760849
dqn reward tensor(208., device='cuda:0') e 0.05 loss_dqn tensor(3.7080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039407577365636826
dqn reward tensor(489.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0977e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1662469208240509
dqn reward tensor(224.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6006e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22279348969459534
dqn reward tensor(114.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4958e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16494373977184296
dqn reward tensor(229.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21631057560443878
dqn reward tensor(260.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5425e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10970821976661682
dqn reward tensor(336.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1625e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04835106059908867
dqn reward tensor(394.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09956397116184235
dqn reward tensor(359.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21430736780166626
dqn reward tensor(292.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5738e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13714192807674408
dqn reward tensor(184.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5500e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039110131561756134
dqn reward tensor(303.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11019042879343033
dqn reward tensor(350.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6153e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0756801962852478
dqn reward tensor(245.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4756e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11760671436786652
dqn reward tensor(-209., device='cuda:0') e 0.05 loss_dqn tensor(7.4894e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27066314220428467
dqn reward tensor(86., device='cuda:0') e 0.05 loss_dqn tensor(3.6025e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1463354527950287
dqn reward tensor(199., device='cuda:0') e 0.05 loss_dqn tensor(1.5629e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02731700986623764
dqn reward tensor(337.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9211e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16839593648910522
dqn reward tensor(316.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3994e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1823979914188385
dqn reward tensor(379.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4079e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0817975103855133
dqn reward tensor(232.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0793e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022138632833957672
dqn reward tensor(481.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4897e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25431710481643677
dqn reward tensor(335.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6629e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18270477652549744
dqn reward tensor(244.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1468798816204071
dqn reward tensor(326.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1314e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05437197908759117
dqn reward tensor(359.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4071e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0729571133852005
dqn reward tensor(368.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12137828022241592
dqn reward tensor(326.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5642e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31525400280952454
dqn reward tensor(521.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4056e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16353118419647217
dqn reward tensor(200.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943155586719513
dqn reward tensor(226.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10419062525033951
dqn reward tensor(327.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5180e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2751741111278534
dqn reward tensor(306.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4249e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20765599608421326
dqn reward tensor(145.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4028e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09952729195356369
dqn reward tensor(124.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603483110666275
dqn reward tensor(263.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4197e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19504967331886292
dqn reward tensor(183.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3564e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21297280490398407
dqn reward tensor(156.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5245e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12355834245681763
dqn reward tensor(200.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3272e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10451245307922363
dqn reward tensor(232.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.6468e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1213512048125267
dqn reward tensor(415., device='cuda:0') e 0.05 loss_dqn tensor(3.4421e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12592050433158875
dqn reward tensor(240.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3689e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12354129552841187
dqn reward tensor(241.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6392e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21483498811721802
dqn reward tensor(199.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4779e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18598586320877075
dqn reward tensor(405.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4763e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1111454889178276
dqn reward tensor(30.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2153e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07373369485139847
dqn reward tensor(218.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4214e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20286956429481506
dqn reward tensor(238.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05455934628844261
dqn reward tensor(327.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4462e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08173507452011108
dqn reward tensor(423.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20671984553337097
dqn reward tensor(235.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06205953657627106
dqn reward tensor(210.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.1785e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14345607161521912
dqn reward tensor(175., device='cuda:0') e 0.05 loss_dqn tensor(2.6725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10079333186149597
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8104e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20062589645385742
dqn reward tensor(235.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0719e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16659455001354218
dqn reward tensor(278.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2244e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19181916117668152
dqn reward tensor(170.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.4023e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1685059517621994
dqn reward tensor(301.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10379424691200256
dqn reward tensor(457.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4155e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1782406121492386
dqn reward tensor(291.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4564e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0743330866098404
dqn reward tensor(488.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4624e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13644465804100037
dqn reward tensor(336.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13706478476524353
dqn reward tensor(397.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.3672e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1371861845254898
dqn reward tensor(211.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2682e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16924071311950684
dqn reward tensor(234.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4794e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15202513337135315
dqn reward tensor(515.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3519e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13775289058685303
dqn reward tensor(126.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6598e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2327602505683899
dqn reward tensor(383., device='cuda:0') e 0.05 loss_dqn tensor(1.4208e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07329560071229935
dqn reward tensor(231.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4849e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11688430607318878
dqn reward tensor(254.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5715e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12018892914056778
dqn reward tensor(418.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3063e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08499245345592499
dqn reward tensor(205.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11735475063323975
dqn reward tensor(349.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3998e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1667727828025818
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(8.9343e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06251703947782516
dqn reward tensor(394.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5624e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20101326704025269
dqn reward tensor(358.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4907e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14882434904575348
dqn reward tensor(341.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0057e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2086266279220581
dqn reward tensor(369.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6461e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08106173574924469
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3175e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17137061059474945
dqn reward tensor(256.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2610e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22296705842018127
dqn reward tensor(293.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16341155767440796
dqn reward tensor(313.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4489e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37648189067840576
dqn reward tensor(368.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1544421911239624
dqn reward tensor(193.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5999e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16550835967063904
dqn reward tensor(326.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4314e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10007549822330475
dqn reward tensor(212.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6916e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11589759588241577
dqn reward tensor(285.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10183651745319366
dqn reward tensor(176.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11996982991695404
dqn reward tensor(196.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9804e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08784575760364532
dqn reward tensor(186.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6790e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10161003470420837
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(3.5491e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25103315711021423
dqn reward tensor(315., device='cuda:0') e 0.05 loss_dqn tensor(5.3730e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24712695181369781
dqn reward tensor(297., device='cuda:0') e 0.05 loss_dqn tensor(3.9855e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08789423108100891
dqn reward tensor(212.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22479742765426636
dqn reward tensor(428., device='cuda:0') e 0.05 loss_dqn tensor(1.4366e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22699932754039764
dqn reward tensor(472.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4528e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06303428113460541
dqn reward tensor(345.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3851e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07324822247028351
dqn reward tensor(372.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4984e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05921792984008789
dqn reward tensor(107., device='cuda:0') e 0.05 loss_dqn tensor(5.7001e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19361612200737
dqn reward tensor(280.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5914e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04100833833217621
dqn reward tensor(321.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5742e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18522553145885468
dqn reward tensor(127., device='cuda:0') e 0.05 loss_dqn tensor(5.4379e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08340657502412796
dqn reward tensor(352.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036080896854400635
dqn reward tensor(268.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1458e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12205588817596436
dqn reward tensor(202.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6538e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027454059571027756
dqn reward tensor(259., device='cuda:0') e 0.05 loss_dqn tensor(7.3553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19291332364082336
dqn reward tensor(331.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4952e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026647420600056648
dqn reward tensor(321.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06127598509192467
dqn reward tensor(352.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.5154e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1829940378665924
dqn reward tensor(312.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1048e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2954748272895813
dqn reward tensor(202., device='cuda:0') e 0.05 loss_dqn tensor(1.5099e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08600887656211853
dqn reward tensor(266.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7793e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12743782997131348
dqn reward tensor(432.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0524897500872612
dqn reward tensor(267.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.6646e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23483800888061523
dqn reward tensor(166.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12312698364257812
dqn reward tensor(574., device='cuda:0') e 0.05 loss_dqn tensor(1.4629e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026910269632935524
dqn reward tensor(175.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7840e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10201974213123322
dqn reward tensor(218.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0719013661146164
dqn reward tensor(411.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8258e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022411011159420013
dqn reward tensor(209.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1951e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10338068008422852
dqn reward tensor(248.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4736e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06622422486543655
dqn reward tensor(186.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5770e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22455617785453796
dqn reward tensor(242.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5370e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1634538769721985
dqn reward tensor(226.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4484e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29363447427749634
dqn reward tensor(481.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6566e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2059570699930191
dqn reward tensor(198., device='cuda:0') e 0.05 loss_dqn tensor(3.2709e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14529697597026825
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5311e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09224170446395874
dqn reward tensor(372.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5345e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16954368352890015
dqn reward tensor(399.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4906e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2257298231124878
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4979e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05168198049068451
dqn reward tensor(302.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07193854451179504
dqn reward tensor(248.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.5446e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039870232343673706
dqn reward tensor(431.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6043e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.086344875395298
dqn reward tensor(343.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.7958e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11332374811172485
dqn reward tensor(452.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1437966525554657
dqn reward tensor(347., device='cuda:0') e 0.05 loss_dqn tensor(1.4837e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20487435162067413
dqn reward tensor(374.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4598e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0779852569103241
dqn reward tensor(206., device='cuda:0') e 0.05 loss_dqn tensor(1.5748e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07574315369129181
dqn reward tensor(122.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6151e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11724146455526352
dqn reward tensor(284.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4912e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14164741337299347
dqn reward tensor(304.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13051193952560425
dqn reward tensor(382.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8596e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08109208196401596
dqn reward tensor(208.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5747e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18675261735916138
dqn reward tensor(312.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027226267382502556
dqn reward tensor(191.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8935e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23597510159015656
dqn reward tensor(345.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4199e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12828484177589417
dqn reward tensor(132.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6053e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11184108257293701
dqn reward tensor(250.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1115e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18955188989639282
dqn reward tensor(354.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1383935809135437
dqn reward tensor(368.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02094692923128605
dqn reward tensor(287.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5342e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0693235993385315
dqn reward tensor(186.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6198e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18960994482040405
dqn reward tensor(100.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10743879526853561
dqn reward tensor(321., device='cuda:0') e 0.05 loss_dqn tensor(1.5062e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16348469257354736
dqn reward tensor(339.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5584e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11153841018676758
dqn reward tensor(500.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4843e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09120053052902222
dqn reward tensor(246.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1570e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18203476071357727
dqn reward tensor(340., device='cuda:0') e 0.05 loss_dqn tensor(1.5769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07015704363584518
dqn reward tensor(338.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04603154957294464
dqn reward tensor(294.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19811013340950012
dqn reward tensor(345.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5746e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26791203022003174
dqn reward tensor(194., device='cuda:0') e 0.05 loss_dqn tensor(5.3002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08841060101985931
dqn reward tensor(241.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6947e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03230121359229088
dqn reward tensor(179.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5764e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10823515802621841
dqn reward tensor(319., device='cuda:0') e 0.05 loss_dqn tensor(4.0078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028227198868989944
dqn reward tensor(298.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.1626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35246121883392334
dqn reward tensor(290.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1236787810921669
dqn reward tensor(282.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5762e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10034109652042389
dqn reward tensor(264.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16924408078193665
dqn reward tensor(456.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4870e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18137383460998535
dqn reward tensor(436.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7341e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24287308752536774
dqn reward tensor(274.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8214e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.183815136551857
dqn reward tensor(-15.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6357e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16853252053260803
dqn reward tensor(370.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3900e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3036350905895233
dqn reward tensor(238.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6869e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17629122734069824
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6997e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052128370851278305
dqn reward tensor(262.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3642e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26104626059532166
dqn reward tensor(517.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1465e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04609335958957672
dqn reward tensor(265.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0462e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14564990997314453
dqn reward tensor(457.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10087278485298157
dqn reward tensor(82.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8214e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05511808022856712
dqn reward tensor(416.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9996e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055094797164201736
dqn reward tensor(200.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8896e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07919162511825562
dqn reward tensor(368.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6471e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24496938288211823
dqn reward tensor(292.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15084639191627502
dqn reward tensor(178.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7393e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2462342381477356
dqn reward tensor(209.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7122e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2737060487270355
dqn reward tensor(325.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08389601856470108
dqn reward tensor(401.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9123e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062108516693115234
dqn reward tensor(237.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10727618634700775
dqn reward tensor(413.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1478e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11527398228645325
dqn reward tensor(226.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7973e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09801872819662094
dqn reward tensor(344.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5137e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23971909284591675
dqn reward tensor(305.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7464e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10793528705835342
dqn reward tensor(578.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6227e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08520196378231049
dqn reward tensor(266.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7671e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0737459808588028
dqn reward tensor(314.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.6872e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20341958105564117
dqn reward tensor(397.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05974127724766731
dqn reward tensor(160.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2280e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06006395444273949
dqn reward tensor(300., device='cuda:0') e 0.05 loss_dqn tensor(1.7201e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.210807204246521
dqn reward tensor(401.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0667e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11963419616222382
dqn reward tensor(434.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6573e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15318553149700165
dqn reward tensor(517.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6966e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1320415735244751
dqn reward tensor(344., device='cuda:0') e 0.05 loss_dqn tensor(6.6560e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18636758625507355
dqn reward tensor(198.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6757e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11280244588851929
dqn reward tensor(293.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0331e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13788463175296783
dqn reward tensor(314.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12497156113386154
dqn reward tensor(144.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7959e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07793103903532028
dqn reward tensor(283.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8341e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2620595097541809
dqn reward tensor(158.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.3679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08742846548557281
dqn reward tensor(274., device='cuda:0') e 0.05 loss_dqn tensor(1.8205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07728365808725357
dqn reward tensor(386.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.7884e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03824898600578308
dqn reward tensor(371.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.4813e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07364913821220398
dqn reward tensor(207.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9364e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02884777821600437
dqn reward tensor(391.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7619e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2128196358680725
dqn reward tensor(268.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8336e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2238578051328659
dqn reward tensor(64.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0792e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2620139718055725
dqn reward tensor(434.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0167e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2428373098373413
dqn reward tensor(108., device='cuda:0') e 0.05 loss_dqn tensor(2.2454e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06656719744205475
dqn reward tensor(240.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8385e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374647468328476
dqn reward tensor(206.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7617e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23178806900978088
dqn reward tensor(210., device='cuda:0') e 0.05 loss_dqn tensor(4.7246e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16618001461029053
dqn reward tensor(364.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7480e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08461377024650574
dqn reward tensor(13.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2788e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2288883626461029
dqn reward tensor(226.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9898e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1273317039012909
dqn reward tensor(355.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.2326e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09644204378128052
dqn reward tensor(202.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8713e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07981933653354645
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08436538279056549
dqn reward tensor(346.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7888e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24325886368751526
dqn reward tensor(27.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09229663014411926
dqn reward tensor(317.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20696333050727844
dqn reward tensor(110.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9934e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12585319578647614
dqn reward tensor(202.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5066e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12743735313415527
dqn reward tensor(281.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5100e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15573051571846008
dqn reward tensor(379., device='cuda:0') e 0.05 loss_dqn tensor(1.7886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12106551229953766
dqn reward tensor(170., device='cuda:0') e 0.05 loss_dqn tensor(8.6204e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18123938143253326
dqn reward tensor(243.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6717e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09794297814369202
dqn reward tensor(220.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7194e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09204596281051636
dqn reward tensor(393., device='cuda:0') e 0.05 loss_dqn tensor(1.0117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15829966962337494
dqn reward tensor(137.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9421e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07847575098276138
dqn reward tensor(338.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16524314880371094
dqn reward tensor(286.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09927012026309967
dqn reward tensor(174.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8676e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17303426563739777
dqn reward tensor(269.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22711069881916046
dqn reward tensor(364., device='cuda:0') e 0.05 loss_dqn tensor(1.7651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1404319703578949
dqn reward tensor(196.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8378e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04607642814517021
dqn reward tensor(262.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5303e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11363373696804047
dqn reward tensor(240.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3295e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17172639071941376
dqn reward tensor(136.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7616e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0529031977057457
dqn reward tensor(210.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7662e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10148394107818604
dqn reward tensor(63.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8247e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17099252343177795
dqn reward tensor(321.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6533e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20368483662605286
dqn reward tensor(198.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6905e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10726538300514221
dqn reward tensor(353.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4201e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1067391186952591
dqn reward tensor(350., device='cuda:0') e 0.05 loss_dqn tensor(5.0098e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08517170697450638
dqn reward tensor(336., device='cuda:0') e 0.05 loss_dqn tensor(1.6742e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24195531010627747
dqn reward tensor(392.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16910980641841888
dqn reward tensor(228.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.7442e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22094425559043884
dqn reward tensor(294.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.7129e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16473746299743652
dqn reward tensor(39., device='cuda:0') e 0.05 loss_dqn tensor(9.7885e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030133845284581184
dqn reward tensor(18., device='cuda:0') e 0.05 loss_dqn tensor(4.6347e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3299851715564728
dqn reward tensor(135.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4908e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17862655222415924
dqn reward tensor(159.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06791189312934875
dqn reward tensor(121.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6627e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07083920389413834
dqn reward tensor(238.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.2876e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07052265107631683
dqn reward tensor(249.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7637e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044135238975286484
dqn reward tensor(242.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6315e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0673011988401413
dqn reward tensor(298.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16417157649993896
dqn reward tensor(347.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24386641383171082
dqn reward tensor(225.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030348924919962883
dqn reward tensor(273., device='cuda:0') e 0.05 loss_dqn tensor(4.3612e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11893105506896973
dqn reward tensor(267., device='cuda:0') e 0.05 loss_dqn tensor(1.7263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17069301009178162
dqn reward tensor(118.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6392e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24502208828926086
dqn reward tensor(321.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5707e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22219297289848328
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(1.7081e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04797952249646187
dqn reward tensor(150.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.6121e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07605259120464325
dqn reward tensor(262.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.1764e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05321265757083893
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6873e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07056467980146408
dqn reward tensor(108.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.1962e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06760802865028381
dqn reward tensor(-2.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1718e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19799374043941498
dqn reward tensor(303.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7138e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16484205424785614
dqn reward tensor(245.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6595e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30258581042289734
dqn reward tensor(202., device='cuda:0') e 0.05 loss_dqn tensor(7.0715e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30246904492378235
dqn reward tensor(146., device='cuda:0') e 0.05 loss_dqn tensor(8.0869e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15214163064956665
dqn reward tensor(32.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9867e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2530212998390198
dqn reward tensor(213.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2714e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09533507376909256
dqn reward tensor(254.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4868e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15670272707939148
dqn reward tensor(385.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6276e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1656285524368286
dqn reward tensor(97., device='cuda:0') e 0.05 loss_dqn tensor(4.7040e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05739729106426239
dqn reward tensor(187.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4666e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043599262833595276
dqn reward tensor(238., device='cuda:0') e 0.05 loss_dqn tensor(1.7202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0975300520658493
dqn reward tensor(217.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7277e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04037356376647949
dqn reward tensor(318.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2808e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1140933707356453
dqn reward tensor(452., device='cuda:0') e 0.05 loss_dqn tensor(1.7044e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29661405086517334
dqn reward tensor(245.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09208302199840546
dqn reward tensor(118.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8640e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03584493696689606
dqn reward tensor(-21.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2994e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12580731511116028
dqn reward tensor(-16.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6633e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09006983041763306
dqn reward tensor(263.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6967e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3634944558143616
dqn reward tensor(172.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5683e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06412354111671448
dqn reward tensor(217.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1472093164920807
dqn reward tensor(103.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8431e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07424233853816986
dqn reward tensor(297.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059607475996017456
dqn reward tensor(369.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6172e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2605976462364197
dqn reward tensor(516.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17017024755477905
dqn reward tensor(265.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6399e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12856003642082214
dqn reward tensor(339.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6416e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05599367246031761
dqn reward tensor(298.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5201e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10189120471477509
dqn reward tensor(405.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5875e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031216515228152275
dqn reward tensor(322.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.7074e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1463455855846405
dqn reward tensor(247., device='cuda:0') e 0.05 loss_dqn tensor(5.6576e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10270863026380539
dqn reward tensor(29.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.3607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14149677753448486
dqn reward tensor(246.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058734260499477386
dqn reward tensor(160., device='cuda:0') e 0.05 loss_dqn tensor(7.1616e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13216644525527954
dqn reward tensor(320.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2720e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1281830370426178
dqn reward tensor(283.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6046e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33873680233955383
dqn reward tensor(321.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9916e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0668727234005928
dqn reward tensor(333.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5886e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0841444656252861
dqn reward tensor(247.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7979e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06799221783876419
dqn reward tensor(464.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3840e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07280568033456802
dqn reward tensor(237.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7382e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29312774538993835
dqn reward tensor(87.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0703e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04494427889585495
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3888e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16844278573989868
dqn reward tensor(157.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3891e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13814985752105713
dqn reward tensor(308.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6901e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11010193079710007
dqn reward tensor(70.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.0806e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07242461293935776
dqn reward tensor(164.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4852e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0922042578458786
dqn reward tensor(214.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6186e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07069634646177292
dqn reward tensor(263.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0634e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10755769908428192
dqn reward tensor(319.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11729909479618073
dqn reward tensor(228.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4555e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22480444610118866
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9827e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0858122855424881
dqn reward tensor(147.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0409e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03015858680009842
dqn reward tensor(375.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5478e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13357847929000854
dqn reward tensor(162.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11153145879507065
dqn reward tensor(35.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1643e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414077877998352
dqn reward tensor(307.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6390e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20822536945343018
dqn reward tensor(349.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.6180e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15106911957263947
dqn reward tensor(231.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5488e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05648857727646828
dqn reward tensor(285.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0805e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05616511404514313
dqn reward tensor(327.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9322e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16778188943862915
dqn reward tensor(276.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.5426e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051685601472854614
dqn reward tensor(110.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2383e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19822394847869873
dqn reward tensor(282., device='cuda:0') e 0.05 loss_dqn tensor(3.6170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17915232479572296
dqn reward tensor(360.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7991e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3086540699005127
dqn reward tensor(117.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6356e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0670640766620636
dqn reward tensor(188.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4722e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0902831107378006
dqn reward tensor(308.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10796042531728745
dqn reward tensor(270.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7580e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14484328031539917
dqn reward tensor(472.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2075e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10957720875740051
dqn reward tensor(255.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03341314196586609
dqn reward tensor(164., device='cuda:0') e 0.05 loss_dqn tensor(1.8195e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840503752231598
dqn reward tensor(14.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8440e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02452833019196987
Evaluating...
Train: {'rocauc': 0.7885307177981007} 3.5879859924316406
=====Epoch 25=====
Training...
dqn reward tensor(228.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6083e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0890250951051712
dqn reward tensor(-235.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9058e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15773451328277588
dqn reward tensor(208.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7570e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12564988434314728
dqn reward tensor(225.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6751e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07996831089258194
dqn reward tensor(280.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.5173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13482001423835754
dqn reward tensor(235.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8287e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06637376546859741
dqn reward tensor(217.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12526729702949524
dqn reward tensor(134.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5825e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12641584873199463
dqn reward tensor(503.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4767e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14525046944618225
dqn reward tensor(129.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.0925e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12456946074962616
dqn reward tensor(126.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5926e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09307676553726196
dqn reward tensor(151.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2378e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05662956088781357
dqn reward tensor(258.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5524e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05498214438557625
dqn reward tensor(167.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3408e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035109929740428925
dqn reward tensor(138.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0263e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02173515595495701
dqn reward tensor(469.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4055e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22919198870658875
dqn reward tensor(211.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09865725040435791
dqn reward tensor(161.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3516e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0930749922990799
dqn reward tensor(292.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2116e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017329581081867218
dqn reward tensor(195.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9497e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08042275905609131
dqn reward tensor(199.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2802e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22175002098083496
dqn reward tensor(-13.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6983e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0754472017288208
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7285e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04739503189921379
dqn reward tensor(274.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3308e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14927692711353302
dqn reward tensor(298.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2247e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04513915255665779
dqn reward tensor(211.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1879e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08568669855594635
dqn reward tensor(285.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2286e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18229079246520996
dqn reward tensor(293.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9198e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020060192793607712
dqn reward tensor(84.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7751e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904488205909729
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31754690408706665
dqn reward tensor(141.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7507e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3081463873386383
dqn reward tensor(128.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2772e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14862903952598572
dqn reward tensor(264.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7501e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14825458824634552
dqn reward tensor(166.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2133e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05376821383833885
dqn reward tensor(-10.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8217e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16826513409614563
dqn reward tensor(215.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2463e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12001895159482956
dqn reward tensor(119., device='cuda:0') e 0.05 loss_dqn tensor(3.4457e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13893342018127441
dqn reward tensor(167.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16436398029327393
dqn reward tensor(25.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12931719422340393
dqn reward tensor(147.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1731e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15257181227207184
dqn reward tensor(224., device='cuda:0') e 0.05 loss_dqn tensor(1.2633e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2621849775314331
dqn reward tensor(-29.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.7112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09083561599254608
dqn reward tensor(141.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2451e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11043180525302887
dqn reward tensor(189.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1459844559431076
dqn reward tensor(242.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.8800e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036182742565870285
dqn reward tensor(170.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7969e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036196134984493256
dqn reward tensor(247.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023835347965359688
dqn reward tensor(216.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7833e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051944833248853683
dqn reward tensor(90.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11283309757709503
dqn reward tensor(211.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9003e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2735934257507324
dqn reward tensor(38.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6870e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.010709356516599655
dqn reward tensor(122.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17193210124969482
dqn reward tensor(256.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7640e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0609622485935688
dqn reward tensor(211.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.7756e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18144893646240234
dqn reward tensor(161.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2750e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22608980536460876
dqn reward tensor(34., device='cuda:0') e 0.05 loss_dqn tensor(4.6574e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11652915179729462
dqn reward tensor(143.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11775808781385422
dqn reward tensor(233.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2722e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04634455218911171
dqn reward tensor(19.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7008e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0696687325835228
dqn reward tensor(337.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3176398277282715
dqn reward tensor(276.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2699e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1408976912498474
dqn reward tensor(249.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3198e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11766248941421509
dqn reward tensor(442.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2671e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12707042694091797
dqn reward tensor(302.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4396e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12178490310907364
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16826456785202026
dqn reward tensor(153.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1349e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20129896700382233
dqn reward tensor(260.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5345e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11490525305271149
dqn reward tensor(295.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2654e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12046242505311966
dqn reward tensor(268., device='cuda:0') e 0.05 loss_dqn tensor(1.7431e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1863066703081131
dqn reward tensor(310.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.1381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28469839692115784
dqn reward tensor(121.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9138e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23221130669116974
dqn reward tensor(449.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8115e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10427908599376678
dqn reward tensor(337.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.7910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13293558359146118
dqn reward tensor(283.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8417e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04546020179986954
dqn reward tensor(330.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18316605687141418
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8592e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03372425585985184
dqn reward tensor(244., device='cuda:0') e 0.05 loss_dqn tensor(5.7651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05727964639663696
dqn reward tensor(235.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1369e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1264905333518982
dqn reward tensor(295., device='cuda:0') e 0.05 loss_dqn tensor(8.3969e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021073104813694954
dqn reward tensor(264.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5714e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13120701909065247
dqn reward tensor(471.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2366e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2180417776107788
dqn reward tensor(278.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017869116738438606
dqn reward tensor(416.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4184e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3890727758407593
dqn reward tensor(280.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7127e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17402195930480957
dqn reward tensor(723.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1937e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1045474037528038
dqn reward tensor(331.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.0344e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09145714342594147
dqn reward tensor(486.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.2257e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08737653493881226
dqn reward tensor(225.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1230e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09491512179374695
dqn reward tensor(503.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2172e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09305861592292786
dqn reward tensor(497.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1417e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20016902685165405
dqn reward tensor(339.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9983e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057899318635463715
dqn reward tensor(378.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2708e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2875737249851227
dqn reward tensor(487.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3779e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1501452624797821
dqn reward tensor(337.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3690e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061548877507448196
dqn reward tensor(556.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2544e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15349476039409637
dqn reward tensor(290.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.5712e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050794363021850586
dqn reward tensor(370.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.2144e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08821471780538559
dqn reward tensor(474.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2412e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036373987793922424
dqn reward tensor(261.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6209e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.306663453578949
dqn reward tensor(231.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1304e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07943783700466156
dqn reward tensor(442.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3638e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14244601130485535
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1876e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049307115375995636
dqn reward tensor(449.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3727e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27101537585258484
dqn reward tensor(437.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3683e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12833106517791748
dqn reward tensor(652.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21030354499816895
dqn reward tensor(380.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1772e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15877534449100494
dqn reward tensor(313.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24139755964279175
dqn reward tensor(301.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3348e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13776874542236328
dqn reward tensor(282.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4677e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09400486201047897
dqn reward tensor(678.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2573e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047477349638938904
dqn reward tensor(298.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7107e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07811469584703445
dqn reward tensor(634.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1823e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11432629078626633
dqn reward tensor(256.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4415e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12429477274417877
dqn reward tensor(412.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2591e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09297038614749908
dqn reward tensor(157.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026496311649680138
dqn reward tensor(552.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0079e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0764593705534935
dqn reward tensor(487., device='cuda:0') e 0.05 loss_dqn tensor(6.3665e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025892727077007294
dqn reward tensor(371.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0591e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11662405729293823
dqn reward tensor(358.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.140981987118721
dqn reward tensor(442., device='cuda:0') e 0.05 loss_dqn tensor(2.3330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14203082025051117
dqn reward tensor(626., device='cuda:0') e 0.05 loss_dqn tensor(2.3250e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04933539032936096
dqn reward tensor(444., device='cuda:0') e 0.05 loss_dqn tensor(2.0281e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28954145312309265
dqn reward tensor(331.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14365822076797485
dqn reward tensor(526.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2503787577152252
dqn reward tensor(432.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1597e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019539330154657364
dqn reward tensor(336.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2504e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22515445947647095
dqn reward tensor(495.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2194e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18772371113300323
dqn reward tensor(214.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2320e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21337619423866272
dqn reward tensor(247.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3326096832752228
dqn reward tensor(447.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1951e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14823110401630402
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05106791853904724
dqn reward tensor(414.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1896e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10304287821054459
dqn reward tensor(315.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3476e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19637788832187653
dqn reward tensor(432., device='cuda:0') e 0.05 loss_dqn tensor(2.0341e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17530971765518188
dqn reward tensor(430.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1938e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15422801673412323
dqn reward tensor(342.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10768918693065643
dqn reward tensor(363.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5081e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14714327454566956
dqn reward tensor(507., device='cuda:0') e 0.05 loss_dqn tensor(2.0272e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.082952581346035
dqn reward tensor(477.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407857984304428
dqn reward tensor(532.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.0473e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10759426653385162
dqn reward tensor(371.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8616e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14382711052894592
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17545296251773834
dqn reward tensor(528., device='cuda:0') e 0.05 loss_dqn tensor(1.6012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22231267392635345
dqn reward tensor(368.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4654e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31900691986083984
dqn reward tensor(306.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4987e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12633644044399261
dqn reward tensor(669., device='cuda:0') e 0.05 loss_dqn tensor(2.1429e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08432181179523468
dqn reward tensor(369.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5392e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03024449199438095
dqn reward tensor(539.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6705e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03111834079027176
dqn reward tensor(390.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.0767e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2854824364185333
dqn reward tensor(404.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1041e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027666956186294556
dqn reward tensor(409.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.3009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2172926962375641
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0627e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13955794274806976
dqn reward tensor(207.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1260540783405304
dqn reward tensor(414.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09455589950084686
dqn reward tensor(188., device='cuda:0') e 0.05 loss_dqn tensor(4.9670e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.202847421169281
dqn reward tensor(318.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7729e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12671902775764465
dqn reward tensor(434.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5353e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03801974281668663
dqn reward tensor(225.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10010913014411926
dqn reward tensor(579.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6934e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17225193977355957
dqn reward tensor(416.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10052724927663803
dqn reward tensor(206.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.0806e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10962367802858353
dqn reward tensor(536.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3348e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09700705856084824
dqn reward tensor(508.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0638611689209938
dqn reward tensor(414.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03403961658477783
dqn reward tensor(432.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2157e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07290074229240417
dqn reward tensor(226.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1712e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11493106186389923
dqn reward tensor(276.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5210e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12483289837837219
dqn reward tensor(372., device='cuda:0') e 0.05 loss_dqn tensor(1.2448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02253633737564087
dqn reward tensor(359.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19700586795806885
dqn reward tensor(364.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03769350424408913
dqn reward tensor(400.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3763e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08363071084022522
dqn reward tensor(143.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0923781543970108
dqn reward tensor(343.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26244235038757324
dqn reward tensor(280.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4379e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23617088794708252
dqn reward tensor(366.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0819e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09125890582799911
dqn reward tensor(311.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019556542858481407
dqn reward tensor(406.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10487087070941925
dqn reward tensor(275.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09534703195095062
dqn reward tensor(377.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3144e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27947136759757996
dqn reward tensor(281.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3524e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21973104774951935
dqn reward tensor(392.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5818e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09181089699268341
dqn reward tensor(478.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4147e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1259819120168686
dqn reward tensor(296., device='cuda:0') e 0.05 loss_dqn tensor(4.6150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10528014600276947
dqn reward tensor(287.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8196e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20414766669273376
dqn reward tensor(-53.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10572084784507751
dqn reward tensor(546.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4114e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07096913456916809
dqn reward tensor(226., device='cuda:0') e 0.05 loss_dqn tensor(3.0995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07300034165382385
dqn reward tensor(431.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5390e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497184932231903
dqn reward tensor(405.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13704606890678406
dqn reward tensor(447., device='cuda:0') e 0.05 loss_dqn tensor(2.3331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1016799658536911
dqn reward tensor(343.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7971e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12410192936658859
dqn reward tensor(485.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6455e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08272796869277954
dqn reward tensor(322.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1866e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06846542656421661
dqn reward tensor(416.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4332e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05978749319911003
dqn reward tensor(367.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0354e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09369613975286484
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7849e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2153095006942749
dqn reward tensor(495.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.6838e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016719646751880646
dqn reward tensor(445.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7106e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11837540566921234
dqn reward tensor(388.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7520e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2746001183986664
dqn reward tensor(509.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9719e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13505399227142334
dqn reward tensor(590., device='cuda:0') e 0.05 loss_dqn tensor(5.1927e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14575128257274628
dqn reward tensor(403.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7649e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07981416583061218
dqn reward tensor(564.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4556e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16642530262470245
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08025794476270676
dqn reward tensor(333.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9873e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06368744373321533
dqn reward tensor(223.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5423e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33611470460891724
dqn reward tensor(320.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7338e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14357540011405945
dqn reward tensor(449.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16751188039779663
dqn reward tensor(414.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9062e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26091909408569336
dqn reward tensor(482.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.6440e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054302603006362915
dqn reward tensor(480.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.4803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2047521471977234
dqn reward tensor(487.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.8576e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06742699444293976
dqn reward tensor(452.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07795853912830353
dqn reward tensor(338.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8657e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09879709035158157
dqn reward tensor(521.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6688e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13107801973819733
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046673014760017395
dqn reward tensor(585.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0375535823404789
dqn reward tensor(419.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5780e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16658510267734528
dqn reward tensor(471.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6142e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07680681347846985
dqn reward tensor(488.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14690658450126648
dqn reward tensor(366., device='cuda:0') e 0.05 loss_dqn tensor(6.7360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13401135802268982
dqn reward tensor(502.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6562e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0164039246737957
dqn reward tensor(457.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13763079047203064
dqn reward tensor(417.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.0588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18807564675807953
dqn reward tensor(415.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31182926893234253
dqn reward tensor(605.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1577e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11413396894931793
dqn reward tensor(457.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7244e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1860828399658203
dqn reward tensor(381.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.7232e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14550748467445374
dqn reward tensor(237.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3005e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04897905886173248
dqn reward tensor(596.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20148205757141113
dqn reward tensor(641.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0419e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14562442898750305
dqn reward tensor(389.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7031e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26975110173225403
dqn reward tensor(409., device='cuda:0') e 0.05 loss_dqn tensor(4.6980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07457186281681061
dqn reward tensor(354.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057950153946876526
dqn reward tensor(469.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03731892257928848
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6156e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17300379276275635
dqn reward tensor(589.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5091e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24706774950027466
dqn reward tensor(358.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1632911115884781
dqn reward tensor(607.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6032e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07541633397340775
dqn reward tensor(508.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7831e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033995550125837326
dqn reward tensor(456.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4958e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31891193985939026
dqn reward tensor(597., device='cuda:0') e 0.05 loss_dqn tensor(4.3951e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09222866594791412
dqn reward tensor(383.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5784e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25238466262817383
dqn reward tensor(451.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1416e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14751973748207092
dqn reward tensor(391.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.8662e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14530256390571594
dqn reward tensor(316.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4735e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22602522373199463
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0478e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11677578091621399
dqn reward tensor(615.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1131e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09612919390201569
dqn reward tensor(458.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19784705340862274
dqn reward tensor(415.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05557965859770775
dqn reward tensor(442.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434508115053177
dqn reward tensor(410.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6718e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16391652822494507
dqn reward tensor(253.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1319e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1322711706161499
dqn reward tensor(402.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8816e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4525698125362396
dqn reward tensor(408.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12330928444862366
dqn reward tensor(567., device='cuda:0') e 0.05 loss_dqn tensor(4.3268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11503464728593826
dqn reward tensor(421.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5113e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02982266992330551
dqn reward tensor(443.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.120772585272789
dqn reward tensor(404.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14260509610176086
dqn reward tensor(393.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2324e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07277418673038483
dqn reward tensor(466.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2649e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1644519865512848
dqn reward tensor(525.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2589e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18453150987625122
dqn reward tensor(400.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.6031e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3436748683452606
dqn reward tensor(419.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6784e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12091296911239624
dqn reward tensor(574.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17062699794769287
dqn reward tensor(401.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4460e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07200993597507477
dqn reward tensor(577.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.9679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03892342373728752
dqn reward tensor(615.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0345e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1102999895811081
dqn reward tensor(477., device='cuda:0') e 0.05 loss_dqn tensor(1.3840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10078918933868408
dqn reward tensor(282.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15243034064769745
dqn reward tensor(595.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0034e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11919713020324707
dqn reward tensor(314.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.6813e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18524885177612305
dqn reward tensor(518.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0670e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33318591117858887
dqn reward tensor(375.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.9886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1504981964826584
dqn reward tensor(284.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18618501722812653
dqn reward tensor(166., device='cuda:0') e 0.05 loss_dqn tensor(4.3550e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14089550077915192
dqn reward tensor(272.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1086445152759552
dqn reward tensor(479.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5609e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0930500328540802
dqn reward tensor(358.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9583e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1985989362001419
dqn reward tensor(596.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.3088e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1211986094713211
dqn reward tensor(390.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5320e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058547813445329666
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7885e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20573942363262177
dqn reward tensor(442.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.1531e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07928027957677841
dqn reward tensor(400.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2616e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19428978860378265
dqn reward tensor(536.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9218e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0341983437538147
dqn reward tensor(515.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2381e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17513418197631836
dqn reward tensor(627.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9454e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09217870980501175
dqn reward tensor(236.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5221e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16668756306171417
dqn reward tensor(468.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1885547786951065
dqn reward tensor(504.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15272089838981628
dqn reward tensor(457.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11781608313322067
dqn reward tensor(543.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031106583774089813
dqn reward tensor(388.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0765e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09717831015586853
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4289e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12267186492681503
dqn reward tensor(311.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.5329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2188296765089035
dqn reward tensor(578.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9085e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12318478524684906
dqn reward tensor(440.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1762748509645462
dqn reward tensor(474.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12941482663154602
dqn reward tensor(456.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19837896525859833
dqn reward tensor(442.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2437e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10051128268241882
dqn reward tensor(342.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16598723828792572
dqn reward tensor(415.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20844393968582153
dqn reward tensor(348.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.220057874917984
dqn reward tensor(331.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8845e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13590751588344574
dqn reward tensor(327.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25978535413742065
dqn reward tensor(361.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.1050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14678049087524414
dqn reward tensor(537.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0571599006652832
dqn reward tensor(387.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1415e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14309674501419067
dqn reward tensor(497.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9735e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0993683785200119
dqn reward tensor(261., device='cuda:0') e 0.05 loss_dqn tensor(3.9746e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07242141664028168
dqn reward tensor(409.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7124e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21001920104026794
dqn reward tensor(366.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.1239e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21277526021003723
dqn reward tensor(367.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1424708366394043
dqn reward tensor(564.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08532188087701797
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1174081340432167
dqn reward tensor(462.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8926e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1419501006603241
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1367279440164566
dqn reward tensor(485.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1038176640868187
dqn reward tensor(242.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2592221796512604
dqn reward tensor(491.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7648e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09121397137641907
dqn reward tensor(465., device='cuda:0') e 0.05 loss_dqn tensor(3.9677e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1582825630903244
dqn reward tensor(366.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9077e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07241959124803543
dqn reward tensor(564., device='cuda:0') e 0.05 loss_dqn tensor(1.7428e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18506044149398804
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6583e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04490794241428375
dqn reward tensor(391.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0689e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21935787796974182
dqn reward tensor(337.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4659e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06140414997935295
dqn reward tensor(727.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6776e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05490041524171829
dqn reward tensor(349.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04888484254479408
dqn reward tensor(389.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.5803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12893851101398468
dqn reward tensor(531., device='cuda:0') e 0.05 loss_dqn tensor(3.9316e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16514067351818085
dqn reward tensor(524.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7282e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31341874599456787
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5536e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03690751641988754
dqn reward tensor(373.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7785e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22442010045051575
dqn reward tensor(245.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08895431458950043
dqn reward tensor(580.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25186267495155334
dqn reward tensor(526., device='cuda:0') e 0.05 loss_dqn tensor(7.6667e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10927106440067291
dqn reward tensor(464.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0167e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11664269864559174
dqn reward tensor(201.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.1086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07465913146734238
dqn reward tensor(308.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04992760717868805
dqn reward tensor(352.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9911e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07039216160774231
dqn reward tensor(573., device='cuda:0') e 0.05 loss_dqn tensor(3.3874e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11682723462581635
dqn reward tensor(249.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12867501378059387
dqn reward tensor(418.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.7087e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2586303949356079
dqn reward tensor(560.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1939e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2116454690694809
dqn reward tensor(478.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06945444643497467
dqn reward tensor(271., device='cuda:0') e 0.05 loss_dqn tensor(4.3351e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10909627377986908
dqn reward tensor(279.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1572573035955429
dqn reward tensor(430.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7464e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2770359516143799
dqn reward tensor(407.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07986443489789963
dqn reward tensor(393.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.8187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09559644758701324
dqn reward tensor(356.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1249e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0725647360086441
dqn reward tensor(485.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1481e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07263866811990738
dqn reward tensor(397.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1391e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1427147537469864
dqn reward tensor(584.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1597e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030349940061569214
dqn reward tensor(449., device='cuda:0') e 0.05 loss_dqn tensor(6.7884e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14069399237632751
dqn reward tensor(433.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0642e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15844251215457916
dqn reward tensor(370.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18817414343357086
dqn reward tensor(502.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.4087e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18949547410011292
dqn reward tensor(318.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4207e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23238573968410492
dqn reward tensor(224.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5848e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08553557097911835
dqn reward tensor(451., device='cuda:0') e 0.05 loss_dqn tensor(9.3676e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09222456812858582
dqn reward tensor(381.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10878707468509674
dqn reward tensor(457.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5281e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1296822428703308
dqn reward tensor(549., device='cuda:0') e 0.05 loss_dqn tensor(2.9730e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13630205392837524
dqn reward tensor(445.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29552799463272095
dqn reward tensor(590.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2648e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11049146205186844
dqn reward tensor(439.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.2390e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07611267268657684
dqn reward tensor(702., device='cuda:0') e 0.05 loss_dqn tensor(2.9075e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17952312529087067
dqn reward tensor(444.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9008e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11710293591022491
dqn reward tensor(319.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1567e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14055390655994415
dqn reward tensor(446.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7125e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09358716011047363
dqn reward tensor(498.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11518624424934387
dqn reward tensor(375.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4602e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13991492986679077
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2445e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09558289498090744
dqn reward tensor(279.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2159840166568756
dqn reward tensor(484.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13597187399864197
dqn reward tensor(609.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7808e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09341167658567429
dqn reward tensor(437.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.6111e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15844911336898804
dqn reward tensor(390.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0621e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08959341049194336
dqn reward tensor(487.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8666e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0762181207537651
dqn reward tensor(442.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8845e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11544838547706604
dqn reward tensor(594.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5671e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09778182208538055
dqn reward tensor(352.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3436e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13260090351104736
dqn reward tensor(485.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2504655122756958
dqn reward tensor(346.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5374e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09649360179901123
dqn reward tensor(530.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1977139711380005
dqn reward tensor(350., device='cuda:0') e 0.05 loss_dqn tensor(5.5917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03658267483115196
dqn reward tensor(333., device='cuda:0') e 0.05 loss_dqn tensor(3.4343e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054134152829647064
dqn reward tensor(484.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09913325309753418
dqn reward tensor(554.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1184e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12378843128681183
dqn reward tensor(380.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10132025927305222
dqn reward tensor(421., device='cuda:0') e 0.05 loss_dqn tensor(1.0217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30248236656188965
dqn reward tensor(351.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09470729529857635
dqn reward tensor(333.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.2019e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17832142114639282
dqn reward tensor(441.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8566e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09373550862073898
dqn reward tensor(398.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1014094352722168
dqn reward tensor(453.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0363e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19421716034412384
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4452e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18082250654697418
dqn reward tensor(409.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2672e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17205177247524261
dqn reward tensor(435.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1396e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12481689453125
dqn reward tensor(530.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12370863556861877
dqn reward tensor(218.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04682052507996559
dqn reward tensor(355.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8197e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08234935253858566
dqn reward tensor(308.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0459786131978035
dqn reward tensor(450.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0527e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03905151039361954
dqn reward tensor(627.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08784963190555573
dqn reward tensor(418.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1884e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13784271478652954
dqn reward tensor(323.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2471e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09157183766365051
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3409e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08608376234769821
dqn reward tensor(503.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041908662766218185
dqn reward tensor(519.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3681e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15670576691627502
dqn reward tensor(507.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14151562750339508
dqn reward tensor(324.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3552e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12376933544874191
dqn reward tensor(403.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18102321028709412
dqn reward tensor(470.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08012065291404724
dqn reward tensor(350., device='cuda:0') e 0.05 loss_dqn tensor(2.4603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2148389369249344
dqn reward tensor(423.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19252963364124298
dqn reward tensor(289., device='cuda:0') e 0.05 loss_dqn tensor(5.5089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06795237213373184
dqn reward tensor(319.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5369e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10437233000993729
dqn reward tensor(382.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4926e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09461483359336853
dqn reward tensor(282.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.2850e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13493971526622772
dqn reward tensor(335.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3985e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21076813340187073
dqn reward tensor(506.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5366e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16340363025665283
dqn reward tensor(395.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3223e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06609690189361572
dqn reward tensor(325.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1129e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09038247913122177
dqn reward tensor(254.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4059e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1196763664484024
dqn reward tensor(399.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1444e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22558027505874634
dqn reward tensor(478.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3594e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12842565774917603
dqn reward tensor(512.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047143589705228806
dqn reward tensor(117.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19097036123275757
dqn reward tensor(403.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1724e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23434580862522125
dqn reward tensor(407.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5011e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10640491545200348
dqn reward tensor(407.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6687e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08063666522502899
dqn reward tensor(620.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.1236e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021196212619543076
dqn reward tensor(453.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.2738e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13281230628490448
dqn reward tensor(276.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.1952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027430031448602676
dqn reward tensor(400.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2585e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06113692373037338
dqn reward tensor(468., device='cuda:0') e 0.05 loss_dqn tensor(3.4950e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062000822275877
dqn reward tensor(540., device='cuda:0') e 0.05 loss_dqn tensor(3.2612e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20887601375579834
dqn reward tensor(503.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5252e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07955609261989594
dqn reward tensor(145.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12035020440816879
dqn reward tensor(392., device='cuda:0') e 0.05 loss_dqn tensor(1.4033e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07772637158632278
dqn reward tensor(299.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3434e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09614452719688416
dqn reward tensor(208.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30346348881721497
dqn reward tensor(348.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1901e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10313822329044342
dqn reward tensor(302.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7105e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08880516141653061
dqn reward tensor(510.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2916e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09014321118593216
dqn reward tensor(412., device='cuda:0') e 0.05 loss_dqn tensor(3.2524e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0771794244647026
dqn reward tensor(546., device='cuda:0') e 0.05 loss_dqn tensor(1.0437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045047152787446976
dqn reward tensor(497.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029658343642950058
dqn reward tensor(309.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1992e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02505694329738617
dqn reward tensor(323., device='cuda:0') e 0.05 loss_dqn tensor(3.5903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22392189502716064
dqn reward tensor(295.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026112504303455353
dqn reward tensor(358.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2354e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022719401866197586
dqn reward tensor(440.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9992e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017919352278113365
dqn reward tensor(483.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2791e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015121376141905785
dqn reward tensor(461.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6036e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26613202691078186
dqn reward tensor(438.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34979572892189026
dqn reward tensor(318.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.8928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16563254594802856
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0607e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05451361462473869
dqn reward tensor(375., device='cuda:0') e 0.05 loss_dqn tensor(1.3931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0760289654135704
dqn reward tensor(504.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09270714968442917
dqn reward tensor(533.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0187e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15539449453353882
dqn reward tensor(292.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.7611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027186209335923195
dqn reward tensor(435.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14234530925750732
dqn reward tensor(584.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31346970796585083
dqn reward tensor(381., device='cuda:0') e 0.05 loss_dqn tensor(1.0184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060070984065532684
dqn reward tensor(325.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2746126651763916
dqn reward tensor(195.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.6909e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365320205688477
dqn reward tensor(572., device='cuda:0') e 0.05 loss_dqn tensor(3.1924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1664791703224182
dqn reward tensor(274.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4285e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10419200360774994
dqn reward tensor(439.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9352e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17749296128749847
dqn reward tensor(346., device='cuda:0') e 0.05 loss_dqn tensor(1.5481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09580545872449875
dqn reward tensor(289.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8446e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15836529433727264
dqn reward tensor(530.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3812e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18080167472362518
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18871518969535828
dqn reward tensor(475.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10125108063220978
dqn reward tensor(453., device='cuda:0') e 0.05 loss_dqn tensor(3.2604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07786432653665543
dqn reward tensor(533., device='cuda:0') e 0.05 loss_dqn tensor(3.1456e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08004207164049149
dqn reward tensor(562., device='cuda:0') e 0.05 loss_dqn tensor(1.3900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1323164701461792
dqn reward tensor(455.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17190159857273102
dqn reward tensor(388.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0267e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12914437055587769
dqn reward tensor(290.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3150e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1412411332130432
dqn reward tensor(355.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3472e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14423510432243347
dqn reward tensor(364., device='cuda:0') e 0.05 loss_dqn tensor(3.3092e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09646806120872498
dqn reward tensor(205.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5259e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11547131836414337
dqn reward tensor(416.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09972114861011505
dqn reward tensor(67.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7188e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025130528956651688
dqn reward tensor(348., device='cuda:0') e 0.05 loss_dqn tensor(1.7637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023517034947872162
dqn reward tensor(437.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0132e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09453573822975159
dqn reward tensor(343.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5786e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1145104393362999
dqn reward tensor(278.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1759e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3315369784832001
dqn reward tensor(180.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24774357676506042
dqn reward tensor(308., device='cuda:0') e 0.05 loss_dqn tensor(3.4310e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08267983049154282
dqn reward tensor(483., device='cuda:0') e 0.05 loss_dqn tensor(8.4728e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0824887678027153
dqn reward tensor(302.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3891e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26179367303848267
dqn reward tensor(409.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7289e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17059265077114105
dqn reward tensor(250.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22642166912555695
dqn reward tensor(411.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08372390270233154
dqn reward tensor(63.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4267e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07464577257633209
dqn reward tensor(301.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16492779552936554
dqn reward tensor(497.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.4201e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2948804199695587
dqn reward tensor(358.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3923e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11523062735795975
dqn reward tensor(446.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4869e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18647512793540955
dqn reward tensor(337., device='cuda:0') e 0.05 loss_dqn tensor(3.5871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13334277272224426
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4032e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14391668140888214
dqn reward tensor(197.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5416e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13790100812911987
dqn reward tensor(308.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.7598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1896996796131134
dqn reward tensor(458.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4773e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14937826991081238
dqn reward tensor(83.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22043636441230774
dqn reward tensor(490.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6167e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09317886829376221
dqn reward tensor(132.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4034e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14840522408485413
dqn reward tensor(397.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1808e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0815621167421341
dqn reward tensor(261.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1211e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11717275530099869
dqn reward tensor(76.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1406e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 1.0613707304000854
Evaluating...
Train: {'rocauc': 0.7901903870309488} 5.420279502868652
=====Epoch 26=====
Training...
dqn reward tensor(280., device='cuda:0') e 0.05 loss_dqn tensor(1.5698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07727357000112534
dqn reward tensor(296.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5091e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12437384575605392
dqn reward tensor(544.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5021e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11731284856796265
dqn reward tensor(266.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8200e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23347048461437225
dqn reward tensor(454.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4847e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20046702027320862
dqn reward tensor(422.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5401e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11332441866397858
dqn reward tensor(173.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7909e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11031340807676315
dqn reward tensor(216.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4410e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13695648312568665
dqn reward tensor(447.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05597228929400444
dqn reward tensor(318.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.4893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12574374675750732
dqn reward tensor(354.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4226e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08696962893009186
dqn reward tensor(480.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5355e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04172927886247635
dqn reward tensor(261., device='cuda:0') e 0.05 loss_dqn tensor(1.2159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11344047635793686
dqn reward tensor(605.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0419e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2041732519865036
dqn reward tensor(305.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9466e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10608351975679398
dqn reward tensor(482.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0828e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07996664941310883
dqn reward tensor(416.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2589e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15278440713882446
dqn reward tensor(519.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6109e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10570451617240906
dqn reward tensor(598.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2070e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07374995946884155
dqn reward tensor(496., device='cuda:0') e 0.05 loss_dqn tensor(2.3893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07232240587472916
dqn reward tensor(338.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9723e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058629121631383896
dqn reward tensor(236.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7134e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03734337538480759
dqn reward tensor(466.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5872e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16705869138240814
dqn reward tensor(325.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8453e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15230146050453186
dqn reward tensor(468.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5843e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16955886781215668
dqn reward tensor(376.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02834347076714039
dqn reward tensor(436.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2527339458465576
dqn reward tensor(428.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6442e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07827598601579666
dqn reward tensor(259.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0864277184009552
dqn reward tensor(167.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0833e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11130392551422119
dqn reward tensor(440., device='cuda:0') e 0.05 loss_dqn tensor(3.7623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07210428267717361
dqn reward tensor(267.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1403e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15977764129638672
dqn reward tensor(224.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5309e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0830291211605072
dqn reward tensor(415., device='cuda:0') e 0.05 loss_dqn tensor(2.2960e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031761664897203445
dqn reward tensor(285., device='cuda:0') e 0.05 loss_dqn tensor(3.7766e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09733453392982483
dqn reward tensor(190., device='cuda:0') e 0.05 loss_dqn tensor(3.8011e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028056306764483452
dqn reward tensor(442.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09408942610025406
dqn reward tensor(93.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1267e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027300702407956123
dqn reward tensor(472.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6679e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1506318300962448
dqn reward tensor(307.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21107089519500732
dqn reward tensor(236.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066010802984238
dqn reward tensor(346.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0832e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09656287729740143
dqn reward tensor(223.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3241e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11250551044940948
dqn reward tensor(271.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06463620066642761
dqn reward tensor(340.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33657610416412354
dqn reward tensor(144., device='cuda:0') e 0.05 loss_dqn tensor(2.2493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20366357266902924
dqn reward tensor(440.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07666604220867157
dqn reward tensor(436.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5862e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03103054314851761
dqn reward tensor(330.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9424e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12237368524074554
dqn reward tensor(420.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4105e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060890600085258484
dqn reward tensor(200., device='cuda:0') e 0.05 loss_dqn tensor(1.4763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06921685487031937
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08562126010656357
dqn reward tensor(465.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16754359006881714
dqn reward tensor(364.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5287e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0931963324546814
dqn reward tensor(339.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08836892992258072
dqn reward tensor(278.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11921139061450958
dqn reward tensor(527.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2925e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13244055211544037
dqn reward tensor(358.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2836e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07150096446275711
dqn reward tensor(305.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3351e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13484692573547363
dqn reward tensor(352.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1904514580965042
dqn reward tensor(327.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5244e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1578739583492279
dqn reward tensor(474., device='cuda:0') e 0.05 loss_dqn tensor(8.3129e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13680049777030945
dqn reward tensor(269.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.3476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07675619423389435
dqn reward tensor(393.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8585e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20307013392448425
dqn reward tensor(429.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09886808693408966
dqn reward tensor(549.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3043e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13337790966033936
dqn reward tensor(330.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7953e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1256650984287262
dqn reward tensor(442., device='cuda:0') e 0.05 loss_dqn tensor(1.3432e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10152878612279892
dqn reward tensor(434.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7976e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20731188356876373
dqn reward tensor(177.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8333e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08669332414865494
dqn reward tensor(489., device='cuda:0') e 0.05 loss_dqn tensor(3.4605e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09401719272136688
dqn reward tensor(257., device='cuda:0') e 0.05 loss_dqn tensor(1.4013e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24302324652671814
dqn reward tensor(258.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0337403304874897
dqn reward tensor(353.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0929318517446518
dqn reward tensor(310.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5780e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021971942856907845
dqn reward tensor(404.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4599e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21287116408348083
dqn reward tensor(234., device='cuda:0') e 0.05 loss_dqn tensor(1.3558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3131744861602783
dqn reward tensor(437.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0482e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08626311272382736
dqn reward tensor(145.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11429433524608612
dqn reward tensor(464.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2984e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10990235209465027
dqn reward tensor(412.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1656203269958496
dqn reward tensor(334.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.2855e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14672379195690155
dqn reward tensor(397.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07337994873523712
dqn reward tensor(478.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2158e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14562851190567017
dqn reward tensor(314.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18586957454681396
dqn reward tensor(249.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3787e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09478913992643356
dqn reward tensor(480.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1593e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13103672862052917
dqn reward tensor(284.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27936306595802307
dqn reward tensor(346.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3641e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1523597240447998
dqn reward tensor(245.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21833157539367676
dqn reward tensor(378., device='cuda:0') e 0.05 loss_dqn tensor(1.1141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30314067006111145
dqn reward tensor(330., device='cuda:0') e 0.05 loss_dqn tensor(3.1667e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13096879422664642
dqn reward tensor(335.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3635e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13606569170951843
dqn reward tensor(553.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3793e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16155458986759186
dqn reward tensor(469.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2036e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10590218007564545
dqn reward tensor(362.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3192e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06174154579639435
dqn reward tensor(206.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4252e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.111855648458004
dqn reward tensor(425.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8714e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06851723790168762
dqn reward tensor(473.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1217e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13582533597946167
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2780e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10233943909406662
dqn reward tensor(436.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1288e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03390364348888397
dqn reward tensor(488.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2406e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14335379004478455
dqn reward tensor(259.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0719740241765976
dqn reward tensor(474.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17943522334098816
dqn reward tensor(377.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1832154393196106
dqn reward tensor(345., device='cuda:0') e 0.05 loss_dqn tensor(7.8609e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18372713029384613
dqn reward tensor(227.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5453e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16105353832244873
dqn reward tensor(502.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19963783025741577
dqn reward tensor(393., device='cuda:0') e 0.05 loss_dqn tensor(1.1193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17652180790901184
dqn reward tensor(507.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038572099059820175
dqn reward tensor(242.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7399e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06198245659470558
dqn reward tensor(335.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.5090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1266767382621765
dqn reward tensor(222.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5107e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11956660449504852
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1786e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08234791457653046
dqn reward tensor(439.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2179e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1971859186887741
dqn reward tensor(430., device='cuda:0') e 0.05 loss_dqn tensor(1.3467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031187526881694794
dqn reward tensor(301.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5045e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15795373916625977
dqn reward tensor(432.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6950e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12027603387832642
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4375e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08765389025211334
dqn reward tensor(343.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05159696191549301
dqn reward tensor(163.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2492e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12961003184318542
dqn reward tensor(571., device='cuda:0') e 0.05 loss_dqn tensor(1.5172e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16960984468460083
dqn reward tensor(374.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3401e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10137537866830826
dqn reward tensor(188.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2541e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049835145473480225
dqn reward tensor(360.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5069e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19606074690818787
dqn reward tensor(454., device='cuda:0') e 0.05 loss_dqn tensor(9.1650e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2879224717617035
dqn reward tensor(399., device='cuda:0') e 0.05 loss_dqn tensor(3.0272e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08553813397884369
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2737e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20826880633831024
dqn reward tensor(366.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7960e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07669766247272491
dqn reward tensor(511.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0044e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15024475753307343
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(3.0853e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07601957023143768
dqn reward tensor(633.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.108930304646492
dqn reward tensor(362., device='cuda:0') e 0.05 loss_dqn tensor(3.3575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1394374966621399
dqn reward tensor(380.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2321e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15717118978500366
dqn reward tensor(352.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1813947558403015
dqn reward tensor(356.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3970e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034470491111278534
dqn reward tensor(348., device='cuda:0') e 0.05 loss_dqn tensor(3.4961e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1187693402171135
dqn reward tensor(287.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0951017215847969
dqn reward tensor(312., device='cuda:0') e 0.05 loss_dqn tensor(1.5440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05850540101528168
dqn reward tensor(236.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23639798164367676
dqn reward tensor(616.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.5367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13903027772903442
dqn reward tensor(281., device='cuda:0') e 0.05 loss_dqn tensor(3.8660e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19519475102424622
dqn reward tensor(325.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2050682157278061
dqn reward tensor(433.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08616656810045242
dqn reward tensor(437.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2340644896030426
dqn reward tensor(420.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3530e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3848520815372467
dqn reward tensor(288.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6359e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20600073039531708
dqn reward tensor(249.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.9300e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036769501864910126
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3676e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14062361419200897
dqn reward tensor(498.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2692e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10234193503856659
dqn reward tensor(465.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9746e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16340240836143494
dqn reward tensor(228.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1066e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13218271732330322
dqn reward tensor(409.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13320927321910858
dqn reward tensor(313.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041849736124277115
dqn reward tensor(356.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7234e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1312086433172226
dqn reward tensor(315.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08643300086259842
dqn reward tensor(487.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5743e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042516689747571945
dqn reward tensor(243.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7467e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15263551473617554
dqn reward tensor(508.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8968e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09577666223049164
dqn reward tensor(390.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24841004610061646
dqn reward tensor(319.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7044e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11403396725654602
dqn reward tensor(249.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15177437663078308
dqn reward tensor(408.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1405729055404663
dqn reward tensor(552.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0953935906291008
dqn reward tensor(494.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08983124047517776
dqn reward tensor(314.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1532028764486313
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9933e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11957517266273499
dqn reward tensor(358.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3714e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05346193164587021
dqn reward tensor(100.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07881857454776764
dqn reward tensor(325.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2685e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10704591870307922
dqn reward tensor(389.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.3130e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05363893881440163
dqn reward tensor(520.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8164e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13416409492492676
dqn reward tensor(392.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2051e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27641263604164124
dqn reward tensor(393., device='cuda:0') e 0.05 loss_dqn tensor(4.8069e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15744704008102417
dqn reward tensor(414.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8803e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14188574254512787
dqn reward tensor(307.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10483147948980331
dqn reward tensor(424.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8926e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19219493865966797
dqn reward tensor(479.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1572345495223999
dqn reward tensor(263.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11204242706298828
dqn reward tensor(436.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.6595e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08829787373542786
dqn reward tensor(488.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.8002e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14903947710990906
dqn reward tensor(408.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3433e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14688770473003387
dqn reward tensor(499.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1921e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046521931886672974
dqn reward tensor(497.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3690e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21605123579502106
dqn reward tensor(386.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04110831022262573
dqn reward tensor(449.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13845476508140564
dqn reward tensor(381.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1512e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044886987656354904
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3906e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13554060459136963
dqn reward tensor(617.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.7216e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1496739238500595
dqn reward tensor(468., device='cuda:0') e 0.05 loss_dqn tensor(4.4003e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07588218152523041
dqn reward tensor(446.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1523e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11256563663482666
dqn reward tensor(320.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5742e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09222929924726486
dqn reward tensor(270.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7346e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16984429955482483
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9526e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06892839819192886
dqn reward tensor(368.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18292878568172455
dqn reward tensor(503.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.8485e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10403978824615479
dqn reward tensor(478.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27832987904548645
dqn reward tensor(172.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9428e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08312144875526428
dqn reward tensor(543.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8364e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16093710064888
dqn reward tensor(294.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1656e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1773265153169632
dqn reward tensor(609.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9914e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16605685651302338
dqn reward tensor(609.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3208e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05091847479343414
dqn reward tensor(538.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7575e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030187491327524185
dqn reward tensor(345.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2584e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12174674868583679
dqn reward tensor(437.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1355181336402893
dqn reward tensor(248.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0381e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2307131141424179
dqn reward tensor(256.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024361666291952133
dqn reward tensor(533.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9450e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09753662347793579
dqn reward tensor(283.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1310039460659027
dqn reward tensor(559.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4455e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14650890231132507
dqn reward tensor(518.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025127582252025604
dqn reward tensor(463., device='cuda:0') e 0.05 loss_dqn tensor(5.1373e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17390143871307373
dqn reward tensor(449.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8206e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015612127259373665
dqn reward tensor(319.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3490e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0362878181040287
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0786321833729744
dqn reward tensor(349.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2898e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10058119893074036
dqn reward tensor(439.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08812932670116425
dqn reward tensor(499., device='cuda:0') e 0.05 loss_dqn tensor(1.6660e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046880386769771576
dqn reward tensor(474.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4307e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26302361488342285
dqn reward tensor(483.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9231e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0796334370970726
dqn reward tensor(446., device='cuda:0') e 0.05 loss_dqn tensor(1.6721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016160674393177032
dqn reward tensor(315.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8329e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12817558646202087
dqn reward tensor(348.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8525e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15880240499973297
dqn reward tensor(354.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1149883046746254
dqn reward tensor(266.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0115e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2890703082084656
dqn reward tensor(445.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7876e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10514379292726517
dqn reward tensor(365.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32268550992012024
dqn reward tensor(382.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9732e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18251490592956543
dqn reward tensor(237.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.4302e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13247019052505493
dqn reward tensor(420.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.8291e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042117755860090256
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7122e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12608221173286438
dqn reward tensor(268.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21471774578094482
dqn reward tensor(475.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3512e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07349187880754471
dqn reward tensor(403., device='cuda:0') e 0.05 loss_dqn tensor(3.1953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15174750983715057
dqn reward tensor(391., device='cuda:0') e 0.05 loss_dqn tensor(1.1193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18423941731452942
dqn reward tensor(197.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0890e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12288819253444672
dqn reward tensor(355.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8753e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07295405119657516
dqn reward tensor(296.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0171e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20425359904766083
dqn reward tensor(255.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1995e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07035191357135773
dqn reward tensor(438., device='cuda:0') e 0.05 loss_dqn tensor(5.0078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09992393851280212
dqn reward tensor(489.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07046019285917282
dqn reward tensor(419.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.9399e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11610022932291031
dqn reward tensor(563.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.8304e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12557066977024078
dqn reward tensor(455.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1866e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2764577567577362
dqn reward tensor(462.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3429e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13421818614006042
dqn reward tensor(506.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7126e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3291594088077545
dqn reward tensor(572.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2645e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09154213964939117
dqn reward tensor(514.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3734e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08895698934793472
dqn reward tensor(346.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09727921336889267
dqn reward tensor(268.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2285e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0821286216378212
dqn reward tensor(363.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5672e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21278591454029083
dqn reward tensor(354.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14691635966300964
dqn reward tensor(440.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3081e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07459346204996109
dqn reward tensor(451.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1692e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19819211959838867
dqn reward tensor(376., device='cuda:0') e 0.05 loss_dqn tensor(2.4616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0388292521238327
dqn reward tensor(438.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9556e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08893539756536484
dqn reward tensor(500.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3073e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17489159107208252
dqn reward tensor(342.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.6570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12491600215435028
dqn reward tensor(306.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9402e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17634594440460205
dqn reward tensor(431., device='cuda:0') e 0.05 loss_dqn tensor(4.9611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09091217070817947
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14882990717887878
dqn reward tensor(191.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7404e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08645109832286835
dqn reward tensor(301.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.1582e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2158311903476715
dqn reward tensor(395.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11764352023601532
dqn reward tensor(96.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0459e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.189890056848526
dqn reward tensor(500.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9278e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13631488382816315
dqn reward tensor(328.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21574096381664276
dqn reward tensor(222.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9047e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053612031042575836
dqn reward tensor(259.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18456298112869263
dqn reward tensor(343.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08396191895008087
dqn reward tensor(535.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.0911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10481563210487366
dqn reward tensor(390.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1929204910993576
dqn reward tensor(404.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8805e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07917680591344833
dqn reward tensor(610.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9245e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14989261329174042
dqn reward tensor(464.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1715e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14042598009109497
dqn reward tensor(442.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3308e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04936283081769943
dqn reward tensor(121.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041123341768980026
dqn reward tensor(413.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05616507679224014
dqn reward tensor(488.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6022e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21606652438640594
dqn reward tensor(622.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7257e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1608903706073761
dqn reward tensor(582.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0671190619468689
dqn reward tensor(475.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9934e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06591600924730301
dqn reward tensor(617.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.7354e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12423409521579742
dqn reward tensor(382.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9714e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21661946177482605
dqn reward tensor(497.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7222e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07954033464193344
dqn reward tensor(476.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2352e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13148804008960724
dqn reward tensor(505.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2773244082927704
dqn reward tensor(440.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9682e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06886778771877289
dqn reward tensor(393.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9656e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13296544551849365
dqn reward tensor(361.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21175378561019897
dqn reward tensor(396.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1837e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08592835068702698
dqn reward tensor(501.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4950e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18796464800834656
dqn reward tensor(519.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17311465740203857
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8808e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14991241693496704
dqn reward tensor(301.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1036e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066632181406021
dqn reward tensor(286.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8081e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14088661968708038
dqn reward tensor(508., device='cuda:0') e 0.05 loss_dqn tensor(4.9119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1513657569885254
dqn reward tensor(559.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2199e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16818609833717346
dqn reward tensor(427.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6297e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13368386030197144
dqn reward tensor(285.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.7761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03479433432221413
dqn reward tensor(283.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11222521960735321
dqn reward tensor(397.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1201e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21725517511367798
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0189e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06082127243280411
dqn reward tensor(234.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1639086753129959
dqn reward tensor(336.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8243e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11926572024822235
dqn reward tensor(415.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8099e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05687158554792404
dqn reward tensor(534.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8167e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2015283703804016
dqn reward tensor(559.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.085824154317379
dqn reward tensor(330.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.6060e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08762643486261368
dqn reward tensor(133.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12529772520065308
dqn reward tensor(319.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5907e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1647474765777588
dqn reward tensor(485.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30505675077438354
dqn reward tensor(444.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12540297210216522
dqn reward tensor(454.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6254e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07414368540048599
dqn reward tensor(429.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0290e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18938106298446655
dqn reward tensor(517.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1202e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1240750104188919
dqn reward tensor(357.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4510e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18809756636619568
dqn reward tensor(276.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4883e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14606714248657227
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0881e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1796143352985382
dqn reward tensor(91.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17917022109031677
dqn reward tensor(339.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5149e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12052790075540543
dqn reward tensor(426., device='cuda:0') e 0.05 loss_dqn tensor(3.1664e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10168100893497467
dqn reward tensor(484.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1741e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14632807672023773
dqn reward tensor(378.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9159e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04855189472436905
dqn reward tensor(310.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2027e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10471371561288834
dqn reward tensor(346.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3173e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038987897336483
dqn reward tensor(343.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6604e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08575805276632309
dqn reward tensor(379.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09431372582912445
dqn reward tensor(376.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09194019436836243
dqn reward tensor(450.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2514e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12109094858169556
dqn reward tensor(342.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22048097848892212
dqn reward tensor(410., device='cuda:0') e 0.05 loss_dqn tensor(1.3371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15913626551628113
dqn reward tensor(415.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15632498264312744
dqn reward tensor(312.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18197792768478394
dqn reward tensor(271.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8946e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21572385728359222
dqn reward tensor(152.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7507e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08332142978906631
dqn reward tensor(440.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9784e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21477611362934113
dqn reward tensor(323.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2178e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24352143704891205
dqn reward tensor(249.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0573e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03174225986003876
dqn reward tensor(500.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9742e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02297080308198929
dqn reward tensor(317.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1778501272201538
dqn reward tensor(85., device='cuda:0') e 0.05 loss_dqn tensor(3.7421e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10508987307548523
dqn reward tensor(409., device='cuda:0') e 0.05 loss_dqn tensor(2.9715e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12715303897857666
dqn reward tensor(310.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1200675517320633
dqn reward tensor(410.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9605e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24086302518844604
dqn reward tensor(519.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6788e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05595210939645767
dqn reward tensor(411.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2534e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057893961668014526
dqn reward tensor(595.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3995e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19060420989990234
dqn reward tensor(335.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11018997430801392
dqn reward tensor(282.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12938889861106873
dqn reward tensor(195.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3955e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.128513902425766
dqn reward tensor(522.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8977e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026204895228147507
dqn reward tensor(395.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1220e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028439637273550034
dqn reward tensor(507.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1774568259716034
dqn reward tensor(518., device='cuda:0') e 0.05 loss_dqn tensor(3.0303e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08163323998451233
dqn reward tensor(246.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22081129252910614
dqn reward tensor(256., device='cuda:0') e 0.05 loss_dqn tensor(3.0192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2690331041812897
dqn reward tensor(321.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.1458e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329086571931839
dqn reward tensor(291.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1122068464756012
dqn reward tensor(165.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4963e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08888783305883408
dqn reward tensor(340.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9430e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3676401674747467
dqn reward tensor(358.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02330031245946884
dqn reward tensor(335., device='cuda:0') e 0.05 loss_dqn tensor(9.5338e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15741442143917084
dqn reward tensor(359.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23015323281288147
dqn reward tensor(210.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1796e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13538530468940735
dqn reward tensor(339., device='cuda:0') e 0.05 loss_dqn tensor(3.1351e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0791105255484581
dqn reward tensor(321.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20945730805397034
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9268e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08768380433320999
dqn reward tensor(150.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14376412332057953
dqn reward tensor(405., device='cuda:0') e 0.05 loss_dqn tensor(8.5857e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25403061509132385
dqn reward tensor(258.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5662e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08695562928915024
dqn reward tensor(389.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1238197460770607
dqn reward tensor(541.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3138e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0618027001619339
dqn reward tensor(552., device='cuda:0') e 0.05 loss_dqn tensor(2.9184e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11037810891866684
dqn reward tensor(434., device='cuda:0') e 0.05 loss_dqn tensor(5.2530e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13090068101882935
dqn reward tensor(258.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.0203e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10512614250183105
dqn reward tensor(448.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12895506620407104
dqn reward tensor(282.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9796e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15844213962554932
dqn reward tensor(254.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7603e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06823907792568207
dqn reward tensor(134.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0290e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06599052250385284
dqn reward tensor(430.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09195850044488907
dqn reward tensor(305.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1543e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09652965515851974
dqn reward tensor(464.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8934e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08735239505767822
dqn reward tensor(304.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6309e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07422930002212524
dqn reward tensor(350., device='cuda:0') e 0.05 loss_dqn tensor(7.9415e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.45853039622306824
dqn reward tensor(299.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2585e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20737265050411224
dqn reward tensor(278.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5285e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023161549121141434
dqn reward tensor(421.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.0871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09876281023025513
dqn reward tensor(187.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09151354432106018
dqn reward tensor(499., device='cuda:0') e 0.05 loss_dqn tensor(2.8497e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11961467564105988
dqn reward tensor(376.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3522e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05854456126689911
dqn reward tensor(202.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3932e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3339122533798218
dqn reward tensor(610.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6260e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10838037729263306
dqn reward tensor(536.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6759e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10903900116682053
dqn reward tensor(523.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7317e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1116262823343277
dqn reward tensor(54.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07219181954860687
dqn reward tensor(134.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3736e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053107112646102905
dqn reward tensor(521.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9089e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040989235043525696
dqn reward tensor(321.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1398e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19734859466552734
dqn reward tensor(133.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1116e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20177392661571503
dqn reward tensor(612.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9486e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08940966427326202
dqn reward tensor(441.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.3997e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29719847440719604
dqn reward tensor(309., device='cuda:0') e 0.05 loss_dqn tensor(1.2976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07738012820482254
dqn reward tensor(329.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3564e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13906040787696838
dqn reward tensor(409.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3154e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15697011351585388
dqn reward tensor(311.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08495974540710449
dqn reward tensor(483.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0956e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17375627160072327
dqn reward tensor(411.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08051395416259766
dqn reward tensor(348.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18558885157108307
dqn reward tensor(341.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3380778431892395
dqn reward tensor(327.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.4462e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03595411032438278
dqn reward tensor(309.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7567e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04788871854543686
dqn reward tensor(281.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2950e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07378856837749481
dqn reward tensor(398.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9678e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04259610176086426
dqn reward tensor(370.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16788996756076813
dqn reward tensor(429.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4402e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06243922933936119
dqn reward tensor(568.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035267241299152374
dqn reward tensor(319., device='cuda:0') e 0.05 loss_dqn tensor(8.0674e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1424550712108612
dqn reward tensor(498.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0155e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01914874091744423
dqn reward tensor(366.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9082e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13487432897090912
dqn reward tensor(551.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9789e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029768314212560654
dqn reward tensor(310.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3305e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15528172254562378
dqn reward tensor(331.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3695e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08709487318992615
dqn reward tensor(325., device='cuda:0') e 0.05 loss_dqn tensor(3.4024e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14368315041065216
dqn reward tensor(374.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16146475076675415
dqn reward tensor(192.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7713e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09393426775932312
dqn reward tensor(500.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20607376098632812
dqn reward tensor(580.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07353044301271439
dqn reward tensor(381.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6382e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05541121959686279
dqn reward tensor(408.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7611e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2947794795036316
dqn reward tensor(481.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3723e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27531683444976807
dqn reward tensor(416.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1796e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27628231048583984
dqn reward tensor(443.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0463e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840690016746521
dqn reward tensor(432.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2725e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14198344945907593
dqn reward tensor(177., device='cuda:0') e 0.05 loss_dqn tensor(3.4815e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08293229341506958
dqn reward tensor(349.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4406e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16368170082569122
dqn reward tensor(467.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1192e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13949164748191833
dqn reward tensor(516.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20055662095546722
dqn reward tensor(689.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4889e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33399301767349243
dqn reward tensor(474.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1163e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15382170677185059
dqn reward tensor(561.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.9759e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15581806004047394
dqn reward tensor(454., device='cuda:0') e 0.05 loss_dqn tensor(1.0446e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05113377422094345
dqn reward tensor(389.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5927e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20482181012630463
dqn reward tensor(457.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05350063741207123
dqn reward tensor(306.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.4893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21207405626773834
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7433e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21161872148513794
dqn reward tensor(661.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4294e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09489800781011581
dqn reward tensor(464.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3972e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12749247252941132
dqn reward tensor(218., device='cuda:0') e 0.05 loss_dqn tensor(1.8093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15951117873191833
dqn reward tensor(352.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1076170951128006
dqn reward tensor(487.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3767e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21731522679328918
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10501328110694885
dqn reward tensor(276.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5755e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04652421921491623
dqn reward tensor(539.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5565e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15469211339950562
dqn reward tensor(147.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8877e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17496590316295624
dqn reward tensor(369.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6477e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21328935027122498
dqn reward tensor(554.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4600e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.133096843957901
dqn reward tensor(464.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05345911532640457
dqn reward tensor(521.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2818e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19608230888843536
dqn reward tensor(209.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2622e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17823128402233124
dqn reward tensor(659.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7793e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10488010197877884
dqn reward tensor(464.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.4380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08022662997245789
dqn reward tensor(369.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1898e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11168719828128815
dqn reward tensor(565., device='cuda:0') e 0.05 loss_dqn tensor(5.4944e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17800821363925934
dqn reward tensor(596.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.4374e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038413792848587036
dqn reward tensor(351.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7573e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17582030594348907
dqn reward tensor(347.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20051798224449158
dqn reward tensor(679.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21797822415828705
dqn reward tensor(440.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24272985756397247
dqn reward tensor(514.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2557199001312256
dqn reward tensor(645., device='cuda:0') e 0.05 loss_dqn tensor(1.6075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2550927996635437
dqn reward tensor(511.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3837e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03878626972436905
dqn reward tensor(517.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1687e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19612501561641693
dqn reward tensor(566.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8642e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13645942509174347
dqn reward tensor(336.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07747019082307816
dqn reward tensor(400.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20127108693122864
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048190973699092865
dqn reward tensor(645.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3341405391693115
dqn reward tensor(453.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11995217204093933
dqn reward tensor(555., device='cuda:0') e 0.05 loss_dqn tensor(1.8271e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15932762622833252
dqn reward tensor(620.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030832048505544662
dqn reward tensor(643.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09188085794448853
dqn reward tensor(735., device='cuda:0') e 0.05 loss_dqn tensor(1.8455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09908421337604523
dqn reward tensor(477.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14893102645874023
dqn reward tensor(509.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023720845580101013
dqn reward tensor(505., device='cuda:0') e 0.05 loss_dqn tensor(1.9637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13875997066497803
dqn reward tensor(491.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1738e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17863446474075317
dqn reward tensor(507.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2160869538784027
dqn reward tensor(473.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08542943000793457
dqn reward tensor(453., device='cuda:0') e 0.05 loss_dqn tensor(2.7617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06344889104366302
dqn reward tensor(512., device='cuda:0') e 0.05 loss_dqn tensor(2.1425e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16710726916790009
dqn reward tensor(434.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09320370852947235
dqn reward tensor(434.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0913e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26835009455680847
dqn reward tensor(606.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20530113577842712
dqn reward tensor(457.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03689549118280411
dqn reward tensor(354.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6843e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0753583014011383
dqn reward tensor(447.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6254e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13567349314689636
dqn reward tensor(441.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23425762355327606
dqn reward tensor(543.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11528556793928146
dqn reward tensor(501.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049706872552633286
dqn reward tensor(477.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08851128071546555
dqn reward tensor(470.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.3707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1729501336812973
dqn reward tensor(456.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052558500319719315
dqn reward tensor(310.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6353e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16776567697525024
dqn reward tensor(489.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17792755365371704
dqn reward tensor(446., device='cuda:0') e 0.05 loss_dqn tensor(2.3922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14453716576099396
dqn reward tensor(431.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018768159672617912
dqn reward tensor(519., device='cuda:0') e 0.05 loss_dqn tensor(2.2572e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06961123645305634
dqn reward tensor(384.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2533324360847473
dqn reward tensor(431.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21508434414863586
dqn reward tensor(500., device='cuda:0') e 0.05 loss_dqn tensor(2.4847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26948103308677673
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6386e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2574320435523987
dqn reward tensor(373.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12072973698377609
dqn reward tensor(647.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14560703933238983
dqn reward tensor(43.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2889e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03770732879638672
Evaluating...
Train: {'rocauc': 0.7840996618636029} 7.623059272766113
=====Epoch 27=====
Training...
dqn reward tensor(371.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1119e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16242167353630066
dqn reward tensor(598.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12032006680965424
dqn reward tensor(460.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1259911209344864
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1381e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1013653352856636
dqn reward tensor(506.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.0402e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10878356546163559
dqn reward tensor(582.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0525e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10840253531932831
dqn reward tensor(444.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2365e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09879599511623383
dqn reward tensor(477.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425676792860031
dqn reward tensor(424.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7115e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21225173771381378
dqn reward tensor(333.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6245e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27596938610076904
dqn reward tensor(508.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.1101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15451836585998535
dqn reward tensor(489.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21434979140758514
dqn reward tensor(435.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1763823926448822
dqn reward tensor(417.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036273881793022156
dqn reward tensor(362.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3492e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1631583273410797
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09694623947143555
dqn reward tensor(516.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2345632016658783
dqn reward tensor(302.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040870778262615204
dqn reward tensor(659.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.097334124147892
dqn reward tensor(541.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5843e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11889849603176117
dqn reward tensor(625.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06189270690083504
dqn reward tensor(503.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12375471740961075
dqn reward tensor(552.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9137e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2175128012895584
dqn reward tensor(599., device='cuda:0') e 0.05 loss_dqn tensor(2.6754e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0703170970082283
dqn reward tensor(525.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19813989102840424
dqn reward tensor(486.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12084131687879562
dqn reward tensor(686.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08931849151849747
dqn reward tensor(507.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09563995897769928
dqn reward tensor(554.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1358279436826706
dqn reward tensor(412.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07576398551464081
dqn reward tensor(432.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07533562928438187
dqn reward tensor(666.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7884e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11708857119083405
dqn reward tensor(469.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.150379940867424
dqn reward tensor(510.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18236851692199707
dqn reward tensor(543.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5801e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13205605745315552
dqn reward tensor(460.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8247e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32614433765411377
dqn reward tensor(353.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07549597322940826
dqn reward tensor(416.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16408059000968933
dqn reward tensor(757.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20070035755634308
dqn reward tensor(521.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04278776794672012
dqn reward tensor(692.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1312e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10190461575984955
dqn reward tensor(483.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1042284145951271
dqn reward tensor(384., device='cuda:0') e 0.05 loss_dqn tensor(3.3739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15593522787094116
dqn reward tensor(595.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14344559609889984
dqn reward tensor(374.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09752482175827026
dqn reward tensor(354.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10454434156417847
dqn reward tensor(644., device='cuda:0') e 0.05 loss_dqn tensor(1.8704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1895737648010254
dqn reward tensor(570.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22144748270511627
dqn reward tensor(465.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8908e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.207138329744339
dqn reward tensor(494.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07662192732095718
dqn reward tensor(440.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21581628918647766
dqn reward tensor(696.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6716e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22845543920993805
dqn reward tensor(663.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5735e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17929305136203766
dqn reward tensor(597.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07230508327484131
dqn reward tensor(523.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08977001160383224
dqn reward tensor(618.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0988573431968689
dqn reward tensor(676.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07090191543102264
dqn reward tensor(465.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8757e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050060153007507324
dqn reward tensor(356.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2906e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14382636547088623
dqn reward tensor(474.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13927170634269714
dqn reward tensor(364.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0754e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03222125768661499
dqn reward tensor(583.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08343363553285599
dqn reward tensor(696.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19789043068885803
dqn reward tensor(602.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6129e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13904878497123718
dqn reward tensor(506.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.7444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2101663500070572
dqn reward tensor(569., device='cuda:0') e 0.05 loss_dqn tensor(2.9715e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.215311199426651
dqn reward tensor(713.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5271e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1292836219072342
dqn reward tensor(446.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24713855981826782
dqn reward tensor(639.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08052700012922287
dqn reward tensor(559.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15875700116157532
dqn reward tensor(730.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07704728096723557
dqn reward tensor(484.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7716e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10570503771305084
dqn reward tensor(494., device='cuda:0') e 0.05 loss_dqn tensor(1.7910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11472403258085251
dqn reward tensor(542.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5328e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19642983376979828
dqn reward tensor(613.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6416e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10857908427715302
dqn reward tensor(644.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03865820914506912
dqn reward tensor(538.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12842553853988647
dqn reward tensor(618.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7873e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12084915488958359
dqn reward tensor(450.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.1826e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07597609609365463
dqn reward tensor(502., device='cuda:0') e 0.05 loss_dqn tensor(1.8323e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.123951755464077
dqn reward tensor(422.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.4921e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08174218982458115
dqn reward tensor(647.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2163725048303604
dqn reward tensor(649., device='cuda:0') e 0.05 loss_dqn tensor(1.5829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030330587178468704
dqn reward tensor(617.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7881e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21541482210159302
dqn reward tensor(528.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1678609997034073
dqn reward tensor(552.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.6197e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04412555694580078
dqn reward tensor(438.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11029261350631714
dqn reward tensor(699.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17367085814476013
dqn reward tensor(644.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036984335631132126
dqn reward tensor(579.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.416309118270874
dqn reward tensor(661.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6342e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05991343408823013
dqn reward tensor(553., device='cuda:0') e 0.05 loss_dqn tensor(1.6607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13171619176864624
dqn reward tensor(640.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15899130702018738
dqn reward tensor(710.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5020e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22137627005577087
dqn reward tensor(475.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2636e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10812335461378098
dqn reward tensor(661., device='cuda:0') e 0.05 loss_dqn tensor(1.5794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17281502485275269
dqn reward tensor(375.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21049846708774567
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11127393692731857
dqn reward tensor(608.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10152919590473175
dqn reward tensor(471.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11363747715950012
dqn reward tensor(428.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20564359426498413
dqn reward tensor(542.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09543335437774658
dqn reward tensor(713.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0224e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10357388108968735
dqn reward tensor(516.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12791159749031067
dqn reward tensor(436.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09439609199762344
dqn reward tensor(629.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05471311882138252
dqn reward tensor(362., device='cuda:0') e 0.05 loss_dqn tensor(3.3600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1391473114490509
dqn reward tensor(530., device='cuda:0') e 0.05 loss_dqn tensor(1.6872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14457711577415466
dqn reward tensor(543.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2786530554294586
dqn reward tensor(533.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8220e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17205141484737396
dqn reward tensor(622.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6604e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.210653617978096
dqn reward tensor(832.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4381e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21916444599628448
dqn reward tensor(521.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15409357845783234
dqn reward tensor(516., device='cuda:0') e 0.05 loss_dqn tensor(1.7697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031196441501379013
dqn reward tensor(589.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15202996134757996
dqn reward tensor(728., device='cuda:0') e 0.05 loss_dqn tensor(1.7335e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06673384457826614
dqn reward tensor(509.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1831681728363037
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6544e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18296843767166138
dqn reward tensor(676.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1724e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1657542884349823
dqn reward tensor(503.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15743190050125122
dqn reward tensor(618.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09906628727912903
dqn reward tensor(542.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03554088622331619
dqn reward tensor(684.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15155968070030212
dqn reward tensor(538.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4572e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10173709690570831
dqn reward tensor(587., device='cuda:0') e 0.05 loss_dqn tensor(1.5010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1366029977798462
dqn reward tensor(370.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23130251467227936
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07391392439603806
dqn reward tensor(670.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07026953250169754
dqn reward tensor(623.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10704050958156586
dqn reward tensor(639.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6157e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3494333028793335
dqn reward tensor(482.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0316e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19054242968559265
dqn reward tensor(572.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.307234525680542
dqn reward tensor(668.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2402e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09632660448551178
dqn reward tensor(657., device='cuda:0') e 0.05 loss_dqn tensor(1.8688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06935835629701614
dqn reward tensor(558.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24152791500091553
dqn reward tensor(582.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7416e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30406975746154785
dqn reward tensor(507.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0814e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15224146842956543
dqn reward tensor(636.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.202904611825943
dqn reward tensor(541.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3341e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10884413868188858
dqn reward tensor(553.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19910785555839539
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06243966519832611
dqn reward tensor(433.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3285e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16860167682170868
dqn reward tensor(654.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11076439172029495
dqn reward tensor(499.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12141188234090805
dqn reward tensor(532., device='cuda:0') e 0.05 loss_dqn tensor(2.4066e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0982644259929657
dqn reward tensor(436.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19069130718708038
dqn reward tensor(461.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7803e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24861374497413635
dqn reward tensor(549.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049320004880428314
dqn reward tensor(663.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2246e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16488716006278992
dqn reward tensor(683.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2342e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06619119644165039
dqn reward tensor(392.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10218789428472519
dqn reward tensor(351.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023711662739515305
dqn reward tensor(460.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.3875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12545792758464813
dqn reward tensor(428.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.9358e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2054366171360016
dqn reward tensor(626.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2774231433868408
dqn reward tensor(575.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.6827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905933916568756
dqn reward tensor(577.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1466e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17269949615001678
dqn reward tensor(706.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8208e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1201084554195404
dqn reward tensor(710.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10064630210399628
dqn reward tensor(342.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07441803067922592
dqn reward tensor(608.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15519759058952332
dqn reward tensor(618.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22601020336151123
dqn reward tensor(677.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0746319591999054
dqn reward tensor(485., device='cuda:0') e 0.05 loss_dqn tensor(2.1993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10672298073768616
dqn reward tensor(596., device='cuda:0') e 0.05 loss_dqn tensor(2.6946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15332892537117004
dqn reward tensor(727., device='cuda:0') e 0.05 loss_dqn tensor(1.7765e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16474789381027222
dqn reward tensor(650.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14009594917297363
dqn reward tensor(433., device='cuda:0') e 0.05 loss_dqn tensor(2.2497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09633664041757584
dqn reward tensor(724., device='cuda:0') e 0.05 loss_dqn tensor(1.6655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11543877422809601
dqn reward tensor(514.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1871371865272522
dqn reward tensor(447.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15493693947792053
dqn reward tensor(620.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1520276665687561
dqn reward tensor(561.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6348e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27085819840431213
dqn reward tensor(590.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22972553968429565
dqn reward tensor(517.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19411161541938782
dqn reward tensor(758.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1656123548746109
dqn reward tensor(639.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.0148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1739368587732315
dqn reward tensor(639.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0938e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15309353172779083
dqn reward tensor(537.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8430e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09867456555366516
dqn reward tensor(646.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13487263023853302
dqn reward tensor(739.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23241069912910461
dqn reward tensor(657.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9254e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19793729484081268
dqn reward tensor(516., device='cuda:0') e 0.05 loss_dqn tensor(2.7266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25001275539398193
dqn reward tensor(638., device='cuda:0') e 0.05 loss_dqn tensor(1.5944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09982722252607346
dqn reward tensor(560.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0581464059650898
dqn reward tensor(655.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10161913931369781
dqn reward tensor(673.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06555499136447906
dqn reward tensor(535.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16091589629650116
dqn reward tensor(646., device='cuda:0') e 0.05 loss_dqn tensor(4.1019e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0776640921831131
dqn reward tensor(545., device='cuda:0') e 0.05 loss_dqn tensor(1.8644e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12340280413627625
dqn reward tensor(404.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4374e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11284735053777695
dqn reward tensor(689.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17082451283931732
dqn reward tensor(710.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058261480182409286
dqn reward tensor(595.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7399e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1016356498003006
dqn reward tensor(625.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08301526308059692
dqn reward tensor(606.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10198550671339035
dqn reward tensor(617.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6516e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1588352918624878
dqn reward tensor(647.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.9070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0882248505949974
dqn reward tensor(451.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2104e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2654169797897339
dqn reward tensor(644.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24891294538974762
dqn reward tensor(618.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17187467217445374
dqn reward tensor(582.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05446217954158783
dqn reward tensor(613.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1506e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16986675560474396
dqn reward tensor(624.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0869e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03258306160569191
dqn reward tensor(729.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11269442737102509
dqn reward tensor(753.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09783592820167542
dqn reward tensor(687.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19496509432792664
dqn reward tensor(733.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6224e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254363059997559
dqn reward tensor(736.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13979826867580414
dqn reward tensor(284.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21823927760124207
dqn reward tensor(636., device='cuda:0') e 0.05 loss_dqn tensor(1.9253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13564959168434143
dqn reward tensor(561.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0496588833630085
dqn reward tensor(562.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6300e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17184408009052277
dqn reward tensor(509.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13845357298851013
dqn reward tensor(649., device='cuda:0') e 0.05 loss_dqn tensor(2.5464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047043733298778534
dqn reward tensor(765., device='cuda:0') e 0.05 loss_dqn tensor(1.9904e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2158401906490326
dqn reward tensor(723.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0795e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0955275446176529
dqn reward tensor(590.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0367e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15700453519821167
dqn reward tensor(636.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7814e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03166962414979935
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8327e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06588168442249298
dqn reward tensor(764.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08931148052215576
dqn reward tensor(681., device='cuda:0') e 0.05 loss_dqn tensor(2.3807e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23164606094360352
dqn reward tensor(577.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8712e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1578391194343567
dqn reward tensor(649.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12920522689819336
dqn reward tensor(619., device='cuda:0') e 0.05 loss_dqn tensor(3.4771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19685634970664978
dqn reward tensor(683.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0766274631023407
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2863e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1960475742816925
dqn reward tensor(700.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10486152023077011
dqn reward tensor(667.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24022391438484192
dqn reward tensor(713.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07960422337055206
dqn reward tensor(637.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10122351348400116
dqn reward tensor(695.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6158e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16075178980827332
dqn reward tensor(683., device='cuda:0') e 0.05 loss_dqn tensor(1.7370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2009306102991104
dqn reward tensor(623.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5901e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17525508999824524
dqn reward tensor(717.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04585309326648712
dqn reward tensor(698., device='cuda:0') e 0.05 loss_dqn tensor(1.5021e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16055485606193542
dqn reward tensor(691.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.4544e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20302221179008484
dqn reward tensor(568.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03652099519968033
dqn reward tensor(670.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05570588633418083
dqn reward tensor(598.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11413167417049408
dqn reward tensor(647.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03403525799512863
dqn reward tensor(545.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07881341874599457
dqn reward tensor(641.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03729686886072159
dqn reward tensor(637.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6828e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07725554704666138
dqn reward tensor(584.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477590024471283
dqn reward tensor(618.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20685306191444397
dqn reward tensor(672.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24742533266544342
dqn reward tensor(556.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.6866e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05402848869562149
dqn reward tensor(681.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18332985043525696
dqn reward tensor(738.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0568881630897522
dqn reward tensor(544.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6026e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2347998321056366
dqn reward tensor(549.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17896607518196106
dqn reward tensor(863.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3478e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10916652530431747
dqn reward tensor(826., device='cuda:0') e 0.05 loss_dqn tensor(1.2761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08249493688344955
dqn reward tensor(806.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037133023142814636
dqn reward tensor(670.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21374459564685822
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10484965145587921
dqn reward tensor(551.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17977848649024963
dqn reward tensor(655.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.9274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.168183371424675
dqn reward tensor(543.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6706e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19851510226726532
dqn reward tensor(731.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3936e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06249577924609184
dqn reward tensor(714.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3158e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03202151507139206
dqn reward tensor(717.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3132e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08300681412220001
dqn reward tensor(550.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7247e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09794601798057556
dqn reward tensor(829.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5945e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020782530307769775
dqn reward tensor(743.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4505e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16910654306411743
dqn reward tensor(729.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05784346163272858
dqn reward tensor(722.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09126590192317963
dqn reward tensor(815.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2962e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05120507627725601
dqn reward tensor(809.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0355866439640522
dqn reward tensor(666.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18343816697597504
dqn reward tensor(745.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3178e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10912616550922394
dqn reward tensor(686.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017146825790405273
dqn reward tensor(854.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2900264263153076
dqn reward tensor(860.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24782724678516388
dqn reward tensor(598.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12643924355506897
dqn reward tensor(868.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08568046241998672
dqn reward tensor(534.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16640515625476837
dqn reward tensor(625.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4104e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07169461250305176
dqn reward tensor(700.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2236233800649643
dqn reward tensor(641.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04704334959387779
dqn reward tensor(608.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.4359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1020730584859848
dqn reward tensor(736.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14328445494174957
dqn reward tensor(796.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1721409112215042
dqn reward tensor(573.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5052e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05151316151022911
dqn reward tensor(711.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0980091542005539
dqn reward tensor(589.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17052824795246124
dqn reward tensor(613.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1739313304424286
dqn reward tensor(667.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4264e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16650623083114624
dqn reward tensor(516.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09548802673816681
dqn reward tensor(647.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13438907265663147
dqn reward tensor(765.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5684e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17772480845451355
dqn reward tensor(524.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0306e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1373130828142166
dqn reward tensor(728.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14296889305114746
dqn reward tensor(619.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13694033026695251
dqn reward tensor(525., device='cuda:0') e 0.05 loss_dqn tensor(1.4952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07894629240036011
dqn reward tensor(804.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2582e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1338074952363968
dqn reward tensor(660.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3343e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13451339304447174
dqn reward tensor(647., device='cuda:0') e 0.05 loss_dqn tensor(1.5093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19679152965545654
dqn reward tensor(701.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2715e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10095170140266418
dqn reward tensor(493.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19508478045463562
dqn reward tensor(870.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1431e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19154348969459534
dqn reward tensor(667.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14281493425369263
dqn reward tensor(559.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4566e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21853423118591309
dqn reward tensor(803.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12283962965011597
dqn reward tensor(606., device='cuda:0') e 0.05 loss_dqn tensor(2.0510e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0966581478714943
dqn reward tensor(733.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4287e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15806207060813904
dqn reward tensor(708.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1306311935186386
dqn reward tensor(642.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3369e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11959028244018555
dqn reward tensor(808.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1866e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0667167454957962
dqn reward tensor(673.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11954693496227264
dqn reward tensor(732.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14254872500896454
dqn reward tensor(567.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3026919364929199
dqn reward tensor(716.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10610994696617126
dqn reward tensor(641.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7811e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15171068906784058
dqn reward tensor(757.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9320e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16909782588481903
dqn reward tensor(805.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5011e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2680286765098572
dqn reward tensor(674., device='cuda:0') e 0.05 loss_dqn tensor(1.3835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19958868622779846
dqn reward tensor(719.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06171688437461853
dqn reward tensor(766.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17573387920856476
dqn reward tensor(769., device='cuda:0') e 0.05 loss_dqn tensor(1.6098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10623683035373688
dqn reward tensor(885.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1244000792503357
dqn reward tensor(616.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23739510774612427
dqn reward tensor(586.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13550639152526855
dqn reward tensor(475.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029370669275522232
dqn reward tensor(691.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22093701362609863
dqn reward tensor(777., device='cuda:0') e 0.05 loss_dqn tensor(1.2638e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08994340896606445
dqn reward tensor(633.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19853852689266205
dqn reward tensor(728.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17595379054546356
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1428e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15648862719535828
dqn reward tensor(668.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3632e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13170449435710907
dqn reward tensor(704.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1912e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2832334339618683
dqn reward tensor(762.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12331685423851013
dqn reward tensor(707.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2348e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21561482548713684
dqn reward tensor(588., device='cuda:0') e 0.05 loss_dqn tensor(2.1509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14350751042366028
dqn reward tensor(886.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05305958911776543
dqn reward tensor(777.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18308080732822418
dqn reward tensor(485.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1025981456041336
dqn reward tensor(843.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3418e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16833354532718658
dqn reward tensor(659.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1438187062740326
dqn reward tensor(806.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11357343941926956
dqn reward tensor(643.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09204481542110443
dqn reward tensor(573.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6611e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12467720359563828
dqn reward tensor(644.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16168293356895447
dqn reward tensor(588.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13244837522506714
dqn reward tensor(587.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7220e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16064487397670746
dqn reward tensor(578.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2398616373538971
dqn reward tensor(558., device='cuda:0') e 0.05 loss_dqn tensor(2.3059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16234907507896423
dqn reward tensor(541.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1537235528230667
dqn reward tensor(460.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0883040726184845
dqn reward tensor(445.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.7240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029863623902201653
dqn reward tensor(273.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8584e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08987213671207428
dqn reward tensor(195., device='cuda:0') e 0.05 loss_dqn tensor(2.8773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032490141689777374
dqn reward tensor(205.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02336212806403637
dqn reward tensor(187.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1352066993713379
dqn reward tensor(167.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1638784259557724
dqn reward tensor(52.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2529e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22122712433338165
dqn reward tensor(-2.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0196e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20494678616523743
dqn reward tensor(-19.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060561344027519226
dqn reward tensor(15.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08924524486064911
dqn reward tensor(51.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045308467000722885
dqn reward tensor(-86., device='cuda:0') e 0.05 loss_dqn tensor(3.1187e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07871977239847183
dqn reward tensor(51.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.1258e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11527199298143387
dqn reward tensor(-91.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2087e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032206419855356216
dqn reward tensor(-52.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3083e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08445626497268677
dqn reward tensor(57.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12588180601596832
dqn reward tensor(-84.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21305665373802185
dqn reward tensor(-93.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07199979573488235
dqn reward tensor(104.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028070997446775436
dqn reward tensor(-23.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6409e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11519734561443329
dqn reward tensor(-81.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21967509388923645
dqn reward tensor(-37.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2299e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10722951591014862
dqn reward tensor(-179.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08547983318567276
dqn reward tensor(-38.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2092e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.079808309674263
dqn reward tensor(-61.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11626852303743362
dqn reward tensor(20.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11123733967542648
dqn reward tensor(82.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3358e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.191758930683136
dqn reward tensor(-34.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1801e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0971781387925148
dqn reward tensor(31.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2902e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10091713070869446
dqn reward tensor(-70.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10158532112836838
dqn reward tensor(-67.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3082e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09571444243192673
dqn reward tensor(-45.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023952197283506393
dqn reward tensor(-100.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026446416974067688
dqn reward tensor(-43.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20155297219753265
dqn reward tensor(-75.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3262295722961426
dqn reward tensor(-31.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2946120500564575
dqn reward tensor(-116.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2385e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09545299410820007
dqn reward tensor(-79.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.2735e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0294767078012228
dqn reward tensor(16.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2238e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05269467085599899
dqn reward tensor(-74.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07559699565172195
dqn reward tensor(-32.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040224380791187286
dqn reward tensor(-84.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2816651463508606
dqn reward tensor(-34.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29064029455184937
dqn reward tensor(-256.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.4024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12023800611495972
dqn reward tensor(-11.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2050e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1220647320151329
dqn reward tensor(-154.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09702733904123306
dqn reward tensor(-9.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059510745108127594
dqn reward tensor(-24.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2254158854484558
dqn reward tensor(-105.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5683e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13754208385944366
dqn reward tensor(-147.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2561856806278229
dqn reward tensor(-12.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09891553968191147
dqn reward tensor(72.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2487e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17009533941745758
dqn reward tensor(-83., device='cuda:0') e 0.05 loss_dqn tensor(3.5968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21753111481666565
dqn reward tensor(-71.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2374e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1834116280078888
dqn reward tensor(1., device='cuda:0') e 0.05 loss_dqn tensor(3.2618e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08708086609840393
dqn reward tensor(-67.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03545321524143219
dqn reward tensor(-145.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06327731907367706
dqn reward tensor(-52.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08647701144218445
dqn reward tensor(-208.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09965480864048004
dqn reward tensor(-24., device='cuda:0') e 0.05 loss_dqn tensor(3.2244e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0943257138133049
dqn reward tensor(-31.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06692492216825485
dqn reward tensor(12.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1658804714679718
dqn reward tensor(-117.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043552741408348083
dqn reward tensor(-111.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09365960955619812
dqn reward tensor(-116.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08030281215906143
dqn reward tensor(-147.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10787470638751984
dqn reward tensor(-93.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2582e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1323249191045761
dqn reward tensor(-54.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13624151051044464
dqn reward tensor(81.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3957e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14789701998233795
dqn reward tensor(-67.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05038358271121979
dqn reward tensor(-67.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1692303568124771
dqn reward tensor(7.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3544e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11743884533643723
dqn reward tensor(-68.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038016367703676224
dqn reward tensor(-50.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21561606228351593
dqn reward tensor(-50.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3744e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06888850033283234
dqn reward tensor(67.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3459e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1885322630405426
dqn reward tensor(-108.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14525558054447174
dqn reward tensor(37.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.3256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026148013770580292
dqn reward tensor(-20.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16558784246444702
dqn reward tensor(-70.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2643e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17283380031585693
dqn reward tensor(-107., device='cuda:0') e 0.05 loss_dqn tensor(3.2573e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2329992949962616
dqn reward tensor(-65.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14428281784057617
dqn reward tensor(-43.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09524261206388474
dqn reward tensor(-27.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254531443119049
dqn reward tensor(-95.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15414687991142273
dqn reward tensor(-145.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03166985511779785
dqn reward tensor(-26.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5842e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1639939248561859
dqn reward tensor(-49.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.5236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03378663957118988
dqn reward tensor(-51.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11457107216119766
dqn reward tensor(-45.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03223797678947449
dqn reward tensor(-126.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08239021897315979
dqn reward tensor(-68., device='cuda:0') e 0.05 loss_dqn tensor(3.4948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018270540982484818
dqn reward tensor(-135.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09624695777893066
dqn reward tensor(-75.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1867e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.152544766664505
dqn reward tensor(-49., device='cuda:0') e 0.05 loss_dqn tensor(3.3923e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08677230030298233
dqn reward tensor(-81.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2936e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09406936168670654
dqn reward tensor(-85.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.5451e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11596141755580902
dqn reward tensor(-82.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3478533923625946
dqn reward tensor(-15.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2422e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03796771168708801
dqn reward tensor(-49.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14628088474273682
dqn reward tensor(-108.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2135060727596283
dqn reward tensor(-102.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20823350548744202
dqn reward tensor(-80.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2089e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12333270907402039
dqn reward tensor(-56.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03547307848930359
dqn reward tensor(-78.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3213e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20767155289649963
dqn reward tensor(-154.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2660e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1729414165019989
dqn reward tensor(-100.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.4283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18613088130950928
dqn reward tensor(4.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2826e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10976878553628922
dqn reward tensor(-44.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15249142050743103
dqn reward tensor(-43.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2238685041666031
dqn reward tensor(-122.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.100859135389328
dqn reward tensor(-157.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12020685523748398
dqn reward tensor(-178., device='cuda:0') e 0.05 loss_dqn tensor(3.3031e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15427425503730774
dqn reward tensor(-283.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2367e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.137637659907341
dqn reward tensor(-120.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2087e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07073803246021271
dqn reward tensor(-226.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.2718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10577299445867538
dqn reward tensor(-53., device='cuda:0') e 0.05 loss_dqn tensor(3.2721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028817465528845787
dqn reward tensor(-89.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12926509976387024
dqn reward tensor(-102., device='cuda:0') e 0.05 loss_dqn tensor(3.3877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11057929694652557
dqn reward tensor(-47.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08531109243631363
dqn reward tensor(-46.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16092199087142944
dqn reward tensor(-220.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2989e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.283369779586792
dqn reward tensor(-82.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10177290439605713
dqn reward tensor(-149.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2566152811050415
dqn reward tensor(76.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4019e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08352567255496979
dqn reward tensor(-14.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2532e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06930015236139297
dqn reward tensor(143.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11300057917833328
dqn reward tensor(-86.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5576e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025021271780133247
dqn reward tensor(150.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12916618585586548
dqn reward tensor(88.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06643085181713104
dqn reward tensor(65.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02900097146630287
dqn reward tensor(98.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5559e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12040427327156067
dqn reward tensor(35.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09962720423936844
dqn reward tensor(30., device='cuda:0') e 0.05 loss_dqn tensor(3.5216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06473346799612045
dqn reward tensor(-99.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3157e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10941459238529205
dqn reward tensor(-3., device='cuda:0') e 0.05 loss_dqn tensor(3.5042e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08814194798469543
dqn reward tensor(74.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15665635466575623
dqn reward tensor(13.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.7807e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08804389834403992
dqn reward tensor(44.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344388872385025
dqn reward tensor(90.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0172e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1656457781791687
dqn reward tensor(122.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1559e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09451957792043686
dqn reward tensor(-69., device='cuda:0') e 0.05 loss_dqn tensor(3.2289e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06473992764949799
dqn reward tensor(2.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07857722789049149
dqn reward tensor(118.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11486987769603729
dqn reward tensor(177.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1579e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17230947315692902
dqn reward tensor(80.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6270e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021411065012216568
dqn reward tensor(43.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.3924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09923622012138367
dqn reward tensor(105.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10474251955747604
dqn reward tensor(-49.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046317365020513535
dqn reward tensor(56.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023856380954384804
dqn reward tensor(-86.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2294e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11207081377506256
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.205559641122818
dqn reward tensor(64.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.1023e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12613032758235931
dqn reward tensor(-3.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17407605051994324
dqn reward tensor(-7.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.1194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13181300461292267
dqn reward tensor(56.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0848e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17650717496871948
dqn reward tensor(114.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.2934e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35387831926345825
dqn reward tensor(21.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1638e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07353116571903229
dqn reward tensor(176.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9183e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09935515373945236
dqn reward tensor(124.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.3190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10665038228034973
dqn reward tensor(21., device='cuda:0') e 0.05 loss_dqn tensor(3.3735e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16729268431663513
dqn reward tensor(211.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06099285930395126
dqn reward tensor(190.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11352114379405975
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1850e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11048121750354767
dqn reward tensor(22.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1379e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028151486068964005
Evaluating...
Train: {'rocauc': 0.7895343776104535} 1.7485876083374023
=====Epoch 28=====
Training...
dqn reward tensor(-16.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.3596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1694868803024292
dqn reward tensor(180.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10274840891361237
dqn reward tensor(-89.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2328951209783554
dqn reward tensor(180.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11060398817062378
dqn reward tensor(9.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21092146635055542
dqn reward tensor(92.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07321859896183014
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3550e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1870856136083603
dqn reward tensor(233.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19699311256408691
dqn reward tensor(194.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2716e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10143868625164032
dqn reward tensor(40., device='cuda:0') e 0.05 loss_dqn tensor(3.2142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11887828260660172
dqn reward tensor(64.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.6223e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06368328630924225
dqn reward tensor(32.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09347739815711975
dqn reward tensor(83.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24291276931762695
dqn reward tensor(39.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15638995170593262
dqn reward tensor(-78.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18480408191680908
dqn reward tensor(-64.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11534329503774643
dqn reward tensor(0.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06970234960317612
dqn reward tensor(64.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08139414340257645
dqn reward tensor(155., device='cuda:0') e 0.05 loss_dqn tensor(3.2187e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1081894040107727
dqn reward tensor(170., device='cuda:0') e 0.05 loss_dqn tensor(3.1598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08425462245941162
dqn reward tensor(147.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1185e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0497671440243721
dqn reward tensor(239.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2271958589553833
dqn reward tensor(194., device='cuda:0') e 0.05 loss_dqn tensor(3.2240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08029481768608093
dqn reward tensor(1.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22203034162521362
dqn reward tensor(-38.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12302859872579575
dqn reward tensor(58., device='cuda:0') e 0.05 loss_dqn tensor(3.5136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04256195202469826
dqn reward tensor(-11.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060936301946640015
dqn reward tensor(150.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10858340561389923
dqn reward tensor(58.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29373201727867126
dqn reward tensor(5., device='cuda:0') e 0.05 loss_dqn tensor(3.4029e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1645640730857849
dqn reward tensor(158., device='cuda:0') e 0.05 loss_dqn tensor(3.3183e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027293847873806953
dqn reward tensor(83.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17901816964149475
dqn reward tensor(163., device='cuda:0') e 0.05 loss_dqn tensor(3.1340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08264680206775665
dqn reward tensor(123.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16653448343276978
dqn reward tensor(79.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1337e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22076204419136047
dqn reward tensor(157.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030651651322841644
dqn reward tensor(168.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19923511147499084
dqn reward tensor(58.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3223e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28685635328292847
dqn reward tensor(-135.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07204116880893707
dqn reward tensor(-80.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2257e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10150522738695145
dqn reward tensor(89.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4801e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13286545872688293
dqn reward tensor(14.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5919e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0839092880487442
dqn reward tensor(2., device='cuda:0') e 0.05 loss_dqn tensor(3.6883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10069085657596588
dqn reward tensor(46.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09110617637634277
dqn reward tensor(189.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1736e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16912749409675598
dqn reward tensor(103.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13237176835536957
dqn reward tensor(83.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.6739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2973097562789917
dqn reward tensor(182., device='cuda:0') e 0.05 loss_dqn tensor(3.2128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24631467461585999
dqn reward tensor(182.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.262556254863739
dqn reward tensor(166.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1906876266002655
dqn reward tensor(-27.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06970261037349701
dqn reward tensor(66.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0607169046998024
dqn reward tensor(70.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12003464996814728
dqn reward tensor(33.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056776683777570724
dqn reward tensor(121.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040517017245292664
dqn reward tensor(75.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5515e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17961561679840088
dqn reward tensor(81.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4327e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1496971994638443
dqn reward tensor(12.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.7583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06718823313713074
dqn reward tensor(98.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11242478340864182
dqn reward tensor(1.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13479825854301453
dqn reward tensor(38.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24121050536632538
dqn reward tensor(219.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0690479502081871
dqn reward tensor(-0.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02395457960665226
dqn reward tensor(151., device='cuda:0') e 0.05 loss_dqn tensor(3.3481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2840877175331116
dqn reward tensor(48.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11472438275814056
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13662806153297424
dqn reward tensor(4.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32764238119125366
dqn reward tensor(252.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09969846904277802
dqn reward tensor(78.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22458845376968384
dqn reward tensor(110.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1085933968424797
dqn reward tensor(-1., device='cuda:0') e 0.05 loss_dqn tensor(3.1968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08368708193302155
dqn reward tensor(52.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2598607540130615
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13328014314174652
dqn reward tensor(174.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2137e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1327410340309143
dqn reward tensor(34., device='cuda:0') e 0.05 loss_dqn tensor(3.8629e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04746704176068306
dqn reward tensor(-55.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12500162422657013
dqn reward tensor(91.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14581552147865295
dqn reward tensor(34., device='cuda:0') e 0.05 loss_dqn tensor(3.2069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14114922285079956
dqn reward tensor(78.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15354160964488983
dqn reward tensor(84.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03027399443089962
dqn reward tensor(31.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2329e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037993885576725006
dqn reward tensor(131.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032892175018787384
dqn reward tensor(82.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.2071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16955281794071198
dqn reward tensor(-23.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5339e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13239775598049164
dqn reward tensor(156.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13530397415161133
dqn reward tensor(71.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.3086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021211108192801476
dqn reward tensor(38.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3305e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044447027146816254
dqn reward tensor(106., device='cuda:0') e 0.05 loss_dqn tensor(3.3442e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1538264900445938
dqn reward tensor(51.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2235e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053976867347955704
dqn reward tensor(-178.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2979e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.114254891872406
dqn reward tensor(17.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.4634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018305320292711258
dqn reward tensor(-15., device='cuda:0') e 0.05 loss_dqn tensor(3.0467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2070707529783249
dqn reward tensor(110.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020111579447984695
dqn reward tensor(-88.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013595334254205227
dqn reward tensor(-65.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2633e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08193904161453247
dqn reward tensor(198.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2057749629020691
dqn reward tensor(6.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3660e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013932313770055771
dqn reward tensor(1.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1380845010280609
dqn reward tensor(171.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13415546715259552
dqn reward tensor(50., device='cuda:0') e 0.05 loss_dqn tensor(3.3533e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08350486308336258
dqn reward tensor(84.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.3454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08553143590688705
dqn reward tensor(-153.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1525e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07927117496728897
dqn reward tensor(64.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1279684156179428
dqn reward tensor(13.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5414e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21581703424453735
dqn reward tensor(-47.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9046e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19006212055683136
dqn reward tensor(36.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.4410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20609942078590393
dqn reward tensor(-78.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4139e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15648405253887177
dqn reward tensor(63.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5167e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1138124093413353
dqn reward tensor(267.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04705972224473953
dqn reward tensor(110.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2762e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0939374715089798
dqn reward tensor(27.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6051e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11355693638324738
dqn reward tensor(155.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2226e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13908806443214417
dqn reward tensor(3.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21764898300170898
dqn reward tensor(-7., device='cuda:0') e 0.05 loss_dqn tensor(3.3490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2461838722229004
dqn reward tensor(20., device='cuda:0') e 0.05 loss_dqn tensor(3.6491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03933408111333847
dqn reward tensor(187.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08536997437477112
dqn reward tensor(69.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2674e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1160786971449852
dqn reward tensor(44.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3629e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032339908182621
dqn reward tensor(96., device='cuda:0') e 0.05 loss_dqn tensor(3.2685e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18774375319480896
dqn reward tensor(-25.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2375144064426422
dqn reward tensor(-36.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5711e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25242459774017334
dqn reward tensor(112.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018489670008420944
dqn reward tensor(88.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029389850795269012
dqn reward tensor(282.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07681585103273392
dqn reward tensor(96.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2450e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16119275987148285
dqn reward tensor(-91., device='cuda:0') e 0.05 loss_dqn tensor(3.5181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03994448482990265
dqn reward tensor(201.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3133e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13738524913787842
dqn reward tensor(69.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6361e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14521785080432892
dqn reward tensor(30.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.4376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10148630291223526
dqn reward tensor(-103.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0623e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2070155143737793
dqn reward tensor(280.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2089e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09932000935077667
dqn reward tensor(84., device='cuda:0') e 0.05 loss_dqn tensor(3.4681e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10491444170475006
dqn reward tensor(71.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1994e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1181585043668747
dqn reward tensor(193., device='cuda:0') e 0.05 loss_dqn tensor(3.2013e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10692457109689713
dqn reward tensor(-117.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2418e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018236860632896423
dqn reward tensor(-97., device='cuda:0') e 0.05 loss_dqn tensor(3.3250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14376792311668396
dqn reward tensor(-100.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18372134864330292
dqn reward tensor(-53.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18745484948158264
dqn reward tensor(-110., device='cuda:0') e 0.05 loss_dqn tensor(3.3655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2976042628288269
dqn reward tensor(-140.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11682963371276855
dqn reward tensor(3.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2254e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0772809088230133
dqn reward tensor(-195.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08385546505451202
dqn reward tensor(-127.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6774e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19462831318378448
dqn reward tensor(-112.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31782031059265137
dqn reward tensor(7.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0816e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666981279850006
dqn reward tensor(-62.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14845071732997894
dqn reward tensor(-159.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2306e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12356524914503098
dqn reward tensor(-66.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06792685389518738
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08279911428689957
dqn reward tensor(34.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1902e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08554515242576599
dqn reward tensor(-60.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10563898831605911
dqn reward tensor(-118.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5125e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0635349452495575
dqn reward tensor(-192.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035603612661361694
dqn reward tensor(-193.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14555451273918152
dqn reward tensor(-94.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5180e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0765129029750824
dqn reward tensor(-157.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04774933308362961
dqn reward tensor(-79.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3749e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1004045233130455
dqn reward tensor(-83.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4291e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23557446897029877
dqn reward tensor(-126.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206290364265442
dqn reward tensor(18.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3998e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14091244339942932
dqn reward tensor(-233.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159701868891716
dqn reward tensor(-14.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14255304634571075
dqn reward tensor(-117.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.4169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2402380406856537
dqn reward tensor(-175.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.1861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018370503559708595
dqn reward tensor(14.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4809e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06694004684686661
dqn reward tensor(-104.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.3916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14604023098945618
dqn reward tensor(-38.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11125542223453522
dqn reward tensor(-62.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4904e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03553670644760132
dqn reward tensor(-23.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.3077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10439752042293549
dqn reward tensor(-162.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12727589905261993
dqn reward tensor(-95., device='cuda:0') e 0.05 loss_dqn tensor(3.2920e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11993333697319031
dqn reward tensor(26.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05636094510555267
dqn reward tensor(-118.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.2496e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11309964954853058
dqn reward tensor(-32.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4472e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08488740026950836
dqn reward tensor(-142., device='cuda:0') e 0.05 loss_dqn tensor(3.2564e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2201845645904541
dqn reward tensor(-125.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3421e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03290930017828941
dqn reward tensor(75.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03426592797040939
dqn reward tensor(-62.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3164851665496826
dqn reward tensor(-119.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5214e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329713612794876
dqn reward tensor(-201.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2754337191581726
dqn reward tensor(-89.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0927351638674736
dqn reward tensor(-117.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017713766545057297
dqn reward tensor(17.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4022e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11301339417695999
dqn reward tensor(-26., device='cuda:0') e 0.05 loss_dqn tensor(3.3043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09251685440540314
dqn reward tensor(-9.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07638678699731827
dqn reward tensor(-148., device='cuda:0') e 0.05 loss_dqn tensor(3.4099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06150951609015465
dqn reward tensor(-58.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15621401369571686
dqn reward tensor(-69.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.6144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09743162244558334
dqn reward tensor(-64.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.8046e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1191486343741417
dqn reward tensor(6.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06643690168857574
dqn reward tensor(-125.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09281884133815765
dqn reward tensor(-58.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3071553409099579
dqn reward tensor(-29.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07641656696796417
dqn reward tensor(-41.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19847358763217926
dqn reward tensor(-27.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08604547381401062
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(3.1979e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13226011395454407
dqn reward tensor(-162., device='cuda:0') e 0.05 loss_dqn tensor(3.5865e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06797774136066437
dqn reward tensor(-62.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10424380004405975
dqn reward tensor(84.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1529e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15131869912147522
dqn reward tensor(-89.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3046e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1145201325416565
dqn reward tensor(-33.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21308399736881256
dqn reward tensor(-125.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18679958581924438
dqn reward tensor(-20.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12667450308799744
dqn reward tensor(-114.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.1378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10054922103881836
dqn reward tensor(-49.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19646435976028442
dqn reward tensor(122.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8880e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1475842297077179
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032821692526340485
dqn reward tensor(22.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1319960057735443
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12438748776912689
dqn reward tensor(-50.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3431e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03999147564172745
dqn reward tensor(6.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5526e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17905133962631226
dqn reward tensor(52.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16549837589263916
dqn reward tensor(-27.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19259749352931976
dqn reward tensor(18.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5589e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23106618225574493
dqn reward tensor(11.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20294320583343506
dqn reward tensor(37.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.7173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22790710628032684
dqn reward tensor(-57.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5511e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03904227539896965
dqn reward tensor(13.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15569764375686646
dqn reward tensor(-57.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2482898235321045
dqn reward tensor(-108.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15086765587329865
dqn reward tensor(20.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24667991697788239
dqn reward tensor(-140.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2994e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29235607385635376
dqn reward tensor(5.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03150811046361923
dqn reward tensor(-24.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8726e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14438742399215698
dqn reward tensor(-68.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.5063e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2075527012348175
dqn reward tensor(-151.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21534651517868042
dqn reward tensor(-98.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10556510090827942
dqn reward tensor(-156.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425071358680725
dqn reward tensor(123.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2439e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1325497329235077
dqn reward tensor(-136.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6381e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15805017948150635
dqn reward tensor(-46.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34584128856658936
dqn reward tensor(-16.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05114812031388283
dqn reward tensor(60.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062133751809597015
dqn reward tensor(98.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10427626967430115
dqn reward tensor(-61.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.2598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11000962555408478
dqn reward tensor(-3.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15655693411827087
dqn reward tensor(78.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19312641024589539
dqn reward tensor(90.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08798453211784363
dqn reward tensor(-31.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3213e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27722102403640747
dqn reward tensor(-23.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.3444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07077813148498535
dqn reward tensor(-60.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04849071800708771
dqn reward tensor(-136.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2369e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11199638247489929
dqn reward tensor(-89.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3157e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1678149700164795
dqn reward tensor(-150.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15338411927223206
dqn reward tensor(8.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3275e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061473630368709564
dqn reward tensor(68.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029658272862434387
dqn reward tensor(54.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05044562742114067
dqn reward tensor(-92.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3741e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1577598750591278
dqn reward tensor(-84.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2486e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06296779215335846
dqn reward tensor(-82., device='cuda:0') e 0.05 loss_dqn tensor(3.8027e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041540808975696564
dqn reward tensor(-24.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09416338056325912
dqn reward tensor(-60.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05355542153120041
dqn reward tensor(9., device='cuda:0') e 0.05 loss_dqn tensor(3.2445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07592839747667313
dqn reward tensor(-6.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041103433817625046
dqn reward tensor(56.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13576306402683258
dqn reward tensor(-36.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06004995107650757
dqn reward tensor(-13.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09667228162288666
dqn reward tensor(48.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2168458104133606
dqn reward tensor(-45.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20870333909988403
dqn reward tensor(-22.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5339e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1532030999660492
dqn reward tensor(275.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08870137482881546
dqn reward tensor(231.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.0837e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10897332429885864
dqn reward tensor(424.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8299e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1914987564086914
dqn reward tensor(404.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27864670753479004
dqn reward tensor(566.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1479811817407608
dqn reward tensor(677.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.7151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042993344366550446
dqn reward tensor(597.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11302556097507477
dqn reward tensor(774.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0552469938993454
dqn reward tensor(660.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6687e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1294582188129425
dqn reward tensor(827.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06678536534309387
dqn reward tensor(700.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4429e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1613726168870926
dqn reward tensor(741.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1552e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06228393316268921
dqn reward tensor(561.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3780e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21017184853553772
dqn reward tensor(739.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27816471457481384
dqn reward tensor(648.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2646e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22504691779613495
dqn reward tensor(676.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17673638463020325
dqn reward tensor(778., device='cuda:0') e 0.05 loss_dqn tensor(2.9780e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1060326099395752
dqn reward tensor(590.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5563e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21207232773303986
dqn reward tensor(824.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0317e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11798368394374847
dqn reward tensor(593.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12094578891992569
dqn reward tensor(657.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36504435539245605
dqn reward tensor(711.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.4358e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09382356703281403
dqn reward tensor(825.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13778406381607056
dqn reward tensor(741.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.1988e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14060045778751373
dqn reward tensor(778.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1793150007724762
dqn reward tensor(670.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6496e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14804384112358093
dqn reward tensor(863.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4807e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16478176414966583
dqn reward tensor(504.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2271708846092224
dqn reward tensor(607.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9385e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12669001519680023
dqn reward tensor(629.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12002447247505188
dqn reward tensor(769.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11196944117546082
dqn reward tensor(666.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10487645864486694
dqn reward tensor(707., device='cuda:0') e 0.05 loss_dqn tensor(7.3218e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374131143093109
dqn reward tensor(677.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7262e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15302124619483948
dqn reward tensor(858.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9050e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26767778396606445
dqn reward tensor(704.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7652e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056642405688762665
dqn reward tensor(653.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18217085301876068
dqn reward tensor(694.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05627885088324547
dqn reward tensor(817., device='cuda:0') e 0.05 loss_dqn tensor(2.4744e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07757633924484253
dqn reward tensor(591.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7435e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21651127934455872
dqn reward tensor(599.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2509e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22167111933231354
dqn reward tensor(817.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.5863e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12737533450126648
dqn reward tensor(615.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08640830218791962
dqn reward tensor(691.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.2153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202062875032425
dqn reward tensor(831.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2893e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09873384237289429
dqn reward tensor(635.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3537e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05167228356003761
dqn reward tensor(743.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.3518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06635772436857224
dqn reward tensor(832., device='cuda:0') e 0.05 loss_dqn tensor(6.3170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15108296275138855
dqn reward tensor(689., device='cuda:0') e 0.05 loss_dqn tensor(3.0719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22635529935359955
dqn reward tensor(697.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3080e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05192576348781586
dqn reward tensor(889.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8260e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17618900537490845
dqn reward tensor(667.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1050e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04608693718910217
dqn reward tensor(590.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15992292761802673
dqn reward tensor(722.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.6610e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11599524319171906
dqn reward tensor(766., device='cuda:0') e 0.05 loss_dqn tensor(6.0117e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07417528331279755
dqn reward tensor(694.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4398e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0685407817363739
dqn reward tensor(668.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3016e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1093713641166687
dqn reward tensor(888.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1211e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1132478341460228
dqn reward tensor(664.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8048e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11233420670032501
dqn reward tensor(987.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3777e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08967944979667664
dqn reward tensor(776.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3298e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26092422008514404
dqn reward tensor(780.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1764e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2253967523574829
dqn reward tensor(634.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0696e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35696327686309814
dqn reward tensor(711.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.3505e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14179295301437378
dqn reward tensor(837.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2611e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2690083980560303
dqn reward tensor(791.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4205e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04825691133737564
dqn reward tensor(632., device='cuda:0') e 0.05 loss_dqn tensor(4.3281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13699355721473694
dqn reward tensor(698.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2256e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2133115828037262
dqn reward tensor(920.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2153317630290985
dqn reward tensor(672.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1126e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09307104349136353
dqn reward tensor(864.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5685e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0778278335928917
dqn reward tensor(699.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7952e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05620017647743225
dqn reward tensor(771.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5821e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07774467021226883
dqn reward tensor(675.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10917004942893982
dqn reward tensor(712.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10381852090358734
dqn reward tensor(777.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03376122564077377
dqn reward tensor(595.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020268652588129044
dqn reward tensor(683.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4697e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19248142838478088
dqn reward tensor(737.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0148e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06219208985567093
dqn reward tensor(594.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8479e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17932410538196564
dqn reward tensor(705.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9649e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15706858038902283
dqn reward tensor(543.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.3038e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26574772596359253
dqn reward tensor(788., device='cuda:0') e 0.05 loss_dqn tensor(6.8629e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02418896183371544
dqn reward tensor(534.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7796e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12216837704181671
dqn reward tensor(716.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7048e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08531554043292999
dqn reward tensor(733.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1791e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.216305211186409
dqn reward tensor(666.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11665353924036026
dqn reward tensor(603.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7900e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10966368764638901
dqn reward tensor(690.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06531622260808945
dqn reward tensor(732.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6918e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12111169099807739
dqn reward tensor(853.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2256e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2591177523136139
dqn reward tensor(506., device='cuda:0') e 0.05 loss_dqn tensor(2.8686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18402358889579773
dqn reward tensor(844.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5579e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12360716611146927
dqn reward tensor(681.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3045e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2947794795036316
dqn reward tensor(773.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7345e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1738206148147583
dqn reward tensor(751.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7435e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0742710754275322
dqn reward tensor(704.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8436e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05560984089970589
dqn reward tensor(740.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1345498263835907
dqn reward tensor(619.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08563363552093506
dqn reward tensor(724.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.0120e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05263541638851166
dqn reward tensor(652.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4358e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16182857751846313
dqn reward tensor(787.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1694091111421585
dqn reward tensor(622.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3723e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16134481132030487
dqn reward tensor(795.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17065805196762085
dqn reward tensor(874.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8550e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18861337006092072
dqn reward tensor(694., device='cuda:0') e 0.05 loss_dqn tensor(8.0375e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1383005529642105
dqn reward tensor(551.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2065e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02329447492957115
dqn reward tensor(575.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1731e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21588248014450073
dqn reward tensor(769.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.6783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11077083647251129
dqn reward tensor(681.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7165e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09783119708299637
dqn reward tensor(705.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14960899949073792
dqn reward tensor(812.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09440980851650238
dqn reward tensor(698.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0826e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12536963820457458
dqn reward tensor(711.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1923978626728058
dqn reward tensor(804., device='cuda:0') e 0.05 loss_dqn tensor(1.7620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16408751904964447
dqn reward tensor(639.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7277e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023065969347953796
dqn reward tensor(670.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0776e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18946708738803864
dqn reward tensor(588.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3941e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13096433877944946
dqn reward tensor(759.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6400e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10602624714374542
dqn reward tensor(704.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3627e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025565026327967644
dqn reward tensor(683.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08032108843326569
dqn reward tensor(704.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.5428e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07966173440217972
dqn reward tensor(753.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.4900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10523399710655212
dqn reward tensor(621.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7251e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09654839336872101
dqn reward tensor(914.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1274026781320572
dqn reward tensor(645.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10754252225160599
dqn reward tensor(723., device='cuda:0') e 0.05 loss_dqn tensor(7.9651e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3943151831626892
dqn reward tensor(669.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.3553e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22087931632995605
dqn reward tensor(845., device='cuda:0') e 0.05 loss_dqn tensor(6.5623e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2194952666759491
dqn reward tensor(713.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6442e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37039151787757874
dqn reward tensor(740., device='cuda:0') e 0.05 loss_dqn tensor(2.2829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14811500906944275
dqn reward tensor(727.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8560e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06594090163707733
dqn reward tensor(708.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15607325732707977
dqn reward tensor(756.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.6225e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09204690158367157
dqn reward tensor(721.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0568e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3197058141231537
dqn reward tensor(763.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9253e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047099411487579346
dqn reward tensor(665.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05779852345585823
dqn reward tensor(837.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4769e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17638921737670898
dqn reward tensor(785.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1838e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19591021537780762
dqn reward tensor(629.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7910e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17623105645179749
dqn reward tensor(804., device='cuda:0') e 0.05 loss_dqn tensor(3.1734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06841781735420227
dqn reward tensor(705.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.9170e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1636602282524109
dqn reward tensor(698.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057969871908426285
dqn reward tensor(726.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0091e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15369686484336853
dqn reward tensor(738., device='cuda:0') e 0.05 loss_dqn tensor(7.1310e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14872586727142334
dqn reward tensor(799.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8397e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08142899721860886
dqn reward tensor(644.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06837053596973419
dqn reward tensor(781.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0326e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2728942930698395
dqn reward tensor(823.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6136e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23707613348960876
dqn reward tensor(832.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4595e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026988498866558075
dqn reward tensor(886.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2924e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08247783035039902
dqn reward tensor(712.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17136552929878235
dqn reward tensor(653.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9419e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059130746871232986
dqn reward tensor(920.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7333e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12560589611530304
dqn reward tensor(716.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4566e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16413873434066772
dqn reward tensor(732.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027016740292310715
dqn reward tensor(632.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07106488198041916
dqn reward tensor(786.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1138e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20507565140724182
dqn reward tensor(846.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7155e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05108633637428284
dqn reward tensor(736.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.072577565908432
dqn reward tensor(748.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15883558988571167
dqn reward tensor(851.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5778e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21555256843566895
dqn reward tensor(924.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04601184278726578
dqn reward tensor(846.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.8040e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1392812877893448
dqn reward tensor(719.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8915e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12015362083911896
dqn reward tensor(755., device='cuda:0') e 0.05 loss_dqn tensor(4.0341e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07509879022836685
dqn reward tensor(765.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9495e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02985677868127823
dqn reward tensor(871.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04017256200313568
dqn reward tensor(859.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4416e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16011491417884827
dqn reward tensor(663.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2591121792793274
dqn reward tensor(757., device='cuda:0') e 0.05 loss_dqn tensor(2.8561e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1858941912651062
dqn reward tensor(877.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9207e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2177305370569229
dqn reward tensor(874.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0837e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.066814124584198
dqn reward tensor(881.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1840e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09249237179756165
dqn reward tensor(780.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15140655636787415
dqn reward tensor(914.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3590e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12419497966766357
dqn reward tensor(877.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0752e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04159831628203392
dqn reward tensor(798.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20560911297798157
dqn reward tensor(573.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13989238440990448
dqn reward tensor(803., device='cuda:0') e 0.05 loss_dqn tensor(2.2740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16676709055900574
dqn reward tensor(774.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19701582193374634
dqn reward tensor(932., device='cuda:0') e 0.05 loss_dqn tensor(9.3310e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11657655239105225
dqn reward tensor(798., device='cuda:0') e 0.05 loss_dqn tensor(1.5445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10011337697505951
dqn reward tensor(856.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8330e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1486414521932602
dqn reward tensor(750.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1836167275905609
dqn reward tensor(770.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08629712462425232
dqn reward tensor(532.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11039029061794281
dqn reward tensor(708.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17987790703773499
dqn reward tensor(894.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1453e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03546219319105148
dqn reward tensor(892.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3185e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060683123767375946
dqn reward tensor(892.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8731e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23548798263072968
dqn reward tensor(906., device='cuda:0') e 0.05 loss_dqn tensor(9.3265e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11246620118618011
dqn reward tensor(874.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6854e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058021582663059235
dqn reward tensor(749.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4146e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09458129853010178
dqn reward tensor(682.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17390809953212738
dqn reward tensor(911.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02887004055082798
dqn reward tensor(721.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.3162e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13834677636623383
dqn reward tensor(859., device='cuda:0') e 0.05 loss_dqn tensor(1.0423e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07834547758102417
dqn reward tensor(804.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0735136866569519
dqn reward tensor(751.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2933e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14399416744709015
dqn reward tensor(855.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6284e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09330113232135773
dqn reward tensor(814.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5018e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03648268058896065
dqn reward tensor(730.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0525e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015890074893832207
dqn reward tensor(732.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0817e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13426172733306885
dqn reward tensor(638.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3056e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12226498126983643
dqn reward tensor(996.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9570e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05771737918257713
dqn reward tensor(660.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6774e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05798051878809929
dqn reward tensor(658., device='cuda:0') e 0.05 loss_dqn tensor(3.4952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1642257571220398
dqn reward tensor(696.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0777149423956871
dqn reward tensor(841., device='cuda:0') e 0.05 loss_dqn tensor(2.9341e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08646244555711746
dqn reward tensor(590.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4495e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20655012130737305
dqn reward tensor(819.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0925e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29398098587989807
dqn reward tensor(703.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2777e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10080191493034363
dqn reward tensor(751.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17960864305496216
dqn reward tensor(863.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0438e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.137384295463562
dqn reward tensor(946., device='cuda:0') e 0.05 loss_dqn tensor(3.0211e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24060189723968506
dqn reward tensor(762.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.6481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0523650236427784
dqn reward tensor(688.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3083e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05938720703125
dqn reward tensor(889.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03575380891561508
dqn reward tensor(773.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13384336233139038
dqn reward tensor(826.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08494208008050919
dqn reward tensor(879.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8786e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06036467105150223
dqn reward tensor(708.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12601661682128906
dqn reward tensor(958., device='cuda:0') e 0.05 loss_dqn tensor(1.0103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06058173626661301
dqn reward tensor(813.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25391456484794617
dqn reward tensor(629.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3676e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10528594255447388
dqn reward tensor(813.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13375359773635864
dqn reward tensor(505.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15513142943382263
dqn reward tensor(742.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19951002299785614
dqn reward tensor(751.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6450e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03644147515296936
dqn reward tensor(918.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0710657387971878
dqn reward tensor(940.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6252e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2922716736793518
dqn reward tensor(779.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.6426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13963693380355835
dqn reward tensor(642., device='cuda:0') e 0.05 loss_dqn tensor(2.0076e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1533982753753662
dqn reward tensor(895., device='cuda:0') e 0.05 loss_dqn tensor(2.3388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12496379762887955
dqn reward tensor(804.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6396e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19331294298171997
dqn reward tensor(759.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0578911229968071
dqn reward tensor(797.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0048e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13850528001785278
dqn reward tensor(853., device='cuda:0') e 0.05 loss_dqn tensor(8.9832e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21900665760040283
dqn reward tensor(925.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2127e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10761682689189911
dqn reward tensor(875.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5449e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12331464141607285
dqn reward tensor(735.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.356601744890213
dqn reward tensor(817.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1098397746682167
dqn reward tensor(893.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0161e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2412017285823822
dqn reward tensor(804.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11398130655288696
dqn reward tensor(650.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9891e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10517054051160812
dqn reward tensor(559.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.2012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10974392294883728
dqn reward tensor(836.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15648335218429565
dqn reward tensor(725.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1406400203704834
dqn reward tensor(741.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05777238309383392
dqn reward tensor(577.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4423e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0875818133354187
dqn reward tensor(692.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062088243663311005
dqn reward tensor(681.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0063e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30509430170059204
dqn reward tensor(798.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018964556977152824
dqn reward tensor(89.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9486e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 1.0531641244888306
Evaluating...
Train: {'rocauc': 0.7961290856353851} 12.423044204711914
=====Epoch 29=====
Training...
dqn reward tensor(822., device='cuda:0') e 0.05 loss_dqn tensor(2.4192e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08452848345041275
dqn reward tensor(723.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4051e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09142313152551651
dqn reward tensor(716.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.3966e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11949163675308228
dqn reward tensor(837.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.159408837556839
dqn reward tensor(803.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8112e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0995500385761261
dqn reward tensor(760.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7388e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13290917873382568
dqn reward tensor(846.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6356e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12277299910783768
dqn reward tensor(783.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06188268959522247
dqn reward tensor(617.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9643e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12169717997312546
dqn reward tensor(766.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12251044809818268
dqn reward tensor(854.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3764e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06871065497398376
dqn reward tensor(925.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4761e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046663813292980194
dqn reward tensor(894.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09146243333816528
dqn reward tensor(855.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4669e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10510443150997162
dqn reward tensor(844.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2960e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13430127501487732
dqn reward tensor(871.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9571e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033204980194568634
dqn reward tensor(772., device='cuda:0') e 0.05 loss_dqn tensor(1.5077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11965551972389221
dqn reward tensor(940.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4146e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013148372992873192
dqn reward tensor(860.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21883711218833923
dqn reward tensor(602.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25881028175354004
dqn reward tensor(736.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5076e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01749425195157528
dqn reward tensor(791.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9211e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3116615414619446
dqn reward tensor(757.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.6495e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37675172090530396
dqn reward tensor(676.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22133585810661316
dqn reward tensor(880.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5755e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018163206055760384
dqn reward tensor(655.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7604e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11916176974773407
dqn reward tensor(937.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7658e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16701841354370117
dqn reward tensor(765.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1576e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2599552869796753
dqn reward tensor(967., device='cuda:0') e 0.05 loss_dqn tensor(8.6122e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08987073600292206
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9061e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05232907831668854
dqn reward tensor(825.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17587780952453613
dqn reward tensor(833.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.7139e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05899398773908615
dqn reward tensor(729.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10036812722682953
dqn reward tensor(790.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5304e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15136906504631042
dqn reward tensor(715.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2685e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07129064202308655
dqn reward tensor(830.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06823089718818665
dqn reward tensor(908.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9980e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12912163138389587
dqn reward tensor(793.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03474836051464081
dqn reward tensor(843.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3171e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10678443312644958
dqn reward tensor(791.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9302e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1280941367149353
dqn reward tensor(881.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08041323721408844
dqn reward tensor(684.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019269373267889023
dqn reward tensor(799.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9078e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06883205473423004
dqn reward tensor(853.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0739390030503273
dqn reward tensor(868.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20343518257141113
dqn reward tensor(865., device='cuda:0') e 0.05 loss_dqn tensor(2.6964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24050283432006836
dqn reward tensor(897.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9319e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10975693166255951
dqn reward tensor(743.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15917696058750153
dqn reward tensor(773.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6118e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12958982586860657
dqn reward tensor(860., device='cuda:0') e 0.05 loss_dqn tensor(7.2349e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12427717447280884
dqn reward tensor(620.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0126e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15943343937397003
dqn reward tensor(739.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0812268853187561
dqn reward tensor(871.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11700421571731567
dqn reward tensor(851.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.7998e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11522714793682098
dqn reward tensor(799.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5296e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24503420293331146
dqn reward tensor(860.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1980e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16842922568321228
dqn reward tensor(702., device='cuda:0') e 0.05 loss_dqn tensor(1.0095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14804957807064056
dqn reward tensor(880., device='cuda:0') e 0.05 loss_dqn tensor(9.0743e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15431928634643555
dqn reward tensor(813.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4633e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10799723863601685
dqn reward tensor(804.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.5003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042757779359817505
dqn reward tensor(838.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6044e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07867588102817535
dqn reward tensor(788.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23868268728256226
dqn reward tensor(700.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4037e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17532223463058472
dqn reward tensor(757.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9555e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13966356217861176
dqn reward tensor(682.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19233810901641846
dqn reward tensor(826.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4554e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07922224700450897
dqn reward tensor(607.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10203738510608673
dqn reward tensor(804.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.153998464345932
dqn reward tensor(824.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4119e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02996079996228218
dqn reward tensor(506.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.115311399102211
dqn reward tensor(679.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1766069382429123
dqn reward tensor(594.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4201e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2802976965904236
dqn reward tensor(685.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09514592587947845
dqn reward tensor(793.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5871e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0480654276907444
dqn reward tensor(864.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4360e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09240438789129257
dqn reward tensor(926.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07837522774934769
dqn reward tensor(963.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8090e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11764203011989594
dqn reward tensor(779.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16922535002231598
dqn reward tensor(995., device='cuda:0') e 0.05 loss_dqn tensor(8.9384e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2140980064868927
dqn reward tensor(689.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.5744e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13394585251808167
dqn reward tensor(743., device='cuda:0') e 0.05 loss_dqn tensor(1.1266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06123571842908859
dqn reward tensor(715.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17489850521087646
dqn reward tensor(604.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08003800362348557
dqn reward tensor(801.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17401738464832306
dqn reward tensor(760.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023896217346191406
dqn reward tensor(742.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06710338592529297
dqn reward tensor(754.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0087e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08840964734554291
dqn reward tensor(829.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.5898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19307446479797363
dqn reward tensor(842.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0034e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.158761665225029
dqn reward tensor(648.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17010892927646637
dqn reward tensor(622.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.218374103307724
dqn reward tensor(760.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07995527237653732
dqn reward tensor(858.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1698e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18687327206134796
dqn reward tensor(776.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19669489562511444
dqn reward tensor(818.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0171e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.164143368601799
dqn reward tensor(562.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05034486949443817
dqn reward tensor(663.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15475311875343323
dqn reward tensor(816.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4506e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04258587583899498
dqn reward tensor(785.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15405285358428955
dqn reward tensor(871.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039201393723487854
dqn reward tensor(900.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0982172042131424
dqn reward tensor(842.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1558155119419098
dqn reward tensor(806.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.0121e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12646174430847168
dqn reward tensor(602.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20145556330680847
dqn reward tensor(767.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.274783194065094
dqn reward tensor(889.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1222442016005516
dqn reward tensor(828.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04291282966732979
dqn reward tensor(668.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09884108603000641
dqn reward tensor(972.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0563e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08020705729722977
dqn reward tensor(740.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3511e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11805353313684464
dqn reward tensor(715.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1932189166545868
dqn reward tensor(520.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5466e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03154022991657257
dqn reward tensor(813.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13212183117866516
dqn reward tensor(804.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09779079258441925
dqn reward tensor(881.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6733e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20023910701274872
dqn reward tensor(782.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0775068998336792
dqn reward tensor(693.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8304e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0785033255815506
dqn reward tensor(652.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03657548129558563
dqn reward tensor(860.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23964138329029083
dqn reward tensor(735.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17414294183254242
dqn reward tensor(813.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4080e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20794986188411713
dqn reward tensor(643.8125, device='cuda:0') e 0.05 loss_dqn tensor(5.5152e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09752847999334335
dqn reward tensor(797.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0950e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17036844789981842
dqn reward tensor(717.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27159827947616577
dqn reward tensor(753.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9054e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10120932757854462
dqn reward tensor(785.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06825803965330124
dqn reward tensor(872.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048161204904317856
dqn reward tensor(909., device='cuda:0') e 0.05 loss_dqn tensor(8.8273e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08844304084777832
dqn reward tensor(880., device='cuda:0') e 0.05 loss_dqn tensor(1.3986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05514061823487282
dqn reward tensor(724.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035667311400175095
dqn reward tensor(763.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.4988e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07801943272352219
dqn reward tensor(840.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3501e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07573791593313217
dqn reward tensor(949.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1363e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033787257969379425
dqn reward tensor(855.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9635e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040989890694618225
dqn reward tensor(726.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6780e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35077449679374695
dqn reward tensor(829.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1288522332906723
dqn reward tensor(755.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12592346966266632
dqn reward tensor(914.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1698753386735916
dqn reward tensor(930.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0312e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07096622884273529
dqn reward tensor(631.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12739336490631104
dqn reward tensor(632.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.6469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1203245297074318
dqn reward tensor(768.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1065e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18528541922569275
dqn reward tensor(895.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8712e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20661769807338715
dqn reward tensor(792.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08861411362886429
dqn reward tensor(888.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07485008239746094
dqn reward tensor(794.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09709526598453522
dqn reward tensor(593.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1342487633228302
dqn reward tensor(835.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8367e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10492159426212311
dqn reward tensor(816.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09769980609416962
dqn reward tensor(662.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10618745535612106
dqn reward tensor(773.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5618e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07284458726644516
dqn reward tensor(709.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2272e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1062224954366684
dqn reward tensor(672.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5692e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12675844132900238
dqn reward tensor(768.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6039e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16441835463047028
dqn reward tensor(700.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9368e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.219306081533432
dqn reward tensor(781.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7028e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09086984395980835
dqn reward tensor(669.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.0569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0729062631726265
dqn reward tensor(714.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03420228883624077
dqn reward tensor(839.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3308e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19566640257835388
dqn reward tensor(854.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1943497657775879
dqn reward tensor(847., device='cuda:0') e 0.05 loss_dqn tensor(9.1540e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08821950852870941
dqn reward tensor(678., device='cuda:0') e 0.05 loss_dqn tensor(3.6610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05512508377432823
dqn reward tensor(790.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21808700263500214
dqn reward tensor(680.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17341279983520508
dqn reward tensor(631.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26857709884643555
dqn reward tensor(866.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.7043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06003915145993233
dqn reward tensor(682.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3466e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04016519710421562
dqn reward tensor(773.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6233e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2039453685283661
dqn reward tensor(730., device='cuda:0') e 0.05 loss_dqn tensor(8.9284e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08050061762332916
dqn reward tensor(729.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2312214970588684
dqn reward tensor(757.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2633023262023926
dqn reward tensor(896.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6546e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18768800795078278
dqn reward tensor(773.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14654351770877838
dqn reward tensor(893.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0285e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07299826294183731
dqn reward tensor(740.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7471e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11668525636196136
dqn reward tensor(781., device='cuda:0') e 0.05 loss_dqn tensor(3.0677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08367380499839783
dqn reward tensor(599.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18792825937271118
dqn reward tensor(908.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.193187415599823
dqn reward tensor(986.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0213e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16889500617980957
dqn reward tensor(563.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2772e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19088003039360046
dqn reward tensor(737.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15621472895145416
dqn reward tensor(654.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09626312553882599
dqn reward tensor(810.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.4036e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1015653982758522
dqn reward tensor(611.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06639505922794342
dqn reward tensor(775.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1266e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18002580106258392
dqn reward tensor(774.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4561e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09955833852291107
dqn reward tensor(763.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.8234e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0985095202922821
dqn reward tensor(892.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0996e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17863450944423676
dqn reward tensor(827.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5970e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18731659650802612
dqn reward tensor(783.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6115e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1871703714132309
dqn reward tensor(733., device='cuda:0') e 0.05 loss_dqn tensor(2.6128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14641477167606354
dqn reward tensor(782.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27936556935310364
dqn reward tensor(718.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9263e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16929133236408234
dqn reward tensor(785.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0709e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14701148867607117
dqn reward tensor(872.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0076e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2137427181005478
dqn reward tensor(655.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5026e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15714073181152344
dqn reward tensor(844., device='cuda:0') e 0.05 loss_dqn tensor(1.4930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374015510082245
dqn reward tensor(831.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2404e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0973336473107338
dqn reward tensor(980.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9927e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10788780450820923
dqn reward tensor(773.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14038239419460297
dqn reward tensor(818.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2125e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1661050170660019
dqn reward tensor(538., device='cuda:0') e 0.05 loss_dqn tensor(8.4130e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06784999370574951
dqn reward tensor(799.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4895e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08615945279598236
dqn reward tensor(784.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1124e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09253648668527603
dqn reward tensor(811., device='cuda:0') e 0.05 loss_dqn tensor(1.2939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14427825808525085
dqn reward tensor(754.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8382e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12147380411624908
dqn reward tensor(905.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1588e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07941383123397827
dqn reward tensor(944.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2448e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1075930967926979
dqn reward tensor(892., device='cuda:0') e 0.05 loss_dqn tensor(8.7413e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29392099380493164
dqn reward tensor(713., device='cuda:0') e 0.05 loss_dqn tensor(1.9347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1294991672039032
dqn reward tensor(634.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35836517810821533
dqn reward tensor(882.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2524740695953369
dqn reward tensor(908., device='cuda:0') e 0.05 loss_dqn tensor(9.8818e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11327396333217621
dqn reward tensor(754.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0609470009803772
dqn reward tensor(897.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0145e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16529792547225952
dqn reward tensor(851.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4614e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11766552925109863
dqn reward tensor(775.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2757393419742584
dqn reward tensor(981.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1882e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05708646774291992
dqn reward tensor(767.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3909e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1464240849018097
dqn reward tensor(900.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3872e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24176229536533356
dqn reward tensor(904.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0962e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1193622350692749
dqn reward tensor(880.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16498029232025146
dqn reward tensor(786.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10109969973564148
dqn reward tensor(641., device='cuda:0') e 0.05 loss_dqn tensor(9.9610e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05579201504588127
dqn reward tensor(884., device='cuda:0') e 0.05 loss_dqn tensor(1.5166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13644033670425415
dqn reward tensor(477.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18074508011341095
dqn reward tensor(637.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0258e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12524005770683289
dqn reward tensor(713., device='cuda:0') e 0.05 loss_dqn tensor(2.2518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14608916640281677
dqn reward tensor(736.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9819e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08658362925052643
dqn reward tensor(714., device='cuda:0') e 0.05 loss_dqn tensor(2.9888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03690920025110245
dqn reward tensor(505.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0207e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15901821851730347
dqn reward tensor(827.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4576e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19657719135284424
dqn reward tensor(864.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5947e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16351670026779175
dqn reward tensor(736., device='cuda:0') e 0.05 loss_dqn tensor(2.0393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09715161472558975
dqn reward tensor(844.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24154070019721985
dqn reward tensor(825.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1084e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056706443428993225
dqn reward tensor(819., device='cuda:0') e 0.05 loss_dqn tensor(1.0286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23986244201660156
dqn reward tensor(835.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2455614060163498
dqn reward tensor(605.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6626e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1332242488861084
dqn reward tensor(763.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0967e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09310012310743332
dqn reward tensor(841.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2252511978149414
dqn reward tensor(705.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1778220385313034
dqn reward tensor(664.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7394e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09923592954874039
dqn reward tensor(821.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0004e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05832803249359131
dqn reward tensor(707.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048685189336538315
dqn reward tensor(611.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.8057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16135311126708984
dqn reward tensor(825.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0994047001004219
dqn reward tensor(848.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22982512414455414
dqn reward tensor(716.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.1833e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13035522401332855
dqn reward tensor(638.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1664009988307953
dqn reward tensor(880.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03979499638080597
dqn reward tensor(712.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23087078332901
dqn reward tensor(937.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05122581869363785
dqn reward tensor(655.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26412153244018555
dqn reward tensor(782.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4374e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10781445354223251
dqn reward tensor(894.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5542e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0730963796377182
dqn reward tensor(643.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6039e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16336044669151306
dqn reward tensor(571.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3652e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11356385052204132
dqn reward tensor(735.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3187111020088196
dqn reward tensor(776.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2132018804550171
dqn reward tensor(902., device='cuda:0') e 0.05 loss_dqn tensor(9.8489e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553040564060211
dqn reward tensor(816.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24875782430171967
dqn reward tensor(766.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08656668663024902
dqn reward tensor(794.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03203045576810837
dqn reward tensor(839.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05559220910072327
dqn reward tensor(711.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.5001e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19974267482757568
dqn reward tensor(906.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9739e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2752648591995239
dqn reward tensor(897.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.5067e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09400951117277145
dqn reward tensor(961.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03256447613239288
dqn reward tensor(834.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0403e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051934562623500824
dqn reward tensor(643.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.6310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17124098539352417
dqn reward tensor(892.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0571e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16422227025032043
dqn reward tensor(697., device='cuda:0') e 0.05 loss_dqn tensor(3.2916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22896529734134674
dqn reward tensor(742.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.7316e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06526248157024384
dqn reward tensor(503.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14475160837173462
dqn reward tensor(784.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4532e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18116620182991028
dqn reward tensor(707.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15215928852558136
dqn reward tensor(810.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18948136270046234
dqn reward tensor(821.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05775364488363266
dqn reward tensor(768.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16563518345355988
dqn reward tensor(876.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06347827613353729
dqn reward tensor(689.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04855909198522568
dqn reward tensor(884.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09215942025184631
dqn reward tensor(978., device='cuda:0') e 0.05 loss_dqn tensor(1.0182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09697210788726807
dqn reward tensor(741.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.2666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05676959082484245
dqn reward tensor(605.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.1700e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10021032392978668
dqn reward tensor(618.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04421178624033928
dqn reward tensor(601.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8964e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10641532391309738
dqn reward tensor(826.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3953e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18754532933235168
dqn reward tensor(726.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1169392317533493
dqn reward tensor(720., device='cuda:0') e 0.05 loss_dqn tensor(2.2274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051614515483379364
dqn reward tensor(583.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12917287647724152
dqn reward tensor(703.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.5044e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033550627529621124
dqn reward tensor(808.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9391e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2026306390762329
dqn reward tensor(686.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.7631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09071415662765503
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6009e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06553006172180176
dqn reward tensor(794.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19936653971672058
dqn reward tensor(701.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04019137844443321
dqn reward tensor(632.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1364823281764984
dqn reward tensor(663., device='cuda:0') e 0.05 loss_dqn tensor(1.8811e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0810612365603447
dqn reward tensor(826.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021269990131258965
dqn reward tensor(743.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027339525520801544
dqn reward tensor(811.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5708e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10074783861637115
dqn reward tensor(804.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2104344666004181
dqn reward tensor(777.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08820265531539917
dqn reward tensor(771.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0432e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10412382334470749
dqn reward tensor(707.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14613762497901917
dqn reward tensor(758.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03935742378234863
dqn reward tensor(754.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7300e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047665007412433624
dqn reward tensor(910.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13845029473304749
dqn reward tensor(865.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6090e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045421093702316284
dqn reward tensor(810.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0554e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27377834916114807
dqn reward tensor(949., device='cuda:0') e 0.05 loss_dqn tensor(1.0559e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0987437516450882
dqn reward tensor(685.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02414405345916748
dqn reward tensor(640., device='cuda:0') e 0.05 loss_dqn tensor(4.1970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09891350567340851
dqn reward tensor(727.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4171e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04750559478998184
dqn reward tensor(923.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3195e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09204546362161636
dqn reward tensor(856.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18735834956169128
dqn reward tensor(700.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1585e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04025262966752052
dqn reward tensor(825.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7372e+11, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05152261257171631
dqn reward tensor(693.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9263e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11521227657794952
dqn reward tensor(532.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1316e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16487038135528564
dqn reward tensor(691.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2675231397151947
dqn reward tensor(644.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10585152357816696
dqn reward tensor(552.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.0625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03035622090101242
dqn reward tensor(742.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5582e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15066653490066528
dqn reward tensor(725.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20820897817611694
dqn reward tensor(851.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2324e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062012724578380585
dqn reward tensor(754.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019378265365958214
dqn reward tensor(853.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1738e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1555662453174591
dqn reward tensor(890.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4722e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1473265290260315
dqn reward tensor(837.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08545073866844177
dqn reward tensor(740.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025266272947192192
dqn reward tensor(770.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1593170315027237
dqn reward tensor(643.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10569385439157486
dqn reward tensor(727., device='cuda:0') e 0.05 loss_dqn tensor(6.8108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059927262365818024
dqn reward tensor(936.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1659e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08834461122751236
dqn reward tensor(727.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13320939242839813
dqn reward tensor(812.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042358554899692535
dqn reward tensor(677.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3495e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018943287432193756
dqn reward tensor(787.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07809177041053772
dqn reward tensor(869.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2523e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11372186243534088
dqn reward tensor(660.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20933887362480164
dqn reward tensor(643.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14449679851531982
dqn reward tensor(606.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.0491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21107923984527588
dqn reward tensor(574.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24252276122570038
dqn reward tensor(487.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9945e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1997249275445938
dqn reward tensor(526.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0703718289732933
dqn reward tensor(485.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13210488855838776
dqn reward tensor(323.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.0196e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26045772433280945
dqn reward tensor(288., device='cuda:0') e 0.05 loss_dqn tensor(1.1263e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15324968099594116
dqn reward tensor(279.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0413e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11939091980457306
dqn reward tensor(231.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.0568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14598071575164795
dqn reward tensor(482.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11230958998203278
dqn reward tensor(186.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3110e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21423813700675964
dqn reward tensor(308.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16810542345046997
dqn reward tensor(137.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9323e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2258375883102417
dqn reward tensor(152.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0629e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30823609232902527
dqn reward tensor(129.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23781894147396088
dqn reward tensor(241.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8559e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16887016594409943
dqn reward tensor(268.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1460387408733368
dqn reward tensor(54.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09838399291038513
dqn reward tensor(130.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12890149652957916
dqn reward tensor(53.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.2393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19984892010688782
dqn reward tensor(303.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058406904339790344
dqn reward tensor(85., device='cuda:0') e 0.05 loss_dqn tensor(5.8024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22624054551124573
dqn reward tensor(209., device='cuda:0') e 0.05 loss_dqn tensor(6.1396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03084634244441986
dqn reward tensor(119.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020790886133909225
dqn reward tensor(110.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2814318835735321
dqn reward tensor(126.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017434529960155487
dqn reward tensor(100., device='cuda:0') e 0.05 loss_dqn tensor(1.0249e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013614309951663017
dqn reward tensor(35.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2890774607658386
dqn reward tensor(7.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.2052e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04558659717440605
dqn reward tensor(83.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8177e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27835482358932495
dqn reward tensor(120.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07211863249540329
dqn reward tensor(122.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0814e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20078405737876892
dqn reward tensor(47.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.6112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08274153620004654
dqn reward tensor(24.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07057523727416992
dqn reward tensor(10.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14130473136901855
dqn reward tensor(-135.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03484707325696945
dqn reward tensor(85.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1020e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1626874953508377
dqn reward tensor(154.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1334470510482788
dqn reward tensor(128., device='cuda:0') e 0.05 loss_dqn tensor(9.3991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23124265670776367
dqn reward tensor(62.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8753e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25359851121902466
dqn reward tensor(-102.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09534283727407455
dqn reward tensor(19.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6833e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09513163566589355
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21260012686252594
dqn reward tensor(56., device='cuda:0') e 0.05 loss_dqn tensor(7.5484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12928757071495056
dqn reward tensor(60.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17040663957595825
dqn reward tensor(44.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15179570019245148
dqn reward tensor(-21.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.3203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17054787278175354
dqn reward tensor(153.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06190323084592819
dqn reward tensor(-50.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13647577166557312
dqn reward tensor(9.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.6284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23572497069835663
dqn reward tensor(99.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09753653407096863
dqn reward tensor(17.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4672e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17397525906562805
dqn reward tensor(183.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2860e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06204446405172348
dqn reward tensor(72.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16104383766651154
dqn reward tensor(170.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16463658213615417
dqn reward tensor(89., device='cuda:0') e 0.05 loss_dqn tensor(7.2560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08158113062381744
dqn reward tensor(-86.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12550567090511322
dqn reward tensor(66.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2276684194803238
dqn reward tensor(35.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6642e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17940665781497955
dqn reward tensor(-57.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.0165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09347201138734818
dqn reward tensor(32.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18054217100143433
dqn reward tensor(-175.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17592450976371765
dqn reward tensor(55.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18525344133377075
dqn reward tensor(122.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056127529591321945
dqn reward tensor(203.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0026e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12414626777172089
dqn reward tensor(-94.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11180660873651505
dqn reward tensor(-157.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09476344287395477
dqn reward tensor(37., device='cuda:0') e 0.05 loss_dqn tensor(7.1386e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03475676476955414
dqn reward tensor(103.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19474691152572632
dqn reward tensor(124.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08448024094104767
dqn reward tensor(15.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14361470937728882
dqn reward tensor(114.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18081849813461304
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6589e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12040171027183533
dqn reward tensor(75., device='cuda:0') e 0.05 loss_dqn tensor(8.6301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13601179420948029
dqn reward tensor(48.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6514e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14689335227012634
dqn reward tensor(185., device='cuda:0') e 0.05 loss_dqn tensor(6.6369e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0672268271446228
dqn reward tensor(179.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10899963974952698
dqn reward tensor(86.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21925778687000275
dqn reward tensor(40.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2356e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1354559063911438
dqn reward tensor(18.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2896e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09062757343053818
dqn reward tensor(30.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7019e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12135222554206848
dqn reward tensor(127., device='cuda:0') e 0.05 loss_dqn tensor(6.8909e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0909690335392952
dqn reward tensor(45.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2583296000957489
dqn reward tensor(118.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10037761926651001
dqn reward tensor(196.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0638e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08656968176364899
dqn reward tensor(241.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09815239161252975
dqn reward tensor(47.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6907e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23822787404060364
dqn reward tensor(-50.6875, device='cuda:0') e 0.05 loss_dqn tensor(7.2255e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05305557698011398
dqn reward tensor(-9.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.0958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09016592800617218
dqn reward tensor(121.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05164891481399536
dqn reward tensor(-32.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13879209756851196
dqn reward tensor(120.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1494e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09132736176252365
dqn reward tensor(87.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1748e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10884296894073486
dqn reward tensor(114.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2643175423145294
dqn reward tensor(24.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2674883306026459
dqn reward tensor(80.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.9161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0839570090174675
dqn reward tensor(-83.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6687e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09316462278366089
dqn reward tensor(93.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1438383311033249
dqn reward tensor(-109.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13761299848556519
dqn reward tensor(-11.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1324e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18421527743339539
dqn reward tensor(0.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08175967633724213
dqn reward tensor(82.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17348825931549072
dqn reward tensor(164.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.5982e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22661030292510986
dqn reward tensor(3.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15023711323738098
dqn reward tensor(99.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.2201e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13042473793029785
dqn reward tensor(172.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.1603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11689528822898865
dqn reward tensor(-34.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.7548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14712010324001312
dqn reward tensor(-52.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0926e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24681587517261505
dqn reward tensor(9.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09743824601173401
dqn reward tensor(-51.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2716e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12102390080690384
dqn reward tensor(52.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10134924948215485
dqn reward tensor(-43.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05872306227684021
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9700e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14407853782176971
dqn reward tensor(54.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3385e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2564569115638733
dqn reward tensor(50., device='cuda:0') e 0.05 loss_dqn tensor(8.5779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2023378312587738
dqn reward tensor(81.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1194383054971695
dqn reward tensor(36.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7252e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08505379408597946
dqn reward tensor(139.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2826e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05381111428141594
dqn reward tensor(62.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0466e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12396836280822754
dqn reward tensor(98.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.4474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13872389495372772
dqn reward tensor(21.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.39224985241889954
dqn reward tensor(-39.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2413453757762909
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5725e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09476566314697266
dqn reward tensor(143.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0017e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23781564831733704
dqn reward tensor(6.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3832e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3443209230899811
dqn reward tensor(80.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4228e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15856999158859253
dqn reward tensor(184.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3412e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11867763102054596
dqn reward tensor(157.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1238856390118599
dqn reward tensor(165.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07091778516769409
dqn reward tensor(93.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3601e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1343592405319214
dqn reward tensor(-93.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16233208775520325
dqn reward tensor(170.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0796e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10463206470012665
dqn reward tensor(45.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23072549700737
dqn reward tensor(115.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21474307775497437
dqn reward tensor(164.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.4057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20551364123821259
dqn reward tensor(148.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16352039575576782
dqn reward tensor(19.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.4798e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.088480144739151
dqn reward tensor(84.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1609e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06200164556503296
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(1.0278e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02515266090631485
dqn reward tensor(86.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15961158275604248
dqn reward tensor(-11.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7055e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08906664699316025
dqn reward tensor(-71.6875, device='cuda:0') e 0.05 loss_dqn tensor(7.6396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09617415070533752
dqn reward tensor(92.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5036e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13304170966148376
dqn reward tensor(102.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1444557011127472
dqn reward tensor(95.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6962e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09214162826538086
dqn reward tensor(23.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18810391426086426
dqn reward tensor(-22.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18439489603042603
dqn reward tensor(27.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0069e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20110367238521576
dqn reward tensor(100.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18389660120010376
dqn reward tensor(12.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1642540991306305
dqn reward tensor(31.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1910628378391266
dqn reward tensor(76.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.9611e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13333848118782043
dqn reward tensor(127.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07699055224657059
dqn reward tensor(45., device='cuda:0') e 0.05 loss_dqn tensor(9.6981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13941743969917297
dqn reward tensor(-44.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1838078498840332
dqn reward tensor(35.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1312810182571411
dqn reward tensor(137.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2535666823387146
dqn reward tensor(169.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1309126615524292
dqn reward tensor(207.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0103e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04707638919353485
dqn reward tensor(45.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1731448769569397
dqn reward tensor(-15.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2590613067150116
dqn reward tensor(120.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.5420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10182241350412369
dqn reward tensor(97.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0325e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28303688764572144
dqn reward tensor(193.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3351e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10734568536281586
dqn reward tensor(90.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11388230323791504
dqn reward tensor(94.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6450e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10775414109230042
dqn reward tensor(154.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7192e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1648385226726532
dqn reward tensor(86.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0077e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06646519899368286
dqn reward tensor(-8.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2458e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0594535693526268
dqn reward tensor(-13., device='cuda:0') e 0.05 loss_dqn tensor(7.7969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06853677332401276
dqn reward tensor(23.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03117331676185131
Evaluating...
Train: {'rocauc': 0.7761395341136177} 0.8630965352058411
=====Epoch 30=====
Training...
dqn reward tensor(68.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9192e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3203897476196289
dqn reward tensor(224.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0660e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06508398056030273
dqn reward tensor(82.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.8870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09940074384212494
dqn reward tensor(-42., device='cuda:0') e 0.05 loss_dqn tensor(9.1336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12148846685886383
dqn reward tensor(-86., device='cuda:0') e 0.05 loss_dqn tensor(8.4439e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05151638016104698
dqn reward tensor(62.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5143e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1393951028585434
dqn reward tensor(51.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13938458263874054
dqn reward tensor(152., device='cuda:0') e 0.05 loss_dqn tensor(1.0500e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16976724565029144
dqn reward tensor(83.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1105053722858429
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027660224586725235
dqn reward tensor(57.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11808185279369354
dqn reward tensor(38.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023705992847681046
dqn reward tensor(167., device='cuda:0') e 0.05 loss_dqn tensor(7.7996e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07592643797397614
dqn reward tensor(15.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0601530596613884
dqn reward tensor(191.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0110e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14259418845176697
dqn reward tensor(63.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1335e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08788011968135834
dqn reward tensor(68.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.8290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23184643685817719
dqn reward tensor(-14.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8929e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22002777457237244
dqn reward tensor(31.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9422e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056658849120140076
dqn reward tensor(94.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5320e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15024830400943756
dqn reward tensor(53.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0828e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18232910335063934
dqn reward tensor(113.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4055e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035644546151161194
dqn reward tensor(66.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04037988930940628
dqn reward tensor(-13.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19474831223487854
dqn reward tensor(236.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0166e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17532558739185333
dqn reward tensor(56., device='cuda:0') e 0.05 loss_dqn tensor(8.4240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10235871374607086
dqn reward tensor(134.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06747429072856903
dqn reward tensor(88.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1401001214981079
dqn reward tensor(-110.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.6006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22457972168922424
dqn reward tensor(-9.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0813e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22507256269454956
dqn reward tensor(-155.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0576e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08563978970050812
dqn reward tensor(-8.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2982e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1284940242767334
dqn reward tensor(101.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0866e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0762266293168068
dqn reward tensor(5.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08135949075222015
dqn reward tensor(123.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12786585092544556
dqn reward tensor(46.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0089e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031631484627723694
dqn reward tensor(-97.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0886e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.235200896859169
dqn reward tensor(77.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2011e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10185261815786362
dqn reward tensor(-21.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1455e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15710681676864624
dqn reward tensor(40.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0335160456597805
dqn reward tensor(204.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9533e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17112912237644196
dqn reward tensor(-51.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13512080907821655
dqn reward tensor(72.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.2207e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2883068025112152
dqn reward tensor(130.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.8498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09814011305570602
dqn reward tensor(-33.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1316e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17883116006851196
dqn reward tensor(-157.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.4487e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21654359996318817
dqn reward tensor(-5.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12138190865516663
dqn reward tensor(-54.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09684358537197113
dqn reward tensor(125.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1646e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09182170033454895
dqn reward tensor(109.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17622524499893188
dqn reward tensor(70.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045841965824365616
dqn reward tensor(5., device='cuda:0') e 0.05 loss_dqn tensor(9.1413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04261631518602371
dqn reward tensor(-0.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.3332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11817746609449387
dqn reward tensor(156.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8536e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18951408565044403
dqn reward tensor(-2.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0075e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2059565633535385
dqn reward tensor(-33.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08701741695404053
dqn reward tensor(217.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08424156904220581
dqn reward tensor(13.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18938982486724854
dqn reward tensor(1.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0355e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2690587639808655
dqn reward tensor(1.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.0890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08355476707220078
dqn reward tensor(113.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5552e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13544681668281555
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08848190307617188
dqn reward tensor(-42.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16644564270973206
dqn reward tensor(83.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7747e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13398082554340363
dqn reward tensor(131.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14759591221809387
dqn reward tensor(162.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3543e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22529470920562744
dqn reward tensor(77.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.1180e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26736828684806824
dqn reward tensor(118.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1234e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07889267802238464
dqn reward tensor(20.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1439313292503357
dqn reward tensor(10.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13628199696540833
dqn reward tensor(-87.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20017683506011963
dqn reward tensor(90.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4540e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051219429820775986
dqn reward tensor(89.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.9098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09147638082504272
dqn reward tensor(203.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13932515680789948
dqn reward tensor(38.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0903109610080719
dqn reward tensor(-96.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0631e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26937684416770935
dqn reward tensor(38.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0094e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12122547626495361
dqn reward tensor(60.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6084e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12802961468696594
dqn reward tensor(159., device='cuda:0') e 0.05 loss_dqn tensor(8.1289e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07170958817005157
dqn reward tensor(-14.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.8988e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09451687335968018
dqn reward tensor(52.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.7517e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030819345265626907
dqn reward tensor(225.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12209183722734451
dqn reward tensor(68.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21062324941158295
dqn reward tensor(135., device='cuda:0') e 0.05 loss_dqn tensor(1.2183e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15170568227767944
dqn reward tensor(112.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1375e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17525416612625122
dqn reward tensor(23.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0251e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2394523322582245
dqn reward tensor(-115.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09373486042022705
dqn reward tensor(-11.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1735e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18168558180332184
dqn reward tensor(-130.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0491e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3586767911911011
dqn reward tensor(182.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4280e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07914850115776062
dqn reward tensor(71.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20957651734352112
dqn reward tensor(-41.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1062e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1717938929796219
dqn reward tensor(105.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0664e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12789100408554077
dqn reward tensor(73.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09128430485725403
dqn reward tensor(99.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10329306125640869
dqn reward tensor(-42.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.8334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27001264691352844
dqn reward tensor(108.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08904428035020828
dqn reward tensor(179., device='cuda:0') e 0.05 loss_dqn tensor(8.3810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041780535131692886
dqn reward tensor(-40.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08921544998884201
dqn reward tensor(-48.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9564e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07519394159317017
dqn reward tensor(49.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08154327422380447
dqn reward tensor(5., device='cuda:0') e 0.05 loss_dqn tensor(1.1139e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1030554324388504
dqn reward tensor(88.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05396486073732376
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0930e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0824478417634964
dqn reward tensor(-112.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2011e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10017068684101105
dqn reward tensor(47.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0041e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08918772637844086
dqn reward tensor(-70.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26700934767723083
dqn reward tensor(-32., device='cuda:0') e 0.05 loss_dqn tensor(1.0288e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08103593438863754
dqn reward tensor(63.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.1483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07616224139928818
dqn reward tensor(23.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5073e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20390892028808594
dqn reward tensor(-26.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9280e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2815980613231659
dqn reward tensor(39.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12291719764471054
dqn reward tensor(102.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10568669438362122
dqn reward tensor(-20.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16113759577274323
dqn reward tensor(46.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0257e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07995657622814178
dqn reward tensor(-49.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3156e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025944894179701805
dqn reward tensor(86.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3943e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07710964977741241
dqn reward tensor(-38.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.0886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15286611020565033
dqn reward tensor(64., device='cuda:0') e 0.05 loss_dqn tensor(9.9275e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15166440606117249
dqn reward tensor(-45.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13643376529216766
dqn reward tensor(-2.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1542e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09984950721263885
dqn reward tensor(-18.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1379e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19277919828891754
dqn reward tensor(61.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0974e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1778494119644165
dqn reward tensor(194.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.9869e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033061642199754715
dqn reward tensor(-65.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10190451890230179
dqn reward tensor(-95.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7343e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07802893966436386
dqn reward tensor(-20.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0471e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06430560350418091
dqn reward tensor(-65.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3357e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1493438482284546
dqn reward tensor(-83.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0567e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16383862495422363
dqn reward tensor(-97.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0214e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10309439152479172
dqn reward tensor(-75.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0850e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08712607622146606
dqn reward tensor(84., device='cuda:0') e 0.05 loss_dqn tensor(1.2101e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17756953835487366
dqn reward tensor(-102.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15981799364089966
dqn reward tensor(36.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5613e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034111008048057556
dqn reward tensor(-106.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21653009951114655
dqn reward tensor(43.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0280e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08180396258831024
dqn reward tensor(-41.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.4905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024143166840076447
dqn reward tensor(-149.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1045e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10944797843694687
dqn reward tensor(23.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027075044810771942
dqn reward tensor(24.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09435052424669266
dqn reward tensor(35.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0550e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09694256633520126
dqn reward tensor(58.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0670541450381279
dqn reward tensor(98.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1750943660736084
dqn reward tensor(155.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24515514075756073
dqn reward tensor(-29.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0853e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18660151958465576
dqn reward tensor(-109., device='cuda:0') e 0.05 loss_dqn tensor(1.3501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2599186301231384
dqn reward tensor(-81.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12889547646045685
dqn reward tensor(-54.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3053e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059383705258369446
dqn reward tensor(-109.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7556e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07208418101072311
dqn reward tensor(-25.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1160e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1090761348605156
dqn reward tensor(-45.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1716e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0822664275765419
dqn reward tensor(83., device='cuda:0') e 0.05 loss_dqn tensor(9.1473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09605144709348679
dqn reward tensor(61.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0965e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2433367222547531
dqn reward tensor(-91.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0653e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1522960662841797
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0195e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18190178275108337
dqn reward tensor(-10.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09180210530757904
dqn reward tensor(16.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21648988127708435
dqn reward tensor(-47.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0373e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0871167927980423
dqn reward tensor(214.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0397e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17660504579544067
dqn reward tensor(-17.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0205e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19793033599853516
dqn reward tensor(-128., device='cuda:0') e 0.05 loss_dqn tensor(1.1512e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17994582653045654
dqn reward tensor(38.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1081e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17077557742595673
dqn reward tensor(-98.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1946e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14311671257019043
dqn reward tensor(-72.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04550020024180412
dqn reward tensor(-41.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0678071528673172
dqn reward tensor(75.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5575e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3191748857498169
dqn reward tensor(-145.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1369e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17882594466209412
dqn reward tensor(46.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1164e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14098545908927917
dqn reward tensor(-116.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0961e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03743427246809006
dqn reward tensor(25.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4023e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34479081630706787
dqn reward tensor(-28.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05844166502356529
dqn reward tensor(24.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2040e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12466520071029663
dqn reward tensor(42.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09942404925823212
dqn reward tensor(-38.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0374e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0947672575712204
dqn reward tensor(-40.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0090e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2406127154827118
dqn reward tensor(58.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0282e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08142910897731781
dqn reward tensor(87.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10893795639276505
dqn reward tensor(-3.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3176e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02641603723168373
dqn reward tensor(135., device='cuda:0') e 0.05 loss_dqn tensor(1.0342e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08673197776079178
dqn reward tensor(-74.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2529e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17626217007637024
dqn reward tensor(-55.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.9779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13735675811767578
dqn reward tensor(137.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.3297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12375706434249878
dqn reward tensor(16.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06745417416095734
dqn reward tensor(-82.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24310871958732605
dqn reward tensor(9.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1514e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1792055070400238
dqn reward tensor(38.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1799e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022096510976552963
dqn reward tensor(57.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27967214584350586
dqn reward tensor(31.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0045e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05331460386514664
dqn reward tensor(-30., device='cuda:0') e 0.05 loss_dqn tensor(1.0802e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07407637685537338
dqn reward tensor(-67.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3803e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17012983560562134
dqn reward tensor(-101.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0046e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07976336777210236
dqn reward tensor(41.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2697e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04362562298774719
dqn reward tensor(60.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1916e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05060306936502457
dqn reward tensor(131.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1409e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19173726439476013
dqn reward tensor(51.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2558590769767761
dqn reward tensor(13.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2206e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23259209096431732
dqn reward tensor(25.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2628e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03368334099650383
dqn reward tensor(4., device='cuda:0') e 0.05 loss_dqn tensor(1.0909e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07967649400234222
dqn reward tensor(-44.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1575e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11046221852302551
dqn reward tensor(-26.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15472471714019775
dqn reward tensor(129.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06746822595596313
dqn reward tensor(-46.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0132e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13309326767921448
dqn reward tensor(38.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3960e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07834488153457642
dqn reward tensor(77.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03289522975683212
dqn reward tensor(-3., device='cuda:0') e 0.05 loss_dqn tensor(9.1709e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1483708918094635
dqn reward tensor(74.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1143e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05285888537764549
dqn reward tensor(-131.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15502861142158508
dqn reward tensor(10.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07491785287857056
dqn reward tensor(57.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5501e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18407750129699707
dqn reward tensor(46.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2176e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24043355882167816
dqn reward tensor(191.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12737077474594116
dqn reward tensor(40.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25835880637168884
dqn reward tensor(49.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1485014408826828
dqn reward tensor(-92.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1285e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10108032822608948
dqn reward tensor(90.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08792676031589508
dqn reward tensor(-31.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0014e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06810835003852844
dqn reward tensor(154.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3674e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12709076702594757
dqn reward tensor(47.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08932382613420486
dqn reward tensor(-27.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0912e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2100091129541397
dqn reward tensor(90.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1634e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1454581320285797
dqn reward tensor(4.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1793e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10175590217113495
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1328e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1098022609949112
dqn reward tensor(41.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1729e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1645447015762329
dqn reward tensor(126.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08561115711927414
dqn reward tensor(184., device='cuda:0') e 0.05 loss_dqn tensor(9.9509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1613496094942093
dqn reward tensor(127.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2734e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17340481281280518
dqn reward tensor(66.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20760926604270935
dqn reward tensor(27.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1447e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18647104501724243
dqn reward tensor(80., device='cuda:0') e 0.05 loss_dqn tensor(1.0953e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12703794240951538
dqn reward tensor(67.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09633125364780426
dqn reward tensor(23.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3366e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11704745888710022
dqn reward tensor(210.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16730433702468872
dqn reward tensor(-65.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08865384757518768
dqn reward tensor(120.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1604e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18541046977043152
dqn reward tensor(23.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1517152041196823
dqn reward tensor(5.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2962e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10453906655311584
dqn reward tensor(255.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11131376028060913
dqn reward tensor(47.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0872e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21816283464431763
dqn reward tensor(29.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1275e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08250422775745392
dqn reward tensor(211.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044068172574043274
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0869e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.188604474067688
dqn reward tensor(130.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0576e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10630220919847488
dqn reward tensor(4.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.140029639005661
dqn reward tensor(51.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0873e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16139453649520874
dqn reward tensor(47.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1408e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1249486654996872
dqn reward tensor(-23.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0476e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08129026740789413
dqn reward tensor(105., device='cuda:0') e 0.05 loss_dqn tensor(1.1088e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2879396378993988
dqn reward tensor(46.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2351e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17266780138015747
dqn reward tensor(114.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1513e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030603427439928055
dqn reward tensor(38.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1367846429347992
dqn reward tensor(12.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5276e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15521815419197083
dqn reward tensor(-67.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.1800e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10338051617145538
dqn reward tensor(34.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0912e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13122323155403137
dqn reward tensor(75.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2863e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053125038743019104
dqn reward tensor(-52.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2993e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12156364321708679
dqn reward tensor(218.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.3446e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27855023741722107
dqn reward tensor(59., device='cuda:0') e 0.05 loss_dqn tensor(1.0799e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10142982006072998
dqn reward tensor(38.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.1767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07777857780456543
dqn reward tensor(12.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0547613725066185
dqn reward tensor(91.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03446836769580841
dqn reward tensor(100.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0431e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04273126274347305
dqn reward tensor(250.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08260055631399155
dqn reward tensor(66., device='cuda:0') e 0.05 loss_dqn tensor(1.0088e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12176696956157684
dqn reward tensor(63.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2712862491607666
dqn reward tensor(27.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4636e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12249430269002914
dqn reward tensor(48.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4573e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28117817640304565
dqn reward tensor(35.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0063e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1170867383480072
dqn reward tensor(119., device='cuda:0') e 0.05 loss_dqn tensor(1.6108e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02697182074189186
dqn reward tensor(185., device='cuda:0') e 0.05 loss_dqn tensor(1.1486e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13805094361305237
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0511484295129776
dqn reward tensor(-123.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3164e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16000787913799286
dqn reward tensor(105.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1743e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08591420203447342
dqn reward tensor(207.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1986e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09828023612499237
dqn reward tensor(82.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09912906587123871
dqn reward tensor(67.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0408e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1650984287261963
dqn reward tensor(29.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1669e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24833229184150696
dqn reward tensor(34.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.4402e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1401410698890686
dqn reward tensor(114.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07035643607378006
dqn reward tensor(142.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4322e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1460663378238678
dqn reward tensor(-25.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3114e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22261348366737366
dqn reward tensor(103., device='cuda:0') e 0.05 loss_dqn tensor(9.4101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28903257846832275
dqn reward tensor(82., device='cuda:0') e 0.05 loss_dqn tensor(1.0050e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08469223976135254
dqn reward tensor(-49.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0248e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1286468356847763
dqn reward tensor(120., device='cuda:0') e 0.05 loss_dqn tensor(9.2084e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11056053638458252
dqn reward tensor(151.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1467074453830719
dqn reward tensor(49.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2733e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03835339844226837
dqn reward tensor(106.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1968e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02894739806652069
dqn reward tensor(173.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28859806060791016
dqn reward tensor(140.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.1824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06348437070846558
dqn reward tensor(267.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7957e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17593036592006683
dqn reward tensor(86.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2223e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22909165918827057
dqn reward tensor(72.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2078111320734024
dqn reward tensor(46.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10531538724899292
dqn reward tensor(31.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4145e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12023729085922241
dqn reward tensor(-7.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2566e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0866188108921051
dqn reward tensor(18.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0118e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20210114121437073
dqn reward tensor(-64.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1942e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051432233303785324
dqn reward tensor(-14., device='cuda:0') e 0.05 loss_dqn tensor(1.0925e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1522972583770752
dqn reward tensor(-98.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1974e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2242240011692047
dqn reward tensor(96.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11783276498317719
dqn reward tensor(-79.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11215183138847351
dqn reward tensor(96.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0001e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03430115431547165
dqn reward tensor(86.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.8671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0951383113861084
dqn reward tensor(36.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0097e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06999264657497406
dqn reward tensor(208., device='cuda:0') e 0.05 loss_dqn tensor(1.0818e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026813115924596786
dqn reward tensor(123.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1267e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11054828017950058
dqn reward tensor(42.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835539847612381
dqn reward tensor(79.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.7308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026414422318339348
dqn reward tensor(26.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1105e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12548275291919708
dqn reward tensor(127., device='cuda:0') e 0.05 loss_dqn tensor(9.3340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08257029950618744
dqn reward tensor(60.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0514e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35657182335853577
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2182e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18250447511672974
dqn reward tensor(40.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3520463705062866
dqn reward tensor(130.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0624e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.257293701171875
dqn reward tensor(189.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1033e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17904703319072723
dqn reward tensor(116.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1388e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14894327521324158
dqn reward tensor(-42.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1848e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07056552171707153
dqn reward tensor(72.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1762e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10288578271865845
dqn reward tensor(-18.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1227e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.168766587972641
dqn reward tensor(108.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2339e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08200756460428238
dqn reward tensor(-44.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1580e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10674207657575607
dqn reward tensor(137.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0547e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12278614938259125
dqn reward tensor(-103.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2290e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07121865451335907
dqn reward tensor(-94.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1573e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05792134627699852
dqn reward tensor(53.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1063e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1371469795703888
dqn reward tensor(107.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0050e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21414470672607422
dqn reward tensor(31.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2607e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09030474722385406
dqn reward tensor(-108.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2270e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22048747539520264
dqn reward tensor(24.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0564e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11568514257669449
dqn reward tensor(14.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1383e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05876484513282776
dqn reward tensor(-10.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0789e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06752758473157883
dqn reward tensor(125., device='cuda:0') e 0.05 loss_dqn tensor(1.0953e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2843768298625946
dqn reward tensor(-187.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1083e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09160150587558746
dqn reward tensor(-66.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.4114e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07062152028083801
dqn reward tensor(107.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13470204174518585
dqn reward tensor(-102.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1021e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03549440950155258
dqn reward tensor(-177.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2122e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24614913761615753
dqn reward tensor(-114.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2762e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14877396821975708
dqn reward tensor(12.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1633e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23738625645637512
dqn reward tensor(19.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2464e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08820940554141998
dqn reward tensor(17.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1085e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25829845666885376
dqn reward tensor(-8., device='cuda:0') e 0.05 loss_dqn tensor(1.2060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1466696858406067
dqn reward tensor(26.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2471e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10426165163516998
dqn reward tensor(-10.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1484958827495575
dqn reward tensor(-53.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1607e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14136621356010437
dqn reward tensor(-90.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2861e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13996900618076324
dqn reward tensor(-162.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2495e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28920978307724
dqn reward tensor(-64.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0995e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10570323467254639
dqn reward tensor(141.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0956e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14453338086605072
dqn reward tensor(-98.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2926e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055419400334358215
dqn reward tensor(-254., device='cuda:0') e 0.05 loss_dqn tensor(1.2222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1752033680677414
dqn reward tensor(-86.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1492e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24976781010627747
dqn reward tensor(-90.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1325e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04817059636116028
dqn reward tensor(-53.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2284e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08300885558128357
dqn reward tensor(20.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3613e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08222237229347229
dqn reward tensor(-111.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2657e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21511134505271912
dqn reward tensor(80.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07701068371534348
dqn reward tensor(-25.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1710e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027231227606534958
dqn reward tensor(160., device='cuda:0') e 0.05 loss_dqn tensor(1.1772e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08183728158473969
dqn reward tensor(-121., device='cuda:0') e 0.05 loss_dqn tensor(1.2728e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07766389846801758
dqn reward tensor(-31.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1465e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09183799475431442
dqn reward tensor(-66., device='cuda:0') e 0.05 loss_dqn tensor(1.1013e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12356887012720108
dqn reward tensor(-6.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1270e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20141324400901794
dqn reward tensor(17., device='cuda:0') e 0.05 loss_dqn tensor(1.2221e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19753827154636383
dqn reward tensor(-43.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.1210e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19705818593502045
dqn reward tensor(-49.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1384e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11572243273258209
dqn reward tensor(-135.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1271e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20418919622898102
dqn reward tensor(-63.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3707e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19804257154464722
dqn reward tensor(-92.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1374e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1524871587753296
dqn reward tensor(8.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1567e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14908283948898315
dqn reward tensor(-103.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2055e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15841251611709595
dqn reward tensor(-12.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3148e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1685226559638977
dqn reward tensor(25.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0918e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1881139874458313
dqn reward tensor(-21.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1510e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039805006235837936
dqn reward tensor(2.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3481e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1640203595161438
dqn reward tensor(-17.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1094e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12068235129117966
dqn reward tensor(-17.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.4202e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15044216811656952
dqn reward tensor(-59.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1592e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07093439251184464
dqn reward tensor(-237.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4798e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2108142226934433
dqn reward tensor(-40.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1468e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10030142217874527
dqn reward tensor(52.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1095e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1830780804157257
dqn reward tensor(-138.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1174e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05667497590184212
dqn reward tensor(-2.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3674e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2616516351699829
dqn reward tensor(69.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12918829917907715
dqn reward tensor(-7.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2086e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0904175266623497
dqn reward tensor(-78., device='cuda:0') e 0.05 loss_dqn tensor(1.0906e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18618565797805786
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2227e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12523174285888672
dqn reward tensor(-119.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3216e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050345513969659805
dqn reward tensor(-75.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2188e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09333859384059906
dqn reward tensor(-13.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2179e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03204525634646416
dqn reward tensor(-75.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2675e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12128116935491562
dqn reward tensor(87.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2157e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08116379380226135
dqn reward tensor(37.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1946e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24296656250953674
dqn reward tensor(-83.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2389e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2105010449886322
dqn reward tensor(-136.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1339e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14779819548130035
dqn reward tensor(41.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1772e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24444416165351868
dqn reward tensor(-75.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0923e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1307724565267563
dqn reward tensor(-95.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1648e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.188180610537529
dqn reward tensor(27.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0858e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3123909533023834
dqn reward tensor(-167.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1616e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.127964049577713
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14293934404850006
dqn reward tensor(-174.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2195e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07193343341350555
dqn reward tensor(-126.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2518e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22953295707702637
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1602e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1619100570678711
dqn reward tensor(-90.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1936e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13618811964988708
dqn reward tensor(75.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1218e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.093199722468853
dqn reward tensor(-139.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0670e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15423473715782166
dqn reward tensor(139.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2584e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670438051223755
dqn reward tensor(-26., device='cuda:0') e 0.05 loss_dqn tensor(1.1036e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1558891087770462
dqn reward tensor(-13.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1473e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2026619017124176
dqn reward tensor(-149.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2824e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2663733661174774
dqn reward tensor(-2.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3261e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19636884331703186
dqn reward tensor(-149.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2681e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08145582675933838
dqn reward tensor(-168.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1334e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1762368381023407
dqn reward tensor(-32.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0879e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2300029695034027
dqn reward tensor(-89.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5476e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09541411697864532
dqn reward tensor(22.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1245e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08336815237998962
dqn reward tensor(-131.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0850e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08572153002023697
dqn reward tensor(-96.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1323e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11398089677095413
dqn reward tensor(-76.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1343e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09256193041801453
dqn reward tensor(101., device='cuda:0') e 0.05 loss_dqn tensor(1.0951e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1402818262577057
dqn reward tensor(-66.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02663414180278778
dqn reward tensor(54.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3428e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08296379446983337
dqn reward tensor(-75., device='cuda:0') e 0.05 loss_dqn tensor(1.2648e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13259005546569824
dqn reward tensor(173.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0631e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0807841420173645
dqn reward tensor(-85.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10280268639326096
dqn reward tensor(-21.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1985e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0847284346818924
dqn reward tensor(-56., device='cuda:0') e 0.05 loss_dqn tensor(1.1689e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1711844503879547
dqn reward tensor(-5.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0868e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12230797111988068
dqn reward tensor(-53.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1261e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13535255193710327
dqn reward tensor(-158., device='cuda:0') e 0.05 loss_dqn tensor(1.1146e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12500505149364471
dqn reward tensor(-104.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0983e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1786191612482071
dqn reward tensor(-128.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1471e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04952751472592354
dqn reward tensor(95.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1025e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1199083924293518
dqn reward tensor(-130.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1874e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14541855454444885
dqn reward tensor(-197.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3492e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07424595952033997
dqn reward tensor(-84.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1614e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15733367204666138
dqn reward tensor(21.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0933e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23157663643360138
dqn reward tensor(-102.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0931e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0925217866897583
dqn reward tensor(34.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3949e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23788610100746155
dqn reward tensor(29.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0995e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07783913612365723
dqn reward tensor(-4.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1544e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18770435452461243
dqn reward tensor(-115.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4661e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15403218567371368
dqn reward tensor(-89.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1212e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10757061839103699
dqn reward tensor(-232.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0806e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15610289573669434
dqn reward tensor(-54.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1714e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1514900177717209
dqn reward tensor(-130.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1343e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.096160888671875
dqn reward tensor(-84.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0816e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15275463461875916
dqn reward tensor(-76.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2164e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1350858509540558
dqn reward tensor(-247.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0868e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08029031753540039
dqn reward tensor(-34.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2190e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07996515184640884
dqn reward tensor(48.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4473e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10689294338226318
dqn reward tensor(45.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1040e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11895928531885147
dqn reward tensor(-10.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0871e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13027866184711456
dqn reward tensor(-1.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0734e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13319280743598938
dqn reward tensor(-24.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1267e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19614124298095703
dqn reward tensor(60.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1816e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07746216654777527
dqn reward tensor(-101.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0746e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07248122990131378
dqn reward tensor(-109.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22401845455169678
dqn reward tensor(13.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1073e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20373745262622833
dqn reward tensor(80.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2984e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027412869036197662
dqn reward tensor(-85.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1198e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19005239009857178
dqn reward tensor(9.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.1058e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11314515769481659
dqn reward tensor(-117.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14255671203136444
dqn reward tensor(-40.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2093e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17125770449638367
dqn reward tensor(-169.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1057e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18284264206886292
dqn reward tensor(64.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1030e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026059657335281372
dqn reward tensor(-111.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.345752477645874
dqn reward tensor(62.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4134e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09449796378612518
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1271e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290692687034607
dqn reward tensor(-124.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1539e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2365177869796753
dqn reward tensor(-63.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0975e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25817009806632996
dqn reward tensor(-30.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0756e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11843761801719666
dqn reward tensor(-200.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0666e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13928228616714478
dqn reward tensor(4.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1414e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621028482913971
dqn reward tensor(-121.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2812e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15994316339492798
dqn reward tensor(40.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2231e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18466949462890625
dqn reward tensor(36.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5598e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12300314754247665
dqn reward tensor(-72.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1094e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15521809458732605
dqn reward tensor(43.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0702e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16163617372512817
dqn reward tensor(8.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0828e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17804905772209167
dqn reward tensor(-6.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0722e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09296397864818573
dqn reward tensor(-57.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0399e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0937357097864151
dqn reward tensor(80.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0653e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29716116189956665
dqn reward tensor(-123.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1324e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05056813731789589
dqn reward tensor(-67.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0603e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666054129600525
dqn reward tensor(-125.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0986e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1902494877576828
dqn reward tensor(-199.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18369704484939575
dqn reward tensor(-49.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2324e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03611613065004349
dqn reward tensor(7.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17124664783477783
dqn reward tensor(-112.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1361e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22417929768562317
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0403e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1593562364578247
dqn reward tensor(-57.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0783e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05649988725781441
dqn reward tensor(-62.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2233e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20688661932945251
dqn reward tensor(23.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1165e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17921331524848938
dqn reward tensor(-84., device='cuda:0') e 0.05 loss_dqn tensor(1.1975e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23112770915031433
dqn reward tensor(25.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0610e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16455279290676117
dqn reward tensor(-56.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1023e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23264652490615845
dqn reward tensor(-111.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0847e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07998605072498322
dqn reward tensor(-30.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2626e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1526426076889038
dqn reward tensor(-49.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1185e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23540043830871582
dqn reward tensor(-53.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0662e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12277303636074066
dqn reward tensor(-36.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0681e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13985145092010498
dqn reward tensor(-190.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2645e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07686000317335129
dqn reward tensor(-92.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2677e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08480691909790039
dqn reward tensor(10.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2790e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06411617994308472
dqn reward tensor(-121.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0863e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16127753257751465
dqn reward tensor(2.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0542e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10717751085758209
dqn reward tensor(-33.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2805e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02684682607650757
dqn reward tensor(118.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0931e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11491848528385162
dqn reward tensor(-266.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1824e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04819532483816147
dqn reward tensor(-101.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1094e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060862477868795395
dqn reward tensor(-146.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1166e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06349655240774155
dqn reward tensor(-191., device='cuda:0') e 0.05 loss_dqn tensor(1.1041e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16645368933677673
dqn reward tensor(-16.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0565e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019975299015641212
Evaluating...
Train: {'rocauc': 0.7693202143068338} -0.6605237722396851
=====Epoch 31=====
Training...
dqn reward tensor(-42., device='cuda:0') e 0.05 loss_dqn tensor(1.0366e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09719128906726837
dqn reward tensor(3.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1378e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0696321427822113
dqn reward tensor(-79.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0818e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1543295830488205
dqn reward tensor(-35.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0416e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021720197051763535
dqn reward tensor(6., device='cuda:0') e 0.05 loss_dqn tensor(1.1355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013838134706020355
dqn reward tensor(76.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0475e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12546205520629883
dqn reward tensor(-25.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0375e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2128753364086151
dqn reward tensor(99.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1460e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20272471010684967
dqn reward tensor(-163.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1010e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07031451910734177
dqn reward tensor(-43.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1851e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149839848279953
dqn reward tensor(-68.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1735e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01454461645334959
dqn reward tensor(-159.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1695e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18222558498382568
dqn reward tensor(89.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1829e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10367463529109955
dqn reward tensor(-28.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0216e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15854722261428833
dqn reward tensor(-32.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.2034e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2715195417404175
dqn reward tensor(-152.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0271e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19460126757621765
dqn reward tensor(-28.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0782e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1994166374206543
dqn reward tensor(-101.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0406e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12141002714633942
dqn reward tensor(-171., device='cuda:0') e 0.05 loss_dqn tensor(1.1247e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06688838452100754
dqn reward tensor(77.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1262e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1132124587893486
dqn reward tensor(105., device='cuda:0') e 0.05 loss_dqn tensor(1.0480e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049359582364559174
dqn reward tensor(32.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0221e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1252908557653427
dqn reward tensor(116.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1860e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13368764519691467
dqn reward tensor(73.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1135e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10232105851173401
dqn reward tensor(-48.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0381e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26439881324768066
dqn reward tensor(-214.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1593e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06684907525777817
dqn reward tensor(-148.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1340e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09501715004444122
dqn reward tensor(-46.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0256e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032334789633750916
dqn reward tensor(-67., device='cuda:0') e 0.05 loss_dqn tensor(1.0254e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031290508806705475
dqn reward tensor(20.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9226e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32914698123931885
dqn reward tensor(38.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0561e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08313104510307312
dqn reward tensor(-96.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26836854219436646
dqn reward tensor(-21., device='cuda:0') e 0.05 loss_dqn tensor(1.0520e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21722641587257385
dqn reward tensor(51.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0114e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15576577186584473
dqn reward tensor(27.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0396e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1921100616455078
dqn reward tensor(-196.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0427e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23662525415420532
dqn reward tensor(-6.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1542e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11375419050455093
dqn reward tensor(-21.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2111e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22884300351142883
dqn reward tensor(-46.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1029e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12688420712947845
dqn reward tensor(-76., device='cuda:0') e 0.05 loss_dqn tensor(1.0145e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18205422163009644
dqn reward tensor(35.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0552e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05708447843790054
dqn reward tensor(-35.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1285e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22614628076553345
dqn reward tensor(-126.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1643e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10948837548494339
dqn reward tensor(107.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16995000839233398
dqn reward tensor(-58.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0121e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1272527575492859
dqn reward tensor(-216.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8364e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09614929556846619
dqn reward tensor(119.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0783e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3073469400405884
dqn reward tensor(-8., device='cuda:0') e 0.05 loss_dqn tensor(1.0830e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1981302797794342
dqn reward tensor(19.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4124e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16517624258995056
dqn reward tensor(-89.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0523e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09729355573654175
dqn reward tensor(2.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1969093382358551
dqn reward tensor(-10.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0112e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1212044358253479
dqn reward tensor(-84.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0068e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12372349202632904
dqn reward tensor(-15.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0223e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043255098164081573
dqn reward tensor(5.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4502e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3000943660736084
dqn reward tensor(-0.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.0051e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07391700148582458
dqn reward tensor(24., device='cuda:0') e 0.05 loss_dqn tensor(1.0560e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10143983364105225
dqn reward tensor(-35.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0104e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1893312931060791
dqn reward tensor(25.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2014e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32880884408950806
dqn reward tensor(39.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22507885098457336
dqn reward tensor(29., device='cuda:0') e 0.05 loss_dqn tensor(9.7640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14874984323978424
dqn reward tensor(26., device='cuda:0') e 0.05 loss_dqn tensor(1.0159e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14048033952713013
dqn reward tensor(18., device='cuda:0') e 0.05 loss_dqn tensor(1.0160e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10246895253658295
dqn reward tensor(-95., device='cuda:0') e 0.05 loss_dqn tensor(1.0878e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12718582153320312
dqn reward tensor(-10.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1100e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1575024425983429
dqn reward tensor(-102., device='cuda:0') e 0.05 loss_dqn tensor(1.1154e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1408800184726715
dqn reward tensor(102.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2047e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08677493780851364
dqn reward tensor(176.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1225e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20825637876987457
dqn reward tensor(-38.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1783e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18186505138874054
dqn reward tensor(21., device='cuda:0') e 0.05 loss_dqn tensor(1.0146e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21896325051784515
dqn reward tensor(54.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2057327777147293
dqn reward tensor(-9.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0132e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1233883947134018
dqn reward tensor(-33.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3081e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15706956386566162
dqn reward tensor(-75.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3324e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11423733085393906
dqn reward tensor(47., device='cuda:0') e 0.05 loss_dqn tensor(1.0013e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20263132452964783
dqn reward tensor(-16., device='cuda:0') e 0.05 loss_dqn tensor(1.1041e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13865964114665985
dqn reward tensor(-59.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9957e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15664823353290558
dqn reward tensor(-4.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0058e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07211747765541077
dqn reward tensor(-0.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1806e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08703635632991791
dqn reward tensor(2.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1132495179772377
dqn reward tensor(-109.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0942e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22225596010684967
dqn reward tensor(0.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0891e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24805325269699097
dqn reward tensor(102.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0191e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04320486634969711
dqn reward tensor(-66., device='cuda:0') e 0.05 loss_dqn tensor(1.0524e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13318069279193878
dqn reward tensor(-73.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2061e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18641935288906097
dqn reward tensor(69.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2911e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06471234560012817
dqn reward tensor(8.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1645e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08615818619728088
dqn reward tensor(-108.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.8498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16747014224529266
dqn reward tensor(45.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0390e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12204638123512268
dqn reward tensor(-122.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11799580603837967
dqn reward tensor(22.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18997910618782043
dqn reward tensor(33.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0314e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20151007175445557
dqn reward tensor(-171.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0806e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16391462087631226
dqn reward tensor(-91.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0456e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.169040709733963
dqn reward tensor(-117.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8432e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03394056484103203
dqn reward tensor(-144.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.9016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18938878178596497
dqn reward tensor(-70.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1282e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06468208879232407
dqn reward tensor(-34.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1199512928724289
dqn reward tensor(73.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1693e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28072336316108704
dqn reward tensor(3.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1937292069196701
dqn reward tensor(-24.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3676e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2513903081417084
dqn reward tensor(53.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2980e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10895105451345444
dqn reward tensor(51., device='cuda:0') e 0.05 loss_dqn tensor(1.1116e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09656760096549988
dqn reward tensor(-137.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0361e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041736096143722534
dqn reward tensor(70.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0699e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06631303578615189
dqn reward tensor(40.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07839252054691315
dqn reward tensor(38.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04254571348428726
dqn reward tensor(-97.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0086e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06541246175765991
dqn reward tensor(8.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0876e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.136997789144516
dqn reward tensor(-152., device='cuda:0') e 0.05 loss_dqn tensor(1.0051e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09716157615184784
dqn reward tensor(7.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1946e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1561586558818817
dqn reward tensor(-57.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0082e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0292799174785614
dqn reward tensor(74.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0867e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08740372955799103
dqn reward tensor(-181.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0062e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07058573514223099
dqn reward tensor(151., device='cuda:0') e 0.05 loss_dqn tensor(1.1943e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11363259702920914
dqn reward tensor(-51.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0394e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1873432993888855
dqn reward tensor(-97., device='cuda:0') e 0.05 loss_dqn tensor(9.8480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1054476797580719
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0145e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05932929366827011
dqn reward tensor(32.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0174e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3068305253982544
dqn reward tensor(-95., device='cuda:0') e 0.05 loss_dqn tensor(1.0173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22156240046024323
dqn reward tensor(34.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2426e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17860081791877747
dqn reward tensor(-105., device='cuda:0') e 0.05 loss_dqn tensor(1.0053e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08420217037200928
dqn reward tensor(-71.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0359e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06902159750461578
dqn reward tensor(-79.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0177e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21996009349822998
dqn reward tensor(-173.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15092627704143524
dqn reward tensor(-56.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7989e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12571224570274353
dqn reward tensor(94.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8700e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07558731734752655
dqn reward tensor(-20.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7391e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27949732542037964
dqn reward tensor(-166.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1114e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23665380477905273
dqn reward tensor(56.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0262e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0986432433128357
dqn reward tensor(-73., device='cuda:0') e 0.05 loss_dqn tensor(9.5937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11147890985012054
dqn reward tensor(-60.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1411e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16771556437015533
dqn reward tensor(-28.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0134e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.148044615983963
dqn reward tensor(-44.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4891e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15359288454055786
dqn reward tensor(-120.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0447e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19350123405456543
dqn reward tensor(52.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20632554590702057
dqn reward tensor(-76.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.128322035074234
dqn reward tensor(-78.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7522e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10618378967046738
dqn reward tensor(-74.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0529e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12536995112895966
dqn reward tensor(-78.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7494e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1647806167602539
dqn reward tensor(57.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.1842e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08647122979164124
dqn reward tensor(-39.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0331e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3421141505241394
dqn reward tensor(25.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.6574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08228437602519989
dqn reward tensor(-5.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0281e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04214658588171005
dqn reward tensor(189.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0180e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2957979142665863
dqn reward tensor(-12.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1147e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18660546839237213
dqn reward tensor(-116.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0167e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026343585923314095
dqn reward tensor(-111.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1372e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2760763466358185
dqn reward tensor(123., device='cuda:0') e 0.05 loss_dqn tensor(1.0077e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2654803693294525
dqn reward tensor(-113.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05864829570055008
dqn reward tensor(-74.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0023e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0866810530424118
dqn reward tensor(130.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1070e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17267215251922607
dqn reward tensor(11.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0345e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20377272367477417
dqn reward tensor(-54.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0076e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025669537484645844
dqn reward tensor(-99.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05240034684538841
dqn reward tensor(-39.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1027e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2072484791278839
dqn reward tensor(95.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15176057815551758
dqn reward tensor(-166.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0690e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08485302329063416
dqn reward tensor(-32.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2594e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2166232466697693
dqn reward tensor(9.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6766e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06254568696022034
dqn reward tensor(-59.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0033e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1038106232881546
dqn reward tensor(-120.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11799878627061844
dqn reward tensor(-48.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06934620440006256
dqn reward tensor(-41.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10440190881490707
dqn reward tensor(83.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0994e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16311655938625336
dqn reward tensor(42.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.5238e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17216593027114868
dqn reward tensor(32.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6337e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21081966161727905
dqn reward tensor(-41.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2620e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21196269989013672
dqn reward tensor(-52.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07972683757543564
dqn reward tensor(-13., device='cuda:0') e 0.05 loss_dqn tensor(9.3670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06895709037780762
dqn reward tensor(-42.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02572428435087204
dqn reward tensor(-84.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1009e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14507147669792175
dqn reward tensor(-80.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0935e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15761181712150574
dqn reward tensor(-44., device='cuda:0') e 0.05 loss_dqn tensor(9.8062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10691531747579575
dqn reward tensor(-240., device='cuda:0') e 0.05 loss_dqn tensor(1.0433e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10063937306404114
dqn reward tensor(-159.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033618394285440445
dqn reward tensor(-112.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.3023e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17400401830673218
dqn reward tensor(-254.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1253e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08839703351259232
dqn reward tensor(-97.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0552e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17599721252918243
dqn reward tensor(-40.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0375e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0669214278459549
dqn reward tensor(-143.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.6632e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09912842512130737
dqn reward tensor(-117.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1133e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08382488787174225
dqn reward tensor(-145.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0544e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1004820317029953
dqn reward tensor(34.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.5879e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3080083727836609
dqn reward tensor(-85.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0976e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03199281543493271
dqn reward tensor(-20.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0452e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12654834985733032
dqn reward tensor(-33.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0464e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09902310371398926
dqn reward tensor(-108., device='cuda:0') e 0.05 loss_dqn tensor(1.0687e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10051044076681137
dqn reward tensor(-75.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1390e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673768162727356
dqn reward tensor(-328.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0106e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435701996088028
dqn reward tensor(-2.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0859e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1620052009820938
dqn reward tensor(-92.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3179693818092346
dqn reward tensor(-72.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9300e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15545707941055298
dqn reward tensor(-83., device='cuda:0') e 0.05 loss_dqn tensor(1.0386e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1427132487297058
dqn reward tensor(-277.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1092e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18488700687885284
dqn reward tensor(-127.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0802e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1051725372672081
dqn reward tensor(-47.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.9390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2844170928001404
dqn reward tensor(-115.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0621e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06990394741296768
dqn reward tensor(-13.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.7914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26177000999450684
dqn reward tensor(-67.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0096e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17269068956375122
dqn reward tensor(-94.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0227e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06684307754039764
dqn reward tensor(-22.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22154907882213593
dqn reward tensor(-185.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8632e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10822315514087677
dqn reward tensor(-175.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0166e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19005948305130005
dqn reward tensor(-44.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0811e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3085533380508423
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0085e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16143788397312164
dqn reward tensor(-173.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1261e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17195966839790344
dqn reward tensor(-302.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0832e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11271855235099792
dqn reward tensor(-118.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6623e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07894180715084076
dqn reward tensor(-131.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0052e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13204383850097656
dqn reward tensor(-106.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0278e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4037286043167114
dqn reward tensor(-92.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1289e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07406729459762573
dqn reward tensor(-126.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0366e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0746428519487381
dqn reward tensor(-247.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8635e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19988760352134705
dqn reward tensor(-125.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0779e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034685179591178894
dqn reward tensor(-184.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0623e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1830228865146637
dqn reward tensor(-178., device='cuda:0') e 0.05 loss_dqn tensor(1.0293e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13014033436775208
dqn reward tensor(-84.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0273e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18520468473434448
dqn reward tensor(22.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0657e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23122543096542358
dqn reward tensor(-195.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020344756543636322
dqn reward tensor(-228.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038986027240753174
dqn reward tensor(-192.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0923e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16046224534511566
dqn reward tensor(-93.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1429893672466278
dqn reward tensor(-86.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.6938e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24234220385551453
dqn reward tensor(-240., device='cuda:0') e 0.05 loss_dqn tensor(9.9152e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18734097480773926
dqn reward tensor(-106.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038228124380111694
dqn reward tensor(-223.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0864e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16508717834949493
dqn reward tensor(-185.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.8793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2707180380821228
dqn reward tensor(-131.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0005e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09443221986293793
dqn reward tensor(-196.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.8733e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13781142234802246
dqn reward tensor(-114., device='cuda:0') e 0.05 loss_dqn tensor(1.0442e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16628004610538483
dqn reward tensor(-297.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1882e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09271056950092316
dqn reward tensor(-2.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0319e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09871522337198257
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1007e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10528720915317535
dqn reward tensor(-53.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0408e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1955280601978302
dqn reward tensor(-46.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13466911017894745
dqn reward tensor(-118.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0386e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19195275008678436
dqn reward tensor(-150.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0042e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13553361594676971
dqn reward tensor(-135.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0509e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07146809995174408
dqn reward tensor(-332.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05189028009772301
dqn reward tensor(-190.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9265e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14766132831573486
dqn reward tensor(-216.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0010e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04684244841337204
dqn reward tensor(-153.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0436e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15741297602653503
dqn reward tensor(-196.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14897987246513367
dqn reward tensor(-184.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.9567e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2043887972831726
dqn reward tensor(-88.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20980896055698395
dqn reward tensor(-145., device='cuda:0') e 0.05 loss_dqn tensor(1.0504e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06047458574175835
dqn reward tensor(-344.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09254655241966248
dqn reward tensor(-40.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0473e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21925193071365356
dqn reward tensor(-213.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05189232528209686
dqn reward tensor(-118.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0740e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16559605300426483
dqn reward tensor(-60.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12840688228607178
dqn reward tensor(-154.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13104310631752014
dqn reward tensor(-136., device='cuda:0') e 0.05 loss_dqn tensor(1.0018e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08279135823249817
dqn reward tensor(-177.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16933245956897736
dqn reward tensor(-122.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13178876042366028
dqn reward tensor(-66.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1079665943980217
dqn reward tensor(-239.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0235e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11172100901603699
dqn reward tensor(-116.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1112e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19651034474372864
dqn reward tensor(-222.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0111e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21141895651817322
dqn reward tensor(-198.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0649e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09873248636722565
dqn reward tensor(-98.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1393e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043655261397361755
dqn reward tensor(-144.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0824844241142273
dqn reward tensor(-81.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0500e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03464099392294884
dqn reward tensor(-88.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12149592489004135
dqn reward tensor(-232.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0080e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09773186594247818
dqn reward tensor(-93.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0530e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031803905963897705
dqn reward tensor(-297.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08374051749706268
dqn reward tensor(-13.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0581e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02499709650874138
dqn reward tensor(-20.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0240e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13061638176441193
dqn reward tensor(-49.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7294e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2273658812046051
dqn reward tensor(-176.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0405e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07820529490709305
dqn reward tensor(-203.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.7154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10810848325490952
dqn reward tensor(-148.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6066e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1736014038324356
dqn reward tensor(-187.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0113e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15135854482650757
dqn reward tensor(-206.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15799912810325623
dqn reward tensor(-122.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.8862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02070874348282814
dqn reward tensor(-146.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2382916808128357
dqn reward tensor(-108.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4795e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12064488977193832
dqn reward tensor(-241.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1065e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17580392956733704
dqn reward tensor(-80.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0853e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16766975820064545
dqn reward tensor(-220.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2131e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11155067384243011
dqn reward tensor(-241.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10881178081035614
dqn reward tensor(-187.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9913e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07637746632099152
dqn reward tensor(-163.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23098604381084442
dqn reward tensor(-203.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0773e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24680884182453156
dqn reward tensor(23.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8076e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14393651485443115
dqn reward tensor(-124., device='cuda:0') e 0.05 loss_dqn tensor(9.4518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1580944061279297
dqn reward tensor(-173.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0074e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08854872733354568
dqn reward tensor(-107.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0392e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0897625982761383
dqn reward tensor(-254.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3866e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18403983116149902
dqn reward tensor(-4.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9422e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19040971994400024
dqn reward tensor(-178.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0313e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13921840488910675
dqn reward tensor(-107., device='cuda:0') e 0.05 loss_dqn tensor(1.1186e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1786903738975525
dqn reward tensor(-181.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0147e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14743958413600922
dqn reward tensor(-140.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0657e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14724460244178772
dqn reward tensor(-131.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.4105e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07700900733470917
dqn reward tensor(-211.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0996e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15109893679618835
dqn reward tensor(-243.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.6280e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04141928628087044
dqn reward tensor(-163.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149495929479599
dqn reward tensor(-119.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0062e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20736369490623474
dqn reward tensor(-67., device='cuda:0') e 0.05 loss_dqn tensor(1.4199e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666710376739502
dqn reward tensor(-116.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1691e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13932757079601288
dqn reward tensor(-6.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1489e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09867415577173233
dqn reward tensor(-208., device='cuda:0') e 0.05 loss_dqn tensor(1.0222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18875665962696075
dqn reward tensor(-179.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0275e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09275451302528381
dqn reward tensor(-163.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0256e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10869070887565613
dqn reward tensor(-105.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8353e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030660264194011688
dqn reward tensor(-246.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10624558478593826
dqn reward tensor(-139.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0513e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1677922159433365
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03654465079307556
dqn reward tensor(-0.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10244694352149963
dqn reward tensor(-10.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11559823155403137
dqn reward tensor(-112.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06523798406124115
dqn reward tensor(-72.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17947663366794586
dqn reward tensor(-184., device='cuda:0') e 0.05 loss_dqn tensor(1.0135e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18412798643112183
dqn reward tensor(-226.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29352837800979614
dqn reward tensor(-177.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14308615028858185
dqn reward tensor(-106., device='cuda:0') e 0.05 loss_dqn tensor(1.0128e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05670391768217087
dqn reward tensor(-218.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12224777787923813
dqn reward tensor(-97.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033317409455776215
dqn reward tensor(57.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.5916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14146700501441956
dqn reward tensor(-223.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0217e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0323433056473732
dqn reward tensor(-187.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0370e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07645100355148315
dqn reward tensor(-46.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0280e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060422103852033615
dqn reward tensor(-198.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10479116439819336
dqn reward tensor(-140.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0282e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16313311457633972
dqn reward tensor(-121.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0052e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17326083779335022
dqn reward tensor(-92.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28623446822166443
dqn reward tensor(-140.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1089e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0426112562417984
dqn reward tensor(-232.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0168e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07315919548273087
dqn reward tensor(-89.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0584e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22452004253864288
dqn reward tensor(-160.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1673780232667923
dqn reward tensor(-156.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1408e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18064722418785095
dqn reward tensor(-61.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030148671939969063
dqn reward tensor(-227.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0080e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036428410559892654
dqn reward tensor(-252.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0038e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09436352550983429
dqn reward tensor(-76.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1031e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06773637235164642
dqn reward tensor(-174., device='cuda:0') e 0.05 loss_dqn tensor(1.0207e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18885740637779236
dqn reward tensor(-239.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1978e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.044585272669792175
dqn reward tensor(-247.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.7965e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07920455187559128
dqn reward tensor(-246.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9405e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07850331813097
dqn reward tensor(-212.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0690e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24903827905654907
dqn reward tensor(-274.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0904e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24240146577358246
dqn reward tensor(-232.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031977251172065735
dqn reward tensor(-73.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15981411933898926
dqn reward tensor(-97.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4860e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09282003343105316
dqn reward tensor(-213.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0182e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.133986696600914
dqn reward tensor(-90.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0411e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027072535827755928
dqn reward tensor(-300.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0412e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08219879120588303
dqn reward tensor(-231.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0059e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25381749868392944
dqn reward tensor(34.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7925e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09424427151679993
dqn reward tensor(-186.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.8002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18132635951042175
dqn reward tensor(-10.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1867e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12752243876457214
dqn reward tensor(-267.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0908e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2326928824186325
dqn reward tensor(-170.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0917e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1002236008644104
dqn reward tensor(-217.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.7921e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030966827645897865
dqn reward tensor(-150.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0625e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14279744029045105
dqn reward tensor(-195.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0435e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020303765311837196
dqn reward tensor(-182.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8642e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2563147246837616
dqn reward tensor(-204.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.6707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24932798743247986
dqn reward tensor(-102.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0783e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15832491219043732
dqn reward tensor(-186.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0435e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029049169272184372
dqn reward tensor(-224.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.7371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1305103302001953
dqn reward tensor(-43.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030776409432291985
dqn reward tensor(-297.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06371190398931503
dqn reward tensor(-162.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026770856231451035
dqn reward tensor(-161.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09025115519762039
dqn reward tensor(-170.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13180314004421234
dqn reward tensor(-103.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0888e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0993841290473938
dqn reward tensor(-266.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03912388160824776
dqn reward tensor(-110.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0608e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13055777549743652
dqn reward tensor(-20.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06955727934837341
dqn reward tensor(-163., device='cuda:0') e 0.05 loss_dqn tensor(1.0583e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15610145032405853
dqn reward tensor(-64.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0103e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.136778324842453
dqn reward tensor(-96.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0595e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02182905003428459
dqn reward tensor(-110.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0086e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07466046512126923
dqn reward tensor(-157.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.9999e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10426796227693558
dqn reward tensor(-115.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5652e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08043234050273895
dqn reward tensor(-67.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10343950986862183
dqn reward tensor(-200.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0236e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2642371952533722
dqn reward tensor(-100.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0918e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07550821453332901
dqn reward tensor(-78.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16313278675079346
dqn reward tensor(-182., device='cuda:0') e 0.05 loss_dqn tensor(1.1123e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11298023164272308
dqn reward tensor(-173.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0103e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682138890028
dqn reward tensor(-201.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039085134863853455
dqn reward tensor(-112.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.4414e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24172049760818481
dqn reward tensor(-146.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8874e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20829275250434875
dqn reward tensor(-270., device='cuda:0') e 0.05 loss_dqn tensor(9.4049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08718203008174896
dqn reward tensor(-82.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14814487099647522
dqn reward tensor(-29., device='cuda:0') e 0.05 loss_dqn tensor(1.0216e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21602365374565125
dqn reward tensor(-183.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2823905348777771
dqn reward tensor(-136.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13951750099658966
dqn reward tensor(-170.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2970968782901764
dqn reward tensor(-186.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0171e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11611322313547134
dqn reward tensor(-112.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.1831e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1911064237356186
dqn reward tensor(-140., device='cuda:0') e 0.05 loss_dqn tensor(1.0231e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06278736889362335
dqn reward tensor(-210.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8090e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12272959202528
dqn reward tensor(-77.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2519798278808594
dqn reward tensor(-236.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0142e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16654767096042633
dqn reward tensor(-259.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.4802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16567128896713257
dqn reward tensor(-103.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2708e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059112608432769775
dqn reward tensor(-110.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.4095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09391236305236816
dqn reward tensor(-185., device='cuda:0') e 0.05 loss_dqn tensor(1.0294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0888613685965538
dqn reward tensor(-186.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12254993617534637
dqn reward tensor(-66.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19496402144432068
dqn reward tensor(-87.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08946492522954941
dqn reward tensor(-220.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0572e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08225072175264359
dqn reward tensor(-133., device='cuda:0') e 0.05 loss_dqn tensor(9.7474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2637467086315155
dqn reward tensor(-144.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26590561866760254
dqn reward tensor(-20.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03105144016444683
dqn reward tensor(-142.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06346815079450607
dqn reward tensor(-301.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2938e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14195799827575684
dqn reward tensor(-53.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0080e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14911529421806335
dqn reward tensor(-67.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13783057034015656
dqn reward tensor(-171.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4506e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08516998589038849
dqn reward tensor(-180., device='cuda:0') e 0.05 loss_dqn tensor(9.7643e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08184556663036346
dqn reward tensor(-276., device='cuda:0') e 0.05 loss_dqn tensor(1.1232e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08998029679059982
dqn reward tensor(-157.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.3032e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2965686321258545
dqn reward tensor(-157.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10666051506996155
dqn reward tensor(-288., device='cuda:0') e 0.05 loss_dqn tensor(9.5368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06760655343532562
dqn reward tensor(-236.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13409796357154846
dqn reward tensor(-119.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.3304e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17735804617404938
dqn reward tensor(-153.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.3994e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08979679644107819
dqn reward tensor(45., device='cuda:0') e 0.05 loss_dqn tensor(9.8915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12500301003456116
dqn reward tensor(-167.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1271e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.090669184923172
dqn reward tensor(-20.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.6056e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1020793616771698
dqn reward tensor(-190.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4566e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13984516263008118
dqn reward tensor(-85.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2538658082485199
dqn reward tensor(-138.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058543939143419266
dqn reward tensor(-55.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0266e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047156110405921936
dqn reward tensor(-157.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08690093457698822
dqn reward tensor(-140.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1421261429786682
dqn reward tensor(-92.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5101e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25593575835227966
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0648e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09097103774547577
dqn reward tensor(-20.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.1057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11212972551584244
dqn reward tensor(-151., device='cuda:0') e 0.05 loss_dqn tensor(8.9200e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11323211342096329
dqn reward tensor(-152.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.9983e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12884509563446045
dqn reward tensor(-28.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0438e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029955660924315453
dqn reward tensor(-282.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.6274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053763702511787415
dqn reward tensor(-255.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.6738e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14212338626384735
dqn reward tensor(-6.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19961583614349365
dqn reward tensor(-63.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12912094593048096
dqn reward tensor(-165., device='cuda:0') e 0.05 loss_dqn tensor(1.1103e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2883715033531189
dqn reward tensor(-152.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1409534513950348
dqn reward tensor(-271., device='cuda:0') e 0.05 loss_dqn tensor(8.7552e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2370711863040924
dqn reward tensor(-134.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.9482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1070215031504631
dqn reward tensor(-45.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0199e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22593995928764343
dqn reward tensor(-277.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10958586633205414
dqn reward tensor(-198.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1112179160118103
dqn reward tensor(-191.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06966286897659302
dqn reward tensor(-160.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0696e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11326104402542114
dqn reward tensor(-179.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1200895756483078
dqn reward tensor(-83.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.9434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1037488579750061
dqn reward tensor(-226.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4642e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18410250544548035
dqn reward tensor(-30.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0275e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25170502066612244
dqn reward tensor(-187.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08227499574422836
dqn reward tensor(-213.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0873e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10360684990882874
dqn reward tensor(-30.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19038067758083344
dqn reward tensor(-130.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6979e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1486218273639679
dqn reward tensor(-21., device='cuda:0') e 0.05 loss_dqn tensor(8.8109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.129902184009552
dqn reward tensor(-169.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.119719959795475
dqn reward tensor(-148.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22333768010139465
dqn reward tensor(-158.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1157e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07531474530696869
dqn reward tensor(-194.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0190e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04355962574481964
dqn reward tensor(-66.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06507588177919388
dqn reward tensor(-242.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0058e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038099825382232666
dqn reward tensor(-216.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8237e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07935648411512375
dqn reward tensor(-197.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3985e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09177413582801819
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.7407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11309576779603958
dqn reward tensor(-258., device='cuda:0') e 0.05 loss_dqn tensor(9.0608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13041381537914276
dqn reward tensor(-60., device='cuda:0') e 0.05 loss_dqn tensor(8.7887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11997674405574799
dqn reward tensor(-204.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1361e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12573277950286865
dqn reward tensor(-175.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7729e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0841500461101532
dqn reward tensor(-164.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0427e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18764129281044006
dqn reward tensor(-37.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.9587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14182870090007782
dqn reward tensor(-200.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09532790631055832
dqn reward tensor(-56.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7780e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2618557810783386
dqn reward tensor(-234.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033699050545692444
dqn reward tensor(-167.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08361976593732834
dqn reward tensor(-82.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09534391760826111
dqn reward tensor(-235.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06444522738456726
dqn reward tensor(-58.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13122175633907318
dqn reward tensor(-172.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8978e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20175552368164062
dqn reward tensor(-115.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0805e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08975718170404434
dqn reward tensor(-102.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06897430121898651
dqn reward tensor(-130.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13222426176071167
dqn reward tensor(-15.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09141019731760025
dqn reward tensor(2.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0508e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09323710948228836
dqn reward tensor(-103.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0574e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03876432031393051
dqn reward tensor(-46.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.4290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05521959066390991
dqn reward tensor(17., device='cuda:0') e 0.05 loss_dqn tensor(8.9948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20926985144615173
dqn reward tensor(-186.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2929575741291046
dqn reward tensor(-78.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12298368662595749
dqn reward tensor(-22.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.2976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344211995601654
dqn reward tensor(-176.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.4405e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07359768450260162
dqn reward tensor(-0.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18408861756324768
dqn reward tensor(-139.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20505595207214355
dqn reward tensor(-265.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3715e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08658036589622498
dqn reward tensor(-152.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0056e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053365353494882584
dqn reward tensor(-223.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.4873e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11796735227108002
dqn reward tensor(-234.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5054e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12310102581977844
dqn reward tensor(-46.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19726581871509552
dqn reward tensor(-104.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.3701e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07238728553056717
dqn reward tensor(-198.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11067274957895279
dqn reward tensor(-200.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1277303695678711
dqn reward tensor(-117., device='cuda:0') e 0.05 loss_dqn tensor(1.0445e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14630445837974548
dqn reward tensor(-24.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08843633532524109
dqn reward tensor(-175.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5572e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09147118777036667
dqn reward tensor(-44.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.5720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09115453064441681
dqn reward tensor(-70.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10159219056367874
dqn reward tensor(-84.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15996377170085907
dqn reward tensor(-252.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11516985297203064
dqn reward tensor(-91.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09337680041790009
dqn reward tensor(-19.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9329e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.865857720375061
Evaluating...
Train: {'rocauc': 0.7709509724778993} -2.0154011249542236
=====Epoch 32=====
Training...
dqn reward tensor(-121.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.5262e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19055885076522827
dqn reward tensor(6.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1933063268661499
dqn reward tensor(-139.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3429e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15623804926872253
dqn reward tensor(-29.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5040e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18713770806789398
dqn reward tensor(-76.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11231572926044464
dqn reward tensor(-126., device='cuda:0') e 0.05 loss_dqn tensor(9.4453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11898670345544815
dqn reward tensor(-100.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16096271574497223
dqn reward tensor(-140.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.0413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1659357100725174
dqn reward tensor(-114.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5032e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.105488620698452
dqn reward tensor(-187.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0961376428604126
dqn reward tensor(-163., device='cuda:0') e 0.05 loss_dqn tensor(8.8134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19399023056030273
dqn reward tensor(-112.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.9200e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15172532200813293
dqn reward tensor(-154.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1391e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07581538707017899
dqn reward tensor(-240.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0840e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11777477711439133
dqn reward tensor(37.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07820264995098114
dqn reward tensor(-101.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0201e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25375157594680786
dqn reward tensor(-126.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1265927404165268
dqn reward tensor(-100.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19114410877227783
dqn reward tensor(-111.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12424018234014511
dqn reward tensor(-199.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09131291508674622
dqn reward tensor(-87.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10075512528419495
dqn reward tensor(-29.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3856e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13095413148403168
dqn reward tensor(-163.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08968277275562286
dqn reward tensor(-130.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18782305717468262
dqn reward tensor(2.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13238124549388885
dqn reward tensor(-99.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.3526e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0700460895895958
dqn reward tensor(-164.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08213671296834946
dqn reward tensor(-156., device='cuda:0') e 0.05 loss_dqn tensor(8.5398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0944426953792572
dqn reward tensor(-53.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8058e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10779058933258057
dqn reward tensor(-23.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5593e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21202579140663147
dqn reward tensor(-148.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.0705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09035932272672653
dqn reward tensor(-249., device='cuda:0') e 0.05 loss_dqn tensor(8.3930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05784553661942482
dqn reward tensor(-88.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.4609e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23347274959087372
dqn reward tensor(-60.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4743e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19334720075130463
dqn reward tensor(-175.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4926e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22718730568885803
dqn reward tensor(-245.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08823006600141525
dqn reward tensor(-134., device='cuda:0') e 0.05 loss_dqn tensor(1.0637e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11950305104255676
dqn reward tensor(-108.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0047e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05180297791957855
dqn reward tensor(-150.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1643507182598114
dqn reward tensor(-121., device='cuda:0') e 0.05 loss_dqn tensor(8.8290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05584196001291275
dqn reward tensor(-135.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2257e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2662771940231323
dqn reward tensor(-114.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22607582807540894
dqn reward tensor(-119.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13112448155879974
dqn reward tensor(-23.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22667820751667023
dqn reward tensor(-182.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059270769357681274
dqn reward tensor(-202.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2562616467475891
dqn reward tensor(-184.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.1068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1547517478466034
dqn reward tensor(-114.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09567639976739883
dqn reward tensor(-97.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3197e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09863651543855667
dqn reward tensor(-66.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12285240739583969
dqn reward tensor(-169.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02785969153046608
dqn reward tensor(-199.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13573846220970154
dqn reward tensor(-81.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24302172660827637
dqn reward tensor(-113., device='cuda:0') e 0.05 loss_dqn tensor(8.3867e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17422384023666382
dqn reward tensor(-177., device='cuda:0') e 0.05 loss_dqn tensor(9.5770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13739150762557983
dqn reward tensor(-120.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4492e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08261986821889877
dqn reward tensor(-172.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7629e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2432810664176941
dqn reward tensor(-70.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26295721530914307
dqn reward tensor(-247.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.8571e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07553727924823761
dqn reward tensor(-301.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.151236891746521
dqn reward tensor(-209.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.4440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0486929751932621
dqn reward tensor(-59.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1351427435874939
dqn reward tensor(-164.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12111644446849823
dqn reward tensor(-108.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.3596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0646761804819107
dqn reward tensor(-127.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.3666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03701254352927208
dqn reward tensor(-227.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1691671758890152
dqn reward tensor(-146.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034685567021369934
dqn reward tensor(-199.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4178e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13385948538780212
dqn reward tensor(-76.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1270e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16855230927467346
dqn reward tensor(-218.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09793806076049805
dqn reward tensor(-132.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8116e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11810857057571411
dqn reward tensor(-65., device='cuda:0') e 0.05 loss_dqn tensor(8.7100e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16391192376613617
dqn reward tensor(-106.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15035955607891083
dqn reward tensor(-76.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6429e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06370958685874939
dqn reward tensor(-231., device='cuda:0') e 0.05 loss_dqn tensor(8.1871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2986817955970764
dqn reward tensor(-159.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.6639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15476498007774353
dqn reward tensor(9.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09670999646186829
dqn reward tensor(24.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07345385104417801
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13836292922496796
dqn reward tensor(-10.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2989e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05916522815823555
dqn reward tensor(-124.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03356795758008957
dqn reward tensor(-237.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5550e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14726562798023224
dqn reward tensor(-119.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24041315913200378
dqn reward tensor(-158.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.3012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24205341935157776
dqn reward tensor(-114.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5746e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15828600525856018
dqn reward tensor(-61.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0990038588643074
dqn reward tensor(-77.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3224e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09694504737854004
dqn reward tensor(-122.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033674366772174835
dqn reward tensor(57.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1636587679386139
dqn reward tensor(-206., device='cuda:0') e 0.05 loss_dqn tensor(9.5368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13288161158561707
dqn reward tensor(-222.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09025150537490845
dqn reward tensor(-314.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06384192407131195
dqn reward tensor(-120.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22549615800380707
dqn reward tensor(-56., device='cuda:0') e 0.05 loss_dqn tensor(8.5976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2177923023700714
dqn reward tensor(-58., device='cuda:0') e 0.05 loss_dqn tensor(8.4071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2419913411140442
dqn reward tensor(-77.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02615484781563282
dqn reward tensor(-225.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.3609e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11047801375389099
dqn reward tensor(-107.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1966e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1493118852376938
dqn reward tensor(-18.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1433500498533249
dqn reward tensor(-33.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.4483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08286453783512115
dqn reward tensor(-140.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7293e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08148649334907532
dqn reward tensor(-127., device='cuda:0') e 0.05 loss_dqn tensor(8.2298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18357092142105103
dqn reward tensor(-103.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7317e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1108013466000557
dqn reward tensor(-113.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4197e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1666257083415985
dqn reward tensor(-234.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3693e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22102171182632446
dqn reward tensor(-133.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15459638833999634
dqn reward tensor(-45.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6676e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0830335021018982
dqn reward tensor(-163.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11391710489988327
dqn reward tensor(-121.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09409311413764954
dqn reward tensor(-164.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3411e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08386771380901337
dqn reward tensor(63., device='cuda:0') e 0.05 loss_dqn tensor(8.0880e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10886462032794952
dqn reward tensor(-23.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09613753855228424
dqn reward tensor(-113.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.3839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03277285397052765
dqn reward tensor(-214.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.3981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028697073459625244
dqn reward tensor(-172.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13295377790927887
dqn reward tensor(-61.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11929492652416229
dqn reward tensor(-111.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3131e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0223836712539196
dqn reward tensor(-241.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11308127641677856
dqn reward tensor(-141., device='cuda:0') e 0.05 loss_dqn tensor(8.6722e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016459712758660316
dqn reward tensor(-185.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10210413485765457
dqn reward tensor(-195.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1582e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02865162305533886
dqn reward tensor(-139.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018733851611614227
dqn reward tensor(-257.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.5475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10251186788082123
dqn reward tensor(-7.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8385e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22140991687774658
dqn reward tensor(-73.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6363e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.011816597543656826
dqn reward tensor(35.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9304e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17319080233573914
dqn reward tensor(-181.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21060043573379517
dqn reward tensor(-31.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2527e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1000479906797409
dqn reward tensor(-105.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.7690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14428743720054626
dqn reward tensor(-147.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3401719331741333
dqn reward tensor(-31.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413731217384338
dqn reward tensor(-96.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8107e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11079148203134537
dqn reward tensor(-123.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09520764648914337
dqn reward tensor(-237.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17672613263130188
dqn reward tensor(-176.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2537e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08744072169065475
dqn reward tensor(-132.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028505569323897362
dqn reward tensor(-76.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0387924462556839
dqn reward tensor(-126.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5311e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21046613156795502
dqn reward tensor(-123., device='cuda:0') e 0.05 loss_dqn tensor(8.6112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2177586555480957
dqn reward tensor(-105.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12506647408008575
dqn reward tensor(-128.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06196281686425209
dqn reward tensor(-243.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.5144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24417820572853088
dqn reward tensor(-194., device='cuda:0') e 0.05 loss_dqn tensor(8.9081e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05353786423802376
dqn reward tensor(-93.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19856497645378113
dqn reward tensor(-271.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06911852955818176
dqn reward tensor(-329.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.1802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05876005440950394
dqn reward tensor(10.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15254318714141846
dqn reward tensor(-36., device='cuda:0') e 0.05 loss_dqn tensor(8.1763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12138742208480835
dqn reward tensor(-11.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07583849132061005
dqn reward tensor(-113.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.9449e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0901799127459526
dqn reward tensor(-61.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5539e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13234640657901764
dqn reward tensor(-172.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0257e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21442681550979614
dqn reward tensor(-60.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13790756464004517
dqn reward tensor(-237.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.3047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042772866785526276
dqn reward tensor(-58.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2177e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04296142980456352
dqn reward tensor(-220., device='cuda:0') e 0.05 loss_dqn tensor(8.1812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13983607292175293
dqn reward tensor(-174.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18883512914180756
dqn reward tensor(-199.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14762933552265167
dqn reward tensor(-176.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13091085851192474
dqn reward tensor(-154.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11134883761405945
dqn reward tensor(-178., device='cuda:0') e 0.05 loss_dqn tensor(8.2660e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08618548512458801
dqn reward tensor(-235.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2759067118167877
dqn reward tensor(-89.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6133e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29490038752555847
dqn reward tensor(-103.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2224092036485672
dqn reward tensor(-43.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3602e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17636719346046448
dqn reward tensor(-19., device='cuda:0') e 0.05 loss_dqn tensor(9.3900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28822600841522217
dqn reward tensor(-201.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09890223294496536
dqn reward tensor(-59.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04913267865777016
dqn reward tensor(-58.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15734010934829712
dqn reward tensor(-105., device='cuda:0') e 0.05 loss_dqn tensor(9.7622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09382054954767227
dqn reward tensor(-218.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0027e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16673842072486877
dqn reward tensor(-137.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14893139898777008
dqn reward tensor(-18., device='cuda:0') e 0.05 loss_dqn tensor(8.3691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1095830574631691
dqn reward tensor(-151.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14006632566452026
dqn reward tensor(-83.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17776577174663544
dqn reward tensor(-35.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17705154418945312
dqn reward tensor(-229.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11860473453998566
dqn reward tensor(-47.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0571e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026767730712890625
dqn reward tensor(-229.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21749302744865417
dqn reward tensor(-195.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2725007236003876
dqn reward tensor(-138.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5744e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10622360557317734
dqn reward tensor(-113.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9341e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08953700214624405
dqn reward tensor(-137.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09509735554456711
dqn reward tensor(-181.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03426423668861389
dqn reward tensor(-133., device='cuda:0') e 0.05 loss_dqn tensor(8.5752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029571112245321274
dqn reward tensor(-255.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13035495579242706
dqn reward tensor(-147.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1352691352367401
dqn reward tensor(-101.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.0984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07473934441804886
dqn reward tensor(-168.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018099721521139145
dqn reward tensor(-201.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17906935513019562
dqn reward tensor(-268.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488850712776184
dqn reward tensor(-63.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6756e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09584201127290726
dqn reward tensor(-118.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.4924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18620337545871735
dqn reward tensor(-93.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05804818123579025
dqn reward tensor(-268., device='cuda:0') e 0.05 loss_dqn tensor(7.9375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07403379678726196
dqn reward tensor(30.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07295048236846924
dqn reward tensor(-147.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1878269910812378
dqn reward tensor(-72.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054931312799453735
dqn reward tensor(-109.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11862242966890335
dqn reward tensor(-87.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12514039874076843
dqn reward tensor(-162.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.2487e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07530732452869415
dqn reward tensor(-37.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10014870762825012
dqn reward tensor(-156.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19418048858642578
dqn reward tensor(-155.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18544673919677734
dqn reward tensor(-208.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1970723271369934
dqn reward tensor(-124.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8678e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09223610162734985
dqn reward tensor(-57.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0457e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13038977980613708
dqn reward tensor(-142.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1832e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16834095120429993
dqn reward tensor(-164.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14319682121276855
dqn reward tensor(-80., device='cuda:0') e 0.05 loss_dqn tensor(8.0565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031588319689035416
dqn reward tensor(-48.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0119e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22542880475521088
dqn reward tensor(-73.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.7009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08291009813547134
dqn reward tensor(-119.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17402788996696472
dqn reward tensor(-159.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5684e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0986282229423523
dqn reward tensor(-195.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23556751012802124
dqn reward tensor(-101., device='cuda:0') e 0.05 loss_dqn tensor(8.1718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2456967979669571
dqn reward tensor(-176.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2409e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23416349291801453
dqn reward tensor(-284.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.8730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07742170989513397
dqn reward tensor(-148.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3180e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0949251726269722
dqn reward tensor(-89.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0744604542851448
dqn reward tensor(-126.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.9740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12989038228988647
dqn reward tensor(-117.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20356833934783936
dqn reward tensor(-143.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05665676295757294
dqn reward tensor(-11.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19043488800525665
dqn reward tensor(-177.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12026020884513855
dqn reward tensor(-206., device='cuda:0') e 0.05 loss_dqn tensor(8.8578e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08911827206611633
dqn reward tensor(-193.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0500e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24425122141838074
dqn reward tensor(-166.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.2483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14526420831680298
dqn reward tensor(-157.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10430224239826202
dqn reward tensor(-93., device='cuda:0') e 0.05 loss_dqn tensor(8.0535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15060286223888397
dqn reward tensor(-185.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1869e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13340574502944946
dqn reward tensor(-72.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.3745e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09108343720436096
dqn reward tensor(-236.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2125e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08398449420928955
dqn reward tensor(-312.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.8355e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05486156791448593
dqn reward tensor(-139.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17913749814033508
dqn reward tensor(-105.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.6488e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12419389188289642
dqn reward tensor(-194.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.5902e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07584823668003082
dqn reward tensor(-72.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1063e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32177188992500305
dqn reward tensor(-94.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049477897584438324
dqn reward tensor(-118.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15273000299930573
dqn reward tensor(-40.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07713721692562103
dqn reward tensor(-408.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.3155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02539833076298237
dqn reward tensor(-24.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15112018585205078
dqn reward tensor(-75.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2189e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09788522869348526
dqn reward tensor(-133.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5412e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33025532960891724
dqn reward tensor(-202.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05340224504470825
dqn reward tensor(-119.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14739856123924255
dqn reward tensor(-192.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0303533636033535
dqn reward tensor(-139.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20822690427303314
dqn reward tensor(-152.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.2321e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04239300265908241
dqn reward tensor(-52.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.6719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10587170720100403
dqn reward tensor(-247.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2644e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11959941685199738
dqn reward tensor(-273.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13681507110595703
dqn reward tensor(-90.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21041962504386902
dqn reward tensor(-184.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09636662900447845
dqn reward tensor(-153.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22722497582435608
dqn reward tensor(-134.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.1083e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12663061916828156
dqn reward tensor(-181., device='cuda:0') e 0.05 loss_dqn tensor(8.0984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17216023802757263
dqn reward tensor(-39., device='cuda:0') e 0.05 loss_dqn tensor(8.5037e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08750230073928833
dqn reward tensor(-182.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4085e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06636886298656464
dqn reward tensor(-226.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3033e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05651481822133064
dqn reward tensor(-39.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2614811360836029
dqn reward tensor(-180.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10919105261564255
dqn reward tensor(-189.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.4190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24246901273727417
dqn reward tensor(-185.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0901e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17881903052330017
dqn reward tensor(-185.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07439630478620529
dqn reward tensor(-50.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16250914335250854
dqn reward tensor(-144.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2591e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0453399196267128
dqn reward tensor(-163.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414123922586441
dqn reward tensor(-170.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.8238e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08409619331359863
dqn reward tensor(-54.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11368347704410553
dqn reward tensor(-32.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1290404200553894
dqn reward tensor(-43.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9013e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23910972476005554
dqn reward tensor(-25.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0533e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10921842604875565
dqn reward tensor(-55.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1705225110054016
dqn reward tensor(-55.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2048e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10717060416936874
dqn reward tensor(-114.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.1975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03478231281042099
dqn reward tensor(-133.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9020e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16390010714530945
dqn reward tensor(-84.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08984595537185669
dqn reward tensor(-213.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7500e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09587971866130829
dqn reward tensor(-38.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1505e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20689663290977478
dqn reward tensor(-254.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04440515488386154
dqn reward tensor(-30.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.9440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0856403335928917
dqn reward tensor(-155., device='cuda:0') e 0.05 loss_dqn tensor(7.9583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22211037576198578
dqn reward tensor(-98.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0827e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27828991413116455
dqn reward tensor(-182.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5005e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09206036478281021
dqn reward tensor(-129.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1901075690984726
dqn reward tensor(-255.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057580117136240005
dqn reward tensor(-175., device='cuda:0') e 0.05 loss_dqn tensor(8.9895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17669768631458282
dqn reward tensor(-151., device='cuda:0') e 0.05 loss_dqn tensor(8.0851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11797497421503067
dqn reward tensor(-162.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.4186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10104650259017944
dqn reward tensor(-143.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1979e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1319446563720703
dqn reward tensor(-6.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2081441581249237
dqn reward tensor(-97.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5287e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19426169991493225
dqn reward tensor(-269.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.1544e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04044511169195175
dqn reward tensor(-127.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20154191553592682
dqn reward tensor(-149.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.244694784283638
dqn reward tensor(-210.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.8495e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17067092657089233
dqn reward tensor(-186.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1994641125202179
dqn reward tensor(-87.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17452570796012878
dqn reward tensor(-51., device='cuda:0') e 0.05 loss_dqn tensor(8.6478e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24995967745780945
dqn reward tensor(-196.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16018614172935486
dqn reward tensor(-139., device='cuda:0') e 0.05 loss_dqn tensor(8.6375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0766976848244667
dqn reward tensor(-210.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.1140e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14247725903987885
dqn reward tensor(-172.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08956488966941833
dqn reward tensor(-128.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.4475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13777436316013336
dqn reward tensor(-105.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.5292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1348472535610199
dqn reward tensor(-60.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22548604011535645
dqn reward tensor(-112.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06625287234783173
dqn reward tensor(61.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08016818761825562
dqn reward tensor(-62.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08992079645395279
dqn reward tensor(-189.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02843307889997959
dqn reward tensor(-59.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.2028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11669426411390305
dqn reward tensor(-70.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4575e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.4124337136745453
dqn reward tensor(-28., device='cuda:0') e 0.05 loss_dqn tensor(8.0375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23587697744369507
dqn reward tensor(-47.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1714620292186737
dqn reward tensor(-121.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9200e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02509138733148575
dqn reward tensor(-107., device='cuda:0') e 0.05 loss_dqn tensor(9.2116e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2204437553882599
dqn reward tensor(-57., device='cuda:0') e 0.05 loss_dqn tensor(8.6498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14420723915100098
dqn reward tensor(-109.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.2779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02927309088408947
dqn reward tensor(-255.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07324284315109253
dqn reward tensor(-310.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0365e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12487965077161789
dqn reward tensor(-144.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8735e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03340408205986023
dqn reward tensor(-54.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13267478346824646
dqn reward tensor(-212.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9728e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23551619052886963
dqn reward tensor(-149.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15852877497673035
dqn reward tensor(-83.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1303e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.41001373529434204
dqn reward tensor(-74.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1976e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16149947047233582
dqn reward tensor(-274.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08192890882492065
dqn reward tensor(-192.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1380573809146881
dqn reward tensor(-98.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08804541826248169
dqn reward tensor(-129., device='cuda:0') e 0.05 loss_dqn tensor(8.0170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16232869029045105
dqn reward tensor(82.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16090013086795807
dqn reward tensor(-211.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21440523862838745
dqn reward tensor(-117.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043083205819129944
dqn reward tensor(-117.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07344364374876022
dqn reward tensor(-169.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26778942346572876
dqn reward tensor(-134., device='cuda:0') e 0.05 loss_dqn tensor(8.7231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1858392357826233
dqn reward tensor(-8.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.089753158390522
dqn reward tensor(-79.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09474951028823853
dqn reward tensor(-199.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13103087246418
dqn reward tensor(-79.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24449975788593292
dqn reward tensor(-174.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08967362344264984
dqn reward tensor(-15.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2579e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0680256113409996
dqn reward tensor(-29.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4379e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414567232131958
dqn reward tensor(-115., device='cuda:0') e 0.05 loss_dqn tensor(8.3375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12240221351385117
dqn reward tensor(-40.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17196589708328247
dqn reward tensor(-16., device='cuda:0') e 0.05 loss_dqn tensor(8.5831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1732180118560791
dqn reward tensor(-147.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6516e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16340048611164093
dqn reward tensor(-225.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.2497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07334622740745544
dqn reward tensor(-140.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16655948758125305
dqn reward tensor(-2.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0380164235830307
dqn reward tensor(-161.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8338e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20494362711906433
dqn reward tensor(-166.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10387662798166275
dqn reward tensor(-112.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0754e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051145512610673904
dqn reward tensor(-114.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17470605671405792
dqn reward tensor(-99., device='cuda:0') e 0.05 loss_dqn tensor(8.1238e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25750449299812317
dqn reward tensor(-183.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14233626425266266
dqn reward tensor(-181.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.9831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12103134393692017
dqn reward tensor(-53.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2508e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10541195422410965
dqn reward tensor(-7., device='cuda:0') e 0.05 loss_dqn tensor(8.0836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15423715114593506
dqn reward tensor(-190.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19557294249534607
dqn reward tensor(-67., device='cuda:0') e 0.05 loss_dqn tensor(8.2962e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20417186617851257
dqn reward tensor(-170.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0036e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14174818992614746
dqn reward tensor(-156.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4685e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12696751952171326
dqn reward tensor(7.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16326290369033813
dqn reward tensor(-150.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.8200e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08650188148021698
dqn reward tensor(-81.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3724e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0635179653763771
dqn reward tensor(-117.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0178e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08639097213745117
dqn reward tensor(-82.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2467338889837265
dqn reward tensor(-0.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.6781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10044506937265396
dqn reward tensor(-79., device='cuda:0') e 0.05 loss_dqn tensor(8.7001e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08082105964422226
dqn reward tensor(-132.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0965e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1758924275636673
dqn reward tensor(-85., device='cuda:0') e 0.05 loss_dqn tensor(7.9754e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2965376675128937
dqn reward tensor(-105.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36729419231414795
dqn reward tensor(-203.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.8791e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0882488340139389
dqn reward tensor(-48.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18077024817466736
dqn reward tensor(-128.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3044e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12456487119197845
dqn reward tensor(-34.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1139993667602539
dqn reward tensor(-63.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07494892179965973
dqn reward tensor(-181.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11186087876558304
dqn reward tensor(-200.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036551423370838165
dqn reward tensor(-25.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1446705013513565
dqn reward tensor(-70.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9961e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12637481093406677
dqn reward tensor(-111.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7381e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12282869219779968
dqn reward tensor(-153.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15249456465244293
dqn reward tensor(-57.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.6811e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0701543316245079
dqn reward tensor(-217.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06836438924074173
dqn reward tensor(-313.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18736566603183746
dqn reward tensor(-176.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.1580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1759117841720581
dqn reward tensor(-103.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07114174216985703
dqn reward tensor(-162.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13204672932624817
dqn reward tensor(-102.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13040485978126526
dqn reward tensor(-140.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.0010e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23185670375823975
dqn reward tensor(-59.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14476177096366882
dqn reward tensor(-213.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.9475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15158328413963318
dqn reward tensor(-42.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09519913792610168
dqn reward tensor(-144.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14439702033996582
dqn reward tensor(-84.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4127e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1481156349182129
dqn reward tensor(-51.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.0216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1298452764749527
dqn reward tensor(-211., device='cuda:0') e 0.05 loss_dqn tensor(7.6413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.084516242146492
dqn reward tensor(-117.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8879e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25356632471084595
dqn reward tensor(-177.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1819949448108673
dqn reward tensor(-127.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20402991771697998
dqn reward tensor(-57.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09432826936244965
dqn reward tensor(-144.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10186107456684113
dqn reward tensor(-194.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9999e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3500431776046753
dqn reward tensor(-230.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.9040e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21739940345287323
dqn reward tensor(-9.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9786e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0632552057504654
dqn reward tensor(-214.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2845e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046555377542972565
dqn reward tensor(-173.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09976041316986084
dqn reward tensor(-68., device='cuda:0') e 0.05 loss_dqn tensor(7.9842e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10132433474063873
dqn reward tensor(10.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1726055145263672
dqn reward tensor(-84.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.7915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12571927905082703
dqn reward tensor(-79.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.3104e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056200578808784485
dqn reward tensor(-131.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19407373666763306
dqn reward tensor(-179.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1158e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22607071697711945
dqn reward tensor(-97.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2179e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12518519163131714
dqn reward tensor(-2.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22846728563308716
dqn reward tensor(-196.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2515106797218323
dqn reward tensor(-64.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11999989300966263
dqn reward tensor(-260.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4865e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13785357773303986
dqn reward tensor(-136.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0932813212275505
dqn reward tensor(-209.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11616192013025284
dqn reward tensor(-171.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1024218499660492
dqn reward tensor(-176.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05365675315260887
dqn reward tensor(11.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.5162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09210434556007385
dqn reward tensor(-287.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13770350813865662
dqn reward tensor(-94.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.2444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1037142276763916
dqn reward tensor(-141.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.255854070186615
dqn reward tensor(-79.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1720237284898758
dqn reward tensor(-70.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16315877437591553
dqn reward tensor(-147.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07085103541612625
dqn reward tensor(-213.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1835828721523285
dqn reward tensor(-99.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0747e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03485669195652008
dqn reward tensor(-27.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9411e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11456413567066193
dqn reward tensor(-193.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17135432362556458
dqn reward tensor(-229.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03255414590239525
dqn reward tensor(-47.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.9647e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2740953266620636
dqn reward tensor(-109.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10091893374919891
dqn reward tensor(42.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08301647007465363
dqn reward tensor(-99.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7800e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1383780539035797
dqn reward tensor(-161.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1778e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09646105766296387
dqn reward tensor(-225.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.4588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2039884775876999
dqn reward tensor(-252.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6001e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12154720723628998
dqn reward tensor(-205.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043944843113422394
dqn reward tensor(-191.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.6988e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10831261426210403
dqn reward tensor(-172.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07453510165214539
dqn reward tensor(-145.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.3856e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09437602758407593
dqn reward tensor(-150.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9982e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1783563643693924
dqn reward tensor(-221.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10828015208244324
dqn reward tensor(-165.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060975171625614166
dqn reward tensor(-188.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1907e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18192897737026215
dqn reward tensor(-41.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12612588703632355
dqn reward tensor(32.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1082717627286911
dqn reward tensor(-133.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08905459195375443
dqn reward tensor(-18.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12072943150997162
dqn reward tensor(-86.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07490507513284683
dqn reward tensor(-161.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2599186897277832
dqn reward tensor(-120., device='cuda:0') e 0.05 loss_dqn tensor(8.2395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07369373738765717
dqn reward tensor(-72.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21043755114078522
dqn reward tensor(-262.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1595e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0947754755616188
dqn reward tensor(-167.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2657955288887024
dqn reward tensor(-215., device='cuda:0') e 0.05 loss_dqn tensor(8.1855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10411372780799866
dqn reward tensor(-126.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1163923367857933
dqn reward tensor(-153.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08216790854930878
dqn reward tensor(-124.5625, device='cuda:0') e 0.05 loss_dqn tensor(8.0194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10763608664274216
dqn reward tensor(-225.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05795024335384369
dqn reward tensor(-164.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14972493052482605
dqn reward tensor(-154.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1479e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09728194028139114
dqn reward tensor(-107.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14869071543216705
dqn reward tensor(-223.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9245e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15165969729423523
dqn reward tensor(-169.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043475888669490814
dqn reward tensor(-113.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.9411e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09731752425432205
dqn reward tensor(-107.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8293e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031221356242895126
dqn reward tensor(-168.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.8053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06742534786462784
dqn reward tensor(-156.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.195022851228714
dqn reward tensor(-227., device='cuda:0') e 0.05 loss_dqn tensor(8.1451e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055285558104515076
dqn reward tensor(-224.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9920e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08475692570209503
dqn reward tensor(-175.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12858518958091736
dqn reward tensor(-95.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22486485540866852
dqn reward tensor(-197.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8511e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10106608271598816
dqn reward tensor(-154.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1117e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11041893064975739
dqn reward tensor(-64.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.7133e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18410658836364746
dqn reward tensor(-123.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.0349e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10023420304059982
dqn reward tensor(-199.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3878e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1104140654206276
dqn reward tensor(-76.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03996621444821358
dqn reward tensor(-141., device='cuda:0') e 0.05 loss_dqn tensor(8.1100e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1325448602437973
dqn reward tensor(-116.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9048e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13432835042476654
dqn reward tensor(-111.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9623e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053564514964818954
dqn reward tensor(-147.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1566e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06923724710941315
dqn reward tensor(-154., device='cuda:0') e 0.05 loss_dqn tensor(8.3476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16284993290901184
dqn reward tensor(-117.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16060927510261536
dqn reward tensor(-311.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8534e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15867723524570465
dqn reward tensor(-174.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2891e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11657737195491791
dqn reward tensor(-128.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12592104077339172
dqn reward tensor(-130.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3988e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16411396861076355
dqn reward tensor(-74.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02723442018032074
dqn reward tensor(-158.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1864e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0805327445268631
dqn reward tensor(-148.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3764e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12641990184783936
dqn reward tensor(-282.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.1904e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15398898720741272
dqn reward tensor(-205.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06703990697860718
dqn reward tensor(-14.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0635e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02546694502234459
dqn reward tensor(-199.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045411959290504456
dqn reward tensor(-108.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08281956613063812
dqn reward tensor(-245.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.0029e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13621079921722412
dqn reward tensor(-141.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5755e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12869271636009216
dqn reward tensor(-158.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02575727179646492
dqn reward tensor(-104.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0423e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20649434626102448
dqn reward tensor(-89.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.1748e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23946988582611084
dqn reward tensor(-133.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23091450333595276
dqn reward tensor(-106.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22213345766067505
dqn reward tensor(-121.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20371146500110626
dqn reward tensor(-70.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0065e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1434612274169922
dqn reward tensor(-4.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046454399824142456
Evaluating...
Train: {'rocauc': 0.7782897046273692} -2.2222933769226074
=====Epoch 33=====
Training...
dqn reward tensor(-155.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11184221506118774
dqn reward tensor(-196.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374700963497162
dqn reward tensor(-148.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3065868616104126
dqn reward tensor(-103.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055561263114213943
dqn reward tensor(-84.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14277154207229614
dqn reward tensor(-79.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8636e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16042228043079376
dqn reward tensor(-34.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.9663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07938018441200256
dqn reward tensor(-225.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10340644419193268
dqn reward tensor(-120.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08879726380109787
dqn reward tensor(-101.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.9095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16460402309894562
dqn reward tensor(-90.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15346255898475647
dqn reward tensor(-42.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04580328240990639
dqn reward tensor(-98.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14419741928577423
dqn reward tensor(-142., device='cuda:0') e 0.05 loss_dqn tensor(7.4290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10056906938552856
dqn reward tensor(-180.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.3092e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09068937599658966
dqn reward tensor(-222.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.1971e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15109197795391083
dqn reward tensor(-157.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03249649703502655
dqn reward tensor(-67.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12298691272735596
dqn reward tensor(-30.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07418188452720642
dqn reward tensor(-44.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07971915602684021
dqn reward tensor(-34.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17014922201633453
dqn reward tensor(-73.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0759e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19868871569633484
dqn reward tensor(-169.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1457e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09917724877595901
dqn reward tensor(15.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2028263509273529
dqn reward tensor(-19.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.4881e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024859558790922165
dqn reward tensor(-91.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32873404026031494
dqn reward tensor(5.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22568103671073914
dqn reward tensor(-3.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10094817727804184
dqn reward tensor(-58.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.1532e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17248374223709106
dqn reward tensor(15.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2532440721988678
dqn reward tensor(-66.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11272454261779785
dqn reward tensor(-3.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19500282406806946
dqn reward tensor(-48.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21624036133289337
dqn reward tensor(-105., device='cuda:0') e 0.05 loss_dqn tensor(6.9069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17823275923728943
dqn reward tensor(-199.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.183388352394104
dqn reward tensor(-21.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.7455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16096064448356628
dqn reward tensor(-108.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1051e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14573150873184204
dqn reward tensor(93., device='cuda:0') e 0.05 loss_dqn tensor(6.9798e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11769039928913116
dqn reward tensor(92.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9386e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23184168338775635
dqn reward tensor(-51.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14351393282413483
dqn reward tensor(23.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06374755501747131
dqn reward tensor(-101.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9556e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05563102662563324
dqn reward tensor(-79.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.6332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1078067421913147
dqn reward tensor(45.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2613e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10799287259578705
dqn reward tensor(-21.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04605964198708534
dqn reward tensor(-51.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1007375717163086
dqn reward tensor(-106., device='cuda:0') e 0.05 loss_dqn tensor(6.4609e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1161440759897232
dqn reward tensor(-89.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.7965e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06493033468723297
dqn reward tensor(-99.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0912565141916275
dqn reward tensor(-173.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15902167558670044
dqn reward tensor(-97.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.8670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19654501974582672
dqn reward tensor(-2.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.9477e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1709185391664505
dqn reward tensor(-88.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8519e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013304396532475948
dqn reward tensor(-105.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07935717701911926
dqn reward tensor(-42.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08503737300634384
dqn reward tensor(-21.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1505519300699234
dqn reward tensor(-129.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11738726496696472
dqn reward tensor(-77.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.4641e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03420967981219292
dqn reward tensor(-35.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6847e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16384004056453705
dqn reward tensor(-108.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1127e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10732994228601456
dqn reward tensor(82.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9546e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12882152199745178
dqn reward tensor(86., device='cuda:0') e 0.05 loss_dqn tensor(7.7850e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07915295660495758
dqn reward tensor(-87.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.9451e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08394429832696915
dqn reward tensor(20.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04450548440217972
dqn reward tensor(23., device='cuda:0') e 0.05 loss_dqn tensor(7.2303e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13562250137329102
dqn reward tensor(-133.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17703020572662354
dqn reward tensor(-80.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4502e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12810146808624268
dqn reward tensor(33.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.6540e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17794480919837952
dqn reward tensor(-202.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149431973695755
dqn reward tensor(-62.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.0522e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06249208003282547
dqn reward tensor(-82.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497184932231903
dqn reward tensor(-33.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6085e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1411287784576416
dqn reward tensor(-13.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.2922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04105029255151749
dqn reward tensor(-25., device='cuda:0') e 0.05 loss_dqn tensor(6.4268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13040801882743835
dqn reward tensor(-92.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4145e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08131822198629379
dqn reward tensor(19.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.2246e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19869083166122437
dqn reward tensor(112.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0352e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1669306755065918
dqn reward tensor(90.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.2474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14468899369239807
dqn reward tensor(-87., device='cuda:0') e 0.05 loss_dqn tensor(7.2243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09504589438438416
dqn reward tensor(-9.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058139994740486145
dqn reward tensor(-64., device='cuda:0') e 0.05 loss_dqn tensor(7.0227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12610924243927002
dqn reward tensor(-53.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.4587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13197410106658936
dqn reward tensor(54.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16429439187049866
dqn reward tensor(2.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04554559662938118
dqn reward tensor(-74.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0054e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16350945830345154
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8089e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.228526309132576
dqn reward tensor(18.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6235e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06628811359405518
dqn reward tensor(-95.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3343634605407715
dqn reward tensor(9.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9929e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11489184200763702
dqn reward tensor(-97.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3293e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10497000813484192
dqn reward tensor(-21.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5230e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12775710225105286
dqn reward tensor(55.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7865e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08754639327526093
dqn reward tensor(-14.4375, device='cuda:0') e 0.05 loss_dqn tensor(7.1342e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08919672667980194
dqn reward tensor(95.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05009191483259201
dqn reward tensor(-54.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3837e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06580908596515656
dqn reward tensor(-139.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23888812959194183
dqn reward tensor(-56., device='cuda:0') e 0.05 loss_dqn tensor(6.5970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14231732487678528
dqn reward tensor(-36.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0404597744345665
dqn reward tensor(43.0625, device='cuda:0') e 0.05 loss_dqn tensor(7.9338e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29450365900993347
dqn reward tensor(-30.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05031638592481613
dqn reward tensor(-69.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5758e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14098802208900452
dqn reward tensor(-126.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4700e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14699940383434296
dqn reward tensor(-74.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0908e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10064338892698288
dqn reward tensor(34.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.7663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21025827527046204
dqn reward tensor(-111.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08771327883005142
dqn reward tensor(-111.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5683e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09507551789283752
dqn reward tensor(-100.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08423246443271637
dqn reward tensor(-61.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5039e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0623430497944355
dqn reward tensor(-183., device='cuda:0') e 0.05 loss_dqn tensor(6.8521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11417721211910248
dqn reward tensor(102.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3185e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09035225212574005
dqn reward tensor(-93.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24704965949058533
dqn reward tensor(-94.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1802845299243927
dqn reward tensor(-71.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02187323570251465
dqn reward tensor(-50.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6672e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11706221103668213
dqn reward tensor(-74.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9691e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01727694272994995
dqn reward tensor(-110.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22819560766220093
dqn reward tensor(-138.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26450538635253906
dqn reward tensor(28.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09810595214366913
dqn reward tensor(-51.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7726e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10844075679779053
dqn reward tensor(-9.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4731e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1441635936498642
dqn reward tensor(-124.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.3108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16324478387832642
dqn reward tensor(6.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9983e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0874350517988205
dqn reward tensor(-4.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0953398197889328
dqn reward tensor(-24.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.1396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03746386617422104
dqn reward tensor(-66.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.8458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11863048374652863
dqn reward tensor(-90.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.092598095536232
dqn reward tensor(5.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10046085715293884
dqn reward tensor(-140.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1790894865989685
dqn reward tensor(-210.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0971e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05744737386703491
dqn reward tensor(22.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2013693004846573
dqn reward tensor(50.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.9973e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07039621472358704
dqn reward tensor(-118.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13876864314079285
dqn reward tensor(-103.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.0151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33078137040138245
dqn reward tensor(-159.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.322384238243103
dqn reward tensor(45.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16735555231571198
dqn reward tensor(33.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09467984735965729
dqn reward tensor(-29.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9158e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03073197230696678
dqn reward tensor(-149.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11362703144550323
dqn reward tensor(-85.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4287e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0351998507976532
dqn reward tensor(-2., device='cuda:0') e 0.05 loss_dqn tensor(6.7918e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10168597102165222
dqn reward tensor(-47.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2627848982810974
dqn reward tensor(132.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042394839227199554
dqn reward tensor(-61.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13821063935756683
dqn reward tensor(-61.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.7588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021082162857055664
dqn reward tensor(180.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4012e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12100844830274582
dqn reward tensor(-115.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0873197391629219
dqn reward tensor(-105.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06764709204435349
dqn reward tensor(10.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.017131468281149864
dqn reward tensor(-79.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025398381054401398
dqn reward tensor(-142.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08100210875272751
dqn reward tensor(100.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5912e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2429690957069397
dqn reward tensor(87.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17785313725471497
dqn reward tensor(-32.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5529e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1481669396162033
dqn reward tensor(25.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4404e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22535884380340576
dqn reward tensor(-64.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7183e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14825206995010376
dqn reward tensor(-9.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0904e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30302292108535767
dqn reward tensor(-106.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7832e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11544816195964813
dqn reward tensor(-90.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4762e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08056828379631042
dqn reward tensor(-59.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.9741e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0961051881313324
dqn reward tensor(30., device='cuda:0') e 0.05 loss_dqn tensor(7.6437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14298740029335022
dqn reward tensor(-150.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.0444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14519836008548737
dqn reward tensor(-98.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09482552111148834
dqn reward tensor(163.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06708034127950668
dqn reward tensor(53.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10040067881345749
dqn reward tensor(-164.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1750e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03887270390987396
dqn reward tensor(-77.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.0061e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.128790944814682
dqn reward tensor(-68., device='cuda:0') e 0.05 loss_dqn tensor(6.4374e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06598754972219467
dqn reward tensor(-183.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7415e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17081810534000397
dqn reward tensor(-43.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20139777660369873
dqn reward tensor(-102.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1007520779967308
dqn reward tensor(81.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.8417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19238752126693726
dqn reward tensor(1.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.7301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13429315388202667
dqn reward tensor(124.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.9768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08986079692840576
dqn reward tensor(63.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19133387506008148
dqn reward tensor(-0.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0753e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08953765034675598
dqn reward tensor(88.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.1016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1117953360080719
dqn reward tensor(-33., device='cuda:0') e 0.05 loss_dqn tensor(7.0136e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03625120222568512
dqn reward tensor(-13.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2482e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15401358902454376
dqn reward tensor(-54.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18060609698295593
dqn reward tensor(95.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7987e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21583154797554016
dqn reward tensor(-35.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4742e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04255756735801697
dqn reward tensor(90.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1172e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12765924632549286
dqn reward tensor(-95.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12035978585481644
dqn reward tensor(-22.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16528402268886566
dqn reward tensor(-22.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27143028378486633
dqn reward tensor(-45.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11443555355072021
dqn reward tensor(-73.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7563e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1581192910671234
dqn reward tensor(-142.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8125e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09910115599632263
dqn reward tensor(-30.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03248487412929535
dqn reward tensor(0.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2064e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11900272965431213
dqn reward tensor(-131.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5723e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19757580757141113
dqn reward tensor(-3., device='cuda:0') e 0.05 loss_dqn tensor(6.6658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09988195449113846
dqn reward tensor(-9.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07700612396001816
dqn reward tensor(-124.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08616439253091812
dqn reward tensor(-83.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.5835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1353030651807785
dqn reward tensor(-36., device='cuda:0') e 0.05 loss_dqn tensor(6.4127e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06220141798257828
dqn reward tensor(-119.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4837e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12558625638484955
dqn reward tensor(89.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08121724426746368
dqn reward tensor(13.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8858e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28234612941741943
dqn reward tensor(-195.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425437033176422
dqn reward tensor(-107.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.9748e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1287429928779602
dqn reward tensor(-38.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.7899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03635222464799881
dqn reward tensor(-147.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8896e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021031074225902557
dqn reward tensor(-11.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08817464113235474
dqn reward tensor(0.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09197212755680084
dqn reward tensor(-130.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13280490040779114
dqn reward tensor(-5., device='cuda:0') e 0.05 loss_dqn tensor(6.8128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08651285618543625
dqn reward tensor(92.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.5434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17041131854057312
dqn reward tensor(-7.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17551608383655548
dqn reward tensor(-188.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.9619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1620655208826065
dqn reward tensor(113.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16200590133666992
dqn reward tensor(-31.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16346824169158936
dqn reward tensor(24.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.7401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10617713630199432
dqn reward tensor(-14.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.6420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19443458318710327
dqn reward tensor(-33.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.5618e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03814567252993584
dqn reward tensor(-87.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6254e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09832146763801575
dqn reward tensor(37.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11638976633548737
dqn reward tensor(-69.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3208e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13701167702674866
dqn reward tensor(20.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10502602159976959
dqn reward tensor(-96.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8214e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29411202669143677
dqn reward tensor(-62., device='cuda:0') e 0.05 loss_dqn tensor(6.9706e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1636514663696289
dqn reward tensor(-111.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3424e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09747084975242615
dqn reward tensor(-122.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.0697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14304639399051666
dqn reward tensor(-71.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16924472153186798
dqn reward tensor(-2.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9058e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23080305755138397
dqn reward tensor(-129., device='cuda:0') e 0.05 loss_dqn tensor(6.5668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18995143473148346
dqn reward tensor(10.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5613e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08648952096700668
dqn reward tensor(-75.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09215544164180756
dqn reward tensor(38.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6641e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1789608895778656
dqn reward tensor(86.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.0267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.096074178814888
dqn reward tensor(-48.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09679610282182693
dqn reward tensor(-110., device='cuda:0') e 0.05 loss_dqn tensor(7.0963e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20446190237998962
dqn reward tensor(74.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08440303802490234
dqn reward tensor(-17.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4327e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027198757976293564
dqn reward tensor(-118., device='cuda:0') e 0.05 loss_dqn tensor(6.8823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08779814094305038
dqn reward tensor(1.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4793e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1465960443019867
dqn reward tensor(-47.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7072e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030224265530705452
dqn reward tensor(-94.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20154684782028198
dqn reward tensor(-16.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21401846408843994
dqn reward tensor(-109.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1162741556763649
dqn reward tensor(139.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7020e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18993151187896729
dqn reward tensor(-89.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.6881e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02374383993446827
dqn reward tensor(-4., device='cuda:0') e 0.05 loss_dqn tensor(6.5380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05396292358636856
dqn reward tensor(-74.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07312441617250443
dqn reward tensor(-155.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8506e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22046533226966858
dqn reward tensor(-35., device='cuda:0') e 0.05 loss_dqn tensor(7.2702e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15894082188606262
dqn reward tensor(-185.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11947444081306458
dqn reward tensor(29.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028480570763349533
dqn reward tensor(25.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1551559716463089
dqn reward tensor(33., device='cuda:0') e 0.05 loss_dqn tensor(6.5287e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03267114609479904
dqn reward tensor(-38.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6988e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07883499562740326
dqn reward tensor(-69.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.1149e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10944940894842148
dqn reward tensor(6.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16087877750396729
dqn reward tensor(-95.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7867e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09473472088575363
dqn reward tensor(63.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.2875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031172756105661392
dqn reward tensor(-126.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.5445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2992004156112671
dqn reward tensor(-116.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.5471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13417170941829681
dqn reward tensor(-93.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14108239114284515
dqn reward tensor(-55.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08515133708715439
dqn reward tensor(-39.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08860878646373749
dqn reward tensor(82.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1335916370153427
dqn reward tensor(8., device='cuda:0') e 0.05 loss_dqn tensor(6.8465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18088385462760925
dqn reward tensor(-3.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6576e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12112492322921753
dqn reward tensor(50.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5634e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15805917978286743
dqn reward tensor(-286.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.4446e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1239275261759758
dqn reward tensor(60.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7649e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12544895708560944
dqn reward tensor(9.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13795697689056396
dqn reward tensor(-135.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12372738122940063
dqn reward tensor(-50.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03754036873579025
dqn reward tensor(-113.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.0927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09648510068655014
dqn reward tensor(-1.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.0127e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20131321251392365
dqn reward tensor(-21.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21378278732299805
dqn reward tensor(106.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1697363555431366
dqn reward tensor(154.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9305e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12249375879764557
dqn reward tensor(-6., device='cuda:0') e 0.05 loss_dqn tensor(6.5249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15951572358608246
dqn reward tensor(70.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.9972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2347675859928131
dqn reward tensor(-88.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4391e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10428322851657867
dqn reward tensor(-152.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1973235309123993
dqn reward tensor(21., device='cuda:0') e 0.05 loss_dqn tensor(7.0625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05622593313455582
dqn reward tensor(-103.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12292557954788208
dqn reward tensor(-21.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14432668685913086
dqn reward tensor(-164.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09461529552936554
dqn reward tensor(-95.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.0888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12665525078773499
dqn reward tensor(-96.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9683e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16635346412658691
dqn reward tensor(-67.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.6737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07470526546239853
dqn reward tensor(-72.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13854220509529114
dqn reward tensor(-55.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08913038671016693
dqn reward tensor(-54.9375, device='cuda:0') e 0.05 loss_dqn tensor(7.9515e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031745702028274536
dqn reward tensor(-96.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09297522902488708
dqn reward tensor(-28.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1589037925004959
dqn reward tensor(-91.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.1972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12758421897888184
dqn reward tensor(-93.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7389e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.273041307926178
dqn reward tensor(-87., device='cuda:0') e 0.05 loss_dqn tensor(6.3641e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14799679815769196
dqn reward tensor(-92.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6577e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02660176530480385
dqn reward tensor(-70.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14041662216186523
dqn reward tensor(8.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13977187871932983
dqn reward tensor(-13.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.4248e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0805346667766571
dqn reward tensor(-22.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22001861035823822
dqn reward tensor(-16.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.2829e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1568925827741623
dqn reward tensor(69.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.5346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21914052963256836
dqn reward tensor(77.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15914225578308105
dqn reward tensor(-57.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5629e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038038305938243866
dqn reward tensor(-21.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13803359866142273
dqn reward tensor(-47.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19944307208061218
dqn reward tensor(-6.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.5181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19220119714736938
dqn reward tensor(-125.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04925118759274483
dqn reward tensor(-16.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08877449482679367
dqn reward tensor(-104., device='cuda:0') e 0.05 loss_dqn tensor(7.2979e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1378265917301178
dqn reward tensor(-8.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7581e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11841800808906555
dqn reward tensor(-2.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8230e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18968845903873444
dqn reward tensor(-127.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3925e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15080946683883667
dqn reward tensor(-14.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6452e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028160151094198227
dqn reward tensor(3.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1576102077960968
dqn reward tensor(-112.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9308e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18906578421592712
dqn reward tensor(-130.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09052938967943192
dqn reward tensor(-73.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.0081e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10208559036254883
dqn reward tensor(-97.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026221122592687607
dqn reward tensor(32.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.7037e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11262795329093933
dqn reward tensor(102.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.9385e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07323803007602692
dqn reward tensor(230.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1069074347615242
dqn reward tensor(116.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1927974820137024
dqn reward tensor(138.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.9331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1937016248703003
dqn reward tensor(189.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13806799054145813
dqn reward tensor(246.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6345e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1375802755355835
dqn reward tensor(294.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2578e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03103148192167282
dqn reward tensor(427.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.0831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09797605872154236
dqn reward tensor(345.3125, device='cuda:0') e 0.05 loss_dqn tensor(8.0127e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14306986331939697
dqn reward tensor(359., device='cuda:0') e 0.05 loss_dqn tensor(7.4415e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1474543809890747
dqn reward tensor(328.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.9392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1719246804714203
dqn reward tensor(96.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2023e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11314409971237183
dqn reward tensor(271.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.185298353433609
dqn reward tensor(352.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6681e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10898950695991516
dqn reward tensor(440.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.8176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09760285168886185
dqn reward tensor(197., device='cuda:0') e 0.05 loss_dqn tensor(7.4605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2592168152332306
dqn reward tensor(454., device='cuda:0') e 0.05 loss_dqn tensor(9.8497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1868024319410324
dqn reward tensor(414.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2569e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2308303564786911
dqn reward tensor(444., device='cuda:0') e 0.05 loss_dqn tensor(7.1612e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14997068047523499
dqn reward tensor(386.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8345e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10625743865966797
dqn reward tensor(494.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2057e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13768911361694336
dqn reward tensor(329.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1512e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1401953399181366
dqn reward tensor(264.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.8783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17779089510440826
dqn reward tensor(537.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.3655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03287266567349434
dqn reward tensor(330.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25166428089141846
dqn reward tensor(474.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.8107e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1349363923072815
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049621254205703735
dqn reward tensor(323.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.7274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16378170251846313
dqn reward tensor(401.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4868e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08260484784841537
dqn reward tensor(273.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10001999884843826
dqn reward tensor(437.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2861e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07082533091306686
dqn reward tensor(287.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1646328568458557
dqn reward tensor(385.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11799851059913635
dqn reward tensor(434.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3076e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18523508310317993
dqn reward tensor(351.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2574e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24127346277236938
dqn reward tensor(451.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2214e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09468778222799301
dqn reward tensor(517.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9252e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031823478639125824
dqn reward tensor(544., device='cuda:0') e 0.05 loss_dqn tensor(7.1836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12241863459348679
dqn reward tensor(381.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9001e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22075264155864716
dqn reward tensor(494.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8138e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13342870771884918
dqn reward tensor(612.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0827249139547348
dqn reward tensor(535.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1056491956114769
dqn reward tensor(603.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17588959634304047
dqn reward tensor(525.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3874e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24575413763523102
dqn reward tensor(357.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5868e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19081610441207886
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8737e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05603045970201492
dqn reward tensor(386.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9019e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08661241829395294
dqn reward tensor(172., device='cuda:0') e 0.05 loss_dqn tensor(1.7313e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05292010307312012
dqn reward tensor(363.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2022019326686859
dqn reward tensor(334.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8369e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05563623830676079
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7935e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1541115641593933
dqn reward tensor(286., device='cuda:0') e 0.05 loss_dqn tensor(1.7024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08674904704093933
dqn reward tensor(484.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6478e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22921259701251984
dqn reward tensor(489.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0013e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08598624914884567
dqn reward tensor(347.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.9209e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18320758640766144
dqn reward tensor(536.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2438516914844513
dqn reward tensor(467.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.7238e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032123249024152756
dqn reward tensor(461.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2747e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.279988169670105
dqn reward tensor(329.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7145e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12333046644926071
dqn reward tensor(370.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11650656163692474
dqn reward tensor(294.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5460e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3183022141456604
dqn reward tensor(408.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6733e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08963607996702194
dqn reward tensor(445.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3383e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14483140408992767
dqn reward tensor(438.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05550865828990936
dqn reward tensor(516.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.9890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10128693282604218
dqn reward tensor(387.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6599e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09052389115095139
dqn reward tensor(517.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6665e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048488274216651917
dqn reward tensor(411.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055883314460515976
dqn reward tensor(441.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.6026e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16836439073085785
dqn reward tensor(354.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.7573e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07285644114017487
dqn reward tensor(365.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7541e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16787303984165192
dqn reward tensor(339.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20541557669639587
dqn reward tensor(324.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1197e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14616641402244568
dqn reward tensor(407.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.1900e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15630674362182617
dqn reward tensor(277.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3981e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19764623045921326
dqn reward tensor(375.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.9990e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20988565683364868
dqn reward tensor(357.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9004e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10042350739240646
dqn reward tensor(332.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6665e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10569062829017639
dqn reward tensor(336.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1490740180015564
dqn reward tensor(400.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.6857e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09058896452188492
dqn reward tensor(407., device='cuda:0') e 0.05 loss_dqn tensor(1.7245e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13437850773334503
dqn reward tensor(192.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6922e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18821492791175842
dqn reward tensor(457.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07619350403547287
dqn reward tensor(357.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.3719e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04804747924208641
dqn reward tensor(442.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6445e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10695406794548035
dqn reward tensor(347.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8423e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21867096424102783
dqn reward tensor(599.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2124125361442566
dqn reward tensor(423.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3089e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07312185317277908
dqn reward tensor(448.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4275e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1641334891319275
dqn reward tensor(367.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2052e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10421603918075562
dqn reward tensor(462.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.0984e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05705701932311058
dqn reward tensor(398.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0707e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027524691075086594
dqn reward tensor(289., device='cuda:0') e 0.05 loss_dqn tensor(1.6188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21161603927612305
dqn reward tensor(379.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3557e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1802191138267517
dqn reward tensor(420.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7259e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0750056579709053
dqn reward tensor(284., device='cuda:0') e 0.05 loss_dqn tensor(5.1020e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08941477537155151
dqn reward tensor(514.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4689e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11369289457798004
dqn reward tensor(499., device='cuda:0') e 0.05 loss_dqn tensor(1.8455e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29787224531173706
dqn reward tensor(475.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1031e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0626286044716835
dqn reward tensor(316.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0728e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04370655491948128
dqn reward tensor(546.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.8107e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17343679070472717
dqn reward tensor(362.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14792372286319733
dqn reward tensor(249.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3756e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17365223169326782
dqn reward tensor(640.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07065574824810028
dqn reward tensor(523.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0682879164814949
dqn reward tensor(526.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1032e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08432601392269135
dqn reward tensor(779.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07458899170160294
dqn reward tensor(578.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032106444239616394
dqn reward tensor(480.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.2098e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1497897505760193
dqn reward tensor(565.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1372857689857483
dqn reward tensor(453.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8987e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10510287433862686
dqn reward tensor(751., device='cuda:0') e 0.05 loss_dqn tensor(3.1665e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3387221097946167
dqn reward tensor(483.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9982e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06704632937908173
dqn reward tensor(698.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10779336094856262
dqn reward tensor(669.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2252e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13993792235851288
dqn reward tensor(630., device='cuda:0') e 0.05 loss_dqn tensor(1.8212e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3024398684501648
dqn reward tensor(408.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9545e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27073949575424194
dqn reward tensor(667.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3044e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026194438338279724
dqn reward tensor(633., device='cuda:0') e 0.05 loss_dqn tensor(4.6894e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038810908794403076
dqn reward tensor(330.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07992926985025406
dqn reward tensor(623.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8581e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.164574533700943
dqn reward tensor(733.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13938851654529572
dqn reward tensor(682.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4605e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06697970628738403
dqn reward tensor(794.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12378758192062378
dqn reward tensor(677.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9567e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11726797372102737
dqn reward tensor(552., device='cuda:0') e 0.05 loss_dqn tensor(2.5762e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07174360007047653
dqn reward tensor(621.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08416378498077393
dqn reward tensor(530.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17146597802639008
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7021e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04454556852579117
dqn reward tensor(616.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6713e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2342725694179535
dqn reward tensor(813.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5864e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16969172656536102
dqn reward tensor(593.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2588e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19698837399482727
dqn reward tensor(635.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1266467124223709
dqn reward tensor(562.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.6540e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30851972103118896
dqn reward tensor(609.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.0298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06557981669902802
dqn reward tensor(544.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2580e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24668021500110626
dqn reward tensor(642.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6281e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16026179492473602
dqn reward tensor(762.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5205e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18439888954162598
dqn reward tensor(690.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1795607954263687
dqn reward tensor(528.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22110062837600708
dqn reward tensor(692.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2078821212053299
dqn reward tensor(526.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.0650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12413914501667023
dqn reward tensor(609.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8337e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0710638016462326
dqn reward tensor(561.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0591e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13650351762771606
dqn reward tensor(813., device='cuda:0') e 0.05 loss_dqn tensor(3.5854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15311120450496674
dqn reward tensor(538., device='cuda:0') e 0.05 loss_dqn tensor(8.3447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13178302347660065
dqn reward tensor(606.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5973e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14433512091636658
dqn reward tensor(442.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1993e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08893528580665588
dqn reward tensor(591., device='cuda:0') e 0.05 loss_dqn tensor(2.6530e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028571855276823044
dqn reward tensor(544.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9675e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09230398386716843
dqn reward tensor(744.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05475092679262161
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7940e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10120950639247894
dqn reward tensor(717.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09261438250541687
dqn reward tensor(674.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5050e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17884235084056854
dqn reward tensor(574.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.9168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21403133869171143
dqn reward tensor(730.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7651e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37910377979278564
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6618e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13710695505142212
dqn reward tensor(738.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.140640988945961
dqn reward tensor(629.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1649908423423767
dqn reward tensor(548., device='cuda:0') e 0.05 loss_dqn tensor(2.3618e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1302899420261383
dqn reward tensor(557.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08317950367927551
dqn reward tensor(781.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09628652036190033
dqn reward tensor(559.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.7227e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03287125378847122
dqn reward tensor(600.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0677e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29406505823135376
dqn reward tensor(717.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9382e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15083537995815277
dqn reward tensor(762.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08475721627473831
dqn reward tensor(547.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8994e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31232360005378723
dqn reward tensor(559.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6822e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.142493337392807
dqn reward tensor(605.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6897e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23504367470741272
dqn reward tensor(660.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16795971989631653
dqn reward tensor(570., device='cuda:0') e 0.05 loss_dqn tensor(1.3425e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09446068108081818
dqn reward tensor(661.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7264e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1283303201198578
dqn reward tensor(513.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2553251385688782
dqn reward tensor(580.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4606e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21400170028209686
dqn reward tensor(534.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7283e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10531830042600632
dqn reward tensor(525.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9670e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06634053587913513
dqn reward tensor(630.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7396e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1704305112361908
dqn reward tensor(559.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13890774548053741
dqn reward tensor(575.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3674e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12616397440433502
dqn reward tensor(731.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20257538557052612
dqn reward tensor(673.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5466e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06591663509607315
dqn reward tensor(650.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0385e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16304130852222443
dqn reward tensor(701.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20742139220237732
dqn reward tensor(602.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25069063901901245
dqn reward tensor(585.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0656e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14034810662269592
dqn reward tensor(623.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1573e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10298214852809906
dqn reward tensor(757., device='cuda:0') e 0.05 loss_dqn tensor(3.8311e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0916101485490799
dqn reward tensor(676., device='cuda:0') e 0.05 loss_dqn tensor(3.5119e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20838552713394165
dqn reward tensor(601.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0724e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24648618698120117
dqn reward tensor(651.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2409e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15595778822898865
dqn reward tensor(703.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6543e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28815948963165283
dqn reward tensor(687.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6556e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11452968418598175
dqn reward tensor(585.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0906e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32150590419769287
dqn reward tensor(417.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.3517e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2118465155363083
dqn reward tensor(642.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09333109855651855
dqn reward tensor(684.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1387e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12541499733924866
dqn reward tensor(77.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7050e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033395182341337204
Evaluating...
Train: {'rocauc': 0.783253052167448} 9.627023696899414
=====Epoch 34=====
Training...
dqn reward tensor(566.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.1632e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1468125581741333
dqn reward tensor(628.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1345093846321106
dqn reward tensor(591., device='cuda:0') e 0.05 loss_dqn tensor(3.9709e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10684552043676376
dqn reward tensor(668.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13019993901252747
dqn reward tensor(700.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0505e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16011399030685425
dqn reward tensor(586.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21995091438293457
dqn reward tensor(694.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5443e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1536426544189453
dqn reward tensor(506.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0943e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04061999171972275
dqn reward tensor(663.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10795477032661438
dqn reward tensor(564.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9843e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10131779313087463
dqn reward tensor(720.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033750321716070175
dqn reward tensor(539.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.5267e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11327217519283295
dqn reward tensor(681.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18160569667816162
dqn reward tensor(723.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1665e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21902841329574585
dqn reward tensor(636., device='cuda:0') e 0.05 loss_dqn tensor(1.1106e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1280822604894638
dqn reward tensor(543.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7940e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22786132991313934
dqn reward tensor(622.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1236e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20778518915176392
dqn reward tensor(672.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13381725549697876
dqn reward tensor(528.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6733e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10878065973520279
dqn reward tensor(664.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15475279092788696
dqn reward tensor(771.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07535304874181747
dqn reward tensor(554., device='cuda:0') e 0.05 loss_dqn tensor(2.6982e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13471809029579163
dqn reward tensor(311.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4133e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07855696976184845
dqn reward tensor(572.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.6092e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14166328310966492
dqn reward tensor(539.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6011e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1326628178358078
dqn reward tensor(677.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12227332592010498
dqn reward tensor(692.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03249875456094742
dqn reward tensor(753.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2096613496541977
dqn reward tensor(730.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2546e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17953507602214813
dqn reward tensor(593.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.085409015417099
dqn reward tensor(626.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.0181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029970739036798477
dqn reward tensor(593.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14844530820846558
dqn reward tensor(571.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6242e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0405762754380703
dqn reward tensor(680.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0724e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07898949086666107
dqn reward tensor(780.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0635e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19945290684700012
dqn reward tensor(628.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5151e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07336199283599854
dqn reward tensor(652.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18924932181835175
dqn reward tensor(514.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0306e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14083141088485718
dqn reward tensor(622.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1770e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16624122858047485
dqn reward tensor(643., device='cuda:0') e 0.05 loss_dqn tensor(4.0531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14172275364398956
dqn reward tensor(539.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6910e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17825548350811005
dqn reward tensor(617.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1882e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19426880776882172
dqn reward tensor(387.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0286e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1425580084323883
dqn reward tensor(730.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3122e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059023015201091766
dqn reward tensor(468.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4185e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15470772981643677
dqn reward tensor(474.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1274772733449936
dqn reward tensor(555.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.1266e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21342989802360535
dqn reward tensor(619.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05565435811877251
dqn reward tensor(801.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19462370872497559
dqn reward tensor(726.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10980647802352905
dqn reward tensor(695.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6239e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11923088133335114
dqn reward tensor(661.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03821922838687897
dqn reward tensor(659.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.8005e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1293288767337799
dqn reward tensor(586.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07831216603517532
dqn reward tensor(545.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09209626913070679
dqn reward tensor(572.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2169140726327896
dqn reward tensor(651.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09477671980857849
dqn reward tensor(450.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1412e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.067905493080616
dqn reward tensor(778., device='cuda:0') e 0.05 loss_dqn tensor(3.8981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2433956265449524
dqn reward tensor(340.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8833e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10656014084815979
dqn reward tensor(693.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6471e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15373368561267853
dqn reward tensor(526., device='cuda:0') e 0.05 loss_dqn tensor(4.2406e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17504018545150757
dqn reward tensor(520.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.9726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2549562156200409
dqn reward tensor(462.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3432e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16870605945587158
dqn reward tensor(730., device='cuda:0') e 0.05 loss_dqn tensor(3.6401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020862093195319176
dqn reward tensor(765.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06742306798696518
dqn reward tensor(488.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11114878952503204
dqn reward tensor(592.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08752374351024628
dqn reward tensor(630.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08825164288282394
dqn reward tensor(781.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23348823189735413
dqn reward tensor(621.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6900e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08793514966964722
dqn reward tensor(508.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6915e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08570300042629242
dqn reward tensor(516.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.6795e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10772059857845306
dqn reward tensor(743.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2203599363565445
dqn reward tensor(735.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0499e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09998776018619537
dqn reward tensor(648.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7267e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11733541637659073
dqn reward tensor(378., device='cuda:0') e 0.05 loss_dqn tensor(2.1351e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08879180252552032
dqn reward tensor(739.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0265e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10743680596351624
dqn reward tensor(678., device='cuda:0') e 0.05 loss_dqn tensor(3.9509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13209712505340576
dqn reward tensor(387.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4356e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414116770029068
dqn reward tensor(525.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09536594152450562
dqn reward tensor(649., device='cuda:0') e 0.05 loss_dqn tensor(3.5726e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06841331720352173
dqn reward tensor(679.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1252e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06573345512151718
dqn reward tensor(690.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8002e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09498235583305359
dqn reward tensor(609.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.5705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03417322039604187
dqn reward tensor(465.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2587e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18022939562797546
dqn reward tensor(674.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7406e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12883096933364868
dqn reward tensor(611.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.1064e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1520867943763733
dqn reward tensor(754.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4383e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17641620337963104
dqn reward tensor(687.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08267618715763092
dqn reward tensor(681.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06446410715579987
dqn reward tensor(723.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6179e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12185987085103989
dqn reward tensor(374., device='cuda:0') e 0.05 loss_dqn tensor(2.4505e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14504331350326538
dqn reward tensor(398.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4068e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0530642606317997
dqn reward tensor(464.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6289e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11081930994987488
dqn reward tensor(578.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05166291445493698
dqn reward tensor(502.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2150e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03011426143348217
dqn reward tensor(649.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7790e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2816582918167114
dqn reward tensor(482.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7325e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07911679148674011
dqn reward tensor(577.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9109e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05915383622050285
dqn reward tensor(751.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6706e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16601760685443878
dqn reward tensor(585.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2324e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06278973817825317
dqn reward tensor(698.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6412e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14385463297367096
dqn reward tensor(567.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.4959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03969243913888931
dqn reward tensor(544.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9765e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031595394015312195
dqn reward tensor(735.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5984e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15414880216121674
dqn reward tensor(638.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19255858659744263
dqn reward tensor(459.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0334e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036512650549411774
dqn reward tensor(630.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15159882605075836
dqn reward tensor(582.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15152671933174133
dqn reward tensor(593.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8386e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11398832499980927
dqn reward tensor(748.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039960458874702454
dqn reward tensor(543.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01695547066628933
dqn reward tensor(652.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9617e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047179222106933594
dqn reward tensor(809.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16664041578769684
dqn reward tensor(559., device='cuda:0') e 0.05 loss_dqn tensor(1.3854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10516349971294403
dqn reward tensor(584.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17539900541305542
dqn reward tensor(640.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14254873991012573
dqn reward tensor(601.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0766e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1955268383026123
dqn reward tensor(613.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9068e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1529570370912552
dqn reward tensor(663.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0798e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.105535127222538
dqn reward tensor(468.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7713e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2625914216041565
dqn reward tensor(569.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.155396968126297
dqn reward tensor(523., device='cuda:0') e 0.05 loss_dqn tensor(1.3987e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15715673565864563
dqn reward tensor(494.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5428e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20825397968292236
dqn reward tensor(545.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8462e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30844882130622864
dqn reward tensor(569.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13755738735198975
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.1942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19240489602088928
dqn reward tensor(644.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04909001290798187
dqn reward tensor(712.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15593357384204865
dqn reward tensor(528.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12168623507022858
dqn reward tensor(586.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12714609503746033
dqn reward tensor(679.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7121e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08091878890991211
dqn reward tensor(623.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26037850975990295
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3300e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12074373662471771
dqn reward tensor(518.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4751e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04056764394044876
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10518770664930344
dqn reward tensor(542.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7916e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19142916798591614
dqn reward tensor(586.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3915e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10770465433597565
dqn reward tensor(641.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2129971832036972
dqn reward tensor(759.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1746213138103485
dqn reward tensor(574.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11390531808137894
dqn reward tensor(547.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5618e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05391111969947815
dqn reward tensor(700.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4762e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2696523368358612
dqn reward tensor(509.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3334e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13090063631534576
dqn reward tensor(495.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6292e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21557331085205078
dqn reward tensor(504.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8748e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25414592027664185
dqn reward tensor(742.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6007e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17237338423728943
dqn reward tensor(850.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11688101291656494
dqn reward tensor(665.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09725002944469452
dqn reward tensor(694.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1486256718635559
dqn reward tensor(654.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3945e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1466766893863678
dqn reward tensor(757.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0814e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23533247411251068
dqn reward tensor(712.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06767839193344116
dqn reward tensor(625.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9646e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10895658284425735
dqn reward tensor(623.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5892e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12481358647346497
dqn reward tensor(630.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.0237e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07518783956766129
dqn reward tensor(579.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8186e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0786493718624115
dqn reward tensor(695.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.1591e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08929477632045746
dqn reward tensor(521.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14934012293815613
dqn reward tensor(515.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1057e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1005326583981514
dqn reward tensor(753.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1243707686662674
dqn reward tensor(745.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07512263208627701
dqn reward tensor(580.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.3893e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13324688374996185
dqn reward tensor(529.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038224853575229645
dqn reward tensor(614., device='cuda:0') e 0.05 loss_dqn tensor(1.4485e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09293685853481293
dqn reward tensor(487.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07650452852249146
dqn reward tensor(736.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15475647151470184
dqn reward tensor(658.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7031e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12839500606060028
dqn reward tensor(586.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5223e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06945916265249252
dqn reward tensor(700.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7490e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2448306381702423
dqn reward tensor(586.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.8130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1787225604057312
dqn reward tensor(789.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4684e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012621384114027023
dqn reward tensor(636., device='cuda:0') e 0.05 loss_dqn tensor(3.9638e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.167763352394104
dqn reward tensor(687.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5633e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2536883056163788
dqn reward tensor(416.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8935e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18716809153556824
dqn reward tensor(676.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06951431930065155
dqn reward tensor(486.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3343e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05745849385857582
dqn reward tensor(632.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0868e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10132598876953125
dqn reward tensor(530.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8115e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08980469405651093
dqn reward tensor(619.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09756075590848923
dqn reward tensor(624.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7344e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21509727835655212
dqn reward tensor(573.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9884e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08161839097738266
dqn reward tensor(609.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5434e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028359796851873398
dqn reward tensor(654.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6389e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09957261383533478
dqn reward tensor(611.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8613e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09223130345344543
dqn reward tensor(684.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11753956228494644
dqn reward tensor(628.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22997602820396423
dqn reward tensor(562.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0438467413187027
dqn reward tensor(587.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2580e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15129457414150238
dqn reward tensor(618.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5265e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21762463450431824
dqn reward tensor(606., device='cuda:0') e 0.05 loss_dqn tensor(4.3464e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1850048303604126
dqn reward tensor(652.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034811049699783325
dqn reward tensor(687.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08008644729852676
dqn reward tensor(446.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8320e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13981015980243683
dqn reward tensor(590., device='cuda:0') e 0.05 loss_dqn tensor(2.1355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15260466933250427
dqn reward tensor(716.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2239508479833603
dqn reward tensor(561.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0290e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12526440620422363
dqn reward tensor(697.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0055e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03405187278985977
dqn reward tensor(381., device='cuda:0') e 0.05 loss_dqn tensor(2.2588e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2911580204963684
dqn reward tensor(615.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07874757051467896
dqn reward tensor(341.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1019684299826622
dqn reward tensor(707.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8485e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04509356617927551
dqn reward tensor(657.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10845673829317093
dqn reward tensor(460.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.1210e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21700720489025116
dqn reward tensor(607.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5245e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11274508386850357
dqn reward tensor(747.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045408062636852264
dqn reward tensor(576.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2522e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1957429200410843
dqn reward tensor(558.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7457e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1465153992176056
dqn reward tensor(762., device='cuda:0') e 0.05 loss_dqn tensor(3.6648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04340095818042755
dqn reward tensor(675., device='cuda:0') e 0.05 loss_dqn tensor(4.2818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12422159314155579
dqn reward tensor(650., device='cuda:0') e 0.05 loss_dqn tensor(3.7336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14431779086589813
dqn reward tensor(822.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12446513772010803
dqn reward tensor(516.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03069854900240898
dqn reward tensor(566., device='cuda:0') e 0.05 loss_dqn tensor(7.2408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03910268843173981
dqn reward tensor(597.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020940333604812622
dqn reward tensor(628.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8901e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02306525968015194
dqn reward tensor(534.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6478e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15663829445838928
dqn reward tensor(675.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0177e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11521811783313751
dqn reward tensor(759.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3820e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06324255466461182
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16262969374656677
dqn reward tensor(509.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5821e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04049548879265785
dqn reward tensor(585.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061231862753629684
dqn reward tensor(589.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.6438e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13677051663398743
dqn reward tensor(487.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8411e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07412127405405045
dqn reward tensor(560.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08511064201593399
dqn reward tensor(641.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2280905544757843
dqn reward tensor(622.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3196e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16024765372276306
dqn reward tensor(685.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23731373250484467
dqn reward tensor(592.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21122708916664124
dqn reward tensor(846.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036456793546676636
dqn reward tensor(602.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0272268895059824
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0706e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1190321072936058
dqn reward tensor(807.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21586348116397858
dqn reward tensor(505.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.3932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1763431429862976
dqn reward tensor(658.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.6515e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16680027544498444
dqn reward tensor(507.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5819e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1912359893321991
dqn reward tensor(699., device='cuda:0') e 0.05 loss_dqn tensor(2.7194e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09380312263965607
dqn reward tensor(484., device='cuda:0') e 0.05 loss_dqn tensor(2.9966e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15486513078212738
dqn reward tensor(719.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14362815022468567
dqn reward tensor(680., device='cuda:0') e 0.05 loss_dqn tensor(3.6824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05506880208849907
dqn reward tensor(518.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0018e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2024819403886795
dqn reward tensor(620.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17359575629234314
dqn reward tensor(635.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07042006403207779
dqn reward tensor(446.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7646e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08939890563488007
dqn reward tensor(587.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04840036854147911
dqn reward tensor(686.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2999865710735321
dqn reward tensor(695.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2431e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09158509969711304
dqn reward tensor(576.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.6614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09340597689151764
dqn reward tensor(518.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0252e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1940060555934906
dqn reward tensor(697.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4975e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2943562865257263
dqn reward tensor(545., device='cuda:0') e 0.05 loss_dqn tensor(3.7776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18077175319194794
dqn reward tensor(547.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10156242549419403
dqn reward tensor(641., device='cuda:0') e 0.05 loss_dqn tensor(2.6895e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03868665173649788
dqn reward tensor(711., device='cuda:0') e 0.05 loss_dqn tensor(1.0879e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13612449169158936
dqn reward tensor(660.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036933112889528275
dqn reward tensor(498.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8153e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050699613988399506
dqn reward tensor(690.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17072424292564392
dqn reward tensor(626., device='cuda:0') e 0.05 loss_dqn tensor(3.9028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12101471424102783
dqn reward tensor(463.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8266e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17777952551841736
dqn reward tensor(732.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2338e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028166934847831726
dqn reward tensor(651.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4732e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12276329100131989
dqn reward tensor(525.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0180e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18739554286003113
dqn reward tensor(609., device='cuda:0') e 0.05 loss_dqn tensor(3.5473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29559552669525146
dqn reward tensor(555.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0277e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2826094925403595
dqn reward tensor(622.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0934e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3185637295246124
dqn reward tensor(697.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05438527092337608
dqn reward tensor(633.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18796560168266296
dqn reward tensor(610.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21492637693881989
dqn reward tensor(730.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10849671065807343
dqn reward tensor(634.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0024e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1991235911846161
dqn reward tensor(606., device='cuda:0') e 0.05 loss_dqn tensor(4.5854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2091488540172577
dqn reward tensor(652.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14708614349365234
dqn reward tensor(682.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.3510e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15514099597930908
dqn reward tensor(590., device='cuda:0') e 0.05 loss_dqn tensor(3.9373e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.163251593708992
dqn reward tensor(591.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0377e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23481008410453796
dqn reward tensor(610., device='cuda:0') e 0.05 loss_dqn tensor(3.9923e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15810240805149078
dqn reward tensor(515.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6681e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17952504754066467
dqn reward tensor(793.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18226218223571777
dqn reward tensor(673.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2845e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09328432381153107
dqn reward tensor(655.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8122e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13043135404586792
dqn reward tensor(441., device='cuda:0') e 0.05 loss_dqn tensor(6.4166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13356393575668335
dqn reward tensor(313.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4521e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23380737006664276
dqn reward tensor(648.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.6128e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035093508660793304
dqn reward tensor(432., device='cuda:0') e 0.05 loss_dqn tensor(1.2335e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08139817416667938
dqn reward tensor(739.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7021e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16904453933238983
dqn reward tensor(706.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.274860143661499
dqn reward tensor(619.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08282846212387085
dqn reward tensor(533.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07476988434791565
dqn reward tensor(736.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4747e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14822977781295776
dqn reward tensor(647.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8765e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07078904658555984
dqn reward tensor(682.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.9808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1471009999513626
dqn reward tensor(626.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6562e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09114109724760056
dqn reward tensor(647.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07331515848636627
dqn reward tensor(886.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014647288247942924
dqn reward tensor(591.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24562986195087433
dqn reward tensor(634.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09377181529998779
dqn reward tensor(624.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2025541365146637
dqn reward tensor(661.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21744105219841003
dqn reward tensor(704.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.089181087911129
dqn reward tensor(739.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7131e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11365360766649246
dqn reward tensor(527.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.5432e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1979082226753235
dqn reward tensor(401., device='cuda:0') e 0.05 loss_dqn tensor(1.2815e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16195757687091827
dqn reward tensor(687.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7322e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06447016447782516
dqn reward tensor(437.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1419e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031303033232688904
dqn reward tensor(496.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07524850964546204
dqn reward tensor(788.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2679656147956848
dqn reward tensor(695.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3765e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05662189796566963
dqn reward tensor(602.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17524366080760956
dqn reward tensor(598.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3140e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038152750581502914
dqn reward tensor(758.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14001382887363434
dqn reward tensor(595.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2479393184185028
dqn reward tensor(689.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7206e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2725107967853546
dqn reward tensor(417.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0438e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2150970697402954
dqn reward tensor(511.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12297038733959198
dqn reward tensor(651.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.2915e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07344260811805725
dqn reward tensor(606.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10040654987096786
dqn reward tensor(516.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9678e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11228682100772858
dqn reward tensor(545., device='cuda:0') e 0.05 loss_dqn tensor(2.1525e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2479453980922699
dqn reward tensor(559.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3829e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06274496018886566
dqn reward tensor(624.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.2804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11124332994222641
dqn reward tensor(709., device='cuda:0') e 0.05 loss_dqn tensor(3.7569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10327940434217453
dqn reward tensor(733.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2339e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15747512876987457
dqn reward tensor(457., device='cuda:0') e 0.05 loss_dqn tensor(5.9408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05395296961069107
dqn reward tensor(689.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1491849720478058
dqn reward tensor(603.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7555e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07013065367937088
dqn reward tensor(670.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05793460085988045
dqn reward tensor(659., device='cuda:0') e 0.05 loss_dqn tensor(4.1007e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23219028115272522
dqn reward tensor(738.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09245322644710541
dqn reward tensor(604.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.5930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10561446845531464
dqn reward tensor(747.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344452202320099
dqn reward tensor(625.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0632e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1261911541223526
dqn reward tensor(740.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.342407763004303
dqn reward tensor(718.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07186467200517654
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3091e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12284709513187408
dqn reward tensor(463.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3486e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1475510150194168
dqn reward tensor(725., device='cuda:0') e 0.05 loss_dqn tensor(4.2392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09113064408302307
dqn reward tensor(442.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5472e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20663993060588837
dqn reward tensor(461.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8017e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19975601136684418
dqn reward tensor(636.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08382783830165863
dqn reward tensor(551.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7492e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21632039546966553
dqn reward tensor(621.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4148e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1282445639371872
dqn reward tensor(618., device='cuda:0') e 0.05 loss_dqn tensor(3.9437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0811205729842186
dqn reward tensor(623.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2981e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17675933241844177
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5549e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032474882900714874
dqn reward tensor(422.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.4493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1763923019170761
dqn reward tensor(717.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8303e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05148312449455261
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15823641419410706
dqn reward tensor(661.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.6587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1361207664012909
dqn reward tensor(596.6875, device='cuda:0') e 0.05 loss_dqn tensor(4.0152e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19811095297336578
dqn reward tensor(615.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14311455190181732
dqn reward tensor(659.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0352e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04280677065253258
dqn reward tensor(622.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20725831389427185
dqn reward tensor(496., device='cuda:0') e 0.05 loss_dqn tensor(3.7510e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06970100849866867
dqn reward tensor(523., device='cuda:0') e 0.05 loss_dqn tensor(2.8611e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22190609574317932
dqn reward tensor(310.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8386e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08528212457895279
dqn reward tensor(607.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07086622714996338
dqn reward tensor(653.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1801852285861969
dqn reward tensor(549.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5745e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22347715497016907
dqn reward tensor(509.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4416e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20914244651794434
dqn reward tensor(557.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1061e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15065918862819672
dqn reward tensor(566.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0169e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10695165395736694
dqn reward tensor(662.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5522e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07289056479930878
dqn reward tensor(641.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07049097120761871
dqn reward tensor(624.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1721690446138382
dqn reward tensor(687.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025020219385623932
dqn reward tensor(430.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1683e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1389371156692505
dqn reward tensor(484.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0361e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20705968141555786
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16521351039409637
dqn reward tensor(664.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.0802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16742894053459167
dqn reward tensor(410.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.3005e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22273977100849152
dqn reward tensor(632., device='cuda:0') e 0.05 loss_dqn tensor(4.0380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15244454145431519
dqn reward tensor(670.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1135e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11507309973239899
dqn reward tensor(689., device='cuda:0') e 0.05 loss_dqn tensor(9.1777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17245815694332123
dqn reward tensor(453.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12776817381381989
dqn reward tensor(473.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8874e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09206846356391907
dqn reward tensor(548.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18148519098758698
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0472e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1399153470993042
dqn reward tensor(658.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3446e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0927823930978775
dqn reward tensor(596.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2749e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07405000180006027
dqn reward tensor(598.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1428e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04758854955434799
dqn reward tensor(542.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6896e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06474436819553375
dqn reward tensor(648.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6184e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1704607605934143
dqn reward tensor(472.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15497857332229614
dqn reward tensor(595.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18948028981685638
dqn reward tensor(501.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11387160420417786
dqn reward tensor(566.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1689e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407552808523178
dqn reward tensor(570.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7824e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042875323444604874
dqn reward tensor(717., device='cuda:0') e 0.05 loss_dqn tensor(3.7501e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1639024019241333
dqn reward tensor(656.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1402e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3883431553840637
dqn reward tensor(654.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14667978882789612
dqn reward tensor(620.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4395e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22679093480110168
dqn reward tensor(463.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3306e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031802836805582047
dqn reward tensor(767.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2013e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12652193009853363
dqn reward tensor(607.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7251e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04267166927456856
dqn reward tensor(601.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7990e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1377834528684616
dqn reward tensor(638.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.0394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04115399718284607
dqn reward tensor(708.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3347e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12383352965116501
dqn reward tensor(488.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.2113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08210477232933044
dqn reward tensor(570.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6023e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15729492902755737
dqn reward tensor(515.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.2395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1693291962146759
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10000242292881012
dqn reward tensor(767.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6730e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08313624560832977
dqn reward tensor(613.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7065e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05325435847043991
dqn reward tensor(620.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07417552173137665
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9183e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033323001116514206
dqn reward tensor(708.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2796e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22000829875469208
dqn reward tensor(754.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05159572884440422
dqn reward tensor(725.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14866618812084198
dqn reward tensor(623.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09214429557323456
dqn reward tensor(607.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08872370421886444
dqn reward tensor(615.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3484e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1703002154827118
dqn reward tensor(490.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01670631393790245
dqn reward tensor(589.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31756263971328735
dqn reward tensor(587.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7058e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15127162635326385
dqn reward tensor(650.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7882e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1278236210346222
dqn reward tensor(468.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2538e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25999149680137634
dqn reward tensor(811.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12274680286645889
dqn reward tensor(532., device='cuda:0') e 0.05 loss_dqn tensor(1.9273e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1812918782234192
dqn reward tensor(550.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9849e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12053433060646057
dqn reward tensor(556.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5765e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049793921411037445
dqn reward tensor(502.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6417e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1267654448747635
dqn reward tensor(680.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.3445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035826537758111954
dqn reward tensor(681.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5007e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1853533685207367
dqn reward tensor(645.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2256e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1687784045934677
dqn reward tensor(441.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3095e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20415788888931274
dqn reward tensor(595.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08754071593284607
dqn reward tensor(667.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0449872761964798
dqn reward tensor(606.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.9467e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18772517144680023
dqn reward tensor(606.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8602e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13483548164367676
dqn reward tensor(689.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8221e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2542015314102173
dqn reward tensor(685.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6015e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.100948765873909
dqn reward tensor(626.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2797e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05415097996592522
dqn reward tensor(804.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10210299491882324
dqn reward tensor(341.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.1559e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03768384829163551
dqn reward tensor(583.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.4509e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06212415546178818
dqn reward tensor(575.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2451e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03000389225780964
dqn reward tensor(641.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6674e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15812064707279205
dqn reward tensor(526.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13178831338882446
dqn reward tensor(510.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5935e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12645979225635529
dqn reward tensor(601.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05496668815612793
dqn reward tensor(620.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9258e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17886027693748474
dqn reward tensor(568.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.3488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07371916621923447
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6159e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10685057193040848
dqn reward tensor(691.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.8397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08510588109493256
dqn reward tensor(633.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8070e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12070298939943314
dqn reward tensor(613., device='cuda:0') e 0.05 loss_dqn tensor(1.5510e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08765150606632233
dqn reward tensor(532.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7301e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11202359199523926
dqn reward tensor(708.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6379e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09770747274160385
dqn reward tensor(604.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.0278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3164772689342499
dqn reward tensor(480.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2582e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13638198375701904
dqn reward tensor(620.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.6129e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09220744669437408
dqn reward tensor(413.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6193e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13274477422237396
dqn reward tensor(669.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1607e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18173269927501678
dqn reward tensor(626.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4491e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17983706295490265
dqn reward tensor(851.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13468395173549652
dqn reward tensor(474.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.8247e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.164504736661911
dqn reward tensor(527.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5470e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3442923426628113
dqn reward tensor(600.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3156e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.099456287920475
dqn reward tensor(573.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0512e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21063004434108734
dqn reward tensor(674.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4432e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24034768342971802
dqn reward tensor(506.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0036e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06348836421966553
dqn reward tensor(478.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9863e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10902731120586395
dqn reward tensor(557., device='cuda:0') e 0.05 loss_dqn tensor(4.2911e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12317050993442535
dqn reward tensor(571.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2659e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2205672413110733
dqn reward tensor(518.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6918e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17641234397888184
dqn reward tensor(505.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1327e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12364986538887024
dqn reward tensor(821., device='cuda:0') e 0.05 loss_dqn tensor(3.9078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14590679109096527
dqn reward tensor(471.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.5053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05646209046244621
dqn reward tensor(288.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32536327838897705
dqn reward tensor(500., device='cuda:0') e 0.05 loss_dqn tensor(1.5591e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04370374232530594
dqn reward tensor(602.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9438e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03296361118555069
dqn reward tensor(576.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4128e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1512843668460846
dqn reward tensor(616.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8526e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2825295925140381
dqn reward tensor(652.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21921540796756744
dqn reward tensor(693., device='cuda:0') e 0.05 loss_dqn tensor(5.2788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15728315711021423
dqn reward tensor(699.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14098842442035675
dqn reward tensor(537.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3986e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06033442169427872
dqn reward tensor(585.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1813e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10003183037042618
dqn reward tensor(453.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9061e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027140172198414803
dqn reward tensor(768.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5599e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21894840896129608
dqn reward tensor(605.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19207437336444855
dqn reward tensor(604.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12018062174320221
dqn reward tensor(665.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03556660935282707
dqn reward tensor(537.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20252877473831177
dqn reward tensor(614.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08788101375102997
dqn reward tensor(633., device='cuda:0') e 0.05 loss_dqn tensor(7.5396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16721659898757935
dqn reward tensor(638.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16460371017456055
dqn reward tensor(749., device='cuda:0') e 0.05 loss_dqn tensor(3.8283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032687708735466
dqn reward tensor(690.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08938455581665039
dqn reward tensor(422.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6644e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08670166879892349
dqn reward tensor(570.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1029e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17400065064430237
dqn reward tensor(550.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9674e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026371225714683533
dqn reward tensor(612.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2376e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15156735479831696
dqn reward tensor(534.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07804788649082184
dqn reward tensor(615.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9863e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15763254463672638
dqn reward tensor(612.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8167e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19412630796432495
dqn reward tensor(545.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2962e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20278112590312958
dqn reward tensor(572.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7616e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23967117071151733
dqn reward tensor(506.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3312e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08348773419857025
dqn reward tensor(616.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11063611507415771
dqn reward tensor(599.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0654e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11758346855640411
dqn reward tensor(734.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08203381299972534
dqn reward tensor(695.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10872684419155121
dqn reward tensor(405.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1576e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14930179715156555
dqn reward tensor(600.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.0739e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04476189613342285
dqn reward tensor(670.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19359903037548065
dqn reward tensor(557., device='cuda:0') e 0.05 loss_dqn tensor(3.7123e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0665629580616951
dqn reward tensor(372.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3631e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14580950140953064
dqn reward tensor(677.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043229810893535614
dqn reward tensor(436.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16807495057582855
dqn reward tensor(721.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7332e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0892442911863327
dqn reward tensor(495.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3459e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10442110896110535
dqn reward tensor(488.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14286541938781738
dqn reward tensor(55.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01047883927822113
Evaluating...
Train: {'rocauc': 0.7906433398140589} 9.367361068725586
=====Epoch 35=====
Training...
dqn reward tensor(634., device='cuda:0') e 0.05 loss_dqn tensor(3.7475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03412599116563797
dqn reward tensor(578.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6639e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18359491229057312
dqn reward tensor(544.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.0358e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3100508451461792
dqn reward tensor(600.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23349326848983765
dqn reward tensor(679.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03551869094371796
dqn reward tensor(492., device='cuda:0') e 0.05 loss_dqn tensor(1.6443e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15969116985797882
dqn reward tensor(728.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8438e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1528579294681549
dqn reward tensor(633.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2799e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09711272269487381
dqn reward tensor(566.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7682e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.205409973859787
dqn reward tensor(421.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7455e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04385322332382202
dqn reward tensor(670.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6682e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10146789252758026
dqn reward tensor(671.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4364e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11722023040056229
dqn reward tensor(752.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7656e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15535549819469452
dqn reward tensor(638.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08601507544517517
dqn reward tensor(631., device='cuda:0') e 0.05 loss_dqn tensor(8.3055e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17856398224830627
dqn reward tensor(618.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16344419121742249
dqn reward tensor(544.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2862e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23889446258544922
dqn reward tensor(508.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06326058506965637
dqn reward tensor(583.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0195e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20746958255767822
dqn reward tensor(482.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.2549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04232092946767807
dqn reward tensor(661.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6441e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16194838285446167
dqn reward tensor(443.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5861e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21975529193878174
dqn reward tensor(761.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14839547872543335
dqn reward tensor(485.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.9601e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1098828911781311
dqn reward tensor(486.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18937484920024872
dqn reward tensor(583.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6317e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04694932699203491
dqn reward tensor(714.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3520e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16843575239181519
dqn reward tensor(574.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0108e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10653771460056305
dqn reward tensor(697.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3726e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16289272904396057
dqn reward tensor(665., device='cuda:0') e 0.05 loss_dqn tensor(2.0228e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14816558361053467
dqn reward tensor(690.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8874e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04995350539684296
dqn reward tensor(575.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.2360e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2837207615375519
dqn reward tensor(683.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09159711003303528
dqn reward tensor(696.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1682577133178711
dqn reward tensor(753.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16975536942481995
dqn reward tensor(545.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0906e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10909615457057953
dqn reward tensor(657.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19869637489318848
dqn reward tensor(410.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8435e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10768258571624756
dqn reward tensor(567.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.4800e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08346254378557205
dqn reward tensor(511.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.9740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10097084194421768
dqn reward tensor(636.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12542589008808136
dqn reward tensor(560.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1582896113395691
dqn reward tensor(525.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07421918958425522
dqn reward tensor(530., device='cuda:0') e 0.05 loss_dqn tensor(2.3474e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14451634883880615
dqn reward tensor(579.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08829198777675629
dqn reward tensor(556.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.4170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17258551716804504
dqn reward tensor(445., device='cuda:0') e 0.05 loss_dqn tensor(2.4593e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08040208369493484
dqn reward tensor(517., device='cuda:0') e 0.05 loss_dqn tensor(1.9051e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06784490495920181
dqn reward tensor(564.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05767079442739487
dqn reward tensor(643.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1388983130455017
dqn reward tensor(663.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16012394428253174
dqn reward tensor(653.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2008e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19419251382350922
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4474e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11329472810029984
dqn reward tensor(647.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.3486e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17823421955108643
dqn reward tensor(600., device='cuda:0') e 0.05 loss_dqn tensor(3.4109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1332540214061737
dqn reward tensor(667.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5270e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2626001238822937
dqn reward tensor(528.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3189e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15990735590457916
dqn reward tensor(636.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0215e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12579308450222015
dqn reward tensor(610., device='cuda:0') e 0.05 loss_dqn tensor(7.6596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2074052095413208
dqn reward tensor(615., device='cuda:0') e 0.05 loss_dqn tensor(3.5234e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19225949048995972
dqn reward tensor(454.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202444344758987
dqn reward tensor(567.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9054e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18276503682136536
dqn reward tensor(531.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2053900510072708
dqn reward tensor(387.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9254e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11301057785749435
dqn reward tensor(263.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7904e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09666403383016586
dqn reward tensor(591.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5366e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12384539097547531
dqn reward tensor(596.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.4836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1199759840965271
dqn reward tensor(792.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1151164248585701
dqn reward tensor(532.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0562889464199543
dqn reward tensor(560.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21138142049312592
dqn reward tensor(532.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.9700e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09931668639183044
dqn reward tensor(529.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.8159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05556904897093773
dqn reward tensor(420.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7863e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11939816921949387
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7029e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2897242605686188
dqn reward tensor(544.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03845001757144928
dqn reward tensor(659.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0823681429028511
dqn reward tensor(579.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7797e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07948634028434753
dqn reward tensor(382.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6746e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066795349121094
dqn reward tensor(540.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.0865e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09540662914514542
dqn reward tensor(531.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.5484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12039422988891602
dqn reward tensor(686.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4208e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17807212471961975
dqn reward tensor(661.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4994e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24985146522521973
dqn reward tensor(551.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8184e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0631210207939148
dqn reward tensor(528.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1923123449087143
dqn reward tensor(495.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25336799025535583
dqn reward tensor(323.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4906e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12973523139953613
dqn reward tensor(525.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.1825e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19023951888084412
dqn reward tensor(505.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5959e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15452896058559418
dqn reward tensor(480., device='cuda:0') e 0.05 loss_dqn tensor(4.0477e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17436698079109192
dqn reward tensor(525.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5435e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14305037260055542
dqn reward tensor(673.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10015232861042023
dqn reward tensor(423.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8962e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329013705253601
dqn reward tensor(738., device='cuda:0') e 0.05 loss_dqn tensor(3.3990e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07718967646360397
dqn reward tensor(367.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9353e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14423322677612305
dqn reward tensor(698.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2646033763885498
dqn reward tensor(515.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5251e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16667801141738892
dqn reward tensor(431.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15237432718276978
dqn reward tensor(668.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3884e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2045605629682541
dqn reward tensor(674.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04732758551836014
dqn reward tensor(507.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6404e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1363382190465927
dqn reward tensor(475.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0427e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03344738110899925
dqn reward tensor(732., device='cuda:0') e 0.05 loss_dqn tensor(3.2604e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18193605542182922
dqn reward tensor(815.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1453e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14394015073776245
dqn reward tensor(683.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9217e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15060733258724213
dqn reward tensor(548.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3514e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18509505689144135
dqn reward tensor(395.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0445190891623497
dqn reward tensor(669.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019088149070739746
dqn reward tensor(553.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4963e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18249791860580444
dqn reward tensor(649.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9850e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14963048696517944
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.1140e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09051727503538132
dqn reward tensor(614.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14662425220012665
dqn reward tensor(463.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09454122185707092
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7163e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1872469186782837
dqn reward tensor(641.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9907e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09537895768880844
dqn reward tensor(460.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.6069e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11294916272163391
dqn reward tensor(470., device='cuda:0') e 0.05 loss_dqn tensor(4.1451e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13836079835891724
dqn reward tensor(620.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8652e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14360995590686798
dqn reward tensor(546.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4699e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19823414087295532
dqn reward tensor(615.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8351e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17846465110778809
dqn reward tensor(617.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10417543351650238
dqn reward tensor(552.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.1845e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.080576092004776
dqn reward tensor(540.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0080e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059735652059316635
dqn reward tensor(495.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.7877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07512684166431427
dqn reward tensor(611.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.7537e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051044318825006485
dqn reward tensor(532.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6583e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08439968526363373
dqn reward tensor(619.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10460293292999268
dqn reward tensor(665.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2793e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3154354989528656
dqn reward tensor(544.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0677e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054794684052467346
dqn reward tensor(676.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5974e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025686832144856453
dqn reward tensor(462.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7497e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12961064279079437
dqn reward tensor(503.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9681e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12418225407600403
dqn reward tensor(626.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05480959266424179
dqn reward tensor(604.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0946e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.145348459482193
dqn reward tensor(463.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0273e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13731098175048828
dqn reward tensor(801.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2891e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16383299231529236
dqn reward tensor(627.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28697043657302856
dqn reward tensor(626., device='cuda:0') e 0.05 loss_dqn tensor(3.5715e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03707211837172508
dqn reward tensor(669.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7721e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0491359643638134
dqn reward tensor(586.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2353e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10178472101688385
dqn reward tensor(494.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.0493e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039313096553087234
dqn reward tensor(291.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9758e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08244746923446655
dqn reward tensor(666.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.8203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22193948924541473
dqn reward tensor(671.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03208034858107567
dqn reward tensor(526.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1321e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0640999972820282
dqn reward tensor(508.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7358e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06681278347969055
dqn reward tensor(544.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.8650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04728980362415314
dqn reward tensor(637.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.7463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2080277055501938
dqn reward tensor(569., device='cuda:0') e 0.05 loss_dqn tensor(2.6490e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02546757087111473
dqn reward tensor(511., device='cuda:0') e 0.05 loss_dqn tensor(3.5696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036195192486047745
dqn reward tensor(776.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2481e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046493664383888245
dqn reward tensor(693., device='cuda:0') e 0.05 loss_dqn tensor(1.5499e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13148224353790283
dqn reward tensor(567.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13842496275901794
dqn reward tensor(486.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.4366e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08010292053222656
dqn reward tensor(589.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10941526293754578
dqn reward tensor(632.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022567663341760635
dqn reward tensor(568.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06782735139131546
dqn reward tensor(676.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2843e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19213886559009552
dqn reward tensor(519.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.4461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1765691190958023
dqn reward tensor(559.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5748e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1770705133676529
dqn reward tensor(586.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5585e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11789104342460632
dqn reward tensor(545.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5664e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22006765007972717
dqn reward tensor(577.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1017671450972557
dqn reward tensor(572.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13260222971439362
dqn reward tensor(775., device='cuda:0') e 0.05 loss_dqn tensor(3.2558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06245843321084976
dqn reward tensor(500.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.7074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.47151732444763184
dqn reward tensor(603.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19813117384910583
dqn reward tensor(452.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08616889268159866
dqn reward tensor(321.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0098e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043911710381507874
dqn reward tensor(405.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.2376e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.212532639503479
dqn reward tensor(646.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6488e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0950937271118164
dqn reward tensor(628.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4788e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11585070192813873
dqn reward tensor(599.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12459774315357208
dqn reward tensor(539.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5904e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16141368448734283
dqn reward tensor(448.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08750227093696594
dqn reward tensor(623.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3265e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25517186522483826
dqn reward tensor(517., device='cuda:0') e 0.05 loss_dqn tensor(1.2370e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04358220472931862
dqn reward tensor(592.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4207e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1081588864326477
dqn reward tensor(555.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.4821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21364878118038177
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11107835173606873
dqn reward tensor(396.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07489903271198273
dqn reward tensor(545.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1704e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09402668476104736
dqn reward tensor(588.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2031e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13041582703590393
dqn reward tensor(326., device='cuda:0') e 0.05 loss_dqn tensor(2.3842e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0509294755756855
dqn reward tensor(762., device='cuda:0') e 0.05 loss_dqn tensor(3.3815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07698774337768555
dqn reward tensor(687.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3763e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13952359557151794
dqn reward tensor(713.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27854210138320923
dqn reward tensor(659.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.41388553380966187
dqn reward tensor(493.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4579e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20351539552211761
dqn reward tensor(515.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1703827828168869
dqn reward tensor(717.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09986944496631622
dqn reward tensor(495.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2712e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25552260875701904
dqn reward tensor(475.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.6215e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19776898622512817
dqn reward tensor(690.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4593e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1601509004831314
dqn reward tensor(586.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7759e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060348816215991974
dqn reward tensor(440.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7552e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21471968293190002
dqn reward tensor(600.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9125e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05356696620583534
dqn reward tensor(573., device='cuda:0') e 0.05 loss_dqn tensor(4.5086e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1547105312347412
dqn reward tensor(577.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3836e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06502649188041687
dqn reward tensor(507.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12178079783916473
dqn reward tensor(577.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1093e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06274127960205078
dqn reward tensor(495.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035437025129795074
dqn reward tensor(630.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5570e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16759787499904633
dqn reward tensor(434.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3453e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.041328590363264084
dqn reward tensor(547.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32297468185424805
dqn reward tensor(599.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2262e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06814082711935043
dqn reward tensor(664.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7144e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19887244701385498
dqn reward tensor(613., device='cuda:0') e 0.05 loss_dqn tensor(3.4783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07577859610319138
dqn reward tensor(555.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7353e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1584807187318802
dqn reward tensor(670.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3037e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20270797610282898
dqn reward tensor(535.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.9849e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28105396032333374
dqn reward tensor(758.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0884e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028807098045945168
dqn reward tensor(537., device='cuda:0') e 0.05 loss_dqn tensor(1.4506e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2485351413488388
dqn reward tensor(782.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3363e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12960349023342133
dqn reward tensor(431.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10290095210075378
dqn reward tensor(370.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07238524407148361
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6626e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22534698247909546
dqn reward tensor(504.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.0070e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02439209818840027
dqn reward tensor(690., device='cuda:0') e 0.05 loss_dqn tensor(3.4678e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07778869569301605
dqn reward tensor(436.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3983e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02715890295803547
dqn reward tensor(478.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19983845949172974
dqn reward tensor(603.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.4250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1289394497871399
dqn reward tensor(538.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.2223e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14180371165275574
dqn reward tensor(590.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1801380217075348
dqn reward tensor(542.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5777e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11394213885068893
dqn reward tensor(647., device='cuda:0') e 0.05 loss_dqn tensor(3.3440e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11114346981048584
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7761e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11464343965053558
dqn reward tensor(644.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3904e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23967523872852325
dqn reward tensor(648.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19754931330680847
dqn reward tensor(730.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.5964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11797674000263214
dqn reward tensor(479.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6708e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30564266443252563
dqn reward tensor(570.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09615317732095718
dqn reward tensor(576.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4125e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16225320100784302
dqn reward tensor(596.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.3581e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16698487102985382
dqn reward tensor(733.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4504e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14362432062625885
dqn reward tensor(654.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24601370096206665
dqn reward tensor(619.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6025e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12133034318685532
dqn reward tensor(674.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3745e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09638980031013489
dqn reward tensor(634.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7103e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11410457640886307
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05191978067159653
dqn reward tensor(596.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2137e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1448557823896408
dqn reward tensor(432., device='cuda:0') e 0.05 loss_dqn tensor(2.4769e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10103795677423477
dqn reward tensor(501.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7038e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1109042763710022
dqn reward tensor(646.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8637e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13520070910453796
dqn reward tensor(556.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7512e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040825605392456055
dqn reward tensor(608.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7833e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10386719554662704
dqn reward tensor(636.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6250e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11680696904659271
dqn reward tensor(743.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1523871272802353
dqn reward tensor(455., device='cuda:0') e 0.05 loss_dqn tensor(3.7980e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030392244458198547
dqn reward tensor(523.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5085e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23971746861934662
dqn reward tensor(512.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6379e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12333306670188904
dqn reward tensor(513.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.2763e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24550463259220123
dqn reward tensor(625.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2425e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23580589890480042
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020543433725833893
dqn reward tensor(632., device='cuda:0') e 0.05 loss_dqn tensor(3.3284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16736537218093872
dqn reward tensor(583.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027042176574468613
dqn reward tensor(542.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3929e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05267339572310448
dqn reward tensor(571.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8860e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13710248470306396
dqn reward tensor(565.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4586e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10035199671983719
dqn reward tensor(587.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1374468058347702
dqn reward tensor(493.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3150e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23454508185386658
dqn reward tensor(499.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1778e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09912960231304169
dqn reward tensor(640.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11161946505308151
dqn reward tensor(649.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14721685647964478
dqn reward tensor(339.4375, device='cuda:0') e 0.05 loss_dqn tensor(3.8418e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10066453367471695
dqn reward tensor(470.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5034e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17190144956111908
dqn reward tensor(669.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5267e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0557965412735939
dqn reward tensor(496., device='cuda:0') e 0.05 loss_dqn tensor(3.3797e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08567164838314056
dqn reward tensor(448.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.9447e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06896055489778519
dqn reward tensor(485.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5807e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20027904212474823
dqn reward tensor(390.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2781e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13569259643554688
dqn reward tensor(534.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13784581422805786
dqn reward tensor(580.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04969850555062294
dqn reward tensor(367.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8602e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24096259474754333
dqn reward tensor(337.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9472e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09527871012687683
dqn reward tensor(625.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1433e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06933023780584335
dqn reward tensor(596.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8721e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2121216356754303
dqn reward tensor(676.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3655e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026729624718427658
dqn reward tensor(666.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9055e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1289025843143463
dqn reward tensor(560.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1440439373254776
dqn reward tensor(515., device='cuda:0') e 0.05 loss_dqn tensor(3.5356e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062223322689533234
dqn reward tensor(666.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1782064288854599
dqn reward tensor(679.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5514e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12374251335859299
dqn reward tensor(790.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0624217614531517
dqn reward tensor(527.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3832e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24952039122581482
dqn reward tensor(786.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4428e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10341055691242218
dqn reward tensor(467.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3895e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043503537774086
dqn reward tensor(661.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14480534195899963
dqn reward tensor(645.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3486e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22780922055244446
dqn reward tensor(592.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3939e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08132680505514145
dqn reward tensor(548.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0595e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1655251681804657
dqn reward tensor(621.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1693e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13507972657680511
dqn reward tensor(580.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3348e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26988548040390015
dqn reward tensor(534.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.6293e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23549491167068481
dqn reward tensor(524.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0981e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15476572513580322
dqn reward tensor(523.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5322e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1219298392534256
dqn reward tensor(700., device='cuda:0') e 0.05 loss_dqn tensor(3.0119e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07693730294704437
dqn reward tensor(518.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7032e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11923423409461975
dqn reward tensor(679., device='cuda:0') e 0.05 loss_dqn tensor(2.5896e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15590688586235046
dqn reward tensor(734.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20714502036571503
dqn reward tensor(322.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0359e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14302417635917664
dqn reward tensor(741., device='cuda:0') e 0.05 loss_dqn tensor(3.0565e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09429875016212463
dqn reward tensor(545.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.9574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10690227150917053
dqn reward tensor(552.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8053e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0709691047668457
dqn reward tensor(591.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0887e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17304928600788116
dqn reward tensor(399.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.7562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2374681532382965
dqn reward tensor(726.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21215248107910156
dqn reward tensor(561.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15719667077064514
dqn reward tensor(497.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4011e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07270847260951996
dqn reward tensor(509.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0075e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06192253530025482
dqn reward tensor(536.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5214e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0820029154419899
dqn reward tensor(705.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6536e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1337539106607437
dqn reward tensor(478.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6826e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022093981504440308
dqn reward tensor(498.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4506e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20936886966228485
dqn reward tensor(569.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.0990e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10812273621559143
dqn reward tensor(559.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.9397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10846692323684692
dqn reward tensor(529.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6094e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06933454424142838
dqn reward tensor(628., device='cuda:0') e 0.05 loss_dqn tensor(3.2940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08104553818702698
dqn reward tensor(569.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08875499665737152
dqn reward tensor(632.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1008e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20761018991470337
dqn reward tensor(670., device='cuda:0') e 0.05 loss_dqn tensor(3.1100e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07557801157236099
dqn reward tensor(623., device='cuda:0') e 0.05 loss_dqn tensor(2.5541e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11321696639060974
dqn reward tensor(700.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.4694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07793215662240982
dqn reward tensor(517.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6941e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15751707553863525
dqn reward tensor(480.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8487e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1626105010509491
dqn reward tensor(664.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6450e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12260165065526962
dqn reward tensor(674., device='cuda:0') e 0.05 loss_dqn tensor(3.5174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1900041401386261
dqn reward tensor(660.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1993e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09506814926862717
dqn reward tensor(599.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1454632580280304
dqn reward tensor(567.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3541e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035370875149965286
dqn reward tensor(635., device='cuda:0') e 0.05 loss_dqn tensor(3.2198e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07813438773155212
dqn reward tensor(590.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.5086e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03333575651049614
dqn reward tensor(617.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3658e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2337847203016281
dqn reward tensor(650.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.4309e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19839900732040405
dqn reward tensor(706.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4011e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08945311605930328
dqn reward tensor(574.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3433e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06501331180334091
dqn reward tensor(445.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04568015784025192
dqn reward tensor(458.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8913e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08676720410585403
dqn reward tensor(530., device='cuda:0') e 0.05 loss_dqn tensor(1.8241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08915714919567108
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4778e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06294768303632736
dqn reward tensor(630.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0617e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08499380946159363
dqn reward tensor(630.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2989e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04557289183139801
dqn reward tensor(648.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.5093e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07916727662086487
dqn reward tensor(641.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29110267758369446
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.7664e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16141623258590698
dqn reward tensor(612.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.7759e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1397918164730072
dqn reward tensor(539.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11056716740131378
dqn reward tensor(705.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0728769451379776
dqn reward tensor(621., device='cuda:0') e 0.05 loss_dqn tensor(6.3212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14167919754981995
dqn reward tensor(609.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24955441057682037
dqn reward tensor(737., device='cuda:0') e 0.05 loss_dqn tensor(3.2232e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030197327956557274
dqn reward tensor(533.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9590e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14144042134284973
dqn reward tensor(543.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3177e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10350847244262695
dqn reward tensor(742.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18601877987384796
dqn reward tensor(626.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1888219714164734
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6149e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13976770639419556
dqn reward tensor(460.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0185e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03980465233325958
dqn reward tensor(490., device='cuda:0') e 0.05 loss_dqn tensor(4.9141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16449467837810516
dqn reward tensor(435.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06203782185912132
dqn reward tensor(790.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10350322723388672
dqn reward tensor(541.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12259961664676666
dqn reward tensor(354.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0107e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1184970960021019
dqn reward tensor(588.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.5212e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07345770299434662
dqn reward tensor(521.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8029e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024328012019395828
dqn reward tensor(448.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.9147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07388490438461304
dqn reward tensor(587.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026933636516332626
dqn reward tensor(618.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7483e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15322443842887878
dqn reward tensor(588., device='cuda:0') e 0.05 loss_dqn tensor(3.1033e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2689583897590637
dqn reward tensor(623.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08636365830898285
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4242e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07598739862442017
dqn reward tensor(671., device='cuda:0') e 0.05 loss_dqn tensor(3.5470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08569149672985077
dqn reward tensor(413.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7689e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04688523709774017
dqn reward tensor(548.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.7751e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3220697045326233
dqn reward tensor(672.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6611e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17255352437496185
dqn reward tensor(588.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3191e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19402769207954407
dqn reward tensor(490.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0043e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14671526849269867
dqn reward tensor(452.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5224e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14304989576339722
dqn reward tensor(625.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2144864946603775
dqn reward tensor(291., device='cuda:0') e 0.05 loss_dqn tensor(1.9438e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13677173852920532
dqn reward tensor(542., device='cuda:0') e 0.05 loss_dqn tensor(3.2030e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12069647759199142
dqn reward tensor(496.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6638e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23892797529697418
dqn reward tensor(631.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3093e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07842553406953812
dqn reward tensor(625., device='cuda:0') e 0.05 loss_dqn tensor(3.4339e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19149240851402283
dqn reward tensor(439.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.0231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13083523511886597
dqn reward tensor(309.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.9876e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07635428011417389
dqn reward tensor(521., device='cuda:0') e 0.05 loss_dqn tensor(6.0703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17478029429912567
dqn reward tensor(670.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1520e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11885009706020355
dqn reward tensor(668.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.2271e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06701730191707611
dqn reward tensor(579.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21147021651268005
dqn reward tensor(515.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11677610874176025
dqn reward tensor(660.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4231e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08908004313707352
dqn reward tensor(507.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1807851940393448
dqn reward tensor(536.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.0053e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12770116329193115
dqn reward tensor(711.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4343e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11762349307537079
dqn reward tensor(410.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02847309596836567
dqn reward tensor(765., device='cuda:0') e 0.05 loss_dqn tensor(3.0253e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0984392911195755
dqn reward tensor(538.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4822e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08804331719875336
dqn reward tensor(666.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12097224593162537
dqn reward tensor(497.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05088958889245987
dqn reward tensor(761.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08876118808984756
dqn reward tensor(695.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0916e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23814260959625244
dqn reward tensor(497.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9682e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12239375710487366
dqn reward tensor(448.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1782e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09318270534276962
dqn reward tensor(397.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.4619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17688360810279846
dqn reward tensor(747.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7307e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09343432635068893
dqn reward tensor(510.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2548e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21437174081802368
dqn reward tensor(557.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20646855235099792
dqn reward tensor(486.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.6336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07029727846384048
dqn reward tensor(644.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5408e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1653217375278473
dqn reward tensor(778., device='cuda:0') e 0.05 loss_dqn tensor(3.2742e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14361077547073364
dqn reward tensor(525., device='cuda:0') e 0.05 loss_dqn tensor(2.3159e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15453138947486877
dqn reward tensor(646.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3072e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13122636079788208
dqn reward tensor(541., device='cuda:0') e 0.05 loss_dqn tensor(4.2908e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09845404326915741
dqn reward tensor(607.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4915e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17176303267478943
dqn reward tensor(535.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13758860528469086
dqn reward tensor(580.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10431647300720215
dqn reward tensor(599.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10777576267719269
dqn reward tensor(599.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11316972225904465
dqn reward tensor(405.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04852032661437988
dqn reward tensor(521.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13466249406337738
dqn reward tensor(448.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5874e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07804742455482483
dqn reward tensor(338.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08384020626544952
dqn reward tensor(447.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.4482e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09791409969329834
dqn reward tensor(598.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6810e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1381961554288864
dqn reward tensor(402.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4684e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16345027089118958
dqn reward tensor(751.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.4697e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10908578336238861
dqn reward tensor(503.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4880e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11165200173854828
dqn reward tensor(594.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1592e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04485473781824112
dqn reward tensor(514.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8170e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2564408481121063
dqn reward tensor(661.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1174e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0773937776684761
dqn reward tensor(579., device='cuda:0') e 0.05 loss_dqn tensor(2.2955e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07563873380422592
dqn reward tensor(387.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5187e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2163231521844864
dqn reward tensor(730.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10061509907245636
dqn reward tensor(510.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4803e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0842491015791893
dqn reward tensor(603.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7914e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14114052057266235
dqn reward tensor(501., device='cuda:0') e 0.05 loss_dqn tensor(2.4549e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14496338367462158
dqn reward tensor(507.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2555e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23270472884178162
dqn reward tensor(758.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0398574098944664
dqn reward tensor(619.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5790e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14488287270069122
dqn reward tensor(622.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.2618e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14876234531402588
dqn reward tensor(487.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7849e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23677587509155273
dqn reward tensor(616.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15239456295967102
dqn reward tensor(674.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.1261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2680235505104065
dqn reward tensor(615.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.7614e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03722113370895386
dqn reward tensor(524., device='cuda:0') e 0.05 loss_dqn tensor(3.2169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26675570011138916
dqn reward tensor(400.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1346e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12052939832210541
dqn reward tensor(696.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.7768e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13142919540405273
dqn reward tensor(470.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19092388451099396
dqn reward tensor(653.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05385828763246536
dqn reward tensor(610.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3370e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12106025218963623
dqn reward tensor(548.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4779e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037385471165180206
dqn reward tensor(533.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8011e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13040417432785034
dqn reward tensor(592.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.5232e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10663002729415894
dqn reward tensor(605.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1313e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17913150787353516
dqn reward tensor(747.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.3922e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15062212944030762
dqn reward tensor(608.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14685222506523132
dqn reward tensor(564.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08303368091583252
dqn reward tensor(705.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2615e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1173243373632431
dqn reward tensor(483.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23016130924224854
dqn reward tensor(594.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202760994434357
dqn reward tensor(581., device='cuda:0') e 0.05 loss_dqn tensor(3.5239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12266083806753159
dqn reward tensor(539.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.4044e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25942346453666687
dqn reward tensor(466.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0565808042883873
dqn reward tensor(702.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13319434225559235
dqn reward tensor(610.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3008e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1484815627336502
dqn reward tensor(661.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8546e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12583331763744354
dqn reward tensor(539., device='cuda:0') e 0.05 loss_dqn tensor(2.4925e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12232904136180878
dqn reward tensor(629.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2692e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042895425111055374
dqn reward tensor(627.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17379841208457947
dqn reward tensor(425.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8807e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10724073648452759
dqn reward tensor(427., device='cuda:0') e 0.05 loss_dqn tensor(1.3703e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03580998629331589
dqn reward tensor(632.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3656e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12328357994556427
dqn reward tensor(555.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4255e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10014703869819641
dqn reward tensor(752.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3885e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16657932102680206
dqn reward tensor(743.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7226e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05905518680810928
dqn reward tensor(444.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5081e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09063142538070679
dqn reward tensor(573.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9787e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19315597414970398
dqn reward tensor(649.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4798e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20480717718601227
dqn reward tensor(567.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14259329438209534
dqn reward tensor(670.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5772e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08361568301916122
dqn reward tensor(689.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5682e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08459857106208801
dqn reward tensor(719.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3809e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03515278548002243
dqn reward tensor(647.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5128e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1132936179637909
dqn reward tensor(696.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02753765508532524
dqn reward tensor(554.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.3876e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1407569944858551
dqn reward tensor(506.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.7910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24729788303375244
dqn reward tensor(611.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9080e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06739792227745056
dqn reward tensor(645.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12082390487194061
dqn reward tensor(535.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.6678e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1882345974445343
dqn reward tensor(425.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1064e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14968065917491913
dqn reward tensor(580.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6215e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14529837667942047
dqn reward tensor(742.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12903262674808502
dqn reward tensor(572.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.5015e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08676713705062866
dqn reward tensor(585.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.4617e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14691230654716492
dqn reward tensor(362.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0787e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06064935401082039
dqn reward tensor(485.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8547e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08794322609901428
dqn reward tensor(494., device='cuda:0') e 0.05 loss_dqn tensor(8.6187e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12423157691955566
dqn reward tensor(699.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1958e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12404150515794754
dqn reward tensor(489.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2111e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14997194707393646
dqn reward tensor(502.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1953e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08131915330886841
dqn reward tensor(575.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21390214562416077
dqn reward tensor(612.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.6464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16290131211280823
dqn reward tensor(397.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.6296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11403638869524002
dqn reward tensor(372.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.5817e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08457905054092407
dqn reward tensor(527.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6078e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30573374032974243
dqn reward tensor(126.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07290606945753098
dqn reward tensor(647.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08806934207677841
dqn reward tensor(581.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2398e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11051302403211594
dqn reward tensor(462.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8727e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35812556743621826
dqn reward tensor(565., device='cuda:0') e 0.05 loss_dqn tensor(1.0849e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09954230487346649
dqn reward tensor(441.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08600787818431854
dqn reward tensor(552.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3406e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1942453682422638
dqn reward tensor(719.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05895303934812546
dqn reward tensor(424.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5825e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06019724905490875
dqn reward tensor(519.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1072218120098114
dqn reward tensor(44.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033435922116041183
Evaluating...
Train: {'rocauc': 0.7920708875654959} 9.034460067749023
=====Epoch 36=====
Training...
dqn reward tensor(546.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8675e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07168872654438019
dqn reward tensor(584., device='cuda:0') e 0.05 loss_dqn tensor(3.3088e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04103386029601097
dqn reward tensor(524.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1731e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2000942975282669
dqn reward tensor(637.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08443818241357803
dqn reward tensor(481., device='cuda:0') e 0.05 loss_dqn tensor(4.2998e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13368836045265198
dqn reward tensor(586.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1699763834476471
dqn reward tensor(630.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028592105954885483
dqn reward tensor(727.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.9035e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2601577639579773
dqn reward tensor(712.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027621358633041382
dqn reward tensor(706.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2432e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1941559910774231
dqn reward tensor(627.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3862e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06664358079433441
dqn reward tensor(510.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.6935e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03577181696891785
dqn reward tensor(582.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.42713966965675354
dqn reward tensor(431.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3699e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06352876126766205
dqn reward tensor(592.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2026e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0916365385055542
dqn reward tensor(606.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.4774e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09903259575366974
dqn reward tensor(480.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.6029e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12492318451404572
dqn reward tensor(752., device='cuda:0') e 0.05 loss_dqn tensor(3.4838e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1661614030599594
dqn reward tensor(699.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20811015367507935
dqn reward tensor(287., device='cuda:0') e 0.05 loss_dqn tensor(3.5414e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1385376900434494
dqn reward tensor(615.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9321e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14572931826114655
dqn reward tensor(354.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03565896302461624
dqn reward tensor(739., device='cuda:0') e 0.05 loss_dqn tensor(3.5729e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12057549506425858
dqn reward tensor(554.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14712479710578918
dqn reward tensor(607.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.5160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16880986094474792
dqn reward tensor(535.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5324e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0869993269443512
dqn reward tensor(443.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12886588275432587
dqn reward tensor(486.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0579e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13794922828674316
dqn reward tensor(346., device='cuda:0') e 0.05 loss_dqn tensor(2.6813e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043990083038806915
dqn reward tensor(697.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.5812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15135608613491058
dqn reward tensor(559.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3839e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10789872705936432
dqn reward tensor(678.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055266372859478
dqn reward tensor(473.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2023e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11299386620521545
dqn reward tensor(646.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2531e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10713084042072296
dqn reward tensor(657., device='cuda:0') e 0.05 loss_dqn tensor(3.3494e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553034603595734
dqn reward tensor(488.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14258769154548645
dqn reward tensor(704.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7084e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08408147096633911
dqn reward tensor(475., device='cuda:0') e 0.05 loss_dqn tensor(1.6795e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10225950181484222
dqn reward tensor(538., device='cuda:0') e 0.05 loss_dqn tensor(4.9897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12730254232883453
dqn reward tensor(558.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.2244e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03937897831201553
dqn reward tensor(636.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2118e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16795966029167175
dqn reward tensor(572.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3907e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013707989826798439
dqn reward tensor(477.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2715e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20090097188949585
dqn reward tensor(511.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.192228764295578
dqn reward tensor(500.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10293799638748169
dqn reward tensor(708.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1196e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026817403733730316
dqn reward tensor(675.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6695e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0800778865814209
dqn reward tensor(356.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0469e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.208058699965477
dqn reward tensor(600.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7994e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03389640152454376
dqn reward tensor(335.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7667e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12368752807378769
dqn reward tensor(429.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3148e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1913471519947052
dqn reward tensor(356.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.8695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09623435139656067
dqn reward tensor(481.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1247e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03271926939487457
dqn reward tensor(281.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1250e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17449025809764862
dqn reward tensor(623.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08988238871097565
dqn reward tensor(622., device='cuda:0') e 0.05 loss_dqn tensor(1.3271e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18460121750831604
dqn reward tensor(601.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3260e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04009808599948883
dqn reward tensor(680.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07546434551477432
dqn reward tensor(426.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.3352e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09184323251247406
dqn reward tensor(635.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14903351664543152
dqn reward tensor(558.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7372e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0416499525308609
dqn reward tensor(683.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8061e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1847705841064453
dqn reward tensor(481.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.2541e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09375794976949692
dqn reward tensor(327.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0556e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11908166110515594
dqn reward tensor(289.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.5605e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18419808149337769
dqn reward tensor(680.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8133e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1042376309633255
dqn reward tensor(549.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3747e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06198054552078247
dqn reward tensor(570., device='cuda:0') e 0.05 loss_dqn tensor(2.8997e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020800255239009857
dqn reward tensor(543.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09601561725139618
dqn reward tensor(452.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0341e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06112886220216751
dqn reward tensor(606.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08587844669818878
dqn reward tensor(546., device='cuda:0') e 0.05 loss_dqn tensor(2.8209e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11499863862991333
dqn reward tensor(97.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.7836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1337762176990509
dqn reward tensor(559.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4735e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03327056020498276
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.5519e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2031387984752655
dqn reward tensor(639.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0661e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1909339874982834
dqn reward tensor(602.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3832e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10749153792858124
dqn reward tensor(195.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5207e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17894312739372253
dqn reward tensor(574., device='cuda:0') e 0.05 loss_dqn tensor(3.0532e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023206710815429688
dqn reward tensor(644.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4540e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10307181626558304
dqn reward tensor(371.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1094856932759285
dqn reward tensor(443.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4398e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042479321360588074
dqn reward tensor(547.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08141300082206726
dqn reward tensor(457.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7491e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0674271509051323
dqn reward tensor(400.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.4277e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10520679503679276
dqn reward tensor(500.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10236842930316925
dqn reward tensor(536.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.5463e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06639920175075531
dqn reward tensor(511.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7216e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376955509185791
dqn reward tensor(612., device='cuda:0') e 0.05 loss_dqn tensor(3.1857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019317615777254105
dqn reward tensor(489.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4158e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16155120730400085
dqn reward tensor(344.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12842616438865662
dqn reward tensor(579.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9505e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09812586009502411
dqn reward tensor(580.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9496e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22678224742412567
dqn reward tensor(540.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09738854318857193
dqn reward tensor(485.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0856e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21536202728748322
dqn reward tensor(581.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1162e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16450709104537964
dqn reward tensor(550.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2879e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07839521765708923
dqn reward tensor(486.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06954093277454376
dqn reward tensor(537.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8623e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09634505957365036
dqn reward tensor(663.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.2259e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13358953595161438
dqn reward tensor(506.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0080e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1910783052444458
dqn reward tensor(657.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9539e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1521119475364685
dqn reward tensor(481.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16571994125843048
dqn reward tensor(356., device='cuda:0') e 0.05 loss_dqn tensor(3.4509e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16262659430503845
dqn reward tensor(578.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5161e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1803271472454071
dqn reward tensor(485.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.2679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11108769476413727
dqn reward tensor(513.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8891e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06612346321344376
dqn reward tensor(505., device='cuda:0') e 0.05 loss_dqn tensor(1.9562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0905882865190506
dqn reward tensor(465.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10224932432174683
dqn reward tensor(566.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11840368062257767
dqn reward tensor(404.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.151800274848938
dqn reward tensor(440.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.8503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11813873052597046
dqn reward tensor(582.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.0025e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03276488557457924
dqn reward tensor(469.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2359e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03486281633377075
dqn reward tensor(653.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.8625e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17943677306175232
dqn reward tensor(475.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9355e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149402916431427
dqn reward tensor(614.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.7098e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10656551271677017
dqn reward tensor(385.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2173362821340561
dqn reward tensor(577.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01495723519474268
dqn reward tensor(517.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.3082e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17329853773117065
dqn reward tensor(622.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4943e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2941356897354126
dqn reward tensor(509.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0735e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0780499055981636
dqn reward tensor(303.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04111744090914726
dqn reward tensor(661.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9403e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019078198820352554
dqn reward tensor(429.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1984e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02299056015908718
dqn reward tensor(564.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0656e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10443577170372009
dqn reward tensor(521.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13988043367862701
dqn reward tensor(536.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4742e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.020529527217149734
dqn reward tensor(444.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2668e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15380260348320007
dqn reward tensor(620.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17390450835227966
dqn reward tensor(479.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2543e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1808924674987793
dqn reward tensor(506.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6379e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15590371191501617
dqn reward tensor(532.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.8456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13948865234851837
dqn reward tensor(612.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6912e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10375121235847473
dqn reward tensor(479.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.5258e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07188057899475098
dqn reward tensor(543.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.8673e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14274142682552338
dqn reward tensor(682.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9401e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10331465303897858
dqn reward tensor(697.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15525609254837036
dqn reward tensor(530.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9188e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10064353048801422
dqn reward tensor(537.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11304778605699539
dqn reward tensor(503., device='cuda:0') e 0.05 loss_dqn tensor(3.0350e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12268334627151489
dqn reward tensor(677.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16112037003040314
dqn reward tensor(337.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3590e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12414398044347763
dqn reward tensor(293., device='cuda:0') e 0.05 loss_dqn tensor(2.5890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20745190978050232
dqn reward tensor(606.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7801e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04240816831588745
dqn reward tensor(555.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.3361e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14427009224891663
dqn reward tensor(574.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.9169e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15755140781402588
dqn reward tensor(521.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4159e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08762575685977936
dqn reward tensor(540.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.4973e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30212071537971497
dqn reward tensor(339.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4642e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1839790940284729
dqn reward tensor(739.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15607593953609467
dqn reward tensor(349.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9480e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09682144224643707
dqn reward tensor(511.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7630e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1369330734014511
dqn reward tensor(497.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4859e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19872857630252838
dqn reward tensor(255.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8926e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11341296136379242
dqn reward tensor(393.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3518e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046767815947532654
dqn reward tensor(129.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.7439e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04322586581110954
dqn reward tensor(590.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1215e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25355687737464905
dqn reward tensor(355.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9829e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27489137649536133
dqn reward tensor(463.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0951e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1593937873840332
dqn reward tensor(564.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0683e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17191669344902039
dqn reward tensor(495.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0140e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05444744974374771
dqn reward tensor(559.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7614e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2605783939361572
dqn reward tensor(521.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5613e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1587643325328827
dqn reward tensor(580.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6212e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08038017153739929
dqn reward tensor(572.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1716e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05971032753586769
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1273e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05542629957199097
dqn reward tensor(486.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0985e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09203548729419708
dqn reward tensor(593.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13602766394615173
dqn reward tensor(361.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027601242065429688
dqn reward tensor(403.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.3158e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06244649365544319
dqn reward tensor(728.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8528e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031168784946203232
dqn reward tensor(430.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9235e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03192605450749397
dqn reward tensor(431., device='cuda:0') e 0.05 loss_dqn tensor(3.2780e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20396985113620758
dqn reward tensor(542.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.1863e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08637655526399612
dqn reward tensor(463.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5660e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11769698560237885
dqn reward tensor(558., device='cuda:0') e 0.05 loss_dqn tensor(2.8607e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23384645581245422
dqn reward tensor(452.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17307285964488983
dqn reward tensor(528.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1187e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08828486502170563
dqn reward tensor(481.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2744499742984772
dqn reward tensor(669.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11442501842975616
dqn reward tensor(590.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5918e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18053635954856873
dqn reward tensor(550.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8388e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10915471613407135
dqn reward tensor(562., device='cuda:0') e 0.05 loss_dqn tensor(2.7458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2351025491952896
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9052e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13293400406837463
dqn reward tensor(475.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3042e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14026018977165222
dqn reward tensor(534.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.1627e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1690702736377716
dqn reward tensor(534.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.9290e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12040743231773376
dqn reward tensor(503.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7056e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2581421732902527
dqn reward tensor(577.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13066056370735168
dqn reward tensor(418.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8637e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16873593628406525
dqn reward tensor(536.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5163e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11837923526763916
dqn reward tensor(438., device='cuda:0') e 0.05 loss_dqn tensor(2.6351e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08395262062549591
dqn reward tensor(481.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6865e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14031894505023956
dqn reward tensor(566.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18176057934761047
dqn reward tensor(480.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.2696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13613083958625793
dqn reward tensor(505.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12010771781206131
dqn reward tensor(609., device='cuda:0') e 0.05 loss_dqn tensor(1.9726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07327558100223541
dqn reward tensor(614.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4229e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16816605627536774
dqn reward tensor(415.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1415363848209381
dqn reward tensor(643.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.107256680727005
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6189e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10797294229269028
dqn reward tensor(607.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1595e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02203299105167389
dqn reward tensor(539.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.7884e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09566505998373032
dqn reward tensor(467.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8039e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08316955715417862
dqn reward tensor(610.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3607e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20865723490715027
dqn reward tensor(575.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1326397955417633
dqn reward tensor(441.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8517e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07263046503067017
dqn reward tensor(479.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.2936e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19695444405078888
dqn reward tensor(612.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.8158e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12109357118606567
dqn reward tensor(477., device='cuda:0') e 0.05 loss_dqn tensor(3.0219e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18766221404075623
dqn reward tensor(449.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6591e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13544557988643646
dqn reward tensor(375.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24485254287719727
dqn reward tensor(606., device='cuda:0') e 0.05 loss_dqn tensor(2.9372e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20959949493408203
dqn reward tensor(674.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10608617961406708
dqn reward tensor(435.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07220238447189331
dqn reward tensor(493.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1005406603217125
dqn reward tensor(417.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3121e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1534719467163086
dqn reward tensor(533., device='cuda:0') e 0.05 loss_dqn tensor(1.6905e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16594530642032623
dqn reward tensor(654.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9318e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1603480875492096
dqn reward tensor(614.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.5649e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20953604578971863
dqn reward tensor(432.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.0921e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05841865390539169
dqn reward tensor(465.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.5530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05092516541481018
dqn reward tensor(533.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0229e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20055516064167023
dqn reward tensor(435.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.3135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2914959788322449
dqn reward tensor(660., device='cuda:0') e 0.05 loss_dqn tensor(2.9463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11782059073448181
dqn reward tensor(516.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1996e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22011546790599823
dqn reward tensor(526.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.1882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19149363040924072
dqn reward tensor(375.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6529e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18179874122142792
dqn reward tensor(642.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.8538e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06299363076686859
dqn reward tensor(633.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5535e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17732678353786469
dqn reward tensor(587.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.4896e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06922721117734909
dqn reward tensor(560.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8512e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10995913296937943
dqn reward tensor(491.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.3695e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05142083019018173
dqn reward tensor(549.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.9785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09575027972459793
dqn reward tensor(741.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4300e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10858424752950668
dqn reward tensor(399.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2342263162136078
dqn reward tensor(366.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.8954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19770757853984833
dqn reward tensor(527.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22279107570648193
dqn reward tensor(532.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.4336e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26125702261924744
dqn reward tensor(468.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17905208468437195
dqn reward tensor(491.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.2090e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09935957193374634
dqn reward tensor(418.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0888e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15327444672584534
dqn reward tensor(359.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6096e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09830834716558456
dqn reward tensor(539.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2964e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11847048997879028
dqn reward tensor(447.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.6006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11364781111478806
dqn reward tensor(520.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3018e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14742425084114075
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6710e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10310797393321991
dqn reward tensor(542.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05653340741991997
dqn reward tensor(500., device='cuda:0') e 0.05 loss_dqn tensor(3.3027e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059766680002212524
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0856e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052248261868953705
dqn reward tensor(604.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13434100151062012
dqn reward tensor(372.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4326e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027664681896567345
dqn reward tensor(469.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.3203e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2226720154285431
dqn reward tensor(541.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2820e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12079080939292908
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1034e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17096959054470062
dqn reward tensor(323.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6126e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07259608060121536
dqn reward tensor(435.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4168e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27285003662109375
dqn reward tensor(554.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.1592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13465309143066406
dqn reward tensor(391., device='cuda:0') e 0.05 loss_dqn tensor(3.2551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06948304176330566
dqn reward tensor(405.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6105e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19841603934764862
dqn reward tensor(394.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17774711549282074
dqn reward tensor(446.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.3675e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048942163586616516
dqn reward tensor(472.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2300e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18968525528907776
dqn reward tensor(451.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8512e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1901339590549469
dqn reward tensor(499.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.0472e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10091026872396469
dqn reward tensor(602.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2272e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24973532557487488
dqn reward tensor(367.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.4146e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12494336068630219
dqn reward tensor(518.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3441e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16198132932186127
dqn reward tensor(359.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4520e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.163786381483078
dqn reward tensor(542.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20336462557315826
dqn reward tensor(606.5625, device='cuda:0') e 0.05 loss_dqn tensor(3.2745e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16081976890563965
dqn reward tensor(582.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7296e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17437657713890076
dqn reward tensor(579.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9302e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09164713323116302
dqn reward tensor(577.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18174917995929718
dqn reward tensor(516.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11436107754707336
dqn reward tensor(608.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.8965e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1669607162475586
dqn reward tensor(554.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9530e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12285768985748291
dqn reward tensor(362.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.1912e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07627934962511063
dqn reward tensor(545.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5506e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058684974908828735
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1216e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08992831408977509
dqn reward tensor(221.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7784e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17314206063747406
dqn reward tensor(406.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9723e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055531106889247894
dqn reward tensor(487., device='cuda:0') e 0.05 loss_dqn tensor(1.0643e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022668080404400826
dqn reward tensor(429.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18202100694179535
dqn reward tensor(583., device='cuda:0') e 0.05 loss_dqn tensor(2.4854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20820924639701843
dqn reward tensor(667.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.021788474172353745
dqn reward tensor(414.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.4260e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21560969948768616
dqn reward tensor(504.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.7872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1485052853822708
dqn reward tensor(482., device='cuda:0') e 0.05 loss_dqn tensor(1.6250e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06813614815473557
dqn reward tensor(430.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1190e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1232023686170578
dqn reward tensor(776.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.9468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06116670370101929
dqn reward tensor(634.0625, device='cuda:0') e 0.05 loss_dqn tensor(3.1524e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3839031457901001
dqn reward tensor(654.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9940e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15032468736171722
dqn reward tensor(457.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1771e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09726785868406296
dqn reward tensor(671.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0910e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11324034631252289
dqn reward tensor(643.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.8883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046955037862062454
dqn reward tensor(584.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1418e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12682026624679565
dqn reward tensor(545.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0499e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.150272399187088
dqn reward tensor(544.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8180e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13708865642547607
dqn reward tensor(307.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.2995e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09113861620426178
dqn reward tensor(411.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0973e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12872563302516937
dqn reward tensor(574.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.5779e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22309160232543945
dqn reward tensor(383., device='cuda:0') e 0.05 loss_dqn tensor(1.2848e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10775893181562424
dqn reward tensor(314.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.8549e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19367802143096924
dqn reward tensor(659.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.8717e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09871283173561096
dqn reward tensor(517.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7352e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1719607412815094
dqn reward tensor(393.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1255e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04475685581564903
dqn reward tensor(290.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2198939025402069
dqn reward tensor(471.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.2568e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2736862897872925
dqn reward tensor(615.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5528e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06480507552623749
dqn reward tensor(637.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08170688152313232
dqn reward tensor(528.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.3713e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09498268365859985
dqn reward tensor(470., device='cuda:0') e 0.05 loss_dqn tensor(1.7117e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05238889157772064
dqn reward tensor(556.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.9647e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1237718015909195
dqn reward tensor(459.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2355e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16211168467998505
dqn reward tensor(706.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.2771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1913049966096878
dqn reward tensor(456.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5288e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07345862686634064
dqn reward tensor(550.3125, device='cuda:0') e 0.05 loss_dqn tensor(7.5720e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12357153743505478
dqn reward tensor(361.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.4645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2646453082561493
dqn reward tensor(684.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2878e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3008018732070923
dqn reward tensor(465.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.3996e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0396135076880455
dqn reward tensor(640.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2339e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2091168761253357
dqn reward tensor(518., device='cuda:0') e 0.05 loss_dqn tensor(2.2511e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07112094014883041
dqn reward tensor(397.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3791e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2807304859161377
dqn reward tensor(419.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.0776e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04255641996860504
dqn reward tensor(606.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.0903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27819111943244934
dqn reward tensor(616., device='cuda:0') e 0.05 loss_dqn tensor(3.0153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07351993769407272
dqn reward tensor(593.1875, device='cuda:0') e 0.05 loss_dqn tensor(3.1395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13277098536491394
dqn reward tensor(579.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2820e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08446882665157318
dqn reward tensor(599.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.1609e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04023343697190285
dqn reward tensor(531.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.1995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0992925614118576
dqn reward tensor(464., device='cuda:0') e 0.05 loss_dqn tensor(3.0372e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16696606576442719
dqn reward tensor(763.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.2070e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13773342967033386
dqn reward tensor(420.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0724e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10504882037639618
dqn reward tensor(380.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1099e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22266510128974915
dqn reward tensor(542.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039844073355197906
dqn reward tensor(632.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.7292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13163882493972778
dqn reward tensor(707.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.8960e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15403036773204803
dqn reward tensor(728.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.6087e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040709808468818665
dqn reward tensor(584.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.7407e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054238460958004
dqn reward tensor(568.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.6259e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1772802174091339
dqn reward tensor(598.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4614e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17400497198104858
dqn reward tensor(612.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9959e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17025697231292725
dqn reward tensor(510.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8219e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20644885301589966
dqn reward tensor(674.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5616e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057595908641815186
dqn reward tensor(485.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2895e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08714909851551056
dqn reward tensor(379.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.3601e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11431889981031418
dqn reward tensor(347.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1457e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04093534126877785
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.2356e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1796732395887375
dqn reward tensor(512.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.2519e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17274199426174164
dqn reward tensor(712.9375, device='cuda:0') e 0.05 loss_dqn tensor(3.7077e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06075207144021988
dqn reward tensor(678.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2392352968454361
dqn reward tensor(422., device='cuda:0') e 0.05 loss_dqn tensor(4.1599e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14811992645263672
dqn reward tensor(526.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2416e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0994698703289032
dqn reward tensor(656., device='cuda:0') e 0.05 loss_dqn tensor(4.2899e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17095068097114563
dqn reward tensor(665.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8325e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11743412911891937
dqn reward tensor(323., device='cuda:0') e 0.05 loss_dqn tensor(5.4003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057075995951890945
dqn reward tensor(500.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.8091e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13262274861335754
dqn reward tensor(666.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.0323e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16420957446098328
dqn reward tensor(589.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9198e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10145311057567596
dqn reward tensor(552.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.8693e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027069132775068283
dqn reward tensor(506.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4251e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16319453716278076
dqn reward tensor(222.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7994e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0773191899061203
dqn reward tensor(514.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8198e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11270545423030853
dqn reward tensor(583.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8300e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08249104022979736
dqn reward tensor(709.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.1820e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024575412273406982
dqn reward tensor(734.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.0250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11703012883663177
dqn reward tensor(555.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02404908835887909
dqn reward tensor(631.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.7166e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1542627513408661
dqn reward tensor(474.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.4182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21174746751785278
dqn reward tensor(636.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1224e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17550599575042725
dqn reward tensor(395.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.8238e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018558740615844727
dqn reward tensor(649.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27138862013816833
dqn reward tensor(531.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4913e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19362574815750122
dqn reward tensor(503.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0436e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06605488806962967
dqn reward tensor(362.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.4892e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20285843312740326
dqn reward tensor(578.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.1426e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10844605416059494
dqn reward tensor(533.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0865e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03599318489432335
dqn reward tensor(646.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.5212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23909704387187958
dqn reward tensor(635.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.0479e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024057108908891678
dqn reward tensor(645.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6226e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18238820135593414
dqn reward tensor(687.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.5758e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13121791183948517
dqn reward tensor(618.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.9326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12439129501581192
dqn reward tensor(541.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.1199e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14211347699165344
dqn reward tensor(586., device='cuda:0') e 0.05 loss_dqn tensor(3.8361e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14029940962791443
dqn reward tensor(659., device='cuda:0') e 0.05 loss_dqn tensor(4.0977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29974788427352905
dqn reward tensor(649.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7700e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15080249309539795
dqn reward tensor(458.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5291e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14949548244476318
dqn reward tensor(620.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.9631e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15993386507034302
dqn reward tensor(625., device='cuda:0') e 0.05 loss_dqn tensor(4.2288e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12132105231285095
dqn reward tensor(651.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.2344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12109395861625671
dqn reward tensor(435., device='cuda:0') e 0.05 loss_dqn tensor(3.9003e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053246863186359406
dqn reward tensor(572.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4454e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19583050906658173
dqn reward tensor(467., device='cuda:0') e 0.05 loss_dqn tensor(2.5159e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0442018024623394
dqn reward tensor(509.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0702e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10227473080158234
dqn reward tensor(432.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.8295e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05682601034641266
dqn reward tensor(711.6875, device='cuda:0') e 0.05 loss_dqn tensor(3.6900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08682519197463989
dqn reward tensor(535.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.4655e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11729799211025238
dqn reward tensor(551.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.1808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14119112491607666
dqn reward tensor(558.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.4781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.176767498254776
dqn reward tensor(621.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.1393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15732917189598083
dqn reward tensor(424.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.2195e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33313703536987305
dqn reward tensor(517.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.8129e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15644040703773499
dqn reward tensor(510.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.4279e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033794742077589035
dqn reward tensor(629.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.9272e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13794350624084473
dqn reward tensor(629.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3197e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.172512024641037
dqn reward tensor(537.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.3991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07225313782691956
dqn reward tensor(512.0625, device='cuda:0') e 0.05 loss_dqn tensor(4.3596e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21313422918319702
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.7925e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09259505569934845
dqn reward tensor(468.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4526e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05086822807788849
dqn reward tensor(439.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7455e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07368150353431702
dqn reward tensor(583.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4360e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.048611119389534
dqn reward tensor(568.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.3928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23305337131023407
dqn reward tensor(561., device='cuda:0') e 0.05 loss_dqn tensor(2.6097e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17726115882396698
dqn reward tensor(540.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3836e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07322140038013458
dqn reward tensor(464.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.5861e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1364266574382782
dqn reward tensor(618.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0070e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22296875715255737
dqn reward tensor(424.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0876e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25597772002220154
dqn reward tensor(645.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.5188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03800483047962189
dqn reward tensor(481.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2269e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13296541571617126
dqn reward tensor(675.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07474353909492493
dqn reward tensor(514.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.5286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08022535592317581
dqn reward tensor(371.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.3173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30691611766815186
dqn reward tensor(476.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.4330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.035328201949596405
dqn reward tensor(430.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1468e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05852450802922249
dqn reward tensor(378.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8352e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05436648055911064
dqn reward tensor(611.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3572e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09285280108451843
dqn reward tensor(555.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.7521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15807287395000458
dqn reward tensor(468.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0537e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20813632011413574
dqn reward tensor(558.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8106e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21492454409599304
dqn reward tensor(608.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9317e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11316385120153427
dqn reward tensor(516.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3098e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15084922313690186
dqn reward tensor(523.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0370e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1017233282327652
dqn reward tensor(572.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08626829087734222
dqn reward tensor(408.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.4422e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.253288209438324
dqn reward tensor(419.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3719e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13957935571670532
dqn reward tensor(575.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1105385273694992
dqn reward tensor(447.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2746e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08629313111305237
dqn reward tensor(409.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.3536e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2829582989215851
dqn reward tensor(551.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10373426973819733
dqn reward tensor(487.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0348e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21442855894565582
dqn reward tensor(620.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2274e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1352533996105194
dqn reward tensor(507.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5002e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1420934498310089
dqn reward tensor(443.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9656e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16449077427387238
dqn reward tensor(316.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4198e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15125015377998352
dqn reward tensor(524.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7457e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05983452498912811
dqn reward tensor(626.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3414e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04332687705755234
dqn reward tensor(432.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5283e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08031480759382248
dqn reward tensor(233.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0627e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09617742896080017
dqn reward tensor(399.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.6452e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02956356108188629
dqn reward tensor(438., device='cuda:0') e 0.05 loss_dqn tensor(5.0670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20093975961208344
dqn reward tensor(481.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03243570774793625
dqn reward tensor(451., device='cuda:0') e 0.05 loss_dqn tensor(1.5608e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024251505732536316
dqn reward tensor(514.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8691e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023765206336975098
dqn reward tensor(461.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5296e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14917224645614624
dqn reward tensor(599.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10682440549135208
dqn reward tensor(430.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8458e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10239869356155396
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4225e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17753136157989502
dqn reward tensor(398., device='cuda:0') e 0.05 loss_dqn tensor(8.3719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08927905559539795
dqn reward tensor(426.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7343e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15941950678825378
dqn reward tensor(540.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1835e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2537207007408142
dqn reward tensor(453.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25053635239601135
dqn reward tensor(431.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1029e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06761439144611359
dqn reward tensor(420.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3734e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1980942189693451
dqn reward tensor(594.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14719638228416443
dqn reward tensor(620.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.6337e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.080038882791996
dqn reward tensor(650.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9708e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08638490736484528
dqn reward tensor(481.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9004e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1037815660238266
dqn reward tensor(547.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4624e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0726899579167366
dqn reward tensor(532.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6218e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07161293178796768
dqn reward tensor(492.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8080e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19142615795135498
dqn reward tensor(471.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.4710e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17300641536712646
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9014e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09740982949733734
dqn reward tensor(701.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5288e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07847520709037781
dqn reward tensor(457.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7025e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12515825033187866
dqn reward tensor(612.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4018e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09473337233066559
dqn reward tensor(361.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.8688e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16652680933475494
dqn reward tensor(564.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2140e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1727479100227356
dqn reward tensor(518.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1570e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3248925507068634
dqn reward tensor(578.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3796e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14397236704826355
dqn reward tensor(432.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.1456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08657127618789673
dqn reward tensor(313.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.7312e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0372394323348999
dqn reward tensor(545.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13268309831619263
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3037e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16297310590744019
dqn reward tensor(534.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1258e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13597066700458527
dqn reward tensor(685.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3875e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09999632835388184
dqn reward tensor(439.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.2164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18524867296218872
dqn reward tensor(479.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2013e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16294628381729126
dqn reward tensor(692.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5859e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25052526593208313
dqn reward tensor(482.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20758122205734253
dqn reward tensor(643.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11517056077718735
dqn reward tensor(519.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6407e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14860601723194122
dqn reward tensor(334.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9011e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23655545711517334
dqn reward tensor(368., device='cuda:0') e 0.05 loss_dqn tensor(1.1622e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18497002124786377
dqn reward tensor(593.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.1116e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15469872951507568
dqn reward tensor(708.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04967116191983223
dqn reward tensor(501.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10844643414020538
dqn reward tensor(604.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08880257606506348
dqn reward tensor(339.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.1106e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052217282354831696
dqn reward tensor(428.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.7617e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14469362795352936
dqn reward tensor(470., device='cuda:0') e 0.05 loss_dqn tensor(2.4830e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3272796869277954
dqn reward tensor(521., device='cuda:0') e 0.05 loss_dqn tensor(5.2410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15477482974529266
dqn reward tensor(487.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7620e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12217146158218384
dqn reward tensor(406.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4662e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0668020248413086
dqn reward tensor(548.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.5997e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13111038506031036
dqn reward tensor(339.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.4208e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07307744026184082
dqn reward tensor(455.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0918e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14118000864982605
dqn reward tensor(673.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08237715810537338
dqn reward tensor(236.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.2444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1304326206445694
dqn reward tensor(513.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8588e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15079323947429657
dqn reward tensor(384.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15195582807064056
dqn reward tensor(546.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6858e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1781584471464157
dqn reward tensor(491.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0712e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3194572329521179
dqn reward tensor(41.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.6298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013069147244095802
Evaluating...
Train: {'rocauc': 0.7945896100410373} 7.749858856201172
=====Epoch 37=====
Training...
dqn reward tensor(534., device='cuda:0') e 0.05 loss_dqn tensor(4.8140e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1251000612974167
dqn reward tensor(586.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9535e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17315691709518433
dqn reward tensor(495.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3970e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13467155396938324
dqn reward tensor(338.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.6343e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11513636261224747
dqn reward tensor(677.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07398034632205963
dqn reward tensor(516.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.7029e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08242316544055939
dqn reward tensor(517.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8749e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11505359411239624
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15380859375
dqn reward tensor(360.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5361e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1670527458190918
dqn reward tensor(495.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4612e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16134829819202423
dqn reward tensor(485.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4711e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23216524720191956
dqn reward tensor(646.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6431e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12767240405082703
dqn reward tensor(458.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.7103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15294861793518066
dqn reward tensor(442.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.8218e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1424168348312378
dqn reward tensor(235.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4079e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20435374975204468
dqn reward tensor(539.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14634644985198975
dqn reward tensor(436.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.4334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15831640362739563
dqn reward tensor(560.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.4834e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12176298350095749
dqn reward tensor(593.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3916e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09950237721204758
dqn reward tensor(423.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4141e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11713526397943497
dqn reward tensor(524.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0644e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09628421068191528
dqn reward tensor(630., device='cuda:0') e 0.05 loss_dqn tensor(2.5516e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1157287061214447
dqn reward tensor(395.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9041e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24373680353164673
dqn reward tensor(379.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9886e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0715101882815361
dqn reward tensor(501.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4801e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.221078559756279
dqn reward tensor(492.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6497e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09104820340871811
dqn reward tensor(524., device='cuda:0') e 0.05 loss_dqn tensor(5.1653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05919749662280083
dqn reward tensor(402.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6359e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18387964367866516
dqn reward tensor(414.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3960e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05556553229689598
dqn reward tensor(569.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05816061049699783
dqn reward tensor(499.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5637e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06130377948284149
dqn reward tensor(566.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3494e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08424942940473557
dqn reward tensor(443.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3165e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042326852679252625
dqn reward tensor(470.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4857e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13130581378936768
dqn reward tensor(429.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.5360e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14993242919445038
dqn reward tensor(569.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1871e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1320083886384964
dqn reward tensor(306.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3154e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2519175410270691
dqn reward tensor(505.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2829049527645111
dqn reward tensor(604.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0709e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2385716736316681
dqn reward tensor(617.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0514e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15429110825061798
dqn reward tensor(445.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.9949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09546297788619995
dqn reward tensor(559., device='cuda:0') e 0.05 loss_dqn tensor(5.3429e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11999746412038803
dqn reward tensor(586.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1333e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1633952111005783
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2253088802099228
dqn reward tensor(534.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2993285357952118
dqn reward tensor(527.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1267e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10566592961549759
dqn reward tensor(471., device='cuda:0') e 0.05 loss_dqn tensor(1.7875e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15787175297737122
dqn reward tensor(450.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5638e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07236896455287933
dqn reward tensor(641.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1576383262872696
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.5581e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18822264671325684
dqn reward tensor(550.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1767902970314026
dqn reward tensor(536., device='cuda:0') e 0.05 loss_dqn tensor(5.3418e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10464253276586533
dqn reward tensor(451., device='cuda:0') e 0.05 loss_dqn tensor(5.9059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06495346873998642
dqn reward tensor(453., device='cuda:0') e 0.05 loss_dqn tensor(8.0763e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2516312897205353
dqn reward tensor(462.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20649415254592896
dqn reward tensor(569.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03672618418931961
dqn reward tensor(401.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6511e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10545185953378677
dqn reward tensor(730., device='cuda:0') e 0.05 loss_dqn tensor(5.2753e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23127485811710358
dqn reward tensor(337.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.9551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21253204345703125
dqn reward tensor(492.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7064e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2631378471851349
dqn reward tensor(416., device='cuda:0') e 0.05 loss_dqn tensor(5.8923e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19480028748512268
dqn reward tensor(462.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.7821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08580764383077621
dqn reward tensor(498.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0484e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07615320384502411
dqn reward tensor(524.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11134796589612961
dqn reward tensor(569.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2920e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1499665081501007
dqn reward tensor(651.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22077907621860504
dqn reward tensor(579.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.1464e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11582773923873901
dqn reward tensor(569.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1597e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17268618941307068
dqn reward tensor(526.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11516930162906647
dqn reward tensor(497.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1765e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16553400456905365
dqn reward tensor(676.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0567e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17452691495418549
dqn reward tensor(595., device='cuda:0') e 0.05 loss_dqn tensor(5.3507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13038063049316406
dqn reward tensor(504.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1730e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10646362602710724
dqn reward tensor(558.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031603239476680756
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4317e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10505322366952896
dqn reward tensor(446.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14615553617477417
dqn reward tensor(188.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3638e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08578015863895416
dqn reward tensor(380.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.2433e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16349825263023376
dqn reward tensor(520.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4019e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12757143378257751
dqn reward tensor(473.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1946e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038835309445858
dqn reward tensor(560.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11595387011766434
dqn reward tensor(427.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14681196212768555
dqn reward tensor(439.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2523e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07417653501033783
dqn reward tensor(577.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.9671e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18932226300239563
dqn reward tensor(503., device='cuda:0') e 0.05 loss_dqn tensor(5.6439e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09919659793376923
dqn reward tensor(536.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23272006213665009
dqn reward tensor(487.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7676e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1480795294046402
dqn reward tensor(229.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17152756452560425
dqn reward tensor(482.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1079e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.297688752412796
dqn reward tensor(549.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1664e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1191682294011116
dqn reward tensor(579., device='cuda:0') e 0.05 loss_dqn tensor(2.0504e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06801080703735352
dqn reward tensor(560.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8102e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14696452021598816
dqn reward tensor(485., device='cuda:0') e 0.05 loss_dqn tensor(1.2641e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11272740364074707
dqn reward tensor(556.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12105408310890198
dqn reward tensor(590.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.8759e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16710126399993896
dqn reward tensor(448.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2706e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2781803011894226
dqn reward tensor(445.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.9832e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18421834707260132
dqn reward tensor(473.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06488574296236038
dqn reward tensor(446.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.8251e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16283701360225677
dqn reward tensor(353.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4977e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14602091908454895
dqn reward tensor(197.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.9390e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15066319704055786
dqn reward tensor(324.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3238e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07401339709758759
dqn reward tensor(449.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.8392e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10184672474861145
dqn reward tensor(385.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1184e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07239329814910889
dqn reward tensor(555.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2968e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08637894690036774
dqn reward tensor(543.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.9832e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14585082232952118
dqn reward tensor(476.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0030e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09328202903270721
dqn reward tensor(423.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7610e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18176057934761047
dqn reward tensor(588., device='cuda:0') e 0.05 loss_dqn tensor(2.1325e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08164578676223755
dqn reward tensor(585.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1894e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15947933495044708
dqn reward tensor(427.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3116e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0988905131816864
dqn reward tensor(460.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0204e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17285871505737305
dqn reward tensor(421.6250, device='cuda:0') e 0.05 loss_dqn tensor(7.2389e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09383071959018707
dqn reward tensor(434.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5421e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11441899091005325
dqn reward tensor(360., device='cuda:0') e 0.05 loss_dqn tensor(1.6034e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08597570657730103
dqn reward tensor(508.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.0096e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045931003987789154
dqn reward tensor(474.8125, device='cuda:0') e 0.05 loss_dqn tensor(9.1648e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1179676279425621
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0843e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11584530025720596
dqn reward tensor(623.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04589961841702461
dqn reward tensor(562.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1246e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1083603948354721
dqn reward tensor(574.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06659136712551117
dqn reward tensor(574.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.8969e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16760075092315674
dqn reward tensor(540.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8884e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12864729762077332
dqn reward tensor(308.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5899e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08334340155124664
dqn reward tensor(691.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.1520e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08474309742450714
dqn reward tensor(564.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8406e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2109839916229248
dqn reward tensor(518.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0538e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10717399418354034
dqn reward tensor(477.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4017e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18654727935791016
dqn reward tensor(618.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4923e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10755258053541183
dqn reward tensor(398., device='cuda:0') e 0.05 loss_dqn tensor(5.9603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3572721481323242
dqn reward tensor(515.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.2920e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20726676285266876
dqn reward tensor(599.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2018e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15503497421741486
dqn reward tensor(536.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1009e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2135424017906189
dqn reward tensor(422.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7220e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14945046603679657
dqn reward tensor(314.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.7855e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10868227481842041
dqn reward tensor(460.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5107e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11134612560272217
dqn reward tensor(613.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1504e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26516810059547424
dqn reward tensor(493.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2268e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16061142086982727
dqn reward tensor(640.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9364e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2726494073867798
dqn reward tensor(382.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4433e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22350671887397766
dqn reward tensor(345.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3174e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07514360547065735
dqn reward tensor(419.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.1854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16566726565361023
dqn reward tensor(530.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2383e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1544189304113388
dqn reward tensor(525., device='cuda:0') e 0.05 loss_dqn tensor(2.0692e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12457558512687683
dqn reward tensor(573.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10536204278469086
dqn reward tensor(532.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.8292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09567203372716904
dqn reward tensor(415.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9688e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14458279311656952
dqn reward tensor(405.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7848e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1784139722585678
dqn reward tensor(505.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1239e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07223603129386902
dqn reward tensor(587.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07904929667711258
dqn reward tensor(429.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.4888e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07635170966386795
dqn reward tensor(524.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6305e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29480329155921936
dqn reward tensor(526.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31646353006362915
dqn reward tensor(531.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8397e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1474284678697586
dqn reward tensor(404.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.4685e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1514153778553009
dqn reward tensor(578.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.7196e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06727752834558487
dqn reward tensor(479.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17152905464172363
dqn reward tensor(514.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.3309e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10392270982265472
dqn reward tensor(549.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.5499e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1527835875749588
dqn reward tensor(456.3125, device='cuda:0') e 0.05 loss_dqn tensor(4.8460e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14888033270835876
dqn reward tensor(554.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21279624104499817
dqn reward tensor(550.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.4645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23029015958309174
dqn reward tensor(535.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9459e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06297191977500916
dqn reward tensor(444.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3194e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13917292654514313
dqn reward tensor(486., device='cuda:0') e 0.05 loss_dqn tensor(7.5073e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13535378873348236
dqn reward tensor(456.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3547e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06264299154281616
dqn reward tensor(599.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2379e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05026544630527496
dqn reward tensor(413.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.3078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10863610357046127
dqn reward tensor(433.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4080e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08493258059024811
dqn reward tensor(462.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2240e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15548492968082428
dqn reward tensor(407.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.6965e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15172900259494781
dqn reward tensor(544.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3500e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08585353940725327
dqn reward tensor(489.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13434866070747375
dqn reward tensor(506.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17673781514167786
dqn reward tensor(524.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6058e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05927477404475212
dqn reward tensor(591.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5893e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.5739582180976868
dqn reward tensor(548.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9183e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2206321656703949
dqn reward tensor(468.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2473e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31117701530456543
dqn reward tensor(554.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07938778400421143
dqn reward tensor(477.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3496e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10456407070159912
dqn reward tensor(330.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.7417e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06996873021125793
dqn reward tensor(425.8125, device='cuda:0') e 0.05 loss_dqn tensor(4.7854e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15364386141300201
dqn reward tensor(522.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.2811e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13796274363994598
dqn reward tensor(480.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.3276e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0974503681063652
dqn reward tensor(337.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0461e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08610910177230835
dqn reward tensor(534.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04667753353714943
dqn reward tensor(425.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8592e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11084599792957306
dqn reward tensor(578.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045107387006282806
dqn reward tensor(360.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2604e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06219182908535004
dqn reward tensor(641., device='cuda:0') e 0.05 loss_dqn tensor(5.5217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09502732753753662
dqn reward tensor(641.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.6228e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13893476128578186
dqn reward tensor(530.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11298096179962158
dqn reward tensor(489.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.6465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16362264752388
dqn reward tensor(489.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1219e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014545646496117115
dqn reward tensor(357.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3069e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19530102610588074
dqn reward tensor(520.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026791904121637344
dqn reward tensor(404.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8073e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10796812921762466
dqn reward tensor(469.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.3448e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15700003504753113
dqn reward tensor(517.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5111e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02423439547419548
dqn reward tensor(591.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6686e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08570951968431473
dqn reward tensor(564.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2389e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014177831821143627
dqn reward tensor(422., device='cuda:0') e 0.05 loss_dqn tensor(1.1241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15854451060295105
dqn reward tensor(553.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9726e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2774851620197296
dqn reward tensor(398.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7845e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11161984503269196
dqn reward tensor(567., device='cuda:0') e 0.05 loss_dqn tensor(2.5129e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0194601621478796
dqn reward tensor(617.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0865e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04156985878944397
dqn reward tensor(476.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0434e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07678999751806259
dqn reward tensor(467.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03275946155190468
dqn reward tensor(661.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3372e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20065955817699432
dqn reward tensor(563., device='cuda:0') e 0.05 loss_dqn tensor(4.9106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05557934194803238
dqn reward tensor(538.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2255e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13227549195289612
dqn reward tensor(589.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7942e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06194140017032623
dqn reward tensor(503.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7108e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2037409543991089
dqn reward tensor(623., device='cuda:0') e 0.05 loss_dqn tensor(5.0666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12521347403526306
dqn reward tensor(480.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4671e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03146567568182945
dqn reward tensor(628.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1813e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061697181314229965
dqn reward tensor(459.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3539e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1488824188709259
dqn reward tensor(596.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1281868815422058
dqn reward tensor(535.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07740288972854614
dqn reward tensor(635.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.5877e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03190751001238823
dqn reward tensor(443., device='cuda:0') e 0.05 loss_dqn tensor(5.7839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11439290642738342
dqn reward tensor(337.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.4956e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06030058115720749
dqn reward tensor(455.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8918e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1733177751302719
dqn reward tensor(528.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4538e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16770225763320923
dqn reward tensor(652.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0819282978773117
dqn reward tensor(548.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2791e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20506472885608673
dqn reward tensor(478.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18010221421718597
dqn reward tensor(567.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2505e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15176570415496826
dqn reward tensor(402.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7897e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07678437978029251
dqn reward tensor(496.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.2421e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1526191085577011
dqn reward tensor(450.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.0539e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061215296387672424
dqn reward tensor(549.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1811e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07825038582086563
dqn reward tensor(611.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3181e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10642760992050171
dqn reward tensor(517.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2233e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14832618832588196
dqn reward tensor(556.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4963e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15504121780395508
dqn reward tensor(464.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.9900e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13611780107021332
dqn reward tensor(536.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.2647e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03208351880311966
dqn reward tensor(502.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.5741e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1536864936351776
dqn reward tensor(409.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0327e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025600196793675423
dqn reward tensor(457.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16376763582229614
dqn reward tensor(507.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1108e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04304738715291023
dqn reward tensor(531.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.6759e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19991432130336761
dqn reward tensor(438.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.9010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050534382462501526
dqn reward tensor(486.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3517e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1539286971092224
dqn reward tensor(363.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.7621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15590664744377136
dqn reward tensor(496.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.3347e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32552558183670044
dqn reward tensor(465.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09574372321367264
dqn reward tensor(383.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.0774e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13761286437511444
dqn reward tensor(573., device='cuda:0') e 0.05 loss_dqn tensor(5.5326e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10094255954027176
dqn reward tensor(478.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15723952651023865
dqn reward tensor(601.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.8560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14336711168289185
dqn reward tensor(478.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.4377e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12936833500862122
dqn reward tensor(610.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7920e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3040546178817749
dqn reward tensor(455.4375, device='cuda:0') e 0.05 loss_dqn tensor(5.7620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10695865750312805
dqn reward tensor(593.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15897463262081146
dqn reward tensor(630.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11301635205745697
dqn reward tensor(455.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2517e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05841969698667526
dqn reward tensor(623.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.3777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11160620301961899
dqn reward tensor(512.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.1647e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12259542942047119
dqn reward tensor(650.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1046e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05661313980817795
dqn reward tensor(559.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5890e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1528702825307846
dqn reward tensor(391.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0757e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13609656691551208
dqn reward tensor(465.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3626e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10816793143749237
dqn reward tensor(440., device='cuda:0') e 0.05 loss_dqn tensor(5.6707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21341994404792786
dqn reward tensor(381.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.1513e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25574496388435364
dqn reward tensor(661.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.7633e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20476040244102478
dqn reward tensor(506.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0693e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024059055373072624
dqn reward tensor(648.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1025e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13432542979717255
dqn reward tensor(478.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.7171e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13638734817504883
dqn reward tensor(650.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05335524305701256
dqn reward tensor(508.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.8079e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14548929035663605
dqn reward tensor(728., device='cuda:0') e 0.05 loss_dqn tensor(5.2480e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0854148343205452
dqn reward tensor(590., device='cuda:0') e 0.05 loss_dqn tensor(5.2025e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21264685690402985
dqn reward tensor(693.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6757e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32208043336868286
dqn reward tensor(424.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5658e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08916821330785751
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4058e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08024781942367554
dqn reward tensor(277.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0270e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1462050974369049
dqn reward tensor(507., device='cuda:0') e 0.05 loss_dqn tensor(4.9975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12446437776088715
dqn reward tensor(501., device='cuda:0') e 0.05 loss_dqn tensor(5.2881e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09240732342004776
dqn reward tensor(613.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03142470121383667
dqn reward tensor(522.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7141e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1079036220908165
dqn reward tensor(554.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1248e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1758885532617569
dqn reward tensor(430.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.5928e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029192708432674408
dqn reward tensor(497.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14645622670650482
dqn reward tensor(346.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1496e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1633247286081314
dqn reward tensor(533.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.5484e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1304614543914795
dqn reward tensor(472.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0944e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09171388298273087
dqn reward tensor(514., device='cuda:0') e 0.05 loss_dqn tensor(1.5024e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19561442732810974
dqn reward tensor(541., device='cuda:0') e 0.05 loss_dqn tensor(6.2344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08124147355556488
dqn reward tensor(419.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0746e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07335124164819717
dqn reward tensor(467.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7515e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1595381498336792
dqn reward tensor(450.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.0524e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10509522259235382
dqn reward tensor(544.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3950e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14068567752838135
dqn reward tensor(381.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19826950132846832
dqn reward tensor(597.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.4870e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1205563098192215
dqn reward tensor(467.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.1084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18210764229297638
dqn reward tensor(621., device='cuda:0') e 0.05 loss_dqn tensor(5.7768e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06222614645957947
dqn reward tensor(622.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.6190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.051709018647670746
dqn reward tensor(633.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6233e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023271791636943817
dqn reward tensor(665.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0412e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07226932048797607
dqn reward tensor(600.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1176764965057373
dqn reward tensor(390.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.208405002951622
dqn reward tensor(632.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.3390e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19314423203468323
dqn reward tensor(558.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13681010901927948
dqn reward tensor(477.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8730e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02212277241051197
dqn reward tensor(506.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2448e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10999608784914017
dqn reward tensor(502.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8574e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08822593092918396
dqn reward tensor(484.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4147e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13223016262054443
dqn reward tensor(661., device='cuda:0') e 0.05 loss_dqn tensor(4.9166e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16205473244190216
dqn reward tensor(478.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7249e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09427869319915771
dqn reward tensor(411.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9770e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037711504846811295
dqn reward tensor(523.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.6977e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034364599734544754
dqn reward tensor(510., device='cuda:0') e 0.05 loss_dqn tensor(5.2112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09221440553665161
dqn reward tensor(510.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2043e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030903223901987076
dqn reward tensor(455.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1506e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10255399346351624
dqn reward tensor(476., device='cuda:0') e 0.05 loss_dqn tensor(2.5948e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22471672296524048
dqn reward tensor(579.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13973572850227356
dqn reward tensor(545.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.7844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10987179726362228
dqn reward tensor(574.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.0192e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07523953169584274
dqn reward tensor(564.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4950e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10801125317811966
dqn reward tensor(640.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1236e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08174633234739304
dqn reward tensor(441.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5857e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06888209283351898
dqn reward tensor(648.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27966511249542236
dqn reward tensor(485.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5515e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05334921181201935
dqn reward tensor(506.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2514e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025320356711745262
dqn reward tensor(597.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05278393626213074
dqn reward tensor(344.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.6344e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16330675780773163
dqn reward tensor(438.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5986e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1194150522351265
dqn reward tensor(593.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3709e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06102253496646881
dqn reward tensor(522.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4366e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1777767688035965
dqn reward tensor(520.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8949e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08983894437551498
dqn reward tensor(515.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2276e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13913020491600037
dqn reward tensor(452.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0440e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10051096975803375
dqn reward tensor(351.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.7436e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13190384209156036
dqn reward tensor(525.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6579e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09805500507354736
dqn reward tensor(588., device='cuda:0') e 0.05 loss_dqn tensor(5.2809e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10283996164798737
dqn reward tensor(473.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1523030698299408
dqn reward tensor(519.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2340e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2783747911453247
dqn reward tensor(384.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2370e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.131902813911438
dqn reward tensor(584.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.6111e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.052676260471343994
dqn reward tensor(371.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.5146e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05494977906346321
dqn reward tensor(500.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3583e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11791805922985077
dqn reward tensor(367.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6971e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18851055204868317
dqn reward tensor(532.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.3186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03284693881869316
dqn reward tensor(553.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.7802e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09709208458662033
dqn reward tensor(655.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.1456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11777891218662262
dqn reward tensor(380.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5503e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13603970408439636
dqn reward tensor(549.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3195e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22393281757831573
dqn reward tensor(462.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18560883402824402
dqn reward tensor(400.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3931e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09672325104475021
dqn reward tensor(478., device='cuda:0') e 0.05 loss_dqn tensor(5.6513e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22332924604415894
dqn reward tensor(482.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6928e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06101866811513901
dqn reward tensor(686.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08184051513671875
dqn reward tensor(261.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23474490642547607
dqn reward tensor(658.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2235e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07932829856872559
dqn reward tensor(402.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0809e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029522869735956192
dqn reward tensor(487.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4536e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14874590933322906
dqn reward tensor(544.3125, device='cuda:0') e 0.05 loss_dqn tensor(3.0992e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08193597942590714
dqn reward tensor(400., device='cuda:0') e 0.05 loss_dqn tensor(5.6475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09449350833892822
dqn reward tensor(481.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.9111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15513956546783447
dqn reward tensor(704.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14119496941566467
dqn reward tensor(359.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.2003e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050179481506347656
dqn reward tensor(371.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1346e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09349077939987183
dqn reward tensor(603.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17221131920814514
dqn reward tensor(516.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.6114e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13815490901470184
dqn reward tensor(624.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9350e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10191798955202103
dqn reward tensor(400.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1125e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.421764612197876
dqn reward tensor(295.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7925e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11200219392776489
dqn reward tensor(446.9375, device='cuda:0') e 0.05 loss_dqn tensor(2.7977e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10728245973587036
dqn reward tensor(537.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.200932115316391
dqn reward tensor(488.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038754239678382874
dqn reward tensor(436.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.4371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23365940153598785
dqn reward tensor(561.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2227e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17877763509750366
dqn reward tensor(581.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16995272040367126
dqn reward tensor(516., device='cuda:0') e 0.05 loss_dqn tensor(5.4135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10720551759004593
dqn reward tensor(546.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.9178e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0905565470457077
dqn reward tensor(636.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07611609995365143
dqn reward tensor(235.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3800e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09720635414123535
dqn reward tensor(419.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8063e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1767159402370453
dqn reward tensor(527.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13670890033245087
dqn reward tensor(389.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2760e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16865071654319763
dqn reward tensor(687.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4089e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11667685955762863
dqn reward tensor(577.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1816e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15103673934936523
dqn reward tensor(714.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6329e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03431055322289467
dqn reward tensor(418., device='cuda:0') e 0.05 loss_dqn tensor(5.3107e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2146340310573578
dqn reward tensor(472.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0448e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2541627585887909
dqn reward tensor(670.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2170e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1800040900707245
dqn reward tensor(491.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8863e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0404854454100132
dqn reward tensor(523.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.1584e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08881612867116928
dqn reward tensor(404.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.8549e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019810793921351433
dqn reward tensor(536.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4487e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1344945728778839
dqn reward tensor(413.8125, device='cuda:0') e 0.05 loss_dqn tensor(6.0917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033158183097839355
dqn reward tensor(331., device='cuda:0') e 0.05 loss_dqn tensor(1.4429e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12114442884922028
dqn reward tensor(655.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02056233212351799
dqn reward tensor(598., device='cuda:0') e 0.05 loss_dqn tensor(4.9444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05573991686105728
dqn reward tensor(454.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.8331e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20416142046451569
dqn reward tensor(362.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6524e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10773992538452148
dqn reward tensor(625.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.3173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06970875710248947
dqn reward tensor(500.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.9700e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2596610188484192
dqn reward tensor(396.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1854e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1482984572649002
dqn reward tensor(450.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.5612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09544725716114044
dqn reward tensor(407.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.9386e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16075681149959564
dqn reward tensor(675.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.2767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11735913157463074
dqn reward tensor(496.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1703e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2729671001434326
dqn reward tensor(565.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5236e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02760966122150421
dqn reward tensor(387.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.3567e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16481146216392517
dqn reward tensor(636.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12338437139987946
dqn reward tensor(500.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4498e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18772011995315552
dqn reward tensor(347.6875, device='cuda:0') e 0.05 loss_dqn tensor(2.1310e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10749021172523499
dqn reward tensor(598.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1519e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08944936841726303
dqn reward tensor(485.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4350e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09824490547180176
dqn reward tensor(489.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.058202989399433136
dqn reward tensor(502.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09004173427820206
dqn reward tensor(532.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.3039e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08637642860412598
dqn reward tensor(511.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2985e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08500039577484131
dqn reward tensor(379.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0014e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0969332754611969
dqn reward tensor(448.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5152e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12277720123529434
dqn reward tensor(402.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8296e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18077059090137482
dqn reward tensor(393.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9934e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0767110288143158
dqn reward tensor(333.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12061136960983276
dqn reward tensor(572.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23794269561767578
dqn reward tensor(644.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4786e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1511818766593933
dqn reward tensor(576.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024421054869890213
dqn reward tensor(647.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0472e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23167428374290466
dqn reward tensor(683., device='cuda:0') e 0.05 loss_dqn tensor(5.6652e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17650195956230164
dqn reward tensor(580.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2506e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10589321702718735
dqn reward tensor(549.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.4917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04715711623430252
dqn reward tensor(553.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.2031e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0980561375617981
dqn reward tensor(663.4375, device='cuda:0') e 0.05 loss_dqn tensor(4.7285e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09020932018756866
dqn reward tensor(515.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.0824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.131637841463089
dqn reward tensor(603.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.4722e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14417926967144012
dqn reward tensor(475.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10135778784751892
dqn reward tensor(398.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.1154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17938479781150818
dqn reward tensor(304.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0308365635573864
dqn reward tensor(462.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.3371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13158240914344788
dqn reward tensor(528.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0893e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10116974264383316
dqn reward tensor(266.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.4622e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13659843802452087
dqn reward tensor(267.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.8319e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16440318524837494
dqn reward tensor(43.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.8703e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14886553585529327
dqn reward tensor(352.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23003843426704407
dqn reward tensor(244.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.3162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09021971374750137
dqn reward tensor(234.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7259e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27675914764404297
dqn reward tensor(247.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.9082e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.171968013048172
dqn reward tensor(243.8125, device='cuda:0') e 0.05 loss_dqn tensor(8.7911e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07874195277690887
dqn reward tensor(289.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.3586e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18778258562088013
dqn reward tensor(90.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.4412e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10046868771314621
dqn reward tensor(327., device='cuda:0') e 0.05 loss_dqn tensor(8.5703e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15095254778862
dqn reward tensor(193.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3498e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15819044411182404
dqn reward tensor(243.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6525e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32601600885391235
dqn reward tensor(250.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9777e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2546728849411011
dqn reward tensor(91.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2385e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09428411722183228
dqn reward tensor(362.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9988e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13372266292572021
dqn reward tensor(305.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.9710e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13196510076522827
dqn reward tensor(214., device='cuda:0') e 0.05 loss_dqn tensor(1.3782e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22802819311618805
dqn reward tensor(223.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0898e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24784480035305023
dqn reward tensor(193.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1163e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13623228669166565
dqn reward tensor(83.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2577e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18753284215927124
dqn reward tensor(198.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3007e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10736449062824249
dqn reward tensor(177.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6364e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11863220483064651
dqn reward tensor(93.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1429e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149690181016922
dqn reward tensor(173.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7804e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19252194464206696
dqn reward tensor(145.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09026113152503967
dqn reward tensor(177.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0461e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16249437630176544
dqn reward tensor(266.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6163e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2585359215736389
dqn reward tensor(188.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9062e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1573670357465744
dqn reward tensor(216., device='cuda:0') e 0.05 loss_dqn tensor(1.7976e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09464901685714722
dqn reward tensor(120.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.0647e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12448795884847641
dqn reward tensor(299.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4155e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06442482769489288
dqn reward tensor(74.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9244e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03141472116112709
dqn reward tensor(209.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0042e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07359085977077484
dqn reward tensor(186.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.5586e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10392951965332031
dqn reward tensor(306., device='cuda:0') e 0.05 loss_dqn tensor(8.8521e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11438024789094925
dqn reward tensor(116.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0785e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.046266455203294754
dqn reward tensor(257.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4572e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0640040934085846
dqn reward tensor(188.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0018e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2193230390548706
dqn reward tensor(214.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18457703292369843
dqn reward tensor(259., device='cuda:0') e 0.05 loss_dqn tensor(9.3955e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24244090914726257
dqn reward tensor(87.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1168e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06272785365581512
dqn reward tensor(-22.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0644e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1305607557296753
dqn reward tensor(192.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6842e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30843979120254517
dqn reward tensor(-42.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6658e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03592817112803459
dqn reward tensor(60.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.6165e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15572375059127808
dqn reward tensor(118., device='cuda:0') e 0.05 loss_dqn tensor(3.0029e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04882976412773132
dqn reward tensor(116.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2584e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25878211855888367
dqn reward tensor(216.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6604e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03537788987159729
dqn reward tensor(83.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1578e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07950803637504578
dqn reward tensor(86.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9371e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14963948726654053
dqn reward tensor(176.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7983e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0828121230006218
dqn reward tensor(143.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9623e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03488291800022125
dqn reward tensor(282.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09883560240268707
dqn reward tensor(177.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7118e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04514894634485245
dqn reward tensor(228.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9472e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17553828656673431
dqn reward tensor(160.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1540e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07097074389457703
dqn reward tensor(172.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3988e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2474687397480011
dqn reward tensor(185.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8518e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1071915403008461
dqn reward tensor(395.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.0488e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04949498921632767
dqn reward tensor(310., device='cuda:0') e 0.05 loss_dqn tensor(8.3400e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.016756415367126465
dqn reward tensor(361.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6128e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18308907747268677
dqn reward tensor(297.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13430921733379364
dqn reward tensor(137., device='cuda:0') e 0.05 loss_dqn tensor(8.4281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0923052504658699
dqn reward tensor(257.0625, device='cuda:0') e 0.05 loss_dqn tensor(9.1134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08372405171394348
dqn reward tensor(146.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.3790e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07464459538459778
dqn reward tensor(257., device='cuda:0') e 0.05 loss_dqn tensor(9.2243e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1268649697303772
dqn reward tensor(188.2500, device='cuda:0') e 0.05 loss_dqn tensor(3.0089e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3144189119338989
dqn reward tensor(96.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0771e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0901075154542923
dqn reward tensor(186., device='cuda:0') e 0.05 loss_dqn tensor(1.0346e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19214385747909546
dqn reward tensor(197.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7453e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08471804112195969
dqn reward tensor(259., device='cuda:0') e 0.05 loss_dqn tensor(1.4827e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04734267294406891
dqn reward tensor(472.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3666e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028817210346460342
dqn reward tensor(246.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7583e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03220323473215103
dqn reward tensor(340.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.8328e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12061144411563873
dqn reward tensor(322.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11395077407360077
dqn reward tensor(235.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1266036033630371
dqn reward tensor(176.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6496e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02272428572177887
dqn reward tensor(15., device='cuda:0') e 0.05 loss_dqn tensor(1.3036e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.008598046377301216
Evaluating...
Train: {'rocauc': 0.7935731350417242} 3.2497198581695557
=====Epoch 38=====
Training...
dqn reward tensor(288.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8619e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07928580790758133
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.4740e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16198334097862244
dqn reward tensor(231.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6714e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09024806320667267
dqn reward tensor(70.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1031e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.01723920740187168
dqn reward tensor(184.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4683e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3215029537677765
dqn reward tensor(159.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1358e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13967828452587128
dqn reward tensor(161.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1882e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10092920809984207
dqn reward tensor(294.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.4413e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.35099953413009644
dqn reward tensor(304.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1855e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1757936030626297
dqn reward tensor(82.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.2100e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06926202028989792
dqn reward tensor(296.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8648e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11510296165943146
dqn reward tensor(221.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9373e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09017336368560791
dqn reward tensor(348.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.0168e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10073037445545197
dqn reward tensor(206.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.5301e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19909444451332092
dqn reward tensor(144.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6551e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15984001755714417
dqn reward tensor(125., device='cuda:0') e 0.05 loss_dqn tensor(8.9156e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20998239517211914
dqn reward tensor(273.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.3069e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2195706069469452
dqn reward tensor(82.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6737e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0368969663977623
dqn reward tensor(242.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17433682084083557
dqn reward tensor(172.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5051e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27227699756622314
dqn reward tensor(333.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1497e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22756516933441162
dqn reward tensor(195.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6861e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14383414387702942
dqn reward tensor(273.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.4647e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03175881505012512
dqn reward tensor(258.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.2572e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03619850054383278
dqn reward tensor(256.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04909558594226837
dqn reward tensor(293.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.1588e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0593130998313427
dqn reward tensor(273.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0097e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07465831190347672
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2376e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0737210288643837
dqn reward tensor(50.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.9420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14351244270801544
dqn reward tensor(185.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6612e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1408705711364746
dqn reward tensor(272.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1174e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08574549108743668
dqn reward tensor(216.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.8368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04428901895880699
dqn reward tensor(298.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3055e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012175698764622211
dqn reward tensor(305.1875, device='cuda:0') e 0.05 loss_dqn tensor(8.3796e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22153709828853607
dqn reward tensor(129.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7810e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0712689757347107
dqn reward tensor(156.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.9368e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12514211237430573
dqn reward tensor(324.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.0542e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12903276085853577
dqn reward tensor(145.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.1118e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10605345666408539
dqn reward tensor(253.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4486e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14341512322425842
dqn reward tensor(152.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6927e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037690844386816025
dqn reward tensor(206.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6975e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16330167651176453
dqn reward tensor(57.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9514e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15445855259895325
dqn reward tensor(244.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.8757e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0902337059378624
dqn reward tensor(230.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1440e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030672745779156685
dqn reward tensor(187.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03748740255832672
dqn reward tensor(-12.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1145063042640686
dqn reward tensor(273.7500, device='cuda:0') e 0.05 loss_dqn tensor(7.7937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.059883348643779755
dqn reward tensor(120.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7914e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16334521770477295
dqn reward tensor(156.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4562e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14540278911590576
dqn reward tensor(133.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0670e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03033558465540409
dqn reward tensor(292.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.2067e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11364136636257172
dqn reward tensor(212., device='cuda:0') e 0.05 loss_dqn tensor(9.0157e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13705137372016907
dqn reward tensor(128.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1600e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0379132442176342
dqn reward tensor(254.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1475e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0752439945936203
dqn reward tensor(236.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1874457597732544
dqn reward tensor(228.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7643e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12348468601703644
dqn reward tensor(191.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7745e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04189068451523781
dqn reward tensor(399.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1463e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11392433941364288
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6249e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07420310378074646
dqn reward tensor(225.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0933e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12425019592046738
dqn reward tensor(89.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.5289e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08493224531412125
dqn reward tensor(308.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2325e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2966874837875366
dqn reward tensor(306.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2314e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23609964549541473
dqn reward tensor(301., device='cuda:0') e 0.05 loss_dqn tensor(7.9698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27125632762908936
dqn reward tensor(99.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.0249e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3343167006969452
dqn reward tensor(260.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4105e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09843079745769501
dqn reward tensor(234.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6762e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12417417019605637
dqn reward tensor(166.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.3945e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08870476484298706
dqn reward tensor(342.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.0630e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15099185705184937
dqn reward tensor(248.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2779e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07797673344612122
dqn reward tensor(395.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.1645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07887260615825653
dqn reward tensor(252.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.0336e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1526378095149994
dqn reward tensor(158.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8732e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12462624907493591
dqn reward tensor(325.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.6927e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20636053383350372
dqn reward tensor(260.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2030e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04055975005030632
dqn reward tensor(408.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5386e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08675450086593628
dqn reward tensor(368.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.9640e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13461992144584656
dqn reward tensor(254.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0609e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0947997197508812
dqn reward tensor(216.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.2580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10102721303701401
dqn reward tensor(285.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0104e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08546896278858185
dqn reward tensor(171.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6570e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17419596016407013
dqn reward tensor(59.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.6757e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10629255324602127
dqn reward tensor(221.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1955e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1076163649559021
dqn reward tensor(347.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.3669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09312782436609268
dqn reward tensor(238.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8852e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1033540889620781
dqn reward tensor(316.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8529e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15103483200073242
dqn reward tensor(209.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08339206874370575
dqn reward tensor(252., device='cuda:0') e 0.05 loss_dqn tensor(1.7314e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2413167804479599
dqn reward tensor(148.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7757e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1730724722146988
dqn reward tensor(243.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.2867e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13450515270233154
dqn reward tensor(133.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0177e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12664079666137695
dqn reward tensor(294.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9407e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08990655094385147
dqn reward tensor(213.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.7560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2056414633989334
dqn reward tensor(325.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.8712e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25948312878608704
dqn reward tensor(154., device='cuda:0') e 0.05 loss_dqn tensor(9.4747e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04354454576969147
dqn reward tensor(376.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8556e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09546102583408356
dqn reward tensor(362.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9135e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21270136535167694
dqn reward tensor(196.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.2582e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038907021284103394
dqn reward tensor(266.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6650e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14713798463344574
dqn reward tensor(309.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.5930e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09657634794712067
dqn reward tensor(268.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0298e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05359496921300888
dqn reward tensor(303.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5334e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10218679904937744
dqn reward tensor(69.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2205e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23645518720149994
dqn reward tensor(195.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1335e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0513167679309845
dqn reward tensor(333.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.4664e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07827956974506378
dqn reward tensor(293.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045632101595401764
dqn reward tensor(215.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3394e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20186883211135864
dqn reward tensor(300.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23945844173431396
dqn reward tensor(266.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3687e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26873263716697693
dqn reward tensor(185.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.6302e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10013054311275482
dqn reward tensor(345.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.2621e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17235371470451355
dqn reward tensor(187.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.0014e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13760846853256226
dqn reward tensor(148.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3540e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028508232906460762
dqn reward tensor(162., device='cuda:0') e 0.05 loss_dqn tensor(3.7698e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11637024581432343
dqn reward tensor(334.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3461e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16603562235832214
dqn reward tensor(163.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0071e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16563457250595093
dqn reward tensor(241., device='cuda:0') e 0.05 loss_dqn tensor(2.0358e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18268194794654846
dqn reward tensor(229., device='cuda:0') e 0.05 loss_dqn tensor(9.2620e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2818622589111328
dqn reward tensor(237.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.0610e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08200612664222717
dqn reward tensor(406.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8059e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14066626131534576
dqn reward tensor(246.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.5998e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09057343751192093
dqn reward tensor(307.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8102e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20252224802970886
dqn reward tensor(369.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2562e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17019805312156677
dqn reward tensor(310., device='cuda:0') e 0.05 loss_dqn tensor(8.3229e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1329553723335266
dqn reward tensor(181.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13233225047588348
dqn reward tensor(358.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8275e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053887635469436646
dqn reward tensor(211.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5475e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1250370442867279
dqn reward tensor(235., device='cuda:0') e 0.05 loss_dqn tensor(8.2910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1421000063419342
dqn reward tensor(196.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.8430e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06268871575593948
dqn reward tensor(207., device='cuda:0') e 0.05 loss_dqn tensor(9.0627e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16386005282402039
dqn reward tensor(300., device='cuda:0') e 0.05 loss_dqn tensor(9.1164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07720756530761719
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9442e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10704869031906128
dqn reward tensor(446.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.3378e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1751408874988556
dqn reward tensor(131.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7710e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2626352906227112
dqn reward tensor(403.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12275867164134979
dqn reward tensor(345.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1697e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11895972490310669
dqn reward tensor(185.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8555e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0959252268075943
dqn reward tensor(214.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.3071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08681587874889374
dqn reward tensor(261.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17904379963874817
dqn reward tensor(118., device='cuda:0') e 0.05 loss_dqn tensor(8.7384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10016556084156036
dqn reward tensor(185.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.7793e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08529220521450043
dqn reward tensor(245.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2821e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1551273912191391
dqn reward tensor(240., device='cuda:0') e 0.05 loss_dqn tensor(1.3948e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14498081803321838
dqn reward tensor(229.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3417e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24939176440238953
dqn reward tensor(161.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3114e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04769332334399223
dqn reward tensor(262.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8074e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0987076610326767
dqn reward tensor(434.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.7014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1833266019821167
dqn reward tensor(269.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.1097e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03341294080018997
dqn reward tensor(228.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.0239e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0881652906537056
dqn reward tensor(317.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1024e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09796811640262604
dqn reward tensor(259.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8633e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09202916920185089
dqn reward tensor(327.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.1862e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21434751152992249
dqn reward tensor(343.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6241e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03862134367227554
dqn reward tensor(189.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.054490454494953156
dqn reward tensor(69.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.6051e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0780746266245842
dqn reward tensor(366.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.8056e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2126733958721161
dqn reward tensor(309.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.2723e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15851442515850067
dqn reward tensor(348.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3208e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08653446286916733
dqn reward tensor(254.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3417e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3087767958641052
dqn reward tensor(320.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09573955833911896
dqn reward tensor(258.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.2598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06708332896232605
dqn reward tensor(337.8750, device='cuda:0') e 0.05 loss_dqn tensor(3.3130e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14661456644535065
dqn reward tensor(164.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0347e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09865006804466248
dqn reward tensor(259.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2178e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14424490928649902
dqn reward tensor(408.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4110e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07791800796985626
dqn reward tensor(237.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.1106e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1685124784708023
dqn reward tensor(387.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.2113e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08412060141563416
dqn reward tensor(301.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1579660177230835
dqn reward tensor(248., device='cuda:0') e 0.05 loss_dqn tensor(8.7060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.134541317820549
dqn reward tensor(242.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3603e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17923706769943237
dqn reward tensor(344.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1226e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14115601778030396
dqn reward tensor(352.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9134e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1639772355556488
dqn reward tensor(329.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.0462e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2564850449562073
dqn reward tensor(323.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7754e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.032287489622831345
dqn reward tensor(360.5000, device='cuda:0') e 0.05 loss_dqn tensor(8.6612e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.042012523859739304
dqn reward tensor(449.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9593e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06955789774656296
dqn reward tensor(165.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8215e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08895407617092133
dqn reward tensor(164.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.6573e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03165479749441147
dqn reward tensor(226.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.7968e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24303117394447327
dqn reward tensor(279.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4708e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024702297523617744
dqn reward tensor(352.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08836166560649872
dqn reward tensor(301.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5914e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1205030083656311
dqn reward tensor(174., device='cuda:0') e 0.05 loss_dqn tensor(1.9634e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08510981500148773
dqn reward tensor(363.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1065e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16207529604434967
dqn reward tensor(338.8125, device='cuda:0') e 0.05 loss_dqn tensor(3.2454e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1697390228509903
dqn reward tensor(147., device='cuda:0') e 0.05 loss_dqn tensor(9.1711e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19304436445236206
dqn reward tensor(239.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04556068778038025
dqn reward tensor(183., device='cuda:0') e 0.05 loss_dqn tensor(2.1122e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07746677100658417
dqn reward tensor(46.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.0230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04735533148050308
dqn reward tensor(246.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2189e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16361218690872192
dqn reward tensor(293.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1168e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18373119831085205
dqn reward tensor(309.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.8176e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16102755069732666
dqn reward tensor(356.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.8202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2660280466079712
dqn reward tensor(238.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1049e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24558553099632263
dqn reward tensor(482.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20861083269119263
dqn reward tensor(242., device='cuda:0') e 0.05 loss_dqn tensor(1.7730e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1284288465976715
dqn reward tensor(432.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.2376e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15465840697288513
dqn reward tensor(236.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4971e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2240504026412964
dqn reward tensor(528.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7803e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1221020370721817
dqn reward tensor(225.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5218e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1511690318584442
dqn reward tensor(260.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1629e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09545612335205078
dqn reward tensor(374.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.2841e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0639745369553566
dqn reward tensor(288.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.6840e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10471728444099426
dqn reward tensor(403., device='cuda:0') e 0.05 loss_dqn tensor(9.8184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10530722141265869
dqn reward tensor(179.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.1574e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06000886857509613
dqn reward tensor(262.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.4310e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21053847670555115
dqn reward tensor(334.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08047253638505936
dqn reward tensor(132.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1075e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07234776020050049
dqn reward tensor(249.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.3247e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13345539569854736
dqn reward tensor(312.1250, device='cuda:0') e 0.05 loss_dqn tensor(3.4658e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03501082956790924
dqn reward tensor(224.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4323e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20170654356479645
dqn reward tensor(320.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5427e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04540703818202019
dqn reward tensor(255.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4762e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22878050804138184
dqn reward tensor(267.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.1690e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0745655745267868
dqn reward tensor(198.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.0558e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.009389439597725868
dqn reward tensor(74., device='cuda:0') e 0.05 loss_dqn tensor(9.4456e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0927809625864029
dqn reward tensor(381.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.6750e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3408240079879761
dqn reward tensor(320.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9987e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11725936084985733
dqn reward tensor(239., device='cuda:0') e 0.05 loss_dqn tensor(1.0219e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.013009719550609589
dqn reward tensor(163.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.9834e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1979822814464569
dqn reward tensor(340.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.7251e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10874728113412857
dqn reward tensor(306.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0584e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15911898016929626
dqn reward tensor(100.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07682422548532486
dqn reward tensor(378.6875, device='cuda:0') e 0.05 loss_dqn tensor(8.3067e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09420481324195862
dqn reward tensor(160.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3655e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11861465871334076
dqn reward tensor(288.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0531e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14996108412742615
dqn reward tensor(169.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.4735e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07809831202030182
dqn reward tensor(195.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3756e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13024090230464935
dqn reward tensor(265.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.0898e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10862523317337036
dqn reward tensor(340.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.6604e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14442148804664612
dqn reward tensor(148.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4795e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1795664131641388
dqn reward tensor(345.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.9775e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0876259133219719
dqn reward tensor(247.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.8641e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07792041450738907
dqn reward tensor(246., device='cuda:0') e 0.05 loss_dqn tensor(1.0602e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14352445304393768
dqn reward tensor(246.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2265e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06506454944610596
dqn reward tensor(216.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1655e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03657308965921402
dqn reward tensor(115.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9475e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10234951972961426
dqn reward tensor(239.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11016815900802612
dqn reward tensor(225.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.7569e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15288051962852478
dqn reward tensor(363.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1034e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16891123354434967
dqn reward tensor(422., device='cuda:0') e 0.05 loss_dqn tensor(9.2324e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04037371277809143
dqn reward tensor(323.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30600282549858093
dqn reward tensor(266.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5905e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.053192369639873505
dqn reward tensor(148.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.3901e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02874772995710373
dqn reward tensor(178.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.9858e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09931665658950806
dqn reward tensor(264.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.5395e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09916308522224426
dqn reward tensor(118.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8822e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1261596530675888
dqn reward tensor(281.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.7555e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20420598983764648
dqn reward tensor(337.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6109e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02355324849486351
dqn reward tensor(202.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.3156e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15353438258171082
dqn reward tensor(515.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.1710e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21237799525260925
dqn reward tensor(281.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.9812e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19787733256816864
dqn reward tensor(188., device='cuda:0') e 0.05 loss_dqn tensor(1.1530e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14591851830482483
dqn reward tensor(122.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1397131085395813
dqn reward tensor(257.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.7882e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03371020779013634
dqn reward tensor(231.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3028e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12742552161216736
dqn reward tensor(241.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0702e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20738419890403748
dqn reward tensor(426.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.1575e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14133413136005402
dqn reward tensor(234.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13097088038921356
dqn reward tensor(246.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1212e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21048468351364136
dqn reward tensor(263.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0069e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21805201470851898
dqn reward tensor(357., device='cuda:0') e 0.05 loss_dqn tensor(8.7314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24532344937324524
dqn reward tensor(281., device='cuda:0') e 0.05 loss_dqn tensor(2.0729e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10911387950181961
dqn reward tensor(317.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.0435e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10992918908596039
dqn reward tensor(261.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7396e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19934502243995667
dqn reward tensor(337., device='cuda:0') e 0.05 loss_dqn tensor(1.0300e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09647288918495178
dqn reward tensor(147.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.5454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12134258449077606
dqn reward tensor(257.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.6013e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21653291583061218
dqn reward tensor(367.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.7114e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1721358299255371
dqn reward tensor(346.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5563e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06505373865365982
dqn reward tensor(318.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.8362e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06601845473051071
dqn reward tensor(255.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.6708e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03424239158630371
dqn reward tensor(202.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.2698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12124896049499512
dqn reward tensor(357.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7828e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10522827506065369
dqn reward tensor(275.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3047e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06796981394290924
dqn reward tensor(330.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3996e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1297261118888855
dqn reward tensor(402.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.4160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16863009333610535
dqn reward tensor(294.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4263e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06855636835098267
dqn reward tensor(200., device='cuda:0') e 0.05 loss_dqn tensor(1.2878e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09954075515270233
dqn reward tensor(217.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0812305361032486
dqn reward tensor(242.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6872e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09442922472953796
dqn reward tensor(398.3125, device='cuda:0') e 0.05 loss_dqn tensor(2.1873e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06988708674907684
dqn reward tensor(313.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0718e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12969934940338135
dqn reward tensor(261.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8420e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24142828583717346
dqn reward tensor(193.1875, device='cuda:0') e 0.05 loss_dqn tensor(9.9059e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.019964680075645447
dqn reward tensor(349.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4616e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2166246473789215
dqn reward tensor(159.4375, device='cuda:0') e 0.05 loss_dqn tensor(9.2297e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08629277348518372
dqn reward tensor(218.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1518e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08830860257148743
dqn reward tensor(205.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2759e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10433383285999298
dqn reward tensor(221.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4789e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03207922726869583
dqn reward tensor(116.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.1130e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03768251836299896
dqn reward tensor(323.0625, device='cuda:0') e 0.05 loss_dqn tensor(8.8539e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05005853623151779
dqn reward tensor(315.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.4382e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21546241641044617
dqn reward tensor(356.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.5471e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2207111418247223
dqn reward tensor(115.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5981e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03255646675825119
dqn reward tensor(308.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.1627e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15156011283397675
dqn reward tensor(282.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3924e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09526616334915161
dqn reward tensor(275.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2429e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10924830287694931
dqn reward tensor(335.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7108e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12835237383842468
dqn reward tensor(331.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.1492e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13177591562271118
dqn reward tensor(294., device='cuda:0') e 0.05 loss_dqn tensor(1.0446e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05218314379453659
dqn reward tensor(321.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0219e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036014944314956665
dqn reward tensor(230.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.9454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07677306234836578
dqn reward tensor(413.3125, device='cuda:0') e 0.05 loss_dqn tensor(9.4464e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10046804696321487
dqn reward tensor(242.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8040e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06720052659511566
dqn reward tensor(268.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0008e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16587240993976593
dqn reward tensor(260.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0952e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11948730051517487
dqn reward tensor(296., device='cuda:0') e 0.05 loss_dqn tensor(9.4122e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19544312357902527
dqn reward tensor(230.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5036e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14644187688827515
dqn reward tensor(251.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0710e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1635773926973343
dqn reward tensor(350.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.3984e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15365834534168243
dqn reward tensor(304.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.7553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062171705067157745
dqn reward tensor(131.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4868e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19993099570274353
dqn reward tensor(103.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1315e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06795470416545868
dqn reward tensor(409.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02725781686604023
dqn reward tensor(264.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.5217e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.104355588555336
dqn reward tensor(229.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.5849e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1448819637298584
dqn reward tensor(288.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.7248e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17629796266555786
dqn reward tensor(351.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.6680e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11501127481460571
dqn reward tensor(274.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2438e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03603115677833557
dqn reward tensor(355.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0833e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.040978994220495224
dqn reward tensor(216.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.8061e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23024091124534607
dqn reward tensor(287., device='cuda:0') e 0.05 loss_dqn tensor(1.1888e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.078492671251297
dqn reward tensor(159.9375, device='cuda:0') e 0.05 loss_dqn tensor(8.7014e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20083428919315338
dqn reward tensor(300., device='cuda:0') e 0.05 loss_dqn tensor(1.4963e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10202448070049286
dqn reward tensor(249.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.1234e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025477301329374313
dqn reward tensor(336.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.0021e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024259138852357864
dqn reward tensor(151.4375, device='cuda:0') e 0.05 loss_dqn tensor(2.2570e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1124236211180687
dqn reward tensor(149.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.4114e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08593040704727173
dqn reward tensor(392.4375, device='cuda:0') e 0.05 loss_dqn tensor(8.7362e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0848817378282547
dqn reward tensor(207.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.4465e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0933326855301857
dqn reward tensor(156.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.4851e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2383701056241989
dqn reward tensor(359.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.6376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12154527008533478
dqn reward tensor(189.9375, device='cuda:0') e 0.05 loss_dqn tensor(9.2000e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08237174153327942
dqn reward tensor(365.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4850e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26627445220947266
dqn reward tensor(305.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0759e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29347965121269226
dqn reward tensor(327.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.4917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17575722932815552
dqn reward tensor(203.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1636e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2190738320350647
dqn reward tensor(372.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1184e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02734512835741043
dqn reward tensor(315.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.8996e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030661581084132195
dqn reward tensor(319.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.0443e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08853128552436829
dqn reward tensor(132.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2873e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08370481431484222
dqn reward tensor(186.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4849e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22248892486095428
dqn reward tensor(366., device='cuda:0') e 0.05 loss_dqn tensor(8.9317e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27129533886909485
dqn reward tensor(347.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.0447e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26083922386169434
dqn reward tensor(210.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8425e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10423846542835236
dqn reward tensor(259.5000, device='cuda:0') e 0.05 loss_dqn tensor(3.9461e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31264954805374146
dqn reward tensor(366.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9947e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12919320166110992
dqn reward tensor(489.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0882e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10664843022823334
dqn reward tensor(477.5625, device='cuda:0') e 0.05 loss_dqn tensor(7.2991e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09889934957027435
dqn reward tensor(366.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.5083e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07566606998443604
dqn reward tensor(426.8750, device='cuda:0') e 0.05 loss_dqn tensor(7.0777e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05942073464393616
dqn reward tensor(214.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.9883e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19539320468902588
dqn reward tensor(455.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.7294e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.125961035490036
dqn reward tensor(420.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.0894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27737635374069214
dqn reward tensor(517., device='cuda:0') e 0.05 loss_dqn tensor(6.9580e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.136214017868042
dqn reward tensor(421.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6501e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025613488629460335
dqn reward tensor(499.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.1483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039885103702545166
dqn reward tensor(283., device='cuda:0') e 0.05 loss_dqn tensor(1.2021e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.33148887753486633
dqn reward tensor(515.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6067e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25739479064941406
dqn reward tensor(522.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049586210399866104
dqn reward tensor(401.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0063e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21060694754123688
dqn reward tensor(444.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.4649e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11576507240533829
dqn reward tensor(613.4375, device='cuda:0') e 0.05 loss_dqn tensor(6.3186e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28460264205932617
dqn reward tensor(537.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2113e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06705105304718018
dqn reward tensor(451.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.2081e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16787365078926086
dqn reward tensor(624.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4146e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1154690608382225
dqn reward tensor(519.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.0910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.045129358768463135
dqn reward tensor(425.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9476e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.034609194844961166
dqn reward tensor(494., device='cuda:0') e 0.05 loss_dqn tensor(6.3241e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10511063039302826
dqn reward tensor(635.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4002e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10318415611982346
dqn reward tensor(557.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9608e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09874074906110764
dqn reward tensor(581.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.5429e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11835683882236481
dqn reward tensor(520.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3524e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07378775626420975
dqn reward tensor(659., device='cuda:0') e 0.05 loss_dqn tensor(5.7821e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1659986972808838
dqn reward tensor(565.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12218523025512695
dqn reward tensor(572.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1960e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08788169920444489
dqn reward tensor(698.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0357e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0311138816177845
dqn reward tensor(478.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.2189e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08787062764167786
dqn reward tensor(740.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3585e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18734543025493622
dqn reward tensor(686.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.0470e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15743212401866913
dqn reward tensor(476.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1420e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033244144171476364
dqn reward tensor(549.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023235980421304703
dqn reward tensor(608.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.9651e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.099880650639534
dqn reward tensor(719.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9936e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28288617730140686
dqn reward tensor(488.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9860e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2541259527206421
dqn reward tensor(582.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1542673110961914
dqn reward tensor(522.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0866e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11179102212190628
dqn reward tensor(454.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2174e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14774303138256073
dqn reward tensor(732.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3384e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28002575039863586
dqn reward tensor(433.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2019e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08741094172000885
dqn reward tensor(438.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.1034e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18485800921916962
dqn reward tensor(659., device='cuda:0') e 0.05 loss_dqn tensor(5.8277e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06817379593849182
dqn reward tensor(666.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7159e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34415072202682495
dqn reward tensor(662.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2278e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09026478230953217
dqn reward tensor(635.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.6336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14957180619239807
dqn reward tensor(662.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1721e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18444165587425232
dqn reward tensor(401.8750, device='cuda:0') e 0.05 loss_dqn tensor(9.3645e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0945267528295517
dqn reward tensor(665., device='cuda:0') e 0.05 loss_dqn tensor(7.9606e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07464991509914398
dqn reward tensor(745.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0263e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1508772224187851
dqn reward tensor(717.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06477324664592743
dqn reward tensor(575.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2512e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11256702244281769
dqn reward tensor(642.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2544e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18345433473587036
dqn reward tensor(599.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1967e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05066540092229843
dqn reward tensor(645.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9791e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2520902454853058
dqn reward tensor(721.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.2774e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0765114277601242
dqn reward tensor(376.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9327e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18464940786361694
dqn reward tensor(674.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7210e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1609608232975006
dqn reward tensor(635.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.9396e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09709837287664413
dqn reward tensor(636.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.3947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20591112971305847
dqn reward tensor(587.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.4188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16464468836784363
dqn reward tensor(734.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8356e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11389259248971939
dqn reward tensor(458.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6187e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20668208599090576
dqn reward tensor(605.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3035e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1841101199388504
dqn reward tensor(640.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4163e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.036347270011901855
dqn reward tensor(642.7500, device='cuda:0') e 0.05 loss_dqn tensor(3.5225e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12406764179468155
dqn reward tensor(559.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.3375e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031232167035341263
dqn reward tensor(724., device='cuda:0') e 0.05 loss_dqn tensor(6.1175e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19641602039337158
dqn reward tensor(682.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.3761e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.31512072682380676
dqn reward tensor(498.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.8687e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13942009210586548
dqn reward tensor(511.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.6038e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1615702211856842
dqn reward tensor(496.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.9380e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12722373008728027
dqn reward tensor(598.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.2701e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08493674546480179
dqn reward tensor(750.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8149e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13759008049964905
dqn reward tensor(536.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1451e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17276310920715332
dqn reward tensor(500.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3664e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0893934816122055
dqn reward tensor(634.0625, device='cuda:0') e 0.05 loss_dqn tensor(6.0952e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1625117063522339
dqn reward tensor(519.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.1880e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17231646180152893
dqn reward tensor(688., device='cuda:0') e 0.05 loss_dqn tensor(1.2496e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11210083961486816
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8097e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2155715376138687
dqn reward tensor(610.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.6502e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34210044145584106
dqn reward tensor(681.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7608e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047908905893564224
dqn reward tensor(722.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0635e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12237779796123505
dqn reward tensor(586.6250, device='cuda:0') e 0.05 loss_dqn tensor(9.2204e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25710535049438477
dqn reward tensor(483.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.9277e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19092217087745667
dqn reward tensor(569.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5679e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22453832626342773
dqn reward tensor(419.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.6767e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2017466425895691
dqn reward tensor(495.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3389e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1791374534368515
dqn reward tensor(593.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09055418521165848
dqn reward tensor(557.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.9038e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24408695101737976
dqn reward tensor(703.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0910e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06701698899269104
dqn reward tensor(576., device='cuda:0') e 0.05 loss_dqn tensor(6.8306e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08907869458198547
dqn reward tensor(618.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.5120e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10962478071451187
dqn reward tensor(537.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9135e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09077270328998566
dqn reward tensor(556.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6823e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10674161463975906
dqn reward tensor(567., device='cuda:0') e 0.05 loss_dqn tensor(1.8540e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03274375945329666
dqn reward tensor(554., device='cuda:0') e 0.05 loss_dqn tensor(6.6284e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2230958342552185
dqn reward tensor(603.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.6919e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.038393959403038025
dqn reward tensor(673.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.6783e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13623201847076416
dqn reward tensor(473.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0533e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11445030570030212
dqn reward tensor(469.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.7781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2100302129983902
dqn reward tensor(497.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5216e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17230409383773804
dqn reward tensor(485.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5127e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24067865312099457
dqn reward tensor(609.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.1153e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.022523025050759315
dqn reward tensor(598.6875, device='cuda:0') e 0.05 loss_dqn tensor(5.6488e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07360167056322098
dqn reward tensor(634.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0808e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1301506608724594
dqn reward tensor(548.3750, device='cuda:0') e 0.05 loss_dqn tensor(3.0616e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24314531683921814
dqn reward tensor(610.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.6387e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2525104582309723
dqn reward tensor(534.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.5862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03972450643777847
dqn reward tensor(607.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.5173e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.047146882861852646
dqn reward tensor(565.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.6277e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1242060661315918
dqn reward tensor(580., device='cuda:0') e 0.05 loss_dqn tensor(6.4769e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15807697176933289
dqn reward tensor(489.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.3826e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10527533292770386
dqn reward tensor(725.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2103e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12845782935619354
dqn reward tensor(600.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4416e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18505921959877014
dqn reward tensor(559., device='cuda:0') e 0.05 loss_dqn tensor(2.2242e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21277360618114471
dqn reward tensor(542.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5598e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09305911511182785
dqn reward tensor(614.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.2298e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08859353512525558
dqn reward tensor(503.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1696e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1395905464887619
dqn reward tensor(608.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.8160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18287716805934906
dqn reward tensor(571.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2862e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08176077157258987
dqn reward tensor(653., device='cuda:0') e 0.05 loss_dqn tensor(6.2369e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07484212517738342
dqn reward tensor(436.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.8367e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09580382704734802
dqn reward tensor(671.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0846e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15781688690185547
dqn reward tensor(665.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7430e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10708676278591156
dqn reward tensor(441.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.3872e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15776991844177246
dqn reward tensor(342.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.7084e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030013155192136765
dqn reward tensor(530.1875, device='cuda:0') e 0.05 loss_dqn tensor(7.8330e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10115931928157806
dqn reward tensor(515.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.6653e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06678426265716553
dqn reward tensor(457.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3191e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1113106906414032
dqn reward tensor(390.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4262e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09553783386945724
dqn reward tensor(533.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8454e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0959131196141243
dqn reward tensor(558.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.9545e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07868021726608276
dqn reward tensor(498.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0680e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0858771950006485
dqn reward tensor(482.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.3172e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09938541054725647
dqn reward tensor(677.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3663e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08790086954832077
dqn reward tensor(392., device='cuda:0') e 0.05 loss_dqn tensor(1.3846e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.050487931817770004
dqn reward tensor(605.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7932e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24038247764110565
dqn reward tensor(630., device='cuda:0') e 0.05 loss_dqn tensor(6.2311e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05583474412560463
dqn reward tensor(611., device='cuda:0') e 0.05 loss_dqn tensor(6.7399e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.317859411239624
dqn reward tensor(536., device='cuda:0') e 0.05 loss_dqn tensor(1.6762e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05273157358169556
dqn reward tensor(508.1250, device='cuda:0') e 0.05 loss_dqn tensor(7.0445e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08604103326797485
dqn reward tensor(622., device='cuda:0') e 0.05 loss_dqn tensor(9.3099e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15328320860862732
dqn reward tensor(560.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.0976e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0842292308807373
dqn reward tensor(654.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0609e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.121033675968647
dqn reward tensor(489.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4743e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10659802705049515
dqn reward tensor(491.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.0801e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18065878748893738
dqn reward tensor(587.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.4250e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13115471601486206
dqn reward tensor(619.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.8895e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09411384910345078
dqn reward tensor(549.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.0327e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13319338858127594
dqn reward tensor(685.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.1981e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1917610466480255
dqn reward tensor(482.7500, device='cuda:0') e 0.05 loss_dqn tensor(8.3513e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15599068999290466
dqn reward tensor(558.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2613e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10916752368211746
dqn reward tensor(543.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.5207e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15351563692092896
dqn reward tensor(629., device='cuda:0') e 0.05 loss_dqn tensor(6.1240e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1744711995124817
dqn reward tensor(689.5000, device='cuda:0') e 0.05 loss_dqn tensor(6.2615e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12343870103359222
dqn reward tensor(468.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.7600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0999835804104805
dqn reward tensor(553.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4865e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18550819158554077
dqn reward tensor(655.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8196e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03887273743748665
dqn reward tensor(651.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.3858e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21442654728889465
dqn reward tensor(559.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7588e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.087124302983284
dqn reward tensor(531.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.5885e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0656789243221283
dqn reward tensor(404.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9088e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1882457137107849
dqn reward tensor(581.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.2104e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1445433646440506
dqn reward tensor(48.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.6177e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.34551408886909485
Evaluating...
Train: {'rocauc': 0.7992084212796897} 8.854488372802734
=====Epoch 39=====
Training...
dqn reward tensor(649.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.2937e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1540123075246811
dqn reward tensor(555.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0946e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22333109378814697
dqn reward tensor(669.5625, device='cuda:0') e 0.05 loss_dqn tensor(6.1948e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16000351309776306
dqn reward tensor(783.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.7537e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14071381092071533
dqn reward tensor(581.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0209e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08914179354906082
dqn reward tensor(739.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6206e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07014039158821106
dqn reward tensor(689.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.4183e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11908410489559174
dqn reward tensor(625.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7477e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16250047087669373
dqn reward tensor(601.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8995e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11306291818618774
dqn reward tensor(566.9375, device='cuda:0') e 0.05 loss_dqn tensor(6.6853e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10987235605716705
dqn reward tensor(669.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3716e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19264301657676697
dqn reward tensor(893.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.5332e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2069513499736786
dqn reward tensor(712.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0393e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06832347065210342
dqn reward tensor(635.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1815e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12648241221904755
dqn reward tensor(532.6875, device='cuda:0') e 0.05 loss_dqn tensor(9.0020e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29452210664749146
dqn reward tensor(758.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.5428e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027173172682523727
dqn reward tensor(849.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.6461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06083338335156441
dqn reward tensor(769.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1576e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10794083774089813
dqn reward tensor(789.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9138e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08195215463638306
dqn reward tensor(865.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.9850e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.135471373796463
dqn reward tensor(763.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.0160e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.025140762329101562
dqn reward tensor(609.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9261e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.26375260949134827
dqn reward tensor(720.5000, device='cuda:0') e 0.05 loss_dqn tensor(9.9493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08167103677988052
dqn reward tensor(795.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9275e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1314445585012436
dqn reward tensor(486.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1266e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18110784888267517
dqn reward tensor(744., device='cuda:0') e 0.05 loss_dqn tensor(6.2242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1680159568786621
dqn reward tensor(817.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.0314e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02684340626001358
dqn reward tensor(831.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3220e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22007128596305847
dqn reward tensor(802.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.2581e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19687077403068542
dqn reward tensor(899.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.4507e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09459605813026428
dqn reward tensor(652., device='cuda:0') e 0.05 loss_dqn tensor(1.4485e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09337946772575378
dqn reward tensor(716.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.7146e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11683979630470276
dqn reward tensor(839.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1446e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05138179287314415
dqn reward tensor(858.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9303e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11707362532615662
dqn reward tensor(794.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4490e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16484025120735168
dqn reward tensor(802.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.4669e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21518667042255402
dqn reward tensor(765.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8056e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14958716928958893
dqn reward tensor(855.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5886e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14940249919891357
dqn reward tensor(778.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.8587e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1059160903096199
dqn reward tensor(870.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.1542e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15630026161670685
dqn reward tensor(857.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.7972e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06778044998645782
dqn reward tensor(605.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2154e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14513665437698364
dqn reward tensor(867., device='cuda:0') e 0.05 loss_dqn tensor(5.0917e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08149583637714386
dqn reward tensor(722.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3694e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1987084448337555
dqn reward tensor(819.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.8698e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030628586187958717
dqn reward tensor(753.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.3142e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.304606556892395
dqn reward tensor(872., device='cuda:0') e 0.05 loss_dqn tensor(5.2126e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10714143514633179
dqn reward tensor(967., device='cuda:0') e 0.05 loss_dqn tensor(4.9479e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17074517905712128
dqn reward tensor(795.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.1898e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2062387615442276
dqn reward tensor(660.3750, device='cuda:0') e 0.05 loss_dqn tensor(9.4053e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07840953767299652
dqn reward tensor(730.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3752e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07338932156562805
dqn reward tensor(653.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.8431e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18876948952674866
dqn reward tensor(770.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4755e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06192037835717201
dqn reward tensor(755.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9773e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08972533792257309
dqn reward tensor(892.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4078e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0626295730471611
dqn reward tensor(894.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.1095e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2920949459075928
dqn reward tensor(802.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6830e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05407799035310745
dqn reward tensor(689.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.0111e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.37499648332595825
dqn reward tensor(728.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.4016e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13211345672607422
dqn reward tensor(700.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.7947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1187787652015686
dqn reward tensor(860., device='cuda:0') e 0.05 loss_dqn tensor(1.7424e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08171425759792328
dqn reward tensor(713.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9292e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2418651282787323
dqn reward tensor(731., device='cuda:0') e 0.05 loss_dqn tensor(4.7657e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23087507486343384
dqn reward tensor(704.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4879e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1592002809047699
dqn reward tensor(883.9375, device='cuda:0') e 0.05 loss_dqn tensor(4.8492e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14915511012077332
dqn reward tensor(681.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.4553e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.039528410881757736
dqn reward tensor(582.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0364e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17148752510547638
dqn reward tensor(885.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4060e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0342412032186985
dqn reward tensor(752.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5047e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14470906555652618
dqn reward tensor(811., device='cuda:0') e 0.05 loss_dqn tensor(5.0992e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0392044298350811
dqn reward tensor(650.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2566e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17102709412574768
dqn reward tensor(621.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5895e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09653215110301971
dqn reward tensor(594.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.3785e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21887460350990295
dqn reward tensor(777.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.1162e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11632192134857178
dqn reward tensor(781.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1355e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24913205206394196
dqn reward tensor(759.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0499e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03917045518755913
dqn reward tensor(687., device='cuda:0') e 0.05 loss_dqn tensor(1.2314e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1585959792137146
dqn reward tensor(834.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.5639e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11402308940887451
dqn reward tensor(844.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4364e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12948058545589447
dqn reward tensor(783.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.1750e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09240739047527313
dqn reward tensor(699.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.9806e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028251800686120987
dqn reward tensor(800.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.3985e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07894157618284225
dqn reward tensor(690.1250, device='cuda:0') e 0.05 loss_dqn tensor(8.5006e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12674090266227722
dqn reward tensor(792.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.6188e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09588193893432617
dqn reward tensor(767.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.4180e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22204147279262543
dqn reward tensor(1034., device='cuda:0') e 0.05 loss_dqn tensor(4.7555e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06123524159193039
dqn reward tensor(824.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.1136e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1435689926147461
dqn reward tensor(949.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.6493e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13728605210781097
dqn reward tensor(755.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.5108e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05832173302769661
dqn reward tensor(780.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.0245e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07492004334926605
dqn reward tensor(719.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8079e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06388579308986664
dqn reward tensor(776.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9298e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0648297667503357
dqn reward tensor(734., device='cuda:0') e 0.05 loss_dqn tensor(7.7045e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07396509498357773
dqn reward tensor(753.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.9311e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12724511325359344
dqn reward tensor(806.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.8611e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062401771545410156
dqn reward tensor(715.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.9670e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028808774426579475
dqn reward tensor(763.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.5719e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12030424177646637
dqn reward tensor(715.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3048e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1800539791584015
dqn reward tensor(711., device='cuda:0') e 0.05 loss_dqn tensor(5.0202e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08823912590742111
dqn reward tensor(850., device='cuda:0') e 0.05 loss_dqn tensor(5.9810e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0835852101445198
dqn reward tensor(772.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0506e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05422870069742203
dqn reward tensor(893.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.9461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.29185524582862854
dqn reward tensor(783.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.2190e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030298151075839996
dqn reward tensor(683.1875, device='cuda:0') e 0.05 loss_dqn tensor(5.6945e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28955405950546265
dqn reward tensor(826.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1839e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15590518712997437
dqn reward tensor(687.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7658e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11768601089715958
dqn reward tensor(840.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.9943e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08968854695558548
dqn reward tensor(780.8750, device='cuda:0') e 0.05 loss_dqn tensor(4.5197e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07135254889726639
dqn reward tensor(803.6250, device='cuda:0') e 0.05 loss_dqn tensor(4.8433e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22834426164627075
dqn reward tensor(733.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0707e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1557033807039261
dqn reward tensor(919.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2529e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02830980345606804
dqn reward tensor(648.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.4222e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04865265637636185
dqn reward tensor(770.1250, device='cuda:0') e 0.05 loss_dqn tensor(9.8500e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2837224006652832
dqn reward tensor(773.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9225e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08385877311229706
dqn reward tensor(912.5625, device='cuda:0') e 0.05 loss_dqn tensor(5.3696e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08247917145490646
dqn reward tensor(722.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.3953e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13409024477005005
dqn reward tensor(797.5625, device='cuda:0') e 0.05 loss_dqn tensor(4.8824e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21520355343818665
dqn reward tensor(862.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1230e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.024203378707170486
dqn reward tensor(748., device='cuda:0') e 0.05 loss_dqn tensor(1.1294e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19656223058700562
dqn reward tensor(799., device='cuda:0') e 0.05 loss_dqn tensor(5.1209e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1307385116815567
dqn reward tensor(770., device='cuda:0') e 0.05 loss_dqn tensor(4.7109e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15073800086975098
dqn reward tensor(897.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.3330e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16846752166748047
dqn reward tensor(606.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.3112e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06845985352993011
dqn reward tensor(752.2500, device='cuda:0') e 0.05 loss_dqn tensor(4.8590e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08448950201272964
dqn reward tensor(674.3125, device='cuda:0') e 0.05 loss_dqn tensor(5.0489e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09147631376981735
dqn reward tensor(781.8125, device='cuda:0') e 0.05 loss_dqn tensor(7.9285e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1905255913734436
dqn reward tensor(779.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.4364e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09563685953617096
dqn reward tensor(792.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.8960e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17287330329418182
dqn reward tensor(820.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.6791e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.028323128819465637
dqn reward tensor(701.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.1860e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23097553849220276
dqn reward tensor(682.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0165e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10703323036432266
dqn reward tensor(721.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.2851e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24654620885849
dqn reward tensor(718.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.3195e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03761579841375351
dqn reward tensor(862.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6410e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07226334512233734
dqn reward tensor(635.5000, device='cuda:0') e 0.05 loss_dqn tensor(7.0324e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08025765419006348
dqn reward tensor(750.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.7731e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04824471101164818
dqn reward tensor(852.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1128e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1463497132062912
dqn reward tensor(828.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.6444e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18988355994224548
dqn reward tensor(889.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5818e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21268659830093384
dqn reward tensor(701., device='cuda:0') e 0.05 loss_dqn tensor(4.9846e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04336930066347122
dqn reward tensor(656.7500, device='cuda:0') e 0.05 loss_dqn tensor(4.9164e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04331488907337189
dqn reward tensor(682.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4307e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17023932933807373
dqn reward tensor(843.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.0426e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06910406053066254
dqn reward tensor(784.6250, device='cuda:0') e 0.05 loss_dqn tensor(8.0189e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19101658463478088
dqn reward tensor(780.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.3705e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.42561012506484985
dqn reward tensor(756.8750, device='cuda:0') e 0.05 loss_dqn tensor(8.4784e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15136942267417908
dqn reward tensor(620.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.2552e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09332622587680817
dqn reward tensor(760.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.8831e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13917212188243866
dqn reward tensor(732.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.3288e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03345091640949249
dqn reward tensor(800.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.1947e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10211730003356934
dqn reward tensor(817.3750, device='cuda:0') e 0.05 loss_dqn tensor(4.9903e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.136857271194458
dqn reward tensor(757.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.7755e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03695632517337799
dqn reward tensor(846.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.3257e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24300318956375122
dqn reward tensor(882.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.1628e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10197299718856812
dqn reward tensor(732.5000, device='cuda:0') e 0.05 loss_dqn tensor(4.9010e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.031975820660591125
dqn reward tensor(557.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0390e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1253441572189331
dqn reward tensor(840.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12116742134094238
dqn reward tensor(715.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0292e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12749765813350677
dqn reward tensor(736.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.2461e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.055857330560684204
dqn reward tensor(811.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.0998e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13132290542125702
dqn reward tensor(678., device='cuda:0') e 0.05 loss_dqn tensor(5.2371e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13294431567192078
dqn reward tensor(911.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.5982e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1414034366607666
dqn reward tensor(583.1875, device='cuda:0') e 0.05 loss_dqn tensor(6.3805e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1213202103972435
dqn reward tensor(755.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.1594e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10106278955936432
dqn reward tensor(740.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.3603e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07359615713357925
dqn reward tensor(677.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.7556e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1800815612077713
dqn reward tensor(581.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.5121e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17004236578941345
dqn reward tensor(783., device='cuda:0') e 0.05 loss_dqn tensor(6.0080e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037754788994789124
dqn reward tensor(730.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.1902e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2908978760242462
dqn reward tensor(701.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.3416e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14161649346351624
dqn reward tensor(749.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5600e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11641326546669006
dqn reward tensor(763.1250, device='cuda:0') e 0.05 loss_dqn tensor(4.9654e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17830434441566467
dqn reward tensor(971., device='cuda:0') e 0.05 loss_dqn tensor(5.1382e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10070133209228516
dqn reward tensor(716.5625, device='cuda:0') e 0.05 loss_dqn tensor(9.3771e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1135198101401329
dqn reward tensor(872.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1539e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08067981898784637
dqn reward tensor(606.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4410e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11307519674301147
dqn reward tensor(580.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.5219e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15078778564929962
dqn reward tensor(791.0625, device='cuda:0') e 0.05 loss_dqn tensor(5.2794e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12318112701177597
dqn reward tensor(827., device='cuda:0') e 0.05 loss_dqn tensor(6.2281e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09352283179759979
dqn reward tensor(820.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3574e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05718626827001572
dqn reward tensor(771.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4791e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1376780867576599
dqn reward tensor(711.2500, device='cuda:0') e 0.05 loss_dqn tensor(7.6457e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.28661590814590454
dqn reward tensor(854., device='cuda:0') e 0.05 loss_dqn tensor(9.2042e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0634879469871521
dqn reward tensor(592.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7868e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08084065467119217
dqn reward tensor(817., device='cuda:0') e 0.05 loss_dqn tensor(5.0685e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08485302329063416
dqn reward tensor(843.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.0889e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09532121568918228
dqn reward tensor(812.1875, device='cuda:0') e 0.05 loss_dqn tensor(4.7469e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1412915140390396
dqn reward tensor(697.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.9781e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12299549579620361
dqn reward tensor(655.3750, device='cuda:0') e 0.05 loss_dqn tensor(8.2268e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09589943289756775
dqn reward tensor(662.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.6785e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08865416049957275
dqn reward tensor(708.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7502e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2606169581413269
dqn reward tensor(696.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.2722e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.36944615840911865
dqn reward tensor(758.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8591e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14125744998455048
dqn reward tensor(850., device='cuda:0') e 0.05 loss_dqn tensor(6.0151e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17713391780853271
dqn reward tensor(596.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.4709e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.057452041655778885
dqn reward tensor(771.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.0437e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24007850885391235
dqn reward tensor(722.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5464e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11046750843524933
dqn reward tensor(711.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.5716e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.049145862460136414
dqn reward tensor(701.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.2966e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1426127701997757
dqn reward tensor(790.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.2543e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06208861991763115
dqn reward tensor(763.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6821e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10020429641008377
dqn reward tensor(661.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8887e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13475999236106873
dqn reward tensor(743.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.7376e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04143436998128891
dqn reward tensor(743.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.4157e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.068895623087883
dqn reward tensor(759.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.4558e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07811626046895981
dqn reward tensor(809.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4318e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10866948217153549
dqn reward tensor(819.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.2602e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16004730761051178
dqn reward tensor(914., device='cuda:0') e 0.05 loss_dqn tensor(6.2522e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2406507432460785
dqn reward tensor(739.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8336e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09614460170269012
dqn reward tensor(650.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4497e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11624708026647568
dqn reward tensor(731.6875, device='cuda:0') e 0.05 loss_dqn tensor(6.0004e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23322805762290955
dqn reward tensor(581.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.5071e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06962180882692337
dqn reward tensor(854.3125, device='cuda:0') e 0.05 loss_dqn tensor(6.1269e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2045714408159256
dqn reward tensor(836.5000, device='cuda:0') e 0.05 loss_dqn tensor(5.7106e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13076251745224
dqn reward tensor(754.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5651e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0637018233537674
dqn reward tensor(725.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4035e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04573681950569153
dqn reward tensor(721.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.0725e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22291365265846252
dqn reward tensor(593.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.0923e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24328789114952087
dqn reward tensor(736.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6526e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04727276414632797
dqn reward tensor(603.3750, device='cuda:0') e 0.05 loss_dqn tensor(7.4420e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0657290518283844
dqn reward tensor(609.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.1081e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04292931407690048
dqn reward tensor(677.8750, device='cuda:0') e 0.05 loss_dqn tensor(5.5792e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07910802960395813
dqn reward tensor(728.3750, device='cuda:0') e 0.05 loss_dqn tensor(5.8584e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.037754133343696594
dqn reward tensor(790.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.7560e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21814781427383423
dqn reward tensor(532.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.4284e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1301065981388092
dqn reward tensor(519., device='cuda:0') e 0.05 loss_dqn tensor(1.7133e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07913323491811752
dqn reward tensor(727., device='cuda:0') e 0.05 loss_dqn tensor(2.4551e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.40229690074920654
dqn reward tensor(789.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3772e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18451042473316193
dqn reward tensor(925.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.7787e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22914785146713257
dqn reward tensor(686.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.1789e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18445730209350586
dqn reward tensor(914.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8931e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17100097239017487
dqn reward tensor(688.9375, device='cuda:0') e 0.05 loss_dqn tensor(5.6382e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12568572163581848
dqn reward tensor(750., device='cuda:0') e 0.05 loss_dqn tensor(5.1462e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.043326687067747116
dqn reward tensor(641.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.0758e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09928237646818161
dqn reward tensor(805.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.8396e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12327435612678528
dqn reward tensor(816.6250, device='cuda:0') e 0.05 loss_dqn tensor(6.3571e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10730653256177902
dqn reward tensor(610., device='cuda:0') e 0.05 loss_dqn tensor(7.3212e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07825516164302826
dqn reward tensor(800., device='cuda:0') e 0.05 loss_dqn tensor(5.9294e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1461283564567566
dqn reward tensor(878.6250, device='cuda:0') e 0.05 loss_dqn tensor(5.6954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.30049264430999756
dqn reward tensor(727.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4535e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23826691508293152
dqn reward tensor(801.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.1844e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1556600034236908
dqn reward tensor(711.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.4179e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.101022869348526
dqn reward tensor(736.2500, device='cuda:0') e 0.05 loss_dqn tensor(6.3865e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07768163830041885
dqn reward tensor(806.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.6807e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06088891252875328
dqn reward tensor(800.7500, device='cuda:0') e 0.05 loss_dqn tensor(9.3954e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10025458037853241
dqn reward tensor(734.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.8242e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1466471254825592
dqn reward tensor(690.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9147e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14380185306072235
dqn reward tensor(601.8750, device='cuda:0') e 0.05 loss_dqn tensor(6.3199e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16428878903388977
dqn reward tensor(777.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5497e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08723520487546921
dqn reward tensor(823., device='cuda:0') e 0.05 loss_dqn tensor(6.2275e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.062436118721961975
dqn reward tensor(620.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8750e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14201505482196808
dqn reward tensor(656.7500, device='cuda:0') e 0.05 loss_dqn tensor(5.2519e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16752596199512482
dqn reward tensor(643.2500, device='cuda:0') e 0.05 loss_dqn tensor(8.0926e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06141833961009979
dqn reward tensor(765.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6233e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1527107059955597
dqn reward tensor(671.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4610e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10540150105953217
dqn reward tensor(770.1250, device='cuda:0') e 0.05 loss_dqn tensor(5.8945e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20255650579929352
dqn reward tensor(862.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.5922e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1471085548400879
dqn reward tensor(796.7500, device='cuda:0') e 0.05 loss_dqn tensor(6.5182e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0609259307384491
dqn reward tensor(684.1875, device='cuda:0') e 0.05 loss_dqn tensor(1.2989e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16436205804347992
dqn reward tensor(821.3750, device='cuda:0') e 0.05 loss_dqn tensor(6.2100e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23634937405586243
dqn reward tensor(798.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5394e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2056559920310974
dqn reward tensor(669., device='cuda:0') e 0.05 loss_dqn tensor(7.5189e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2211829125881195
dqn reward tensor(712.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0873e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.149995356798172
dqn reward tensor(785.2500, device='cuda:0') e 0.05 loss_dqn tensor(5.9286e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07131718099117279
dqn reward tensor(582.6250, device='cuda:0') e 0.05 loss_dqn tensor(3.8738e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13929253816604614
dqn reward tensor(750.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4670e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2189059853553772
dqn reward tensor(698.1250, device='cuda:0') e 0.05 loss_dqn tensor(6.0894e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1422416716814041
dqn reward tensor(559.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6454e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0796460509300232
dqn reward tensor(700.2500, device='cuda:0') e 0.05 loss_dqn tensor(9.4324e+12, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14252695441246033
dqn reward tensor(685.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.1706e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.169972226023674
dqn reward tensor(711.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7105e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21861758828163147
dqn reward tensor(673.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1318e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.056920744478702545
dqn reward tensor(865.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.1945e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09870259463787079
dqn reward tensor(571.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4219e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.165449857711792
dqn reward tensor(792.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.2528e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11648355424404144
dqn reward tensor(731.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.1223e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20774103701114655
dqn reward tensor(814.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15983808040618896
dqn reward tensor(640.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.4408e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.033692002296447754
dqn reward tensor(691., device='cuda:0') e 0.05 loss_dqn tensor(1.7862e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23223325610160828
dqn reward tensor(797.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7993e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.030759714543819427
dqn reward tensor(821.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7297e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.023394402116537094
dqn reward tensor(668.8750, device='cuda:0') e 0.05 loss_dqn tensor(2.1273e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12590551376342773
dqn reward tensor(553.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8708e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15561118721961975
dqn reward tensor(797.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.6733e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15689347684383392
dqn reward tensor(803., device='cuda:0') e 0.05 loss_dqn tensor(1.9522e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23752960562705994
dqn reward tensor(640.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.0887e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2844657301902771
dqn reward tensor(536.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.3358e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17327409982681274
dqn reward tensor(640.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.0574e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09188990294933319
dqn reward tensor(741., device='cuda:0') e 0.05 loss_dqn tensor(1.5224e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09271407872438431
dqn reward tensor(734.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.5667e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19767308235168457
dqn reward tensor(816.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6705e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11109042167663574
dqn reward tensor(812.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7920e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04775290936231613
dqn reward tensor(743.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.8669e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12768355011940002
dqn reward tensor(746.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7177e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10454888641834259
dqn reward tensor(932., device='cuda:0') e 0.05 loss_dqn tensor(2.2537e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23950111865997314
dqn reward tensor(797.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.6915e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10119307786226273
dqn reward tensor(740., device='cuda:0') e 0.05 loss_dqn tensor(1.8620e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12997977435588837
dqn reward tensor(737.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7744e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03688422217965126
dqn reward tensor(759., device='cuda:0') e 0.05 loss_dqn tensor(1.6229e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08140060305595398
dqn reward tensor(602., device='cuda:0') e 0.05 loss_dqn tensor(1.7641e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12308120727539062
dqn reward tensor(812.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8278e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05685636028647423
dqn reward tensor(711.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.8037e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1370227187871933
dqn reward tensor(703.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7255e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1556263267993927
dqn reward tensor(602.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.5229e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.027222421020269394
dqn reward tensor(844.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6986e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10283777862787247
dqn reward tensor(788.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.5935e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02809424139559269
dqn reward tensor(699.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8170e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08586777001619339
dqn reward tensor(671.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.7880e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14125901460647583
dqn reward tensor(819.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.0787e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0767604261636734
dqn reward tensor(791.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.3952e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14536958932876587
dqn reward tensor(776., device='cuda:0') e 0.05 loss_dqn tensor(2.0960e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14454184472560883
dqn reward tensor(622.1875, device='cuda:0') e 0.05 loss_dqn tensor(2.2237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018539369106292725
dqn reward tensor(595.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.4948e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12296098470687866
dqn reward tensor(681., device='cuda:0') e 0.05 loss_dqn tensor(1.8240e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07746383547782898
dqn reward tensor(804., device='cuda:0') e 0.05 loss_dqn tensor(2.2250e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16260911524295807
dqn reward tensor(824.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1332e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2692003846168518
dqn reward tensor(688.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.8327e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.060205090790987015
dqn reward tensor(690.8125, device='cuda:0') e 0.05 loss_dqn tensor(2.4726e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.026461662724614143
dqn reward tensor(623.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7761e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18660421669483185
dqn reward tensor(558.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7099e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03157668560743332
dqn reward tensor(656.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.2483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20473690330982208
dqn reward tensor(764.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6667e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2016412913799286
dqn reward tensor(709.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9778e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12149668484926224
dqn reward tensor(581.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.4286e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2039984166622162
dqn reward tensor(622.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8824e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08779481053352356
dqn reward tensor(628.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7773e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1283060759305954
dqn reward tensor(671., device='cuda:0') e 0.05 loss_dqn tensor(2.3220e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.130960151553154
dqn reward tensor(823.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1616e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20462974905967712
dqn reward tensor(617.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.0938e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20131854712963104
dqn reward tensor(750.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0100e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05665084719657898
dqn reward tensor(749.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.5202e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04540719464421272
dqn reward tensor(730.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9733e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07987076044082642
dqn reward tensor(686.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.7719e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06133100762963295
dqn reward tensor(794.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8066e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11059293150901794
dqn reward tensor(720.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2334e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0963016077876091
dqn reward tensor(839.0625, device='cuda:0') e 0.05 loss_dqn tensor(1.9694e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15349915623664856
dqn reward tensor(693.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04406449943780899
dqn reward tensor(709.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.9359e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03022470325231552
dqn reward tensor(850.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0301e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.2596632242202759
dqn reward tensor(697.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.6866e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13258911669254303
dqn reward tensor(737.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.4460e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20093613862991333
dqn reward tensor(709.0625, device='cuda:0') e 0.05 loss_dqn tensor(2.2697e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20623888075351715
dqn reward tensor(803.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.9871e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08894498646259308
dqn reward tensor(625.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.6152e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22729255259037018
dqn reward tensor(686.6875, device='cuda:0') e 0.05 loss_dqn tensor(1.6440e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1269669383764267
dqn reward tensor(702.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0004e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09244722127914429
dqn reward tensor(599.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9617e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09732064604759216
dqn reward tensor(664.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9076e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09884761273860931
dqn reward tensor(779.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.0193e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0702841654419899
dqn reward tensor(614.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8005e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08662891387939453
dqn reward tensor(689.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.1867e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0694999173283577
dqn reward tensor(614.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0906e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.32830220460891724
dqn reward tensor(595.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.6380e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07204625755548477
dqn reward tensor(500.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6485e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1373485028743744
dqn reward tensor(668.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9258e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04803244024515152
dqn reward tensor(714., device='cuda:0') e 0.05 loss_dqn tensor(2.0100e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21420913934707642
dqn reward tensor(757.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9008e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07560794055461884
dqn reward tensor(605., device='cuda:0') e 0.05 loss_dqn tensor(1.9960e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20592215657234192
dqn reward tensor(785., device='cuda:0') e 0.05 loss_dqn tensor(1.8289e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06841147691011429
dqn reward tensor(758.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3360e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.27270472049713135
dqn reward tensor(769.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9929e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13478800654411316
dqn reward tensor(703.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1778e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.25473111867904663
dqn reward tensor(665.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.9709e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05175498127937317
dqn reward tensor(800.9375, device='cuda:0') e 0.05 loss_dqn tensor(1.8538e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1110524982213974
dqn reward tensor(772.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.6077e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1396849900484085
dqn reward tensor(670.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4471e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.22552484273910522
dqn reward tensor(634.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.4112e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1621997207403183
dqn reward tensor(732.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8804e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13809654116630554
dqn reward tensor(718.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.0346e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07216071337461472
dqn reward tensor(734.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8612e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06495369225740433
dqn reward tensor(746.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7173e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08938015252351761
dqn reward tensor(831.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.8230e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03766672685742378
dqn reward tensor(613.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8059e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1738574206829071
dqn reward tensor(732.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.2451e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1185370460152626
dqn reward tensor(752., device='cuda:0') e 0.05 loss_dqn tensor(2.0888e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05307877063751221
dqn reward tensor(761.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.1143e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10719043016433716
dqn reward tensor(729.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8866e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08622310310602188
dqn reward tensor(689.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8183e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.10509458184242249
dqn reward tensor(708., device='cuda:0') e 0.05 loss_dqn tensor(1.7242e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.061435747891664505
dqn reward tensor(769.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8668e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.015663951635360718
dqn reward tensor(796.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9811e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1498691439628601
dqn reward tensor(750.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0433e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.3050669729709625
dqn reward tensor(689.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.0277e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.014883039519190788
dqn reward tensor(833.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9222e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.14571180939674377
dqn reward tensor(711., device='cuda:0') e 0.05 loss_dqn tensor(1.6501e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11076786369085312
dqn reward tensor(750.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.3969e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15349361300468445
dqn reward tensor(522.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.3832e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.012050192803144455
dqn reward tensor(800.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.8913e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08387009799480438
dqn reward tensor(770.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9878e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.21916523575782776
dqn reward tensor(724.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.8149e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07044480741024017
dqn reward tensor(787.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9237e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08814780414104462
dqn reward tensor(634.7500, device='cuda:0') e 0.05 loss_dqn tensor(2.1564e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.018873421475291252
dqn reward tensor(578.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2483e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06945282220840454
dqn reward tensor(800.5625, device='cuda:0') e 0.05 loss_dqn tensor(2.1356e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.06952007114887238
dqn reward tensor(760.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.7538e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15347974002361298
dqn reward tensor(668.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.9290e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.20742817223072052
dqn reward tensor(629.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8118e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15568895637989044
dqn reward tensor(660., device='cuda:0') e 0.05 loss_dqn tensor(1.6257e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19415001571178436
dqn reward tensor(740.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.2365e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03930128365755081
dqn reward tensor(760.8125, device='cuda:0') e 0.05 loss_dqn tensor(1.9229e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07735344022512436
dqn reward tensor(745.1250, device='cuda:0') e 0.05 loss_dqn tensor(2.3738e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07378807663917542
dqn reward tensor(757., device='cuda:0') e 0.05 loss_dqn tensor(1.9415e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15836301445960999
dqn reward tensor(593.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.4122e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11545223742723465
dqn reward tensor(689.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.6296e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11061316728591919
dqn reward tensor(783.3750, device='cuda:0') e 0.05 loss_dqn tensor(2.0010e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.09363894909620285
dqn reward tensor(597.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.8904e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.03236665576696396
dqn reward tensor(708.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4248e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11412136256694794
dqn reward tensor(618.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7330e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.15109363198280334
dqn reward tensor(632.8750, device='cuda:0') e 0.05 loss_dqn tensor(1.7663e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1186366081237793
dqn reward tensor(670.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9432e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.23416370153427124
dqn reward tensor(680.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.9997e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.07467418164014816
dqn reward tensor(628.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.9775e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19828137755393982
dqn reward tensor(689.3750, device='cuda:0') e 0.05 loss_dqn tensor(1.4449e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.12178180366754532
dqn reward tensor(606.2500, device='cuda:0') e 0.05 loss_dqn tensor(2.0723e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.24549323320388794
dqn reward tensor(838.5625, device='cuda:0') e 0.05 loss_dqn tensor(1.8178e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.11340748518705368
dqn reward tensor(822., device='cuda:0') e 0.05 loss_dqn tensor(1.4042e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.05114711821079254
dqn reward tensor(705.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.9620e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16263602674007416
dqn reward tensor(514., device='cuda:0') e 0.05 loss_dqn tensor(1.9695e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.02462155371904373
dqn reward tensor(710.4375, device='cuda:0') e 0.05 loss_dqn tensor(1.9048e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.19849687814712524
dqn reward tensor(795.7500, device='cuda:0') e 0.05 loss_dqn tensor(1.9689e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.17016226053237915
dqn reward tensor(878.5000, device='cuda:0') e 0.05 loss_dqn tensor(1.7859e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.08970731496810913
dqn reward tensor(733.1250, device='cuda:0') e 0.05 loss_dqn tensor(1.7060e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.1095445454120636
dqn reward tensor(715., device='cuda:0') e 0.05 loss_dqn tensor(2.0448e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.16884078085422516
dqn reward tensor(682.3125, device='cuda:0') e 0.05 loss_dqn tensor(1.7211e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.18055981397628784
dqn reward tensor(574.6250, device='cuda:0') e 0.05 loss_dqn tensor(2.0468e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.029943570494651794
dqn reward tensor(615.5000, device='cuda:0') e 0.05 loss_dqn tensor(2.4540e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.04845723882317543
dqn reward tensor(591.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.7883e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.0931597352027893
dqn reward tensor(704.2500, device='cuda:0') e 0.05 loss_dqn tensor(1.3838e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13098792731761932
dqn reward tensor(733.6250, device='cuda:0') e 0.05 loss_dqn tensor(1.5114e+13, device='cuda:0', grad_fn=<MeanBackward1>) agent_loss 0.13978394865989685
dqn reward 